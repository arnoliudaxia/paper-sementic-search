{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install openai arxiv\n",
        "!rm *.json*\n",
        "# !wget https://raw.githubusercontent.com/Papercopilot/paperlists/main/iclr/iclr2025.json\n",
        "!wget https://raw.githubusercontent.com/Papercopilot/paperlists/main/iclr/iclr2024.json\n",
        "!wget https://raw.githubusercontent.com/Papercopilot/paperlists/main/iclr/iclr2023.json\n",
        "!wget https://raw.githubusercontent.com/Papercopilot/paperlists/main/iclr/iclr2022.json\n",
        "!wget https://raw.githubusercontent.com/Papercopilot/paperlists/main/iclr/iclr2021.json\n",
        "\n",
        "!wget https://github.com/papercopilot/paperlists/raw/refs/heads/main/nips/nips2024.json\n",
        "!wget https://github.com/papercopilot/paperlists/raw/refs/heads/main/nips/nips2023.json\n",
        "!wget https://github.com/papercopilot/paperlists/raw/refs/heads/main/nips/nips2022.json\n",
        "!wget https://github.com/papercopilot/paperlists/raw/refs/heads/main/nips/nips2021.json\n",
        "!wget https://github.com/papercopilot/paperlists/raw/refs/heads/main/nips/nips2020.json\n",
        "\n",
        "!wget https://github.com/papercopilot/paperlists/raw/refs/heads/main/icml/icml2024.json\n",
        "!wget https://github.com/papercopilot/paperlists/raw/refs/heads/main/icml/icml2023.json\n",
        "!wget https://github.com/papercopilot/paperlists/raw/refs/heads/main/icml/icml2022.json\n",
        "!wget https://github.com/papercopilot/paperlists/raw/refs/heads/main/icml/icml2021.json\n",
        "\n",
        "\n",
        "\n",
        "!wget https://raw.githubusercontent.com/Papercopilot/paperlists/main/siggraph/siggraph2023.json\n",
        "\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "clear_output()\n"
      ],
      "metadata": {
        "id": "urRvVsgUA1NI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# 挂载 Google Drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMVyYdWE2sTA",
        "outputId": "3bf89eef-a452-4683-979a-ddfec7174efe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vGEaCgKgOb7R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from tqdm import tqdm  # 导入tqdm"
      ],
      "metadata": {
        "id": "kV2y7B66HFZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loadFile=\"iclr2024\"\n",
        "\n",
        "with open(loadFile+'.json','r') as file:\n",
        "  paperData=json.load(file)\n",
        "\n",
        "df = pd.DataFrame(paperData)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "VNIERJPCTKv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key='sk-uGDpj82N23f70b22B0ECT3BLBKFJ4cD20d910aDD4df69e1D',\n",
        "    base_url='https://c-z0-api-01.hash070.com/v1'\n",
        ")\n",
        "\n",
        "print(df[\"status\"].unique())\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant. 简洁地返回结果，不要包含无关内容。返回python友好源代码（非markdown格式）\"},\n",
        "    {\"role\": \"user\", \"content\": f\"从下面这个array中，筛选正常的paper状态（非拒稿撤回等）{df['status'].unique()}，返回一个list。\"}\n",
        "  ]\n",
        ")\n",
        "\n",
        "print(completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yjs3w1LnrAaG",
        "outputId": "b1bf9d20-4f83-427c-87dc-9edd2495c276"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Withdraw' 'Reject' 'Poster' 'Spotlight' 'Oral' 'Desk Reject']\n",
            "['Poster', 'Spotlight', 'Oral']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 提取返回的结果并转换为 Python 列表\n",
        "response_content = completion.choices[0].message.content.strip()\n",
        "normal_statuses = eval(response_content)  # 注意：eval 可能有安全风险，请确保 API 返回的内容是可信的\n",
        "\n",
        "# 使用返回的正常状态列表筛选 DataFrame\n",
        "df = df[df['status'].isin(normal_statuses)]\n"
      ],
      "metadata": {
        "id": "DHwchMInw5Z7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df = df[df['status'].isin(['Poster', 'Oral', 'Spotlight'])]\n",
        "# df = df[df['status'].isin(['Active'])]\n",
        "# df = df[df['status'].isin(['Poster', 'Journal', 'Highlighted'])]\n",
        "# df['primary_area'].unique()\n",
        "\n",
        "#SIG 无需\n",
        "\n",
        "df['sess'].unique()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "DuO3OyjZxsyc",
        "outputId": "921243b5-6ae6-4aa3-9abf-f31d99203137"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'sess'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'sess'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-3b685c9c16f8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#SIG 无需\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sess'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'sess'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "获取Abstract内容"
      ],
      "metadata": {
        "id": "m6TBsTNCBqsg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import arxiv\n",
        "import pandas as pd\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 定义函数，从 arXiv 获取摘要\n",
        "def get_abstract_from_arxiv(title):\n",
        "    search = arxiv.Search(\n",
        "        query=f\"ti:{title}\",\n",
        "        max_results=1,\n",
        "        sort_by=arxiv.SortCriterion.Relevance\n",
        "    )\n",
        "    try:\n",
        "        for result in search.results():\n",
        "            return result.summary\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching abstract for title '{title}': {e}\")\n",
        "        return None\n",
        "\n",
        "# 使用多线程批量获取摘要\n",
        "def fetch_all_abstracts(titles, max_workers=10):\n",
        "    abstracts = []\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        futures = [executor.submit(get_abstract_from_arxiv, title) for title in titles]\n",
        "        for future in tqdm(as_completed(futures), total=len(titles), desc=\"Fetching abstracts\"):\n",
        "            abstracts.append(future.result())\n",
        "    return abstracts\n",
        "\n",
        "\n",
        "\n",
        "# 多线程获取摘要\n",
        "df['abstract'] = fetch_all_abstracts(df['title'])\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DrDUfZQqyjmz",
        "outputId": "909fe92c-a39e-42e6-9c85-0e626c1293b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-27-45b45ebe061b>:14: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
            "  for result in search.results():\n",
            "Fetching abstracts: 100%|██████████| 2260/2260 [15:48<00:00,  2.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              id                                              title track  \\\n",
            "5     02f3mUtqnM  Hybrid LLM: Cost-Efficient and Quality-Aware Q...  main   \n",
            "10    06lrITXVAx                  Dropout Enhanced Bilevel Training  main   \n",
            "14    09iOdaeOzp  Sheared LLaMA: Accelerating Language Model Pre...  main   \n",
            "15    09xFexjhqE  AutoLoRa: An Automated Robust Fine-Tuning Fram...  main   \n",
            "18    0BqyZSWfzo  One-shot Empirical Privacy Estimation for Fede...  main   \n",
            "...          ...                                                ...   ...   \n",
            "7395  ztpy1gsUpT  Enhancing Small Medical Learners with Privacy-...  main   \n",
            "7398  zwMfg9PfPs  Out-of-Variable Generalisation for Discriminat...  main   \n",
            "7399  zwU9scoU4A  Learning Mean Field Games on Sparse Graphs: A ...  main   \n",
            "7403  zyBJodMrn5  On the generalization capacity of neural netwo...  main   \n",
            "7405  zzqn5G9fjn  Breaking Physical and Linguistic Borders: Mult...  main   \n",
            "\n",
            "         status                                           keywords  \\\n",
            "5        Poster   Large language models;Efficient ML;Query Routing   \n",
            "10    Spotlight                   Bilevel Optimization;Overfitting   \n",
            "14       Poster  pruning;efficiency;large language models;pre-t...   \n",
            "15       Poster          robust fine-tuning;adversarial robustness   \n",
            "18         Oral  differential privacy;federated learning;empiri...   \n",
            "...         ...                                                ...   \n",
            "7395     Poster  natural language processing;large language mod...   \n",
            "7398     Poster           Out-of-Variable Generalization;Causality   \n",
            "7399     Poster  Mean Field Games;Equilibrium Learning;Networks...   \n",
            "7403     Poster  compositional generalization;compositionality;...   \n",
            "7405     Poster  Multilingual Federated Learning;Natural Langua...   \n",
            "\n",
            "                                           primary_area  \\\n",
            "5     general machine learning (i.e., none of the ab...   \n",
            "10                                         optimization   \n",
            "14    representation learning for computer vision, a...   \n",
            "15    societal considerations including fairness, sa...   \n",
            "18    societal considerations including fairness, sa...   \n",
            "...                                                 ...   \n",
            "7395  applications to physical sciences (physics, ch...   \n",
            "7398  transfer learning, meta learning, and lifelong...   \n",
            "7399  general machine learning (i.e., none of the ab...   \n",
            "7403   applications to neuroscience & cognitive science   \n",
            "7405  societal considerations including fairness, sa...   \n",
            "\n",
            "                                                 author  \\\n",
            "5     Dujian Ding;Ankur Mallick;Chi Wang;Robert Sim;...   \n",
            "10                        Peiran Yu;Junyi Li;Heng Huang   \n",
            "14      Mengzhou Xia;Tianyu Gao;Zhiyuan Zeng;Danqi Chen   \n",
            "15            Xilie Xu;Jingfeng Zhang;Mohan Kankanhalli   \n",
            "18    Galen Andrew;Peter Kairouz;Sewoong Oh;Alina Op...   \n",
            "...                                                 ...   \n",
            "7395  Xinlu Zhang;Shiyang Li;Xianjun Yang;Chenxin Ti...   \n",
            "7398  Siyuan Guo;Jonas Bernhard Wildberger;Bernhard ...   \n",
            "7399              Christian Fabian;Kai Cui;Heinz Koeppl   \n",
            "7403  Takuya Ito;Soham Dan;Mattia Rigotti;James Kozl...   \n",
            "7405  Wanru Zhao;Yihong Chen;Royson Lee;Xinchi Qiu;Y...   \n",
            "\n",
            "                                              authorids  \\\n",
            "5     ~Dujian_Ding1;~Ankur_Mallick1;~Chi_Wang3;~Robe...   \n",
            "10                  ~Peiran_Yu1;~Junyi_Li1;~Heng_Huang1   \n",
            "14    ~Mengzhou_Xia1;~Tianyu_Gao1;~Zhiyuan_Zeng3;~Da...   \n",
            "15      ~Xilie_Xu1;~Jingfeng_Zhang1;~Mohan_Kankanhalli1   \n",
            "18    ~Galen_Andrew1;~Peter_Kairouz1;~Sewoong_Oh3;~A...   \n",
            "...                                                 ...   \n",
            "7395  ~Xinlu_Zhang1;~Shiyang_Li1;~Xianjun_Yang1;~Che...   \n",
            "7398  ~Siyuan_Guo1;~Jonas_Bernhard_Wildberger1;~Bern...   \n",
            "7399        ~Christian_Fabian1;~Kai_Cui3;~Heinz_Koeppl1   \n",
            "7403  ~Takuya_Ito1;~Soham_Dan1;~Mattia_Rigotti1;~Jam...   \n",
            "7405  ~Wanru_Zhao1;~Yihong_Chen3;~Royson_Lee1;~Xinch...   \n",
            "\n",
            "                                                    aff  \\\n",
            "5     Computing Science, University of British Colum...   \n",
            "10    University of Maryland;University of Pittsburg...   \n",
            "14    Princeton University;Princeton University;Tsin...   \n",
            "15    National University of Singapore;University of...   \n",
            "18    Google;Google;Google Research;Google;Massachus...   \n",
            "...                                                 ...   \n",
            "7395  University of California, Santa Barbara;Peking...   \n",
            "7398  Max Planck Institute for Intelligent Systems, ...   \n",
            "7399  Technische Universität Darmstadt;Technische Un...   \n",
            "7403  International Business Machines;International ...   \n",
            "7405  FAIR;Samsung AI Center, Cambridge;University o...   \n",
            "\n",
            "                                             aff_domain  ... confidence_avg  \\\n",
            "5     cs.ubc.ca;microsoft.com;microsoft.com;microsof...  ...       3.250000   \n",
            "10                            umd.edu;pitt.edu;pitt.edu  ...       3.500000   \n",
            "14    princeton.edu;princeton.edu;tsinghua.edu.cn;cs...  ...       4.000000   \n",
            "15                 nus.edu.sg;auckland.ac.nz;nus.edu.sg  ...       3.750000   \n",
            "18    google.com;google.com;google.com;google.com;mi...  ...       3.333333   \n",
            "...                                                 ...  ...            ...   \n",
            "7395              ucsb.edu;pumc.edu;google.com;ucsb.edu  ...       3.500000   \n",
            "7398                  tuebingen.mpg.de;tuebingen.mpg.de  ...       3.000000   \n",
            "7399    tu-darmstadt.de;tu-darmstadt.de;tu-darmstadt.de  ...       3.000000   \n",
            "7403            ibm.com;ibm.com;ibm.com;ibm.com;ibm.com  ...       3.666667   \n",
            "7405  meta.com;samsung.com;cam.ac.uk;cam.ac.uk;cam.a...  ...       4.500000   \n",
            "\n",
            "     soundness_avg contribution_avg presentation_avg replies_avg  \\\n",
            "5         2.750000         2.250000         3.000000          16   \n",
            "10        3.250000         2.750000         3.000000          17   \n",
            "14        3.000000         2.750000         2.750000          18   \n",
            "15        2.500000         2.250000         3.000000          24   \n",
            "18        3.666667         2.666667         3.333333          23   \n",
            "...            ...              ...              ...         ...   \n",
            "7395      2.750000         3.000000         2.500000          15   \n",
            "7398      3.000000         2.250000         3.250000          19   \n",
            "7399      3.000000         3.000000         3.000000          11   \n",
            "7403      2.333333         2.666667         2.666667          17   \n",
            "7405      3.000000         2.750000         2.500000          30   \n",
            "\n",
            "     corr_rating_confidence  project  github  \\\n",
            "5                  0.134742                    \n",
            "10                -1.000000                    \n",
            "14                 0.000000                    \n",
            "15                 0.132453                    \n",
            "18                 0.000000                    \n",
            "...                     ...      ...     ...   \n",
            "7395               0.000000                    \n",
            "7398               0.816497                    \n",
            "7399              -0.866025                    \n",
            "7403               0.917663                    \n",
            "7405               0.096674                    \n",
            "\n",
            "                                           site  \\\n",
            "5     https://iclr.cc/virtual/2024/poster/19625   \n",
            "10    https://iclr.cc/virtual/2024/poster/19624   \n",
            "14    https://iclr.cc/virtual/2024/poster/19623   \n",
            "15    https://iclr.cc/virtual/2024/poster/19622   \n",
            "18    https://iclr.cc/virtual/2024/poster/19621   \n",
            "...                                         ...   \n",
            "7395  https://iclr.cc/virtual/2024/poster/17369   \n",
            "7398  https://iclr.cc/virtual/2024/poster/17368   \n",
            "7399  https://iclr.cc/virtual/2024/poster/17367   \n",
            "7403  https://iclr.cc/virtual/2024/poster/17366   \n",
            "7405  https://iclr.cc/virtual/2024/poster/17365   \n",
            "\n",
            "                                               abstract  \n",
            "5                                                  None  \n",
            "10                                                 None  \n",
            "14                                                 None  \n",
            "15    Predicting and reasoning about the future lie ...  \n",
            "18                                                 None  \n",
            "...                                                 ...  \n",
            "7395  The success of recent text-to-image diffusion ...  \n",
            "7398  Large language models (LLMs) demonstrate remar...  \n",
            "7399  Particle distinguishability is a significant c...  \n",
            "7403  Learning the behavior of large agent populatio...  \n",
            "7405  The advent of the Transformer has led to the d...  \n",
            "\n",
            "[2260 rows x 27 columns]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "什么年代了还在生成embedding，直接构造prompt！"
      ],
      "metadata": {
        "id": "XySO3joHBxAO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 生成 text prompt\n",
        "def generate_text_prompt(df):\n",
        "    # 确保只关注 title 和 abstract 列\n",
        "    df = df[['title', 'abstract']].dropna()  # 去除缺失值\n",
        "    prompt = \"\"\n",
        "    for idx, row in enumerate(df.itertuples(index=False), start=1):\n",
        "        title, abstract = row.title, row.abstract\n",
        "        prompt += f\"{idx}. Title: {title}\\n   Abstract: {abstract}\\n\\n\"\n",
        "    return prompt.strip()  # 去除末尾多余的换行符\n",
        "\n",
        "# 调用函数生成 prompt\n",
        "text_prompt = generate_text_prompt(df)\n",
        "\n",
        "# 打印结果\n",
        "print(text_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITkU3OoVBpx3",
        "outputId": "ff142d20-7467-4b7c-896d-2f4aeae2797a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Title: AutoLoRa: An Automated Robust Fine-Tuning Framework\n",
            "   Abstract: Predicting and reasoning about the future lie at the heart of many\n",
            "time-series questions. For example, goal-conditioned reinforcement learning can\n",
            "be viewed as learning representations to predict which states are likely to be\n",
            "visited in the future. While prior methods have used contrastive predictive\n",
            "coding to model time series data, learning representations that encode\n",
            "long-term dependencies usually requires large amounts of data. In this paper,\n",
            "we introduce a temporal difference version of contrastive predictive coding\n",
            "that stitches together pieces of different time series data to decrease the\n",
            "amount of data required to learn predictions of future events. We apply this\n",
            "representation learning method to derive an off-policy algorithm for\n",
            "goal-conditioned RL. Experiments demonstrate that, compared with prior RL\n",
            "methods, ours achieves $2 \\times$ median improvement in success rates and can\n",
            "better cope with stochastic environments. In tabular settings, we show that our\n",
            "method is about $20 \\times$ more sample efficient than the successor\n",
            "representation and $1500 \\times$ more sample efficient than the standard (Monte\n",
            "Carlo) version of contrastive predictive coding.\n",
            "\n",
            "2. Title: From Latent Graph to Latent Topology Inference: Differentiable Cell Complex Module\n",
            "   Abstract: Dropout is a common operator in deep learning, aiming to prevent overfitting\n",
            "by randomly dropping neurons during training. This paper introduces a new\n",
            "family of poisoning attacks against neural networks named DROPOUTATTACK.\n",
            "DROPOUTATTACK attacks the dropout operator by manipulating the selection of\n",
            "neurons to drop instead of selecting them uniformly at random. We design,\n",
            "implement, and evaluate four DROPOUTATTACK variants that cover a broad range of\n",
            "scenarios. These attacks can slow or stop training, destroy prediction accuracy\n",
            "of target classes, and sabotage either precision or recall of a target class.\n",
            "In our experiments of training a VGG-16 model on CIFAR-100, our attack can\n",
            "reduce the precision of the victim class by 34.6% (from 81.7% to 47.1%) without\n",
            "incurring any degradation in model accuracy\n",
            "\n",
            "3. Title: MOFDiff: Coarse-grained Diffusion for Metal-Organic Framework Design\n",
            "   Abstract: The great success of Large Language Models (LLMs) has expanded the potential\n",
            "of multimodality, contributing to the gradual evolution of General Artificial\n",
            "Intelligence (AGI). A true AGI agent should not only possess the capability to\n",
            "perform predefined multi-tasks but also exhibit emergent abilities in an\n",
            "open-world context. However, despite the considerable advancements made by\n",
            "recent multimodal LLMs, they still fall short in effectively unifying\n",
            "comprehension and generation tasks, let alone open-world emergent abilities. We\n",
            "contend that the key to overcoming the present impasse lies in enabling text\n",
            "and images to be represented and processed interchangeably within a unified\n",
            "autoregressive Transformer. To this end, we introduce SEED, an elaborate image\n",
            "tokenizer that empowers LLMs with the ability to SEE and Draw at the same time.\n",
            "We identify two crucial design principles: (1) Image tokens should be\n",
            "independent of 2D physical patch positions and instead be produced with a 1D\n",
            "causal dependency, exhibiting intrinsic interdependence that aligns with the\n",
            "left-to-right autoregressive prediction mechanism in LLMs. (2) Image tokens\n",
            "should capture high-level semantics consistent with the degree of semantic\n",
            "abstraction in words, and be optimized for both discriminativeness and\n",
            "reconstruction during the tokenizer training phase. With SEED tokens, LLM is\n",
            "able to perform scalable multimodal autoregression under its original training\n",
            "recipe, i.e., next-word prediction. SEED-LLaMA is therefore produced by\n",
            "large-scale pretraining and instruction tuning on the interleaved textual and\n",
            "visual data, demonstrating impressive performance on a broad range of\n",
            "multimodal comprehension and generation tasks. More importantly, SEED-LLaMA has\n",
            "exhibited compositional emergent abilities such as multi-turn in-context\n",
            "multimodal generation, acting like your AI assistant.\n",
            "\n",
            "4. Title: Contrastive Difference Predictive Coding\n",
            "   Abstract: Auditing mechanisms for differential privacy use probabilistic means to\n",
            "empirically estimate the privacy level of an algorithm. For private machine\n",
            "learning, existing auditing mechanisms are tight: the empirical privacy\n",
            "estimate (nearly) matches the algorithm's provable privacy guarantee. But these\n",
            "auditing techniques suffer from two limitations. First, they only give tight\n",
            "estimates under implausible worst-case assumptions (e.g., a fully adversarial\n",
            "dataset). Second, they require thousands or millions of training runs to\n",
            "produce non-trivial statistical estimates of the privacy leakage.\n",
            "  This work addresses both issues. We design an improved auditing scheme that\n",
            "yields tight privacy estimates for natural (not adversarially crafted) datasets\n",
            "-- if the adversary can see all model updates during training. Prior auditing\n",
            "works rely on the same assumption, which is permitted under the standard\n",
            "differential privacy threat model. This threat model is also applicable, e.g.,\n",
            "in federated learning settings. Moreover, our auditing scheme requires only two\n",
            "training runs (instead of thousands) to produce tight privacy estimates, by\n",
            "adapting recent advances in tight composition theorems for differential\n",
            "privacy. We demonstrate the utility of our improved auditing schemes by\n",
            "surfacing implementation bugs in private machine learning code that eluded\n",
            "prior auditing techniques.\n",
            "\n",
            "5. Title: NEFTune: Noisy Embeddings Improve Instruction Finetuning\n",
            "   Abstract: The popularity of LLaMA (Touvron et al., 2023a;b) and other recently emerged\n",
            "moderate-sized large language models (LLMs) highlights the potential of\n",
            "building smaller yet powerful LLMs. Regardless, the cost of training such\n",
            "models from scratch on trillions of tokens remains high. In this work, we study\n",
            "structured pruning as an effective means to develop smaller LLMs from\n",
            "pre-trained, larger models. Our approach employs two key techniques: (1)\n",
            "targeted structured pruning, which prunes a larger model to a specified target\n",
            "shape by removing layers, heads, and intermediate and hidden dimensions in an\n",
            "end-to-end manner, and (2) dynamic batch loading, which dynamically updates the\n",
            "composition of sampled data in each training batch based on varying losses\n",
            "across different domains. We demonstrate the efficacy of our approach by\n",
            "presenting the Sheared-LLaMA series, pruning the LLaMA2-7B model down to 1.3B\n",
            "and 2.7B parameters. Sheared-LLaMA models outperform state-of-the-art\n",
            "open-source models of equivalent sizes, such as Pythia, INCITE, OpenLLaMA and\n",
            "the concurrent TinyLlama models, on a wide range of downstream and instruction\n",
            "tuning evaluations, while requiring only 3% of compute compared to training\n",
            "such models from scratch. This work provides compelling evidence that\n",
            "leveraging existing LLMs with structured pruning is a far more cost-effective\n",
            "approach for building competitive small-scale LLMs\n",
            "\n",
            "6. Title: LiDAR-PTQ: Post-Training Quantization for Point Cloud 3D Object Detection\n",
            "   Abstract: Latent Graph Inference (LGI) relaxed the reliance of Graph Neural Networks\n",
            "(GNNs) on a given graph topology by dynamically learning it. However, most of\n",
            "LGI methods assume to have a (noisy, incomplete, improvable, ...) input graph\n",
            "to rewire and can solely learn regular graph topologies. In the wake of the\n",
            "success of Topological Deep Learning (TDL), we study Latent Topology Inference\n",
            "(LTI) for learning higher-order cell complexes (with sparse and not regular\n",
            "topology) describing multi-way interactions between data points. To this aim,\n",
            "we introduce the Differentiable Cell Complex Module (DCM), a novel learnable\n",
            "function that computes cell probabilities in the complex to improve the\n",
            "downstream task. We show how to integrate DCM with cell complex message passing\n",
            "networks layers and train it in a end-to-end fashion, thanks to a two-step\n",
            "inference procedure that avoids an exhaustive search across all possible cells\n",
            "in the input, thus maintaining scalability. Our model is tested on several\n",
            "homophilic and heterophilic graph datasets and it is shown to outperform other\n",
            "state-of-the-art techniques, offering significant improvements especially in\n",
            "cases where an input graph is not provided.\n",
            "\n",
            "7. Title: TopoMLP: A Simple yet Strong Pipeline for Driving Topology Reasoning\n",
            "   Abstract: Designing applications for hybrid cloud has many features, including dynamic\n",
            "virtualization management and route switching. This makes it impossible to\n",
            "evaluate the query and hence the optimal distribution of data. In this paper,\n",
            "we formulate the main challenges of designing and simulation, offer\n",
            "installation for processing.\n",
            "\n",
            "8. Title: Variance Reduced Halpern Iteration for Finite-Sum Monotone Inclusions\n",
            "   Abstract: Plasticity, the ability of a neural network to evolve with new data, is\n",
            "crucial for high-performance and sample-efficient visual reinforcement learning\n",
            "(VRL). Although methods like resetting and regularization can potentially\n",
            "mitigate plasticity loss, the influences of various components within the VRL\n",
            "framework on the agent's plasticity are still poorly understood. In this work,\n",
            "we conduct a systematic empirical exploration focusing on three primary\n",
            "underexplored facets and derive the following insightful conclusions: (1) data\n",
            "augmentation is essential in maintaining plasticity; (2) the critic's\n",
            "plasticity loss serves as the principal bottleneck impeding efficient training;\n",
            "and (3) without timely intervention to recover critic's plasticity in the early\n",
            "stages, its loss becomes catastrophic. These insights suggest a novel strategy\n",
            "to address the high replay ratio (RR) dilemma, where exacerbated plasticity\n",
            "loss hinders the potential improvements of sample efficiency brought by\n",
            "increased reuse frequency. Rather than setting a static RR for the entire\n",
            "training process, we propose Adaptive RR, which dynamically adjusts the RR\n",
            "based on the critic's plasticity level. Extensive evaluations indicate that\n",
            "Adaptive RR not only avoids catastrophic plasticity loss in the early stages\n",
            "but also benefits from more frequent reuse in later phases, resulting in\n",
            "superior sample efficiency.\n",
            "\n",
            "9. Title: UNR-Explainer: Counterfactual Explanations for Unsupervised Node Representation Learning Models\n",
            "   Abstract: In this paper we characterize the zero sets of functions from $\\ell^{p}_{A}$\n",
            "(the analytic functions on the open unit disk $D$ whose Taylor coefficients\n",
            "form an $\\ell^p$ sequence) by developing a concept of an \"inner function\"\n",
            "modeled by Beurling's discussion of the Hilbert space $\\ell^{2}_{A}$ (the\n",
            "classical Hardy space). The zero set criterion is used to construct families of\n",
            "zero sets which are not covered by classical results. In particular, it is\n",
            "proved that when $p > 2$, there are zero sets for $\\ell^{p}_{A}$ which are not\n",
            "Blaschke sequences.\n",
            "\n",
            "10. Title: MINDE: Mutual Information Neural Diffusion Estimation\n",
            "   Abstract: Recurrent Spiking Neural Networks (RSNNs) have emerged as a computationally\n",
            "efficient and brain-inspired learning model. The design of sparse RSNNs with\n",
            "fewer neurons and synapses helps reduce the computational complexity of RSNNs.\n",
            "Traditionally, sparse SNNs are obtained by first training a dense and complex\n",
            "SNN for a target task, and, then, pruning neurons with low activity\n",
            "(activity-based pruning) while maintaining task performance. In contrast, this\n",
            "paper presents a task-agnostic methodology for designing sparse RSNNs by\n",
            "pruning a large randomly initialized model. We introduce a novel Lyapunov Noise\n",
            "Pruning (LNP) algorithm that uses graph sparsification methods and utilizes\n",
            "Lyapunov exponents to design a stable sparse RSNN from a randomly initialized\n",
            "RSNN. We show that the LNP can leverage diversity in neuronal timescales to\n",
            "design a sparse Heterogeneous RSNN (HRSNN). Further, we show that the same\n",
            "sparse HRSNN model can be trained for different tasks, such as image\n",
            "classification and temporal prediction. We experimentally show that, in spite\n",
            "of being task-agnostic, LNP increases computational efficiency (fewer neurons\n",
            "and synapses) and prediction performance of RSNNs compared to traditional\n",
            "activity-based pruning of trained dense models.\n",
            "\n",
            "11. Title: Retrieval-Guided Reinforcement Learning for Boolean Circuit Minimization\n",
            "   Abstract: Machine learning approaches relying on such criteria as adversarial\n",
            "robustness or multi-agent settings have raised the need for solving\n",
            "game-theoretic equilibrium problems. Of particular relevance to these\n",
            "applications are methods targeting finite-sum structure, which generically\n",
            "arises in empirical variants of learning problems in these contexts. Further,\n",
            "methods with computable approximation errors are highly desirable, as they\n",
            "provide verifiable exit criteria. Motivated by these applications, we study\n",
            "finite-sum monotone inclusion problems, which model broad classes of\n",
            "equilibrium problems. Our main contributions are variants of the classical\n",
            "Halpern iteration that employ variance reduction to obtain improved complexity\n",
            "guarantees in which $n$ component operators in the finite sum are ``on\n",
            "average'' either cocoercive or Lipschitz continuous and monotone, with\n",
            "parameter $L$. The resulting oracle complexity of our methods, which provide\n",
            "guarantees for the last iterate and for a (computable) operator norm residual,\n",
            "is $\\widetilde{\\mathcal{O}}( n + \\sqrt{n}L\\varepsilon^{-1})$, which improves\n",
            "upon existing methods by a factor up to $\\sqrt{n}$. This constitutes the first\n",
            "variance reduction-type result for general finite-sum monotone inclusions and\n",
            "for more specific problems such as convex-concave optimization when operator\n",
            "norm residual is the optimality measure. We further argue that, up to\n",
            "poly-logarithmic factors, this complexity is unimprovable in the monotone\n",
            "Lipschitz setting; i.e., the provided result is near-optimal.\n",
            "\n",
            "12. Title: Distributional Preference Learning: Understanding and Accounting for Hidden Context in RLHF\n",
            "   Abstract: As language models continue to scale in size and capability, they display an\n",
            "array of emerging behaviors, both beneficial and concerning. This heightens the\n",
            "need to control model behaviors. We hope to be able to control the personality\n",
            "traits of language models at the inference-time so as to have various character\n",
            "features, on top of which the requirements of different types of tasks can be\n",
            "met. Personality is a higher-level and more abstract behavioral representation\n",
            "for language models. We introduce ControlLM, which leverages differential\n",
            "activation patterns, derived from contrasting behavioral prompts in the model's\n",
            "latent space, to influence the model's personality traits at inference. This\n",
            "approach allows for the precise, real-time adjustment of model behavior. First,\n",
            "we demonstrate ControlLM's capacity to elicit diverse persona behaviors without\n",
            "any training, while precision control allows personality traits to closely\n",
            "match average human values. Subsequently, we showcase improved reasoning and\n",
            "question answering through selective amplification of beneficial attributes\n",
            "like conscientiousness and friendliness. We hope that this work will inspire\n",
            "research on controlling human-like behaviors of language models and provide\n",
            "insights for future research. Our code is publicly available at:\n",
            "https://github.com/wengsyx/ControlLM.\n",
            "\n",
            "13. Title: Linear attention is (maybe) all you need (to understand Transformer optimization)\n",
            "   Abstract: Boolean circuit is a computational graph that consists of the dynamic\n",
            "directed graph structure and static functionality. The commonly used logic\n",
            "optimization and Boolean matching-based transformation can change the behavior\n",
            "of the Boolean circuit for its graph structure and functionality in logic\n",
            "synthesis. The graph structure-based Boolean circuit classification can be\n",
            "grouped into the graph classification task, however, the functionality-based\n",
            "Boolean circuit classification remains an open problem for further research. In\n",
            "this paper, we first define the proposed matching-equivalent class based on its\n",
            "``Boolean-aware'' property. The Boolean circuits in the proposed class can be\n",
            "transformed into each other. Then, we present a commonly study framework based\n",
            "on graph neural network~(GNN) to analyze the key factors that can affect the\n",
            "Boolean-aware Boolean circuit classification. The empirical experiment results\n",
            "verify the proposed analysis, and it also shows the direction and opportunity\n",
            "to improve the proposed problem. The code and dataset will be released after\n",
            "acceptance.\n",
            "\n",
            "14. Title: LipSim: A Provably Robust Perceptual Similarity Metric\n",
            "   Abstract: Supervised learning with large scale labeled datasets and deep layered models\n",
            "has made a paradigm shift in diverse areas in learning and recognition.\n",
            "However, this approach still suffers generalization issues under the presence\n",
            "of a domain shift between the training and the test data distribution. In this\n",
            "regard, unsupervised domain adaptation algorithms have been proposed to\n",
            "directly address the domain shift problem. In this paper, we approach the\n",
            "problem from a transductive perspective. We incorporate the domain shift and\n",
            "the transductive target inference into our framework by jointly solving for an\n",
            "asymmetric similarity metric and the optimal transductive target label\n",
            "assignment. We also show that our model can easily be extended for deep feature\n",
            "learning in order to learn features which are discriminative in the target\n",
            "domain. Our experiments show that the proposed method significantly outperforms\n",
            "state-of-the-art algorithms in both object recognition and digit classification\n",
            "experiments by a large margin.\n",
            "\n",
            "15. Title: SALMONN: Towards Generic Hearing Abilities for Large Language Models\n",
            "   Abstract: With the increasing number of new neural architecture designs and substantial\n",
            "existing neural architectures, it becomes difficult for the researchers to\n",
            "situate their contributions compared with existing neural architectures or\n",
            "establish the connections between their designs and other relevant ones. To\n",
            "discover similar neural architectures in an efficient and automatic manner, we\n",
            "define a new problem Neural Architecture Retrieval which retrieves a set of\n",
            "existing neural architectures which have similar designs to the query neural\n",
            "architecture. Existing graph pre-training strategies cannot address the\n",
            "computational graph in neural architectures due to the graph size and motifs.\n",
            "To fulfill this potential, we propose to divide the graph into motifs which are\n",
            "used to rebuild the macro graph to tackle these issues, and introduce\n",
            "multi-level contrastive learning to achieve accurate graph representation\n",
            "learning. Extensive evaluations on both human-designed and synthesized neural\n",
            "architectures demonstrate the superiority of our algorithm. Such a dataset\n",
            "which contains 12k real-world network architectures, as well as their\n",
            "embedding, is built for neural architecture retrieval.\n",
            "\n",
            "16. Title: Tensor Programs VI: Feature Learning in Infinite Depth Neural Networks\n",
            "   Abstract: Generating 3D scenes is a challenging open problem, which requires\n",
            "synthesizing plausible content that is fully consistent in 3D space. While\n",
            "recent methods such as neural radiance fields excel at view synthesis and 3D\n",
            "reconstruction, they cannot synthesize plausible details in unobserved regions\n",
            "since they lack a generative capability. Conversely, existing generative\n",
            "methods are typically not capable of reconstructing detailed, large-scale\n",
            "scenes in the wild, as they use limited-capacity 3D scene representations,\n",
            "require aligned camera poses, or rely on additional regularizers. In this work,\n",
            "we introduce the first diffusion model able to perform fast, detailed\n",
            "reconstruction and generation of real-world 3D scenes. To achieve this, we make\n",
            "three contributions. First, we introduce a new neural scene representation,\n",
            "IB-planes, that can efficiently and accurately represent large 3D scenes,\n",
            "dynamically allocating more capacity as needed to capture details visible in\n",
            "each image. Second, we propose a denoising-diffusion framework to learn a prior\n",
            "over this novel 3D scene representation, using only 2D images without the need\n",
            "for any additional supervision signal such as masks or depths. This supports 3D\n",
            "reconstruction and generation in a unified architecture. Third, we develop a\n",
            "principled approach to avoid trivial 3D solutions when integrating the\n",
            "image-based rendering with the diffusion model, by dropping out representations\n",
            "of some images. We evaluate the model on several challenging datasets of real\n",
            "and synthetic images, and demonstrate superior results on generation, novel\n",
            "view synthesis and 3D reconstruction.\n",
            "\n",
            "17. Title: Enhancing Transferable Adversarial Attacks on Vision Transformers through Gradient Normalization Scaling and High-Frequency Adaptation\n",
            "   Abstract: Cross-encoder (CE) models which compute similarity by jointly encoding a\n",
            "query-item pair perform better than embedding-based models (dual-encoders) at\n",
            "estimating query-item relevance. Existing approaches perform k-NN search with\n",
            "CE by approximating the CE similarity with a vector embedding space fit either\n",
            "with dual-encoders (DE) or CUR matrix factorization. DE-based\n",
            "retrieve-and-rerank approaches suffer from poor recall on new domains and the\n",
            "retrieval with DE is decoupled from the CE. While CUR-based approaches can be\n",
            "more accurate than the DE-based approach, they require a prohibitively large\n",
            "number of CE calls to compute item embeddings, thus making it impractical for\n",
            "deployment at scale. In this paper, we address these shortcomings with our\n",
            "proposed sparse-matrix factorization based method that efficiently computes\n",
            "latent query and item embeddings to approximate CE scores and performs k-NN\n",
            "search with the approximate CE similarity. We compute item embeddings offline\n",
            "by factorizing a sparse matrix containing query-item CE scores for a set of\n",
            "train queries. Our method produces a high-quality approximation while requiring\n",
            "only a fraction of CE calls as compared to CUR-based methods, and allows for\n",
            "leveraging DE to initialize the embedding space while avoiding compute- and\n",
            "resource-intensive finetuning of DE via distillation. At test time, the item\n",
            "embeddings remain fixed and retrieval occurs over rounds, alternating between\n",
            "a) estimating the test query embedding by minimizing error in approximating CE\n",
            "scores of items retrieved thus far, and b) using the updated test query\n",
            "embedding for retrieving more items. Our k-NN search method improves recall by\n",
            "up to 5% (k=1) and 54% (k=100) over DE-based approaches. Additionally, our\n",
            "indexing approach achieves a speedup of up to 100x over CUR-based and 5x over\n",
            "DE distillation methods, while matching or improving k-NN search recall over\n",
            "baselines.\n",
            "\n",
            "18. Title: Unsupervised Order Learning\n",
            "   Abstract: Differentiable programming is a fresh programming paradigm which composes\n",
            "parameterized algorithmic components and trains them using automatic\n",
            "differentiation (AD). The concept emerges from deep learning but is not only\n",
            "limited to training neural networks. We present theory and practice of\n",
            "programming tensor network algorithms in a fully differentiable way. By\n",
            "formulating the tensor network algorithm as a computation graph, one can\n",
            "compute higher order derivatives of the program accurately and efficiently\n",
            "using AD. We present essential techniques to differentiate through the tensor\n",
            "networks contractions, including stable AD for tensor decomposition and\n",
            "efficient backpropagation through fixed point iterations. As a demonstration,\n",
            "we compute the specific heat of the Ising model directly by taking the second\n",
            "order derivative of the free energy obtained in the tensor renormalization\n",
            "group calculation. Next, we perform gradient based variational optimization of\n",
            "infinite projected entangled pair states for quantum antiferromagnetic\n",
            "Heisenberg model and obtain start-of-the-art variational energy and\n",
            "magnetization with moderate efforts. Differentiable programming removes\n",
            "laborious human efforts in deriving and implementing analytical gradients for\n",
            "tensor network programs, which opens the door to more innovations in tensor\n",
            "network algorithms and applications.\n",
            "\n",
            "19. Title: Adaptive Retrieval and Scalable Indexing for k-NN Search with Cross-Encoders\n",
            "   Abstract: Reasoning on knowledge graphs is a challenging task because it utilizes\n",
            "observed information to predict the missing one. Particularly, answering\n",
            "complex queries based on first-order logic is one of the crucial tasks to\n",
            "verify learning to reason abilities for generalization and composition.\n",
            "Recently, the prevailing method is query embedding which learns the embedding\n",
            "of a set of entities and treats logic operations as set operations and has\n",
            "shown great empirical success. Though there has been much research following\n",
            "the same formulation, many of its claims lack a formal and systematic\n",
            "inspection. In this paper, we rethink this formulation and justify many of the\n",
            "previous claims by characterizing the scope of queries investigated previously\n",
            "and precisely identifying the gap between its formulation and its goal, as well\n",
            "as providing complexity analysis for the currently investigated queries.\n",
            "Moreover, we develop a new dataset containing ten new types of queries with\n",
            "features that have never been considered and therefore can provide a thorough\n",
            "investigation of complex queries. Finally, we propose a new neural-symbolic\n",
            "method, Fuzzy Inference with Truth value (FIT), where we equip the neural link\n",
            "predictors with fuzzy logic theory to support end-to-end learning using complex\n",
            "queries with provable reasoning capability. Empirical results show that our\n",
            "method outperforms previous methods significantly in the new dataset and also\n",
            "surpasses previous methods in the existing dataset at the same time.\n",
            "\n",
            "20. Title: Denoising Diffusion via Image-Based Rendering\n",
            "   Abstract: Modern machine learning (ML) systems demand substantial training data, often\n",
            "resorting to external sources. Nevertheless, this practice renders them\n",
            "vulnerable to backdoor poisoning attacks. Prior backdoor defense strategies\n",
            "have primarily focused on the identification of backdoored models or poisoned\n",
            "data characteristics, typically operating under the assumption of access to\n",
            "clean data. In this work, we delve into a relatively underexplored challenge:\n",
            "the automatic identification of backdoor data within a poisoned dataset, all\n",
            "under realistic conditions, i.e., without the need for additional clean data or\n",
            "without manually defining a threshold for backdoor detection. We draw an\n",
            "inspiration from the scaled prediction consistency (SPC) technique, which\n",
            "exploits the prediction invariance of poisoned data to an input scaling factor.\n",
            "Based on this, we pose the backdoor data identification problem as a\n",
            "hierarchical data splitting optimization problem, leveraging a novel SPC-based\n",
            "loss function as the primary optimization objective. Our innovation unfolds in\n",
            "several key aspects. First, we revisit the vanilla SPC method, unveiling its\n",
            "limitations in addressing the proposed backdoor identification problem.\n",
            "Subsequently, we develop a bi-level optimization-based approach to precisely\n",
            "identify backdoor data by minimizing the advanced SPC loss. Finally, we\n",
            "demonstrate the efficacy of our proposal against a spectrum of backdoor\n",
            "attacks, encompassing basic label-corrupted attacks as well as more\n",
            "sophisticated clean-label attacks, evaluated across various benchmark datasets.\n",
            "Experiment results show that our approach often surpasses the performance of\n",
            "current baselines in identifying backdoor data points, resulting in about\n",
            "4%-36% improvement in average AUROC. Codes are available at\n",
            "https://github.com/OPTML-Group/BackdoorMSPC.\n",
            "\n",
            "21. Title: Neural Architecture Retrieval\n",
            "   Abstract: Markov Decision Processes (MDPs) are a formal framework for modeling and\n",
            "solving sequential decision-making problems. In finite-time horizons such\n",
            "problems are relevant for instance for optimal stopping or specific supply\n",
            "chain problems, but also in the training of large language models. In contrast\n",
            "to infinite horizon MDPs optimal policies are not stationary, policies must be\n",
            "learned for every single epoch. In practice all parameters are often trained\n",
            "simultaneously, ignoring the inherent structure suggested by dynamic\n",
            "programming. This paper introduces a combination of dynamic programming and\n",
            "policy gradient called dynamic policy gradient, where the parameters are\n",
            "trained backwards in time. For the tabular softmax parametrisation we carry out\n",
            "the convergence analysis for simultaneous and dynamic policy gradient towards\n",
            "global optima, both in the exact and sampled gradient settings without\n",
            "regularisation. It turns out that the use of dynamic policy gradient training\n",
            "much better exploits the structure of finite- time problems which is reflected\n",
            "in improved convergence bounds.\n",
            "\n",
            "22. Title: Data Distillation Can Be Like Vodka: Distilling More Times For Better Quality\n",
            "   Abstract: The massive interest in deep neural networks (DNNs) for both computer vision\n",
            "and natural language processing has been sparked by the growth in computational\n",
            "power. However, this led to an increase in the memory footprint, to a point\n",
            "where it can be challenging to simply load a model on commodity devices such as\n",
            "mobile phones. To address this limitation, quantization is a favored solution\n",
            "as it maps high precision tensors to a low precision, memory efficient format.\n",
            "In terms of memory footprint reduction, its most effective variants are based\n",
            "on codebooks. These methods, however, suffer from two limitations. First, they\n",
            "either define a single codebook for each tensor, or use a memory-expensive\n",
            "mapping to multiple codebooks. Second, gradient descent optimization of the\n",
            "mapping favors jumps toward extreme values, hence not defining a proximal\n",
            "search. In this work, we propose to address these two limitations. First, we\n",
            "initially group similarly distributed neurons and leverage the re-ordered\n",
            "structure to either apply different scale factors to the different groups, or\n",
            "map weights that fall in these groups to several codebooks, without any mapping\n",
            "overhead. Second, stemming from this initialization, we propose a joint\n",
            "learning of the codebook and weight mappings that bears similarities with\n",
            "recent gradient-based post-training quantization techniques. Third, drawing\n",
            "estimation from straight-through estimation techniques, we introduce a novel\n",
            "gradient update definition to enable a proximal search of the codebooks and\n",
            "their mappings. The proposed jointly learnable codebooks and mappings (JLCM)\n",
            "method allows a very efficient approximation of any DNN: as such, a Llama 7B\n",
            "can be compressed down to 2Go and loaded on 5-year-old smartphones.\n",
            "\n",
            "23. Title: Backdoor Secrets Unveiled: Identifying Backdoor Data with Optimized Scaled Prediction Consistency\n",
            "   Abstract: The infinitely wide neural network has been proven a useful and manageable\n",
            "mathematical model that enables the understanding of many phenomena appearing\n",
            "in deep learning. One example is the convergence of random deep networks to\n",
            "Gaussian processes that allows a rigorous analysis of the way the choice of\n",
            "activation function and network weights impacts the training dynamics. In this\n",
            "paper, we extend the seminal proof of Matthews et al. (2018) to a larger class\n",
            "of initial weight distributions (which we call PSEUDO-IID), including the\n",
            "established cases of IID and orthogonal weights, as well as the emerging\n",
            "low-rank and structured sparse settings celebrated for their computational\n",
            "speed-up benefits. We show that fully-connected and convolutional networks\n",
            "initialized with PSEUDO-IID distributions are all effectively equivalent up to\n",
            "their variance. Using our results, one can identify the Edge-of-Chaos for a\n",
            "broader class of neural networks and tune them at criticality in order to\n",
            "enhance their training. Moreover, they enable the posterior distribution of\n",
            "Bayesian Neural Networks to be tractable across these various initialization\n",
            "schemes.\n",
            "\n",
            "24. Title: Bespoke Solvers for Generative Flow Models\n",
            "   Abstract: Transformer training is notoriously difficult, requiring a careful design of\n",
            "optimizers and use of various heuristics. We make progress towards\n",
            "understanding the subtleties of training Transformers by carefully studying a\n",
            "simple yet canonical linearized shallow Transformer model. Specifically, we\n",
            "train linear Transformers to solve regression tasks, inspired by J.~von Oswald\n",
            "et al.~(ICML 2023), and K.~Ahn et al.~(NeurIPS 2023). Most importantly, we\n",
            "observe that our proposed linearized models can reproduce several prominent\n",
            "aspects of Transformer training dynamics. Consequently, the results obtained in\n",
            "this paper suggest that a simple linearized Transformer model could actually be\n",
            "a valuable, realistic abstraction for understanding Transformer optimization.\n",
            "\n",
            "25. Title: Network Memory Footprint Compression Through Jointly Learnable Codebooks and Mappings\n",
            "   Abstract: Diffusion or flow-based models are powerful generative paradigms that are\n",
            "notoriously hard to sample as samples are defined as solutions to\n",
            "high-dimensional Ordinary or Stochastic Differential Equations (ODEs/SDEs)\n",
            "which require a large Number of Function Evaluations (NFE) to approximate well.\n",
            "Existing methods to alleviate the costly sampling process include model\n",
            "distillation and designing dedicated ODE solvers. However, distillation is\n",
            "costly to train and sometimes can deteriorate quality, while dedicated solvers\n",
            "still require relatively large NFE to produce high quality samples. In this\n",
            "paper we introduce \"Bespoke solvers\", a novel framework for constructing custom\n",
            "ODE solvers tailored to the ODE of a given pre-trained flow model. Our approach\n",
            "optimizes an order consistent and parameter-efficient solver (e.g., with 80\n",
            "learnable parameters), is trained for roughly 1% of the GPU time required for\n",
            "training the pre-trained model, and significantly improves approximation and\n",
            "generation quality compared to dedicated solvers. For example, a Bespoke solver\n",
            "for a CIFAR10 model produces samples with Fr\\'echet Inception Distance (FID) of\n",
            "2.73 with 10 NFE, and gets to 1% of the Ground Truth (GT) FID (2.59) for this\n",
            "model with only 20 NFE. On the more challenging ImageNet-64$\\times$64, Bespoke\n",
            "samples at 2.2 FID with 10 NFE, and gets within 2% of GT FID (1.71) with 20\n",
            "NFE.\n",
            "\n",
            "26. Title: Spike-driven Transformer V2: Meta Spiking Neural Network Architecture Inspiring the Design of Next-generation Neuromorphic Chips\n",
            "   Abstract: In practice, preference learning from human feedback depends on incomplete\n",
            "data with hidden context. Hidden context refers to data that affects the\n",
            "feedback received, but which is not represented in the data used to train a\n",
            "preference model. This captures common issues of data collection, such as\n",
            "having human annotators with varied preferences, cognitive processes that\n",
            "result in seemingly irrational behavior, and combining data labeled according\n",
            "to different criteria. We prove that standard applications of preference\n",
            "learning, including reinforcement learning from human feedback (RLHF),\n",
            "implicitly aggregate over hidden contexts according to a well-known voting rule\n",
            "called Borda count. We show this can produce counter-intuitive results that are\n",
            "very different from other methods which implicitly aggregate via expected\n",
            "utility. Furthermore, our analysis formalizes the way that preference learning\n",
            "from users with diverse values tacitly implements a social choice function. A\n",
            "key implication of this result is that annotators have an incentive to\n",
            "misreport their preferences in order to influence the learned model, leading to\n",
            "vulnerabilities in the deployment of RLHF. As a step towards mitigating these\n",
            "problems, we introduce a class of methods called distributional preference\n",
            "learning (DPL). DPL methods estimate a distribution of possible score values\n",
            "for each alternative in order to better account for hidden context.\n",
            "Experimental results indicate that applying DPL to RLHF for LLM chatbots\n",
            "identifies hidden context in the data and significantly reduces subsequent\n",
            "jailbreak vulnerability. Our code and data are available at\n",
            "https://github.com/cassidylaidlaw/hidden-context\n",
            "\n",
            "27. Title: Beyond IID weights: sparse and low-rank deep Neural Networks are also Gaussian Processes\n",
            "   Abstract: Neuromorphic computing, which exploits Spiking Neural Networks (SNNs) on\n",
            "neuromorphic chips, is a promising energy-efficient alternative to traditional\n",
            "AI. CNN-based SNNs are the current mainstream of neuromorphic computing. By\n",
            "contrast, no neuromorphic chips are designed especially for Transformer-based\n",
            "SNNs, which have just emerged, and their performance is only on par with\n",
            "CNN-based SNNs, offering no distinct advantage. In this work, we propose a\n",
            "general Transformer-based SNN architecture, termed as ``Meta-SpikeFormer\",\n",
            "whose goals are: 1) Lower-power, supports the spike-driven paradigm that there\n",
            "is only sparse addition in the network; 2) Versatility, handles various vision\n",
            "tasks; 3) High-performance, shows overwhelming performance advantages over\n",
            "CNN-based SNNs; 4) Meta-architecture, provides inspiration for future\n",
            "next-generation Transformer-based neuromorphic chip designs. Specifically, we\n",
            "extend the Spike-driven Transformer in \\citet{yao2023spike} into a meta\n",
            "architecture, and explore the impact of structure, spike-driven self-attention,\n",
            "and skip connection on its performance. On ImageNet-1K, Meta-SpikeFormer\n",
            "achieves 80.0\\% top-1 accuracy (55M), surpassing the current state-of-the-art\n",
            "(SOTA) SNN baselines (66M) by 3.7\\%. This is the first direct training SNN\n",
            "backbone that can simultaneously supports classification, detection, and\n",
            "segmentation, obtaining SOTA results in SNNs. Finally, we discuss the\n",
            "inspiration of the meta SNN architecture for neuromorphic chip design. Source\n",
            "code and models are available at\n",
            "\\url{https://github.com/BICLab/Spike-Driven-Transformer-V2}.\n",
            "\n",
            "28. Title: Defining Expertise: Applications to Treatment Effect Estimation\n",
            "   Abstract: Decision-makers are often experts of their domain and take actions based on\n",
            "their domain knowledge. Doctors, for instance, may prescribe treatments by\n",
            "predicting the likely outcome of each available treatment. Actions of an expert\n",
            "thus naturally encode part of their domain knowledge, and can help make\n",
            "inferences within the same domain: Knowing doctors try to prescribe the best\n",
            "treatment for their patients, we can tell treatments prescribed more frequently\n",
            "are likely to be more effective. Yet in machine learning, the fact that most\n",
            "decision-makers are experts is often overlooked, and \"expertise\" is seldom\n",
            "leveraged as an inductive bias. This is especially true for the literature on\n",
            "treatment effect estimation, where often the only assumption made about actions\n",
            "is that of overlap. In this paper, we argue that expertise - particularly the\n",
            "type of expertise the decision-makers of a domain are likely to have - can be\n",
            "informative in designing and selecting methods for treatment effect estimation.\n",
            "We formally define two types of expertise, predictive and prognostic, and\n",
            "demonstrate empirically that: (i) the prominent type of expertise in a domain\n",
            "significantly influences the performance of different methods in treatment\n",
            "effect estimation, and (ii) it is possible to predict the type of expertise\n",
            "present in a dataset, which can provide a quantitative basis for model\n",
            "selection.\n",
            "\n",
            "29. Title: DreamTime: An Improved Optimization Strategy for Diffusion-Guided 3D Generation\n",
            "   Abstract: Deep learning models are known to be vulnerable to adversarial examples\n",
            "crafted by adding human-imperceptible perturbations on benign images. Many\n",
            "existing adversarial attack methods have achieved great white-box attack\n",
            "performance, but exhibit low transferability when attacking other models.\n",
            "Various momentum iterative gradient-based methods are shown to be effective to\n",
            "improve the adversarial transferability. In what follows, we propose an\n",
            "enhanced momentum iterative gradient-based method to further enhance the\n",
            "adversarial transferability. Specifically, instead of only accumulating the\n",
            "gradient during the iterative process, we additionally accumulate the average\n",
            "gradient of the data points sampled in the gradient direction of the previous\n",
            "iteration so as to stabilize the update direction and escape from poor local\n",
            "maxima. Extensive experiments on the standard ImageNet dataset demonstrate that\n",
            "our method could improve the adversarial transferability of momentum-based\n",
            "methods by a large margin of 11.1% on average. Moreover, by incorporating with\n",
            "various input transformation methods, the adversarial transferability could be\n",
            "further improved significantly. We also attack several extra advanced defense\n",
            "models under the ensemble-model setting, and the enhancements are remarkable\n",
            "with at least 7.8% on average.\n",
            "\n",
            "30. Title: I-PHYRE: Interactive Physical Reasoning\n",
            "   Abstract: Label smoothing -- using softened labels instead of hard ones -- is a widely\n",
            "adopted regularization method for deep learning, showing diverse benefits such\n",
            "as enhanced generalization and calibration. Its implications for preserving\n",
            "model privacy, however, have remained unexplored. To fill this gap, we\n",
            "investigate the impact of label smoothing on model inversion attacks (MIAs),\n",
            "which aim to generate class-representative samples by exploiting the knowledge\n",
            "encoded in a classifier, thereby inferring sensitive information about its\n",
            "training data. Through extensive analyses, we uncover that traditional label\n",
            "smoothing fosters MIAs, thereby increasing a model's privacy leakage. Even\n",
            "more, we reveal that smoothing with negative factors counters this trend,\n",
            "impeding the extraction of class-related information and leading to privacy\n",
            "preservation, beating state-of-the-art defenses. This establishes a practical\n",
            "and powerful novel way for enhancing model resilience against MIAs.\n",
            "\n",
            "31. Title: ReSimAD: Zero-Shot 3D Domain Transfer for Autonomous Driving with Source Reconstruction and Target Simulation\n",
            "   Abstract: Diffusion models have emerged as a key pillar of foundation models in visual\n",
            "domains. One of their critical applications is to universally solve different\n",
            "downstream inverse tasks via a single diffusion prior without re-training for\n",
            "each task. Most inverse tasks can be formulated as inferring a posterior\n",
            "distribution over data (e.g., a full image) given a measurement (e.g., a masked\n",
            "image). This is however challenging in diffusion models since the nonlinear and\n",
            "iterative nature of the diffusion process renders the posterior intractable. To\n",
            "cope with this challenge, we propose a variational approach that by design\n",
            "seeks to approximate the true posterior distribution. We show that our approach\n",
            "naturally leads to regularization by denoising diffusion process (RED-Diff)\n",
            "where denoisers at different timesteps concurrently impose different structural\n",
            "constraints over the image. To gauge the contribution of denoisers from\n",
            "different timesteps, we propose a weighting mechanism based on\n",
            "signal-to-noise-ratio (SNR). Our approach provides a new variational\n",
            "perspective for solving inverse problems with diffusion models, allowing us to\n",
            "formulate sampling as stochastic optimization, where one can simply apply\n",
            "off-the-shelf solvers with lightweight iterates. Our experiments for image\n",
            "restoration tasks such as inpainting and superresolution demonstrate the\n",
            "strengths of our method compared with state-of-the-art sampling-based diffusion\n",
            "models.\n",
            "\n",
            "32. Title: Sample-Efficient Learning of POMDPs with Multiple Observations In Hindsight\n",
            "   Abstract: Dataset distillation aims to minimize the time and memory needed for training\n",
            "deep networks on large datasets, by creating a small set of synthetic images\n",
            "that has a similar generalization performance to that of the full dataset.\n",
            "However, current dataset distillation techniques fall short, showing a notable\n",
            "performance gap when compared to training on the original data. In this work,\n",
            "we are the first to argue that using just one synthetic subset for distillation\n",
            "will not yield optimal generalization performance. This is because the training\n",
            "dynamics of deep networks drastically change during the training. Hence,\n",
            "multiple synthetic subsets are required to capture the training dynamics at\n",
            "different phases of training. To address this issue, we propose Progressive\n",
            "Dataset Distillation (PDD). PDD synthesizes multiple small sets of synthetic\n",
            "images, each conditioned on the previous sets, and trains the model on the\n",
            "cumulative union of these subsets without requiring additional training time.\n",
            "Our extensive experiments show that PDD can effectively improve the performance\n",
            "of existing dataset distillation methods by up to 4.3%. In addition, our method\n",
            "for the first time enable generating considerably larger synthetic datasets.\n",
            "\n",
            "33. Title: Simple Minimax Optimal Byzantine Robust Algorithm for Nonconvex Objectives with Uniform Gradient Heterogeneity\n",
            "   Abstract: Improving the alignment of Large Language Models (LLMs) with respect to the\n",
            "cultural values that they encode has become an increasingly important topic. In\n",
            "this work, we study whether we can exploit existing knowledge about cultural\n",
            "values at inference time to adjust model responses to cultural value probes. We\n",
            "present a simple and inexpensive method that uses a combination of in-context\n",
            "learning (ICL) and human survey data, and show that we can improve the\n",
            "alignment to cultural values across 5 models that include both English-centric\n",
            "and multilingual LLMs. Importantly, we show that our method could prove useful\n",
            "in test languages other than English and can improve alignment to the cultural\n",
            "values that correspond to a range of culturally diverse countries.\n",
            "\n",
            "34. Title: InstaFlow: One Step is Enough for High-Quality Diffusion-Based Text-to-Image Generation\n",
            "   Abstract: Foundation models have emerged as a powerful tool for many AI problems.\n",
            "Despite the tremendous success of foundation models, effective adaptation to\n",
            "new tasks, particularly those with limited labels, remains an open question and\n",
            "lacks theoretical understanding. An emerging solution with recent success in\n",
            "vision and NLP involves finetuning a foundation model on a selection of\n",
            "relevant tasks, before its adaptation to a target task with limited labeled\n",
            "samples. In this paper, we study the theoretical justification of this\n",
            "multitask finetuning approach. Our theoretical analysis reveals that with a\n",
            "diverse set of related tasks, this multitask finetuning leads to reduced error\n",
            "in the target task, in comparison to directly adapting the same pretrained\n",
            "model. We quantify the relationship between finetuning tasks and target tasks\n",
            "by diversity and consistency metrics, and further propose a practical task\n",
            "selection algorithm. We substantiate our theoretical claims with extensive\n",
            "empirical evidence. Further, we present results affirming our task selection\n",
            "algorithm adeptly chooses related finetuning tasks, providing advantages to the\n",
            "model performance on target tasks. We believe our study shed new light on the\n",
            "effective adaptation of foundation models to new tasks that lack abundant\n",
            "labels. Our code is available at\n",
            "https://github.com/OliverXUZY/Foudation-Model_Multitask.\n",
            "\n",
            "35. Title: Unsupervised Pretraining for Fact Verification by Language Model Distillation\n",
            "   Abstract: Currently, increasingly deeper neural networks have been applied to improve\n",
            "their accuracy. In contrast, We propose a novel wider Convolutional Neural\n",
            "Networks (CNN) architecture, motivated by the Multi-column Deep Neural Networks\n",
            "and the Network In Network(NIN), aiming for higher accuracy without input data\n",
            "transmutation. In our architecture, namely \"CNN In Convolution\"(CNNIC), a small\n",
            "CNN, instead of the original generalized liner model(GLM) based filters, is\n",
            "convoluted as kernel on the original image, serving as feature extracting layer\n",
            "of this networks. And further classifications are then carried out by a global\n",
            "average pooling layer and a softmax layer. Dropout and orthonormal\n",
            "initialization are applied to overcome training difficulties including slow\n",
            "convergence and over-fitting. Persuasive classification performance is\n",
            "demonstrated on MNIST.\n",
            "\n",
            "36. Title: Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs\n",
            "   Abstract: Current model-based reinforcement learning (MBRL) agents struggle with\n",
            "long-term dependencies. This limits their ability to effectively solve tasks\n",
            "involving extended time gaps between actions and outcomes, or tasks demanding\n",
            "the recalling of distant observations to inform current actions. To improve\n",
            "temporal coherence, we integrate a new family of state space models (SSMs) in\n",
            "world models of MBRL agents to present a new method, Recall to Imagine (R2I).\n",
            "This integration aims to enhance both long-term memory and long-horizon credit\n",
            "assignment. Through a diverse set of illustrative tasks, we systematically\n",
            "demonstrate that R2I not only establishes a new state-of-the-art for\n",
            "challenging memory and credit assignment RL tasks, such as BSuite and POPGym,\n",
            "but also showcases superhuman performance in the complex memory domain of\n",
            "Memory Maze. At the same time, it upholds comparable performance in classic RL\n",
            "tasks, such as Atari and DMC, suggesting the generality of our method. We also\n",
            "show that R2I is faster than the state-of-the-art MBRL method, DreamerV3,\n",
            "resulting in faster wall-time convergence.\n",
            "\n",
            "37. Title: Self-Alignment with Instruction Backtranslation\n",
            "   Abstract: We present Direct Reward Fine-Tuning (DRaFT), a simple and effective method\n",
            "for fine-tuning diffusion models to maximize differentiable reward functions,\n",
            "such as scores from human preference models. We first show that it is possible\n",
            "to backpropagate the reward function gradient through the full sampling\n",
            "procedure, and that doing so achieves strong performance on a variety of\n",
            "rewards, outperforming reinforcement learning-based approaches. We then propose\n",
            "more efficient variants of DRaFT: DRaFT-K, which truncates backpropagation to\n",
            "only the last K steps of sampling, and DRaFT-LV, which obtains lower-variance\n",
            "gradient estimates for the case when K=1. We show that our methods work well\n",
            "for a variety of reward functions and can be used to substantially improve the\n",
            "aesthetic quality of images generated by Stable Diffusion 1.4. Finally, we draw\n",
            "connections between our approach and prior work, providing a unifying\n",
            "perspective on the design space of gradient-based fine-tuning algorithms.\n",
            "\n",
            "38. Title: ConjNorm: Tractable Density Estimation for Out-of-Distribution Detection\n",
            "   Abstract: Federated learning (FL) for minimax optimization has emerged as a powerful\n",
            "paradigm for training models across distributed nodes/clients while preserving\n",
            "data privacy and model robustness on data heterogeneity. In this work, we delve\n",
            "into the decentralized implementation of federated minimax optimization by\n",
            "proposing \\texttt{K-GT-Minimax}, a novel decentralized minimax optimization\n",
            "algorithm that combines local updates and gradient tracking techniques. Our\n",
            "analysis showcases the algorithm's communication efficiency and convergence\n",
            "rate for nonconvex-strongly-concave (NC-SC) minimax optimization, demonstrating\n",
            "a superior convergence rate compared to existing methods.\n",
            "\\texttt{K-GT-Minimax}'s ability to handle data heterogeneity and ensure\n",
            "robustness underscores its significance in advancing federated learning\n",
            "research and applications.\n",
            "\n",
            "39. Title: MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models\n",
            "   Abstract: Adversarial training (AT) is a canonical method for enhancing the robustness\n",
            "of deep neural networks (DNNs). However, recent studies empirically\n",
            "demonstrated that it suffers from robust overfitting, i.e., a long time AT can\n",
            "be detrimental to the robustness of DNNs. This paper presents a theoretical\n",
            "explanation of robust overfitting for DNNs. Specifically, we non-trivially\n",
            "extend the neural tangent kernel (NTK) theory to AT and prove that an\n",
            "adversarially trained wide DNN can be well approximated by a linearized DNN.\n",
            "Moreover, for squared loss, closed-form AT dynamics for the linearized DNN can\n",
            "be derived, which reveals a new AT degeneration phenomenon: a long-term AT will\n",
            "result in a wide DNN degenerates to that obtained without AT and thus cause\n",
            "robust overfitting. Based on our theoretical results, we further design a\n",
            "method namely Adv-NTK, the first AT algorithm for infinite-width DNNs.\n",
            "Experiments on real-world datasets show that Adv-NTK can help infinite-width\n",
            "DNNs enhance comparable robustness to that of their finite-width counterparts,\n",
            "which in turn justifies our theoretical findings. The code is available at\n",
            "https://github.com/fshp971/adv-ntk.\n",
            "\n",
            "40. Title: Unified Human-Scene Interaction via Prompted Chain-of-Contacts\n",
            "   Abstract: The ever-increasing large language models (LLMs), though opening a potential\n",
            "path for the upcoming artificial general intelligence, sadly drops a daunting\n",
            "obstacle on the way towards their on-device deployment. As one of the most\n",
            "well-established pre-LLMs approaches in reducing model complexity, network\n",
            "pruning appears to lag behind in the era of LLMs, due mostly to its costly\n",
            "fine-tuning (or re-training) necessity under the massive volumes of model\n",
            "parameter and training data. To close this industry-academia gap, we introduce\n",
            "Dynamic Sparse No Training (DSnoT), a training-free fine-tuning approach that\n",
            "slightly updates sparse LLMs without the expensive backpropagation and any\n",
            "weight updates. Inspired by the Dynamic Sparse Training, DSnoT minimizes the\n",
            "reconstruction error between the dense and sparse LLMs, in the fashion of\n",
            "performing iterative weight pruning-and-growing on top of sparse LLMs. To\n",
            "accomplish this purpose, DSnoT particularly takes into account the anticipated\n",
            "reduction in reconstruction error for pruning and growing, as well as the\n",
            "variance w.r.t. different input data for growing each weight. This practice can\n",
            "be executed efficiently in linear time since its obviates the need of\n",
            "backpropagation for fine-tuning LLMs. Extensive experiments on LLaMA-V1/V2,\n",
            "Vicuna, and OPT across various benchmarks demonstrate the effectiveness of\n",
            "DSnoT in enhancing the performance of sparse LLMs, especially at high sparsity\n",
            "levels. For instance, DSnoT is able to outperform the state-of-the-art Wanda by\n",
            "26.79 perplexity at 70% sparsity with LLaMA-7B. Our paper offers fresh insights\n",
            "into how to fine-tune sparse LLMs in an efficient training-free manner and open\n",
            "new venues to scale the great potential of sparsity to LLMs. Codes are\n",
            "available at https://github.com/zyxxmu/DSnoT.\n",
            "\n",
            "41. Title: MUFFIN: Curating Multi-Faceted Instructions for Improving Instruction Following\n",
            "   Abstract: Conversational recommender systems (CRS) aim to proactively elicit user\n",
            "preference and recommend high-quality items through natural language\n",
            "conversations. Typically, a CRS consists of a recommendation module to predict\n",
            "preferred items for users and a conversation module to generate appropriate\n",
            "responses. To develop an effective CRS, it is essential to seamlessly integrate\n",
            "the two modules. Existing works either design semantic alignment strategies, or\n",
            "share knowledge resources and representations between the two modules. However,\n",
            "these approaches still rely on different architectures or techniques to develop\n",
            "the two modules, making it difficult for effective module integration.\n",
            "  To address this problem, we propose a unified CRS model named UniCRS based on\n",
            "knowledge-enhanced prompt learning. Our approach unifies the recommendation and\n",
            "conversation subtasks into the prompt learning paradigm, and utilizes\n",
            "knowledge-enhanced prompts based on a fixed pre-trained language model (PLM) to\n",
            "fulfill both subtasks in a unified approach. In the prompt design, we include\n",
            "fused knowledge representations, task-specific soft tokens, and the dialogue\n",
            "context, which can provide sufficient contextual information to adapt the PLM\n",
            "for the CRS task. Besides, for the recommendation subtask, we also incorporate\n",
            "the generated response template as an important part of the prompt, to enhance\n",
            "the information interaction between the two subtasks. Extensive experiments on\n",
            "two public CRS datasets have demonstrated the effectiveness of our approach.\n",
            "\n",
            "42. Title: RA-DIT: Retrieval-Augmented Dual Instruction Tuning\n",
            "   Abstract: Machine learning models are often trained to predict the outcome resulting\n",
            "from a human decision. For example, if a doctor decides to test a patient for\n",
            "disease, will the patient test positive? A challenge is that historical\n",
            "decision-making determines whether the outcome is observed: we only observe\n",
            "test outcomes for patients doctors historically tested. Untested patients, for\n",
            "whom outcomes are unobserved, may differ from tested patients along observed\n",
            "and unobserved dimensions. We propose a Bayesian model class which captures\n",
            "this setting. The purpose of the model is to accurately estimate risk for both\n",
            "tested and untested patients. Estimating this model is challenging due to the\n",
            "wide range of possibilities for untested patients. To address this, we propose\n",
            "two domain constraints which are plausible in health settings: a prevalence\n",
            "constraint, where the overall disease prevalence is known, and an expertise\n",
            "constraint, where the human decision-maker deviates from purely risk-based\n",
            "decision-making only along a constrained feature set. We show theoretically and\n",
            "on synthetic data that domain constraints improve parameter inference. We apply\n",
            "our model to a case study of cancer risk prediction, showing that the model's\n",
            "inferred risk predicts cancer diagnoses, its inferred testing policy captures\n",
            "known public health policies, and it can identify suboptimalities in test\n",
            "allocation. Though our case study is in healthcare, our analysis reveals a\n",
            "general class of domain constraints which can improve model estimation in many\n",
            "settings.\n",
            "\n",
            "43. Title: Neural Spectral Methods: Self-supervised learning in the spectral domain\n",
            "   Abstract: Fact verification aims to verify a claim using evidence from a trustworthy\n",
            "knowledge base. To address this challenge, algorithms must produce features for\n",
            "every claim that are both semantically meaningful, and compact enough to find a\n",
            "semantic alignment with the source information. In contrast to previous work,\n",
            "which tackled the alignment problem by learning over annotated corpora of\n",
            "claims and their corresponding labels, we propose SFAVEL (Self-supervised Fact\n",
            "Verification via Language Model Distillation), a novel unsupervised pretraining\n",
            "framework that leverages pre-trained language models to distil self-supervised\n",
            "features into high-quality claim-fact alignments without the need for\n",
            "annotations. This is enabled by a novel contrastive loss function that\n",
            "encourages features to attain high-quality claim and evidence alignments whilst\n",
            "preserving the semantic relationships across the corpora. Notably, we present\n",
            "results that achieve a new state-of-the-art on FB15k-237 (+5.3% Hits@1) and\n",
            "FEVER (+8% accuracy) with linear evaluation.\n",
            "\n",
            "44. Title: What does the Knowledge Neuron Thesis Have to do with Knowledge?\n",
            "   Abstract: This paper studies the sample-efficiency of learning in Partially Observable\n",
            "Markov Decision Processes (POMDPs), a challenging problem in reinforcement\n",
            "learning that is known to be exponentially hard in the worst-case. Motivated by\n",
            "real-world settings such as loading in game playing, we propose an enhanced\n",
            "feedback model called ``multiple observations in hindsight'', where after each\n",
            "episode of interaction with the POMDP, the learner may collect multiple\n",
            "additional observations emitted from the encountered latent states, but may not\n",
            "observe the latent states themselves. We show that sample-efficient learning\n",
            "under this feedback model is possible for two new subclasses of POMDPs:\n",
            "\\emph{multi-observation revealing POMDPs} and \\emph{distinguishable POMDPs}.\n",
            "Both subclasses generalize and substantially relax \\emph{revealing POMDPs} -- a\n",
            "widely studied subclass for which sample-efficient learning is possible under\n",
            "standard trajectory feedback. Notably, distinguishable POMDPs only require the\n",
            "emission distributions from different latent states to be \\emph{different}\n",
            "instead of \\emph{linearly independent} as required in revealing POMDPs.\n",
            "\n",
            "45. Title: Lipsum-FT: Robust Fine-Tuning of Zero-Shot Models Using Random Text Guidance\n",
            "   Abstract: Novelty Detection (ND) plays a crucial role in machine learning by\n",
            "identifying new or unseen data during model inference. This capability is\n",
            "especially important for the safe and reliable operation of automated systems.\n",
            "Despite advances in this field, existing techniques often fail to maintain\n",
            "their performance when subject to adversarial attacks. Our research addresses\n",
            "this gap by marrying the merits of nearest-neighbor algorithms with robust\n",
            "features obtained from models pretrained on ImageNet. We focus on enhancing the\n",
            "robustness and performance of ND algorithms. Experimental results demonstrate\n",
            "that our approach significantly outperforms current state-of-the-art methods\n",
            "across various benchmarks, particularly under adversarial conditions. By\n",
            "incorporating robust pretrained features into the k-NN algorithm, we establish\n",
            "a new standard for performance and robustness in the field of robust ND. This\n",
            "work opens up new avenues for research aimed at fortifying machine learning\n",
            "systems against adversarial vulnerabilities. Our implementation is publicly\n",
            "available at https://github.com/rohban-lab/ZARND.\n",
            "\n",
            "46. Title: Pooling Image Datasets with Multiple Covariate Shift and Imbalance\n",
            "   Abstract: Pathfinding makes up an important sub-component of a broad range of complex\n",
            "tasks in AI, such as robot path planning, transport routing, and game playing.\n",
            "While classical algorithms can efficiently compute shortest paths, neural\n",
            "networks could be better suited to adapting these sub-routines to more complex\n",
            "and intractable tasks. As a step toward developing such networks, we hand-code\n",
            "and learn models for Breadth-First Search (BFS), i.e. shortest path finding,\n",
            "using the unified architectural framework of Neural Cellular Automata, which\n",
            "are iterative neural networks with equal-size inputs and outputs. Similarly, we\n",
            "present a neural implementation of Depth-First Search (DFS), and outline how it\n",
            "can be combined with neural BFS to produce an NCA for computing diameter of a\n",
            "graph. We experiment with architectural modifications inspired by these\n",
            "hand-coded NCAs, training networks from scratch to solve the diameter problem\n",
            "on grid mazes while exhibiting strong generalization ability. Finally, we\n",
            "introduce a scheme in which data points are mutated adversarially during\n",
            "training. We find that adversarially evolving mazes leads to increased\n",
            "generalization on out-of-distribution examples, while at the same time\n",
            "generating data-sets with significantly more complex solutions for reasoning\n",
            "tasks.\n",
            "\n",
            "47. Title: PerceptionCLIP: Visual Classification by Inferring and Conditioning on Contexts\n",
            "   Abstract: Small sample sizes are common in many disciplines, which necessitates pooling\n",
            "roughly similar datasets across multiple institutions to study weak but\n",
            "relevant associations between images and disease outcomes. Such data often\n",
            "manifest shift/imbalance in covariates (i.e., secondary non-imaging data).\n",
            "Controlling for such nuisance variables is common within standard statistical\n",
            "analysis, but the ideas do not directly apply to overparameterized models.\n",
            "Consequently, recent work has shown how strategies from invariant\n",
            "representation learning provides a meaningful starting point, but the current\n",
            "repertoire of methods is limited to accounting for shifts/imbalances in just a\n",
            "couple of covariates at a time. In this paper, we show how viewing this problem\n",
            "from the perspective of Category theory provides a simple and effective\n",
            "solution that completely avoids elaborate multi-stage training pipelines that\n",
            "would otherwise be needed. We show the effectiveness of this approach via\n",
            "extensive experiments on real datasets. Further, we discuss how this style of\n",
            "formulation offers a unified perspective on at least 5+ distinct problem\n",
            "settings, from self-supervised learning to matching problems in 3D\n",
            "reconstruction.\n",
            "\n",
            "48. Title: GOAt: Explaining Graph Neural Networks via Graph Output Attribution\n",
            "   Abstract: We reassess the Knowledge Neuron (KN) Thesis: an interpretation of the\n",
            "mechanism underlying the ability of large language models to recall facts from\n",
            "a training corpus. This nascent thesis proposes that facts are recalled from\n",
            "the training corpus through the MLP weights in a manner resembling key-value\n",
            "memory, implying in effect that \"knowledge\" is stored in the network.\n",
            "Furthermore, by modifying the MLP modules, one can control the language model's\n",
            "generation of factual information. The plausibility of the KN thesis has been\n",
            "demonstrated by the success of KN-inspired model editing methods (Dai et al.,\n",
            "2022; Meng et al., 2022).\n",
            "  We find that this thesis is, at best, an oversimplification. Not only have we\n",
            "found that we can edit the expression of certain linguistic phenomena using the\n",
            "same model editing methods but, through a more comprehensive evaluation, we\n",
            "have found that the KN thesis does not adequately explain the process of\n",
            "factual expression. While it is possible to argue that the MLP weights store\n",
            "complex patterns that are interpretable both syntactically and semantically,\n",
            "these patterns do not constitute \"knowledge.\" To gain a more comprehensive\n",
            "understanding of the knowledge representation process, we must look beyond the\n",
            "MLP weights and explore recent models' complex layer structures and attention\n",
            "mechanisms.\n",
            "\n",
            "49. Title: Time Travel in LLMs: Tracing Data Contamination in Large Language Models\n",
            "   Abstract: In optimal transport (OT), a Monge map is known as a mapping that transports\n",
            "a source distribution to a target distribution in the most cost-efficient way.\n",
            "Recently, multiple neural estimators for Monge maps have been developed and\n",
            "applied in diverse unpaired domain translation tasks, e.g. in single-cell\n",
            "biology and computer vision. However, the classic OT framework enforces mass\n",
            "conservation, which makes it prone to outliers and limits its applicability in\n",
            "real-world scenarios. The latter can be particularly harmful in OT domain\n",
            "translation tasks, where the relative position of a sample within a\n",
            "distribution is explicitly taken into account. While unbalanced OT tackles this\n",
            "challenge in the discrete setting, its integration into neural Monge map\n",
            "estimators has received limited attention. We propose a theoretically grounded\n",
            "method to incorporate unbalancedness into any Monge map estimator. We improve\n",
            "existing estimators to model cell trajectories over time and to predict\n",
            "cellular responses to perturbations. Moreover, our approach seamlessly\n",
            "integrates with the OT flow matching (OT-FM) framework. While we show that\n",
            "OT-FM performs competitively in image translation, we further improve\n",
            "performance by incorporating unbalancedness (UOT-FM), which better preserves\n",
            "relevant features. We hence establish UOT-FM as a principled method for\n",
            "unpaired image translation.\n",
            "\n",
            "50. Title: Unbalancedness in Neural Monge Maps Improves Unpaired Domain Translation\n",
            "   Abstract: Data contamination, i.e., the presence of test data from downstream tasks in\n",
            "the training data of large language models (LLMs), is a potential major issue\n",
            "in measuring LLMs' real effectiveness on other tasks. We propose a\n",
            "straightforward yet effective method for identifying data contamination within\n",
            "LLMs. At its core, our approach starts by identifying potential contamination\n",
            "at the instance level; using this information, our approach then assesses wider\n",
            "contamination at the partition level. To estimate contamination of individual\n",
            "instances, we employ \"guided instruction:\" a prompt consisting of the dataset\n",
            "name, partition type, and the random-length initial segment of a reference\n",
            "instance, asking the LLM to complete it. An instance is flagged as contaminated\n",
            "if the LLM's output either exactly or nearly matches the latter segment of the\n",
            "reference. To understand if an entire partition is contaminated, we propose two\n",
            "ideas. The first idea marks a dataset partition as contaminated if the average\n",
            "overlap score with the reference instances (as measured by ROUGE-L or BLEURT)\n",
            "is statistically significantly better with the completions from guided\n",
            "instruction compared to a \"general instruction\" that does not include the\n",
            "dataset and partition name. The second idea marks a dataset partition as\n",
            "contaminated if a classifier based on GPT-4 with few-shot in-context learning\n",
            "prompt marks multiple generated completions as exact/near-exact matches of the\n",
            "corresponding reference instances. Our best method achieves an accuracy between\n",
            "92% and 100% in detecting if an LLM is contaminated with seven datasets,\n",
            "containing train and test/validation partitions, when contrasted with manual\n",
            "evaluation by human experts. Further, our findings indicate that GPT-4 is\n",
            "contaminated with AG News, WNLI, and XSum datasets.\n",
            "\n",
            "51. Title: On the Over-Memorization During Natural, Robust and Catastrophic Overfitting\n",
            "   Abstract: Noise modeling and reduction are fundamental tasks in low-level computer\n",
            "vision. They are particularly important for smartphone cameras relying on small\n",
            "sensors that exhibit visually noticeable noise. There has recently been renewed\n",
            "interest in using data-driven approaches to improve camera noise models via\n",
            "neural networks. These data-driven approaches target noise present in the\n",
            "raw-sensor image before it has been processed by the camera's image signal\n",
            "processor (ISP). Modeling noise in the RAW-rgb domain is useful for improving\n",
            "and testing the in-camera denoising algorithm; however, there are situations\n",
            "where the camera's ISP does not apply denoising or additional denoising is\n",
            "desired when the RAW-rgb domain image is no longer available. In such cases,\n",
            "the sensor noise propagates through the ISP to the final rendered image encoded\n",
            "in standard RGB (sRGB). The nonlinear steps on the ISP culminate in a\n",
            "significantly more complex noise distribution in the sRGB domain and existing\n",
            "raw-domain noise models are unable to capture the sRGB noise distribution. We\n",
            "propose a new sRGB-domain noise model based on normalizing flows that is\n",
            "capable of learning the complex noise distribution found in sRGB images under\n",
            "various ISO levels. Our normalizing flows-based approach outperforms other\n",
            "models by a large margin in noise modeling and synthesis tasks. We also show\n",
            "that image denoisers trained on noisy images synthesized with our noise model\n",
            "outperforms those trained with noise from baselines models.\n",
            "\n",
            "52. Title: sRGB Real Noise Modeling via Noise-Aware Sampling with Normalizing Flows\n",
            "   Abstract: Learning in multi-agent environments is difficult due to the non-stationarity\n",
            "introduced by an opponent's or partner's changing behaviors. Instead of\n",
            "reactively adapting to the other agent's (opponent or partner) behavior, we\n",
            "propose an algorithm to proactively influence the other agent's strategy to\n",
            "stabilize -- which can restrain the non-stationarity caused by the other agent.\n",
            "We learn a low-dimensional latent representation of the other agent's strategy\n",
            "and the dynamics of how the latent strategy evolves with respect to our robot's\n",
            "behavior. With this learned dynamics model, we can define an unsupervised\n",
            "stability reward to train our robot to deliberately influence the other agent\n",
            "to stabilize towards a single strategy. We demonstrate the effectiveness of\n",
            "stabilizing in improving efficiency of maximizing the task reward in a variety\n",
            "of simulated environments, including autonomous driving, emergent\n",
            "communication, and robotic manipulation. We show qualitative results on our\n",
            "website: https://sites.google.com/view/stable-marl/.\n",
            "\n",
            "53. Title: Efficient and Scalable Graph Generation through Iterative Local Expansion\n",
            "   Abstract: Abstract reasoning and logic inference are difficult problems for neural\n",
            "networks, yet essential to their applicability in highly structured domains. In\n",
            "this work we demonstrate that a well known technique such as spectral\n",
            "regularization can significantly boost the capabilities of a neural learner. We\n",
            "introduce the Neural Abstract Reasoner (NAR), a memory augmented architecture\n",
            "capable of learning and using abstract rules. We show that, when trained with\n",
            "spectral regularization, NAR achieves $78.8\\%$ accuracy on the Abstraction and\n",
            "Reasoning Corpus, improving performance 4 times over the best known human\n",
            "hand-crafted symbolic solvers. We provide some intuition for the effects of\n",
            "spectral regularization in the domain of abstract reasoning based on\n",
            "theoretical generalization bounds and Solomonoff's theory of inductive\n",
            "inference.\n",
            "\n",
            "54. Title: BroGNet: Momentum-Conserving Graph Neural Stochastic Differential Equation for Learning Brownian Dynamics\n",
            "   Abstract: Transformers have recently emerged as a powerful tool for learning visual\n",
            "representations. In this paper, we identify and characterize artifacts in\n",
            "feature maps of both supervised and self-supervised ViT networks. The artifacts\n",
            "correspond to high-norm tokens appearing during inference primarily in\n",
            "low-informative background areas of images, that are repurposed for internal\n",
            "computations. We propose a simple yet effective solution based on providing\n",
            "additional tokens to the input sequence of the Vision Transformer to fill that\n",
            "role. We show that this solution fixes that problem entirely for both\n",
            "supervised and self-supervised models, sets a new state of the art for\n",
            "self-supervised visual models on dense visual prediction tasks, enables object\n",
            "discovery methods with larger models, and most importantly leads to smoother\n",
            "feature maps and attention maps for downstream visual processing.\n",
            "\n",
            "55. Title: Deep SE(3)-Equivariant Geometric Reasoning for Precise Placement Tasks\n",
            "   Abstract: Overfitting negatively impacts the generalization ability of deep neural\n",
            "networks (DNNs) in both natural and adversarial training. Existing methods\n",
            "struggle to consistently address different types of overfitting, typically\n",
            "designing strategies that focus separately on either natural or adversarial\n",
            "patterns. In this work, we adopt a unified perspective by solely focusing on\n",
            "natural patterns to explore different types of overfitting. Specifically, we\n",
            "examine the memorization effect in DNNs and reveal a shared behaviour termed\n",
            "over-memorization, which impairs their generalization capacity. This behaviour\n",
            "manifests as DNNs suddenly becoming high-confidence in predicting certain\n",
            "training patterns and retaining a persistent memory for them. Furthermore, when\n",
            "DNNs over-memorize an adversarial pattern, they tend to simultaneously exhibit\n",
            "high-confidence prediction for the corresponding natural pattern. These\n",
            "findings motivate us to holistically mitigate different types of overfitting by\n",
            "hindering the DNNs from over-memorization training patterns. To this end, we\n",
            "propose a general framework, Distraction Over-Memorization (DOM), which\n",
            "explicitly prevents over-memorization by either removing or augmenting the\n",
            "high-confidence natural patterns. Extensive experiments demonstrate the\n",
            "effectiveness of our proposed method in mitigating overfitting across various\n",
            "training paradigms.\n",
            "\n",
            "56. Title: Instant3D: Fast Text-to-3D with Sparse-view Generation and Large Reconstruction Model\n",
            "   Abstract: In the realm of generative models for graphs, extensive research has been\n",
            "conducted. However, most existing methods struggle with large graphs due to the\n",
            "complexity of representing the entire joint distribution across all node pairs\n",
            "and capturing both global and local graph structures simultaneously. To\n",
            "overcome these issues, we introduce a method that generates a graph by\n",
            "progressively expanding a single node to a target graph. In each step, nodes\n",
            "and edges are added in a localized manner through denoising diffusion, building\n",
            "first the global structure, and then refining the local details. The local\n",
            "generation avoids modeling the entire joint distribution over all node pairs,\n",
            "achieving substantial computational savings with subquadratic runtime relative\n",
            "to node count while maintaining high expressivity through multiscale\n",
            "generation. Our experiments show that our model achieves state-of-the-art\n",
            "performance on well-established benchmark datasets while successfully scaling\n",
            "to graphs with at least 5000 nodes. Our method is also the first to\n",
            "successfully extrapolate to graphs outside of the training distribution,\n",
            "showcasing a much better generalization capability over existing methods.\n",
            "\n",
            "57. Title: Light-MILPopt: Solving Large-scale Mixed Integer Linear Programs with Lightweight Optimizer and Small-scale Training Dataset\n",
            "   Abstract: We consider Convolutional Neural Networks (CNNs) with 2D structured features\n",
            "that are symmetric in the spatial dimensions. Such networks arise in modeling\n",
            "pairwise relationships for a sequential recommendation problem, as well as\n",
            "secondary structure inference problems of RNA and protein sequences. We develop\n",
            "a CNN architecture that generates and preserves the symmetry structure in the\n",
            "network's convolutional layers. We present parameterizations for the\n",
            "convolutional kernels that produce update rules to maintain symmetry throughout\n",
            "the training. We apply this architecture to the sequential recommendation\n",
            "problem, the RNA secondary structure inference problem, and the protein contact\n",
            "map prediction problem, showing that the symmetric structured networks produce\n",
            "improved results using fewer numbers of machine parameters.\n",
            "\n",
            "58. Title: SparseFormer: Sparse Visual Recognition via Limited Latent Tokens\n",
            "   Abstract: The increasing capabilities of large language models (LLMs) raise\n",
            "opportunities for artificial general intelligence but concurrently amplify\n",
            "safety concerns, such as potential misuse of AI systems, necessitating\n",
            "effective AI alignment. Reinforcement Learning from Human Feedback (RLHF) has\n",
            "emerged as a promising pathway towards AI alignment but brings forth challenges\n",
            "due to its complexity and dependence on a separate reward model. Direct\n",
            "Preference Optimization (DPO) has been proposed as an alternative, and it\n",
            "remains equivalent to RLHF under the reverse KL regularization constraint. This\n",
            "paper presents $f$-DPO, a generalized approach to DPO by incorporating diverse\n",
            "divergence constraints. We show that under certain $f$-divergences, including\n",
            "Jensen-Shannon divergence, forward KL divergences and $\\alpha$-divergences, the\n",
            "complex relationship between the reward and optimal policy can also be\n",
            "simplified by addressing the Karush-Kuhn-Tucker conditions. This eliminates the\n",
            "need for estimating the normalizing constant in the Bradley-Terry model and\n",
            "enables a tractable mapping between the reward function and the optimal policy.\n",
            "Our approach optimizes LLMs to align with human preferences in a more efficient\n",
            "and supervised manner under a broad set of divergence constraints. Empirically,\n",
            "adopting these divergences ensures a balance between alignment performance and\n",
            "generation diversity. Importantly, $f$-DPO outperforms PPO-based methods in\n",
            "divergence efficiency, and divergence constraints directly influence expected\n",
            "calibration error (ECE).\n",
            "\n",
            "59. Title: Enabling Lanuguage Models to Implicitly Learn Self-Improvement\n",
            "   Abstract: In this work, we present a novel approach to ontology reasoning that is based\n",
            "on deep learning rather than logic-based formal reasoning. To this end, we\n",
            "introduce a new model for statistical relational learning that is built upon\n",
            "deep recursive neural networks, and give experimental evidence that it can\n",
            "easily compete with, or even outperform, existing logic-based reasoners on the\n",
            "task of ontology reasoning. More precisely, we compared our implemented system\n",
            "with one of the best logic-based ontology reasoners at present, RDFox, on a\n",
            "number of large standard benchmark datasets, and found that our system attained\n",
            "high reasoning quality, while being up to two orders of magnitude faster.\n",
            "\n",
            "60. Title: Separating common from salient patterns with Contrastive Representation Learning\n",
            "   Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in\n",
            "open-ended text generation tasks. However, the inherent open-ended nature of\n",
            "these tasks implies that there is always room for improvement in the quality of\n",
            "model responses. To address this challenge, various approaches have been\n",
            "proposed to enhance the performance of LLMs. There has been a growing focus on\n",
            "enabling LLMs to self-improve their response quality, thereby reducing the\n",
            "reliance on extensive human annotation efforts for collecting diverse and\n",
            "high-quality training data. Recently, prompting-based methods have been widely\n",
            "explored among self-improvement methods owing to their effectiveness,\n",
            "efficiency, and convenience. However, those methods usually require explicitly\n",
            "and thoroughly written rubrics as inputs to LLMs. It is expensive and\n",
            "challenging to manually derive and provide all necessary rubrics with a\n",
            "real-world complex goal for improvement (e.g., being more helpful and less\n",
            "harmful). To this end, we propose an ImPlicit Self-ImprovemenT (PIT) framework\n",
            "that implicitly learns the improvement goal from human preference data. PIT\n",
            "only requires preference data that are used to train reward models without\n",
            "extra human efforts. Specifically, we reformulate the training objective of\n",
            "reinforcement learning from human feedback (RLHF) -- instead of maximizing\n",
            "response quality for a given input, we maximize the quality gap of the response\n",
            "conditioned on a reference response. In this way, PIT is implicitly trained\n",
            "with the improvement goal of better aligning with human preferences.\n",
            "Experiments on two real-world datasets and one synthetic dataset show that our\n",
            "method significantly outperforms prompting-based methods.\n",
            "\n",
            "61. Title: Matrix Manifold Neural Networks++\n",
            "   Abstract: We present a novel quasi-Monte Carlo mechanism to improve graph-based\n",
            "sampling, coined repelling random walks. By inducing correlations between the\n",
            "trajectories of an interacting ensemble such that their marginal transition\n",
            "probabilities are unmodified, we are able to explore the graph more\n",
            "efficiently, improving the concentration of statistical estimators whilst\n",
            "leaving them unbiased. The mechanism has a trivial drop-in implementation. We\n",
            "showcase the effectiveness of repelling random walks in a range of settings\n",
            "including estimation of graph kernels, the PageRank vector and graphlet\n",
            "concentrations. We provide detailed experimental evaluation and robust\n",
            "theoretical guarantees. To our knowledge, repelling random walks constitute the\n",
            "first rigorously studied quasi-Monte Carlo scheme correlating the directions of\n",
            "walkers on a graph, inviting new research in this exciting nascent domain.\n",
            "\n",
            "62. Title: Repelling Random Walks\n",
            "   Abstract: Deep neural networks (DNNs) on Riemannian manifolds have garnered increasing\n",
            "interest in various applied areas. For instance, DNNs on spherical and\n",
            "hyperbolic manifolds have been designed to solve a wide range of computer\n",
            "vision and nature language processing tasks. One of the key factors that\n",
            "contribute to the success of these networks is that spherical and hyperbolic\n",
            "manifolds have the rich algebraic structures of gyrogroups and gyrovector\n",
            "spaces. This enables principled and effective generalizations of the most\n",
            "successful DNNs to these manifolds. Recently, some works have shown that many\n",
            "concepts in the theory of gyrogroups and gyrovector spaces can also be\n",
            "generalized to matrix manifolds such as Symmetric Positive Definite (SPD) and\n",
            "Grassmann manifolds. As a result, some building blocks for SPD and Grassmann\n",
            "neural networks, e.g., isometric models and multinomial logistic regression\n",
            "(MLR) can be derived in a way that is fully analogous to their spherical and\n",
            "hyperbolic counterparts. Building upon these works, we design fully-connected\n",
            "(FC) and convolutional layers for SPD neural networks. We also develop MLR on\n",
            "Symmetric Positive Semi-definite (SPSD) manifolds, and propose a method for\n",
            "performing backpropagation with the Grassmann logarithmic map in the projector\n",
            "perspective. We demonstrate the effectiveness of the proposed approach in the\n",
            "human action recognition and node classification tasks.\n",
            "\n",
            "63. Title: DDMI: Domain-agnostic Latent Diffusion Models for Synthesizing High-Quality Implicit Neural Representations\n",
            "   Abstract: Contrastive Analysis is a sub-field of Representation Learning that aims at\n",
            "separating common factors of variation between two datasets, a background\n",
            "(i.e., healthy subjects) and a target (i.e., diseased subjects), from the\n",
            "salient factors of variation, only present in the target dataset. Despite their\n",
            "relevance, current models based on Variational Auto-Encoders have shown poor\n",
            "performance in learning semantically-expressive representations. On the other\n",
            "hand, Contrastive Representation Learning has shown tremendous performance\n",
            "leaps in various applications (classification, clustering, etc.). In this work,\n",
            "we propose to leverage the ability of Contrastive Learning to learn\n",
            "semantically expressive representations well adapted for Contrastive Analysis.\n",
            "We reformulate it under the lens of the InfoMax Principle and identify two\n",
            "Mutual Information terms to maximize and one to minimize. We decompose the\n",
            "first two terms into an Alignment and a Uniformity term, as commonly done in\n",
            "Contrastive Learning. Then, we motivate a novel Mutual Information minimization\n",
            "strategy to prevent information leakage between common and salient\n",
            "distributions. We validate our method, called SepCLR, on three visual datasets\n",
            "and three medical datasets, specifically conceived to assess the pattern\n",
            "separation capability in Contrastive Analysis. Code available at\n",
            "https://github.com/neurospin-projects/2024_rlouiset_sep_clr.\n",
            "\n",
            "64. Title: Conformal Risk Control\n",
            "   Abstract: We extend conformal prediction to control the expected value of any monotone\n",
            "loss function. The algorithm generalizes split conformal prediction together\n",
            "with its coverage guarantee. Like conformal prediction, the conformal risk\n",
            "control procedure is tight up to an $\\mathcal{O}(1/n)$ factor. We also\n",
            "introduce extensions of the idea to distribution shift, quantile risk control,\n",
            "multiple and adversarial risk control, and expectations of U-statistics. Worked\n",
            "examples from computer vision and natural language processing demonstrate the\n",
            "usage of our algorithm to bound the false negative rate, graph distance, and\n",
            "token-level F1-score.\n",
            "\n",
            "65. Title: Polynomial Width is Sufficient for Set Representation with High-dimensional Features\n",
            "   Abstract: In this paper, we study polynomial norms, i.e. norms that are the\n",
            "$d^{\\text{th}}$ root of a degree-$d$ homogeneous polynomial $f$. We first show\n",
            "that a necessary and sufficient condition for $f^{1/d}$ to be a norm is for $f$\n",
            "to be strictly convex, or equivalently, convex and positive definite. Though\n",
            "not all norms come from $d^{\\text{th}}$ roots of polynomials, we prove that any\n",
            "norm can be approximated arbitrarily well by a polynomial norm. We then\n",
            "investigate the computational problem of testing whether a form gives a\n",
            "polynomial norm. We show that this problem is strongly NP-hard already when the\n",
            "degree of the form is 4, but can always be answered by testing feasibility of a\n",
            "semidefinite program (of possibly large size). We further study the problem of\n",
            "optimizing over the set of polynomial norms using semidefinite programming. To\n",
            "do this, we introduce the notion of r-sos-convexity and extend a result of\n",
            "Reznick on sum of squares representation of positive definite forms to positive\n",
            "definite biforms. We conclude with some applications of polynomial norms to\n",
            "statistics and dynamical systems.\n",
            "\n",
            "66. Title: Beating Price of Anarchy and Gradient Descent without Regret in Potential Games\n",
            "   Abstract: Large Language Models (LLMs) have exhibited remarkable performance across\n",
            "various natural language processing (NLP) tasks. However, fine-tuning these\n",
            "models often necessitates substantial supervision, which can be expensive and\n",
            "time-consuming to obtain. This paper introduces a novel unsupervised method\n",
            "called LanguageModel Self-Improvement by Reinforcement Learning Contemplation\n",
            "(SIRLC) that improves LLMs without reliance on external labels. Our approach is\n",
            "grounded in the observation that it is simpler for language models to assess\n",
            "text quality than to generate text. Building on this insight, SIRLC assigns\n",
            "LLMs dual roles as both student and teacher. As a student, the LLM generates\n",
            "answers to unlabeled questions, while as a teacher, it evaluates the generated\n",
            "text and assigns scores accordingly. The model parameters are updated using\n",
            "reinforcement learning to maximize the evaluation score. We demonstrate that\n",
            "SIRLC can be applied to various NLP tasks, such as reasoning problems, text\n",
            "generation, and machine translation. Our experiments show that SIRLC\n",
            "effectively improves LLM performance without external supervision, resulting in\n",
            "a 5.6% increase in answering accuracy for reasoning tasks and a rise in\n",
            "BERTScore from 0.82 to 0.86 for translation tasks. Furthermore, SIRLC can be\n",
            "applied to models of different sizes, showcasing its broad applicability.\n",
            "\n",
            "67. Title: Language Model Self-improvement by Reinforcement Learning Contemplation\n",
            "   Abstract: With significant advancements in diffusion models, addressing the potential\n",
            "risks of dataset bias becomes increasingly important. Since generated outputs\n",
            "directly suffer from dataset bias, mitigating latent bias becomes a key factor\n",
            "in improving sample quality and proportion. This paper proposes time-dependent\n",
            "importance reweighting to mitigate the bias for the diffusion models. We\n",
            "demonstrate that the time-dependent density ratio becomes more precise than\n",
            "previous approaches, thereby minimizing error propagation in generative\n",
            "learning. While directly applying it to score-matching is intractable, we\n",
            "discover that using the time-dependent density ratio both for reweighting and\n",
            "score correction can lead to a tractable form of the objective function to\n",
            "regenerate the unbiased data density. Furthermore, we theoretically establish a\n",
            "connection with traditional score-matching, and we demonstrate its convergence\n",
            "to an unbiased distribution. The experimental evidence supports the usefulness\n",
            "of the proposed method, which outperforms baselines including time-independent\n",
            "importance reweighting on CIFAR-10, CIFAR-100, FFHQ, and CelebA with various\n",
            "bias settings. Our code is available at https://github.com/alsdudrla10/TIW-DSM.\n",
            "\n",
            "68. Title: Training Unbiased Diffusion Models From Biased Dataset\n",
            "   Abstract: We analyze the convergence rate of the random reshuffling (RR) method, which\n",
            "is a randomized first-order incremental algorithm for minimizing a finite sum\n",
            "of convex component functions. RR proceeds in cycles, picking a uniformly\n",
            "random order (permutation) and processing the component functions one at a time\n",
            "according to this order, i.e., at each cycle, each component function is\n",
            "sampled without replacement from the collection. Though RR has been numerically\n",
            "observed to outperform its with-replacement counterpart stochastic gradient\n",
            "descent (SGD), characterization of its convergence rate has been a long\n",
            "standing open question. In this paper, we answer this question by showing that\n",
            "when the component functions are quadratics or smooth and the sum function is\n",
            "strongly convex, RR with iterate averaging and a diminishing stepsize\n",
            "$\\alpha_k=\\Theta(1/k^s)$ for $s\\in (1/2,1)$ converges at rate\n",
            "$\\Theta(1/k^{2s})$ with probability one in the suboptimality of the objective\n",
            "value, thus improving upon the $\\Omega(1/k)$ rate of SGD. Our analysis draws on\n",
            "the theory of Polyak-Ruppert averaging and relies on decoupling the dependent\n",
            "cycle gradient error into an independent term over cycles and another term\n",
            "dominated by $\\alpha_k^2$. This allows us to apply law of large numbers to an\n",
            "appropriately weighted version of the cycle gradient errors, where the weights\n",
            "depend on the stepsize. We also provide high probability convergence rate\n",
            "estimates that shows decay rate of different terms and allows us to propose a\n",
            "modification of RR with convergence rate ${\\cal O}(\\frac{1}{k^2})$.\n",
            "\n",
            "69. Title: Self-Supervised Heterogeneous Graph Learning: a Homophily and Heterogeneity View\n",
            "   Abstract: In this paper, we present the implicit equations for one special class of\n",
            "real-valued spherical harmonics with octahedral symmetry. Based on this\n",
            "representation, we construct the rotationally invariant measure of deviation\n",
            "from the specified symmetry. The spherical harmonics we consider have some\n",
            "applications in the area of directional fields design due to their ability to\n",
            "represent mutually orthogonal axes in 3D space, not relative to their order and\n",
            "orientation.\n",
            "\n",
            "70. Title: The Reasonableness Behind Unreasonable Translation Capability of Large Language Model\n",
            "   Abstract: Instructing the model to generate a sequence of intermediate steps, a.k.a., a\n",
            "chain of thought (CoT), is a highly effective method to improve the accuracy of\n",
            "large language models (LLMs) on arithmetics and symbolic reasoning tasks.\n",
            "However, the mechanism behind CoT remains unclear. This work provides a\n",
            "theoretical understanding of the power of CoT for decoder-only transformers\n",
            "through the lens of expressiveness. Conceptually, CoT empowers the model with\n",
            "the ability to perform inherently serial computation, which is otherwise\n",
            "lacking in transformers, especially when depth is low. Given input length $n$,\n",
            "previous works have shown that constant-depth transformers with finite\n",
            "precision $\\mathsf{poly}(n)$ embedding size can only solve problems in\n",
            "$\\mathsf{TC}^0$ without CoT. We first show an even tighter expressiveness upper\n",
            "bound for constant-depth transformers with constant-bit precision, which can\n",
            "only solve problems in $\\mathsf{AC}^0$, a proper subset of $ \\mathsf{TC}^0$.\n",
            "However, with $T$ steps of CoT, constant-depth transformers using constant-bit\n",
            "precision and $O(\\log n)$ embedding size can solve any problem solvable by\n",
            "boolean circuits of size $T$. Empirically, enabling CoT dramatically improves\n",
            "the accuracy for tasks that are hard for parallel computation, including the\n",
            "composition of permutation groups, iterated squaring, and circuit value\n",
            "problems, especially for low-depth transformers.\n",
            "\n",
            "71. Title: CrIBo: Self-Supervised Learning via Cross-Image Object-Level Bootstrapping\n",
            "   Abstract: Homophily, the tendency of similar nodes to connect, is a fundamental\n",
            "phenomenon in network science and a critical factor in the performance of graph\n",
            "neural networks (GNNs). While existing studies primarily explore homophily in\n",
            "homogeneous graphs, where nodes share the same type, real-world networks are\n",
            "often more accurately modeled as heterogeneous graphs (HGs) with diverse node\n",
            "types and intricate cross-type interactions. This structural diversity\n",
            "complicates the analysis of homophily, as traditional homophily metrics fail to\n",
            "account for distinct label spaces across node types. To address this\n",
            "limitation, we introduce the Cross-Type Homophily Ratio, a novel metric that\n",
            "quantifies homophily based on the similarity of target information across\n",
            "different node types. Furthermore, we introduce Cross-Type Homophily-guided\n",
            "Heterogeneous Graph Pruning, a method designed to selectively remove\n",
            "low-homophily crosstype edges, thereby enhancing the Cross-Type Homophily Ratio\n",
            "and boosting the performance of heterogeneous graph neural networks (HGNNs).\n",
            "Extensive experiments on five real-world HG datasets validate the effectiveness\n",
            "of our approach, which delivers up to 13.36% average relative performance\n",
            "improvement for HGNNs, offering a fresh perspective on cross-type homophily in\n",
            "heterogeneous graph learning.\n",
            "\n",
            "72. Title: Don't Play Favorites: Minority Guidance for Diffusion Models\n",
            "   Abstract: Reinforcement learning methods are increasingly used to optimise dialogue\n",
            "policies from experience. Most current techniques are model-free: they directly\n",
            "estimate the utility of various actions, without explicit model of the\n",
            "interaction dynamics. In this paper, we investigate an alternative strategy\n",
            "grounded in model-based Bayesian reinforcement learning. Bayesian inference is\n",
            "used to maintain a posterior distribution over the model parameters, reflecting\n",
            "the model uncertainty. This parameter distribution is gradually refined as more\n",
            "data is collected and simultaneously used to plan the agent's actions. Within\n",
            "this learning framework, we carried out experiments with two alternative\n",
            "formalisations of the transition model, one encoded with standard multinomial\n",
            "distributions, and one structured with probabilistic rules. We demonstrate the\n",
            "potential of our approach with empirical results on a user simulator\n",
            "constructed from Wizard-of-Oz data in a human-robot interaction scenario. The\n",
            "results illustrate in particular the benefits of capturing prior domain\n",
            "knowledge with high-level rules.\n",
            "\n",
            "73. Title: GIO: Gradient Information Optimization for Training Dataset Selection\n",
            "   Abstract: Deep learning models achieve excellent performance in numerous machine\n",
            "learning tasks. Yet, they suffer from security-related issues such as\n",
            "adversarial examples and poisoning (backdoor) attacks. A deep learning model\n",
            "may be poisoned by training with backdoored data or by modifying inner network\n",
            "parameters. Then, a backdoored model performs as expected when receiving a\n",
            "clean input, but it misclassifies when receiving a backdoored input stamped\n",
            "with a pre-designed pattern called \"trigger\". Unfortunately, it is difficult to\n",
            "distinguish between clean and backdoored models without prior knowledge of the\n",
            "trigger. This paper proposes a backdoor detection method by utilizing a special\n",
            "type of adversarial attack, universal adversarial perturbation (UAP), and its\n",
            "similarities with a backdoor trigger. We observe an intuitive phenomenon: UAPs\n",
            "generated from backdoored models need fewer perturbations to mislead the model\n",
            "than UAPs from clean models. UAPs of backdoored models tend to exploit the\n",
            "shortcut from all classes to the target class, built by the backdoor trigger.\n",
            "We propose a novel method called Universal Soldier for Backdoor detection (USB)\n",
            "and reverse engineering potential backdoor triggers via UAPs. Experiments on\n",
            "345 models trained on several datasets show that USB effectively detects the\n",
            "injected backdoor and provides comparable or better results than\n",
            "state-of-the-art methods.\n",
            "\n",
            "74. Title: SPTNet: An Efficient Alternative Framework for Generalized Category Discovery with Spatial Prompt Tuning\n",
            "   Abstract: Neural autoregressive sequence models smear the probability among many\n",
            "possible sequences including degenerate ones, such as empty or repetitive\n",
            "sequences. In this work, we tackle one specific case where the model assigns a\n",
            "high probability to unreasonably short sequences. We define the oversmoothing\n",
            "rate to quantify this issue. After confirming the high degree of oversmoothing\n",
            "in neural machine translation, we propose to explicitly minimize the\n",
            "oversmoothing rate during training. We conduct a set of experiments to study\n",
            "the effect of the proposed regularization on both model distribution and\n",
            "decoding performance. We use a neural machine translation task as the testbed\n",
            "and consider three different datasets of varying size. Our experiments reveal\n",
            "three major findings. First, we can control the oversmoothing rate of the model\n",
            "by tuning the strength of the regularization. Second, by enhancing the\n",
            "oversmoothing loss contribution, the probability and the rank of <eos> token\n",
            "decrease heavily at positions where it is not supposed to be. Third, the\n",
            "proposed regularization impacts the outcome of beam search especially when a\n",
            "large beam is used. The degradation of translation quality (measured in BLEU)\n",
            "with a large beam significantly lessens with lower oversmoothing rate, but the\n",
            "degradation compared to smaller beam sizes remains to exist. From these\n",
            "observations, we conclude that the high degree of oversmoothing is the main\n",
            "reason behind the degenerate case of overly probable short sequences in a\n",
            "neural autoregressive model.\n",
            "\n",
            "75. Title: Universal Backdoor Attacks\n",
            "   Abstract: Momentum is known to accelerate the convergence of gradient descent in\n",
            "strongly convex settings without stochastic gradient noise. In stochastic\n",
            "optimization, such as training neural networks, folklore suggests that momentum\n",
            "may help deep learning optimization by reducing the variance of the stochastic\n",
            "gradient update, but previous theoretical analyses do not find momentum to\n",
            "offer any provable acceleration. Theoretical results in this paper clarify the\n",
            "role of momentum in stochastic settings where the learning rate is small and\n",
            "gradient noise is the dominant source of instability, suggesting that SGD with\n",
            "and without momentum behave similarly in the short and long time horizons.\n",
            "Experiments show that momentum indeed has limited benefits for both\n",
            "optimization and generalization in practical training regimes where the optimal\n",
            "learning rate is not very large, including small- to medium-batch training from\n",
            "scratch on ImageNet and fine-tuning language models on downstream tasks.\n",
            "\n",
            "76. Title: Grokking as a First Order Phase Transition in Two Layer Networks\n",
            "   Abstract: We explore the problem of generating minority samples using diffusion models.\n",
            "The minority samples are instances that lie on low-density regions of a data\n",
            "manifold. Generating a sufficient number of such minority instances is\n",
            "important, since they often contain some unique attributes of the data.\n",
            "However, the conventional generation process of the diffusion models mostly\n",
            "yields majority samples (that lie on high-density regions of the manifold) due\n",
            "to their high likelihoods, making themselves ineffective and time-consuming for\n",
            "the minority generating task. In this work, we present a novel framework that\n",
            "can make the generation process of the diffusion models focus on the minority\n",
            "samples. We first highlight that Tweedie's denoising formula yields favorable\n",
            "results for majority samples. The observation motivates us to introduce a\n",
            "metric that describes the uniqueness of a given sample. To address the inherent\n",
            "preference of the diffusion models w.r.t. the majority samples, we further\n",
            "develop minority guidance, a sampling technique that can guide the generation\n",
            "process toward regions with desired likelihood levels. Experiments on benchmark\n",
            "real datasets demonstrate that our minority guidance can greatly improve the\n",
            "capability of generating high-quality minority samples over existing generative\n",
            "samplers. We showcase that the performance benefit of our framework persists\n",
            "even in demanding real-world scenarios such as medical imaging, further\n",
            "underscoring the practical significance of our work. Code is available at\n",
            "https://github.com/soobin-um/minority-guidance.\n",
            "\n",
            "77. Title: Generalization error of spectral algorithms\n",
            "   Abstract: Many computational algorithms applied to geometry operate on discrete\n",
            "representations of shape. It is sometimes necessary to first simplify, or\n",
            "coarsen, representations found in modern datasets for practicable or expedited\n",
            "processing. The utility of a coarsening algorithm depends on both, the choice\n",
            "of representation as well as the specific processing algorithm or operator.\n",
            "e.g. simulation using the Finite Element Method, calculating Betti numbers,\n",
            "etc. We propose a novel method that can coarsen triangle meshes, tetrahedral\n",
            "meshes and simplicial complexes. Our method allows controllable preservation of\n",
            "salient features from the high-resolution geometry and can therefore be\n",
            "customized to different applications.\n",
            "  Salient properties are typically captured by local shape descriptors via\n",
            "linear differential operators -- variants of Laplacians. Eigenvectors of their\n",
            "discretized matrices yield a useful spectral domain for geometry processing\n",
            "(akin to the famous Fourier spectrum which uses eigenfunctions of the\n",
            "derivative operator). Existing methods for spectrum-preserving coarsening use\n",
            "zero-dimensional discretizations of Laplacian operators (defined on vertices).\n",
            "We propose a generalized spectral coarsening method that considers multiple\n",
            "Laplacian operators defined in different dimensionalities in tandem. Our simple\n",
            "algorithm greedily decides the order of contractions of simplices based on a\n",
            "quality function per simplex. The quality function quantifies the error due to\n",
            "removal of that simplex on a chosen band within the spectrum of the coarsened\n",
            "geometry.\n",
            "\n",
            "78. Title: Fusion Is Not Enough: Single Modal Attacks on Fusion Models for 3D Object Detection\n",
            "   Abstract: This paper presents a framework for learning state and action abstractions in\n",
            "sequential decision-making domains. Our framework, planning abstraction from\n",
            "language (PARL), utilizes language-annotated demonstrations to automatically\n",
            "discover a symbolic and abstract action space and induce a latent state\n",
            "abstraction based on it. PARL consists of three stages: 1) recovering\n",
            "object-level and action concepts, 2) learning state abstractions, abstract\n",
            "action feasibility, and transition models, and 3) applying low-level policies\n",
            "for abstract actions. During inference, given the task description, PARL first\n",
            "makes abstract action plans using the latent transition and feasibility\n",
            "functions, then refines the high-level plan using low-level policies. PARL\n",
            "generalizes across scenarios involving novel object instances and environments,\n",
            "unseen concept compositions, and tasks that require longer planning horizons\n",
            "than settings it is trained on.\n",
            "\n",
            "79. Title: Continual Learning in the Presence of Spurious Correlations: Analyses and a Simple Baseline\n",
            "   Abstract: Neural networks sometimes exhibit grokking, a phenomenon where perfect or\n",
            "near-perfect performance is achieved on a validation set well after the same\n",
            "performance has been obtained on the corresponding training set. In this\n",
            "workshop paper, we introduce a robust technique for measuring grokking, based\n",
            "on fitting an appropriate functional form. We then use this to investigate the\n",
            "sharpness of transitions in training and validation accuracy under two\n",
            "settings. The first setting is the theoretical framework developed by Levi et\n",
            "al. (2023) where closed form expressions are readily accessible. The second\n",
            "setting is a two-layer MLP trained to predict the parity of bits, with grokking\n",
            "induced by the concealment strategy of Miller et al. (2023). We find that\n",
            "trends between relative grokking gap and grokking sharpness are similar in both\n",
            "settings when using absolute and relative measures of sharpness. Reflecting on\n",
            "this, we make progress toward explaining some trends and identify the need for\n",
            "further study to untangle the various mechanisms which influence the sharpness\n",
            "of grokking.\n",
            "\n",
            "80. Title: Evaluating Language Model Agency Through Negotiations\n",
            "   Abstract: We introduce an approach to evaluate language model (LM) agency using\n",
            "negotiation games. This approach better reflects real-world use cases and\n",
            "addresses some of the shortcomings of alternative LM benchmarks. Negotiation\n",
            "games enable us to study multi-turn, and cross-model interactions, modulate\n",
            "complexity, and side-step accidental evaluation data leakage. We use our\n",
            "approach to test six widely used and publicly accessible LMs, evaluating\n",
            "performance and alignment in both self-play and cross-play settings. Noteworthy\n",
            "findings include: (i) only closed-source models tested here were able to\n",
            "complete these tasks; (ii) cooperative bargaining games proved to be most\n",
            "challenging to the models; and (iii) even the most powerful models sometimes\n",
            "\"lose\" to weaker opponents\n",
            "\n",
            "81. Title: Why is SAM Robust to Label Noise?\n",
            "   Abstract: Multi-sensor fusion (MSF) is widely used in autonomous vehicles (AVs) for\n",
            "perception, particularly for 3D object detection with camera and LiDAR sensors.\n",
            "The purpose of fusion is to capitalize on the advantages of each modality while\n",
            "minimizing its weaknesses. Advanced deep neural network (DNN)-based fusion\n",
            "techniques have demonstrated the exceptional and industry-leading performance.\n",
            "Due to the redundant information in multiple modalities, MSF is also recognized\n",
            "as a general defence strategy against adversarial attacks. In this paper, we\n",
            "attack fusion models from the camera modality that is considered to be of\n",
            "lesser importance in fusion but is more affordable for attackers. We argue that\n",
            "the weakest link of fusion models depends on their most vulnerable modality,\n",
            "and propose an attack framework that targets advanced camera-LiDAR fusion-based\n",
            "3D object detection models through camera-only adversarial attacks. Our\n",
            "approach employs a two-stage optimization-based strategy that first thoroughly\n",
            "evaluates vulnerable image areas under adversarial attacks, and then applies\n",
            "dedicated attack strategies for different fusion models to generate deployable\n",
            "patches. The evaluations with six advanced camera-LiDAR fusion models and one\n",
            "camera-only model indicate that our attacks successfully compromise all of\n",
            "them. Our approach can either decrease the mean average precision (mAP) of\n",
            "detection performance from 0.824 to 0.353, or degrade the detection score of a\n",
            "target object from 0.728 to 0.156, demonstrating the efficacy of our proposed\n",
            "attack framework. Code is available.\n",
            "\n",
            "82. Title: Privately Aligning Language Models with Reinforcement Learning\n",
            "   Abstract: Positioned between pre-training and user deployment, aligning large language\n",
            "models (LLMs) through reinforcement learning (RL) has emerged as a prevailing\n",
            "strategy for training instruction following-models such as ChatGPT. In this\n",
            "work, we initiate the study of privacy-preserving alignment of LLMs through\n",
            "Differential Privacy (DP) in conjunction with RL. Following the influential\n",
            "work of Ziegler et al. (2020), we study two dominant paradigms: (i) alignment\n",
            "via RL without human in the loop (e.g., positive review generation) and (ii)\n",
            "alignment via RL from human feedback (RLHF) (e.g., summarization in a\n",
            "human-preferred way). We give a new DP framework to achieve alignment via RL,\n",
            "and prove its correctness. Our experimental results validate the effectiveness\n",
            "of our approach, offering competitive utility while ensuring strong privacy\n",
            "protections.\n",
            "\n",
            "83. Title: Efficient-3Dim: Learning a Generalizable Single-image Novel-view Synthesizer in One Day\n",
            "   Abstract: We study pre-training representations for decision-making using video data,\n",
            "which is abundantly available for tasks such as game agents and software\n",
            "testing. Even though significant empirical advances have been made on this\n",
            "problem, a theoretical understanding remains absent. We initiate the\n",
            "theoretical investigation into principled approaches for representation\n",
            "learning and focus on learning the latent state representations of the\n",
            "underlying MDP using video data. We study two types of settings: one where\n",
            "there is iid noise in the observation, and a more challenging setting where\n",
            "there is also the presence of exogenous noise, which is non-iid noise that is\n",
            "temporally correlated, such as the motion of people or cars in the background.\n",
            "We study three commonly used approaches: autoencoding, temporal contrastive\n",
            "learning, and forward modeling. We prove upper bounds for temporal contrastive\n",
            "learning and forward modeling in the presence of only iid noise. We show that\n",
            "these approaches can learn the latent state and use it to do efficient\n",
            "downstream RL with polynomial sample complexity. When exogenous noise is also\n",
            "present, we establish a lower bound result showing that the sample complexity\n",
            "of learning from video data can be exponentially worse than learning from\n",
            "action-labeled trajectory data. This partially explains why reinforcement\n",
            "learning with video pre-training is hard. We evaluate these representational\n",
            "learning methods in two visual domains, yielding results that are consistent\n",
            "with our theoretical findings.\n",
            "\n",
            "84. Title: LEGO-Prover: Neural Theorem Proving with Growing Libraries\n",
            "   Abstract: We present Step-Back Prompting, a simple prompting technique that enables\n",
            "LLMs to do abstractions to derive high-level concepts and first principles from\n",
            "instances containing specific details. Using the concepts and principles to\n",
            "guide reasoning, LLMs significantly improve their abilities in following a\n",
            "correct reasoning path towards the solution. We conduct experiments of\n",
            "Step-Back Prompting with PaLM-2L, GPT-4 and Llama2-70B models, and observe\n",
            "substantial performance gains on various challenging reasoning-intensive tasks\n",
            "including STEM, Knowledge QA, and Multi-Hop Reasoning. For instance, Step-Back\n",
            "Prompting improves PaLM-2L performance on MMLU (Physics and Chemistry) by 7%\n",
            "and 11% respectively, TimeQA by 27%, and MuSiQue by 7%.\n",
            "\n",
            "85. Title: Detecting Machine-Generated Texts by Multi-Population Aware Optimization for Maximum Mean Discrepancy\n",
            "   Abstract: Sharpness-Aware Minimization (SAM) is most known for achieving state-of\n",
            "the-art performances on natural image and language tasks. However, its most\n",
            "pronounced improvements (of tens of percent) is rather in the presence of label\n",
            "noise. Understanding SAM's label noise robustness requires a departure from\n",
            "characterizing the robustness of minimas lying in \"flatter\" regions of the loss\n",
            "landscape. In particular, the peak performance under label noise occurs with\n",
            "early stopping, far before the loss converges. We decompose SAM's robustness\n",
            "into two effects: one induced by changes to the logit term and the other\n",
            "induced by changes to the network Jacobian. The first can be observed in linear\n",
            "logistic regression where SAM provably up-weights the gradient contribution\n",
            "from clean examples. Although this explicit up-weighting is also observable in\n",
            "neural networks, when we intervene and modify SAM to remove this effect,\n",
            "surprisingly, we see no visible degradation in performance. We infer that SAM's\n",
            "effect in deeper networks is instead explained entirely by the effect SAM has\n",
            "on the network Jacobian. We theoretically derive the implicit regularization\n",
            "induced by this Jacobian effect in two layer linear networks. Motivated by our\n",
            "analysis, we see that cheaper alternatives to SAM that explicitly induce these\n",
            "regularization effects largely recover the benefits in deep networks trained on\n",
            "real-world datasets.\n",
            "\n",
            "86. Title: Towards Principled Representation Learning from Videos for Reinforcement Learning\n",
            "   Abstract: This paper focuses on the challenging crowd counting task. As large-scale\n",
            "variations often exist within crowd images, neither fixed-size convolution\n",
            "kernel of CNN nor fixed-size attention of recent vision transformers can well\n",
            "handle this kind of variation. To address this problem, we propose a\n",
            "Multifaceted Attention Network (MAN) to improve transformer models in local\n",
            "spatial relation encoding. MAN incorporates global attention from a vanilla\n",
            "transformer, learnable local attention, and instance attention into a counting\n",
            "model. Firstly, the local Learnable Region Attention (LRA) is proposed to\n",
            "assign attention exclusively for each feature location dynamically. Secondly,\n",
            "we design the Local Attention Regularization to supervise the training of LRA\n",
            "by minimizing the deviation among the attention for different feature\n",
            "locations. Finally, we provide an Instance Attention mechanism to focus on the\n",
            "most important instances dynamically during training. Extensive experiments on\n",
            "four challenging crowd counting datasets namely ShanghaiTech, UCF-QNRF, JHU++,\n",
            "and NWPU have validated the proposed method. Codes:\n",
            "https://github.com/LoraLinH/Boosting-Crowd-Counting-via-Multifaceted-Attention.\n",
            "\n",
            "87. Title: SEGNO: Generalizing Equivariant Graph Neural Networks with Physical Inductive Biases\n",
            "   Abstract: The premise of identifiable and causal representation learning is to improve\n",
            "the current representation learning paradigm in terms of generalizability or\n",
            "robustness. Despite recent progress in questions of identifiability, more\n",
            "theoretical results demonstrating concrete advantages of these methods for\n",
            "downstream tasks are needed. In this paper, we consider the task of\n",
            "intervention extrapolation: predicting how interventions affect an outcome,\n",
            "even when those interventions are not observed at training time, and show that\n",
            "identifiable representations can provide an effective solution to this task\n",
            "even if the interventions affect the outcome non-linearly. Our setup includes\n",
            "an outcome Y, observed features X, which are generated as a non-linear\n",
            "transformation of latent features Z, and exogenous action variables A, which\n",
            "influence Z. The objective of intervention extrapolation is to predict how\n",
            "interventions on A that lie outside the training support of A affect Y. Here,\n",
            "extrapolation becomes possible if the effect of A on Z is linear and the\n",
            "residual when regressing Z on A has full support. As Z is latent, we combine\n",
            "the task of intervention extrapolation with identifiable representation\n",
            "learning, which we call Rep4Ex: we aim to map the observed features X into a\n",
            "subspace that allows for non-linear extrapolation in A. We show that the hidden\n",
            "representation is identifiable up to an affine transformation in Z-space, which\n",
            "is sufficient for intervention extrapolation. The identifiability is\n",
            "characterized by a novel constraint describing the linearity assumption of A on\n",
            "Z. Based on this insight, we propose a method that enforces the linear\n",
            "invariance constraint and can be combined with any type of autoencoder. We\n",
            "validate our theoretical findings through synthetic experiments and show that\n",
            "our approach succeeds in predicting the effects of unseen interventions.\n",
            "\n",
            "88. Title: LRR: Language-Driven Resamplable Continuous Representation against Adversarial Tracking Attacks\n",
            "   Abstract: Forecasting the behavior of other agents is an integral part of the modern\n",
            "robotic autonomy stack, especially in safety-critical scenarios with\n",
            "human-robot interaction, such as autonomous driving. In turn, there has been a\n",
            "significant amount of interest and research in trajectory forecasting,\n",
            "resulting in a wide variety of approaches. Common to all works, however, is the\n",
            "use of the same few accuracy-based evaluation metrics, e.g., displacement error\n",
            "and log-likelihood. While these metrics are informative, they are task-agnostic\n",
            "and predictions that are evaluated as equal can lead to vastly different\n",
            "outcomes, e.g., in downstream planning and decision making. In this work, we\n",
            "take a step back and critically evaluate current trajectory forecasting\n",
            "metrics, proposing task-aware metrics as a better measure of performance in\n",
            "systems where prediction is being deployed. We additionally present one example\n",
            "of such a metric, incorporating planning-awareness within existing trajectory\n",
            "forecasting metrics.\n",
            "\n",
            "89. Title: Generative Learning for Solving Non-Convex Problem with Multi-Valued Input-Solution Mapping\n",
            "   Abstract: We present SAM, a biologically-plausible selective attention-driven\n",
            "modulation approach to enhance classification models in a continual learning\n",
            "setting. Inspired by neurophysiological evidence that the primary visual cortex\n",
            "does not contribute to object manifold untangling for categorization and that\n",
            "primordial attention biases are still embedded in the modern brain, we propose\n",
            "to employ auxiliary saliency prediction features as a modulation signal to\n",
            "drive and stabilize the learning of a sequence of non-i.i.d. classification\n",
            "tasks. Experimental results confirm that SAM effectively enhances the\n",
            "performance (in some cases up to about twenty percent points) of\n",
            "state-of-the-art continual learning methods, both in class-incremental and\n",
            "task-incremental settings. Moreover, we show that attention-based modulation\n",
            "successfully encourages the learning of features that are more robust to the\n",
            "presence of spurious features and to adversarial attacks than baseline methods.\n",
            "Code is available at: https://github.com/perceivelab/SAM.\n",
            "\n",
            "90. Title: The Generalization Gap in Offline Reinforcement Learning\n",
            "   Abstract: This study aims to prove the emergence of symbolic concepts (or more\n",
            "precisely, sparse primitive inference patterns) in well-trained deep neural\n",
            "networks (DNNs). Specifically, we prove the following three conditions for the\n",
            "emergence. (i) The high-order derivatives of the network output with respect to\n",
            "the input variables are all zero. (ii) The DNN can be used on occluded samples\n",
            "and when the input sample is less occluded, the DNN will yield higher\n",
            "confidence. (iii) The confidence of the DNN does not significantly degrade on\n",
            "occluded samples. These conditions are quite common, and we prove that under\n",
            "these conditions, the DNN will only encode a relatively small number of sparse\n",
            "interactions between input variables. Moreover, we can consider such\n",
            "interactions as symbolic primitive inference patterns encoded by a DNN, because\n",
            "we show that inference scores of the DNN on an exponentially large number of\n",
            "randomly masked samples can always be well mimicked by numerical effects of\n",
            "just a few interactions.\n",
            "\n",
            "91. Title: Sharpness-Aware Minimization Enhances Feature Quality via Balanced Learning\n",
            "   Abstract: Large language models (LLMs) such as ChatGPT have exhibited remarkable\n",
            "performance in generating human-like texts. However, machine-generated texts\n",
            "(MGTs) may carry critical risks, such as plagiarism issues, misleading\n",
            "information, or hallucination issues. Therefore, it is very urgent and\n",
            "important to detect MGTs in many situations. Unfortunately, it is challenging\n",
            "to distinguish MGTs and human-written texts because the distributional\n",
            "discrepancy between them is often very subtle due to the remarkable performance\n",
            "of LLMs. In this paper, we seek to exploit \\textit{maximum mean discrepancy}\n",
            "(MMD) to address this issue in the sense that MMD can well identify\n",
            "distributional discrepancies. However, directly training a detector with MMD\n",
            "using diverse MGTs will incur a significantly increased variance of MMD since\n",
            "MGTs may contain \\textit{multiple text populations} due to various LLMs. This\n",
            "will severely impair MMD's ability to measure the difference between two\n",
            "samples. To tackle this, we propose a novel \\textit{multi-population} aware\n",
            "optimization method for MMD called MMD-MP, which can \\textit{avoid variance\n",
            "increases} and thus improve the stability to measure the distributional\n",
            "discrepancy. Relying on MMD-MP, we develop two methods for paragraph-based and\n",
            "sentence-based detection, respectively. Extensive experiments on various LLMs,\n",
            "\\eg, GPT2 and ChatGPT, show superior detection performance of our MMD-MP. The\n",
            "source code is available at \\url{https://github.com/ZSHsh98/MMD-MP}.\n",
            "\n",
            "92. Title: Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training\n",
            "   Abstract: Sharpness-Aware Minimization (SAM) has emerged as a promising alternative\n",
            "optimizer to stochastic gradient descent (SGD). The originally-proposed\n",
            "motivation behind SAM was to bias neural networks towards flatter minima that\n",
            "are believed to generalize better. However, recent studies have shown\n",
            "conflicting evidence on the relationship between flatness and generalization,\n",
            "suggesting that flatness does fully explain SAM's success. Sidestepping this\n",
            "debate, we identify an orthogonal effect of SAM that is beneficial\n",
            "out-of-distribution: we argue that SAM implicitly balances the quality of\n",
            "diverse features. SAM achieves this effect by adaptively suppressing\n",
            "well-learned features which gives remaining features opportunity to be learned.\n",
            "We show that this mechanism is beneficial in datasets that contain redundant or\n",
            "spurious features where SGD falls for the simplicity bias and would not\n",
            "otherwise learn all available features. Our insights are supported by\n",
            "experiments on real data: we demonstrate that SAM improves the quality of\n",
            "features in datasets containing redundant or spurious features, including\n",
            "CelebA, Waterbirds, CIFAR-MNIST, and DomainBed.\n",
            "\n",
            "93. Title: T-Rep: Representation Learning for Time Series using Time-Embeddings\n",
            "   Abstract: Despite recent progress in offline learning, these methods are still trained\n",
            "and tested on the same environment. In this paper, we compare the\n",
            "generalization abilities of widely used online and offline learning methods\n",
            "such as online reinforcement learning (RL), offline RL, sequence modeling, and\n",
            "behavioral cloning. Our experiments show that offline learning algorithms\n",
            "perform worse on new environments than online learning ones. We also introduce\n",
            "the first benchmark for evaluating generalization in offline learning,\n",
            "collecting datasets of varying sizes and skill-levels from Procgen (2D video\n",
            "games) and WebShop (e-commerce websites). The datasets contain trajectories for\n",
            "a limited number of game levels or natural language instructions and at test\n",
            "time, the agent has to generalize to new levels or instructions. Our\n",
            "experiments reveal that existing offline learning algorithms struggle to match\n",
            "the performance of online RL on both train and test environments. Behavioral\n",
            "cloning is a strong baseline, outperforming state-of-the-art offline RL and\n",
            "sequence modeling approaches when trained on data from multiple environments\n",
            "and tested on new ones. Finally, we find that increasing the diversity of the\n",
            "data, rather than its size, improves performance on new environments for all\n",
            "offline learning algorithms. Our study demonstrates the limited generalization\n",
            "of current offline learning algorithms highlighting the need for more research\n",
            "in this area.\n",
            "\n",
            "94. Title: Geometrically Aligned Transfer Encoder for Inductive Transfer in Regression Tasks\n",
            "   Abstract: Deciphering the human visual experience through brain activities captured by\n",
            "fMRI represents a compelling and cutting-edge challenge in the field of\n",
            "neuroscience research. Compared to merely predicting the viewed image itself,\n",
            "decoding brain activity into meaningful captions provides a higher-level\n",
            "interpretation and summarization of visual information, which naturally\n",
            "enhances the application flexibility in real-world situations. In this work, we\n",
            "introduce MindSemantix, a novel multi-modal framework that enables LLMs to\n",
            "comprehend visually-evoked semantic content in brain activity. Our MindSemantix\n",
            "explores a more ideal brain captioning paradigm by weaving LLMs into brain\n",
            "activity analysis, crafting a seamless, end-to-end Brain-Language Model. To\n",
            "effectively capture semantic information from brain responses, we propose\n",
            "Brain-Text Transformer, utilizing a Brain Q-Former as its core architecture. It\n",
            "integrates a pre-trained brain encoder with a frozen LLM to achieve multi-modal\n",
            "alignment of brain-vision-language and establish a robust brain-language\n",
            "correspondence. To enhance the generalizability of neural representations, we\n",
            "pre-train our brain encoder on a large-scale, cross-subject fMRI dataset using\n",
            "self-supervised learning techniques. MindSemantix provides more feasibility to\n",
            "downstream brain decoding tasks such as stimulus reconstruction. Conditioned by\n",
            "MindSemantix captioning, our framework facilitates this process by integrating\n",
            "with advanced generative models like Stable Diffusion and excels in\n",
            "understanding brain visual perception. MindSemantix generates high-quality\n",
            "captions that are deeply rooted in the visual and semantic information derived\n",
            "from brain activity. This approach has demonstrated substantial quantitative\n",
            "improvements over prior art. Our code will be released.\n",
            "\n",
            "95. Title: On-Policy Distillation of Language Models: Learning from Self-Generated Mistakes\n",
            "   Abstract: Modern distribution matching algorithms for training diffusion or flow models\n",
            "directly prescribe the time evolution of the marginal distributions between two\n",
            "boundary distributions. In this work, we consider a generalized distribution\n",
            "matching setup, where these marginals are only implicitly described as a\n",
            "solution to some task-specific objective function. The problem setup, known as\n",
            "the Generalized Schr\\\"odinger Bridge (GSB), appears prevalently in many\n",
            "scientific areas both within and without machine learning. We propose\n",
            "Generalized Schr\\\"odinger Bridge Matching (GSBM), a new matching algorithm\n",
            "inspired by recent advances, generalizing them beyond kinetic energy\n",
            "minimization and to account for task-specific state costs. We show that such a\n",
            "generalization can be cast as solving conditional stochastic optimal control,\n",
            "for which efficient variational approximations can be used, and further\n",
            "debiased with the aid of path integral theory. Compared to prior methods for\n",
            "solving GSB problems, our GSBM algorithm better preserves a feasible transport\n",
            "map between the boundary distributions throughout training, thereby enabling\n",
            "stable convergence and significantly improved scalability. We empirically\n",
            "validate our claims on an extensive suite of experimental setups, including\n",
            "crowd navigation, opinion depolarization, LiDAR manifolds, and image domain\n",
            "transfer. Our work brings new algorithmic opportunities for training diffusion\n",
            "models enhanced with task-specific optimality structures. Code available at\n",
            "https://github.com/facebookresearch/generalized-schrodinger-bridge-matching\n",
            "\n",
            "96. Title: REBAR: Retrieval-Based Reconstruction for Time-series Contrastive Learning\n",
            "   Abstract: Transfer learning is a crucial technique for handling a small amount of data\n",
            "that is potentially related to other abundant data. However, most of the\n",
            "existing methods are focused on classification tasks using images and language\n",
            "datasets. Therefore, in order to expand the transfer learning scheme to\n",
            "regression tasks, we propose a novel transfer technique based on differential\n",
            "geometry, namely the Geometrically Aligned Transfer Encoder (GATE). In this\n",
            "method, we interpret the latent vectors from the model to exist on a Riemannian\n",
            "curved manifold. We find a proper diffeomorphism between pairs of tasks to\n",
            "ensure that every arbitrary point maps to a locally flat coordinate in the\n",
            "overlapping region, allowing the transfer of knowledge from the source to the\n",
            "target data. This also serves as an effective regularizer for the model to\n",
            "behave in extrapolation regions. In this article, we demonstrate that GATE\n",
            "outperforms conventional methods and exhibits stable behavior in both the\n",
            "latent space and extrapolation regions for various molecular graph datasets.\n",
            "\n",
            "97. Title: DORSal: Diffusion for Object-centric Representations of Scenes $\\textit{et al.}$\n",
            "   Abstract: Knowledge distillation (KD) is widely used for compressing a teacher model to\n",
            "reduce its inference cost and memory footprint, by training a smaller student\n",
            "model. However, current KD methods for auto-regressive sequence models suffer\n",
            "from distribution mismatch between output sequences seen during training and\n",
            "those generated by the student during inference. To address this issue, we\n",
            "introduce Generalized Knowledge Distillation (GKD). Instead of solely relying\n",
            "on a fixed set of output sequences, GKD trains the student on its\n",
            "self-generated output sequences by leveraging feedback from the teacher on such\n",
            "sequences. Unlike supervised KD approaches, GKD also offers the flexibility to\n",
            "employ alternative loss functions between the student and teacher, which can be\n",
            "useful when the student lacks the expressivity to mimic the teacher's\n",
            "distribution. Furthermore, GKD facilitates the seamless integration of\n",
            "distillation with RL fine-tuning (RLHF). We demonstrate the efficacy of GKD for\n",
            "distilling auto-regressive language models on summarization, translation, and\n",
            "arithmetic reasoning tasks, and task-agnostic distillation for\n",
            "instruction-tuning.\n",
            "\n",
            "98. Title: Language Model Decoding as Direct Metrics Optimization\n",
            "   Abstract: We describe Substitutional Neural Image Compression (SNIC), a general\n",
            "approach for enhancing any neural image compression model, that requires no\n",
            "data or additional tuning of the trained model. It boosts compression\n",
            "performance toward a flexible distortion metric and enables bit-rate control\n",
            "using a single model instance. The key idea is to replace the image to be\n",
            "compressed with a substitutional one that outperforms the original one in a\n",
            "desired way. Finding such a substitute is inherently difficult for conventional\n",
            "codecs, yet surprisingly favorable for neural compression models thanks to\n",
            "their fully differentiable structures. With gradients of a particular loss\n",
            "backpropogated to the input, a desired substitute can be efficiently crafted\n",
            "iteratively. We demonstrate the effectiveness of SNIC, when combined with\n",
            "various neural compression models and target metrics, in improving compression\n",
            "quality and performing bit-rate control measured by rate-distortion curves.\n",
            "Empirical results of control precision and generation speed are also discussed.\n",
            "\n",
            "99. Title: LCOT: Linear Circular Optimal Transport\n",
            "   Abstract: Despite the remarkable advances in language modeling, current mainstream\n",
            "decoding methods still struggle to generate texts that align with human texts\n",
            "across different aspects. In particular, sampling-based methods produce\n",
            "less-repetitive texts which are often disjunctive in discourse, while\n",
            "search-based methods maintain topic coherence at the cost of increased\n",
            "repetition. Overall, these methods fall short in achieving holistic alignment\n",
            "across a broad range of aspects. In this work, we frame decoding from a\n",
            "language model as an optimization problem with the goal of strictly matching\n",
            "the expected performance with human texts measured by multiple metrics of\n",
            "desired aspects simultaneously. The resulting decoding distribution enjoys an\n",
            "analytical solution that scales the input language model distribution via a\n",
            "sequence-level energy function defined by these metrics. And most importantly,\n",
            "we prove that this induced distribution is guaranteed to improve the perplexity\n",
            "on human texts, which suggests a better approximation to the underlying\n",
            "distribution of human texts. To facilitate tractable sampling from this\n",
            "globally normalized distribution, we adopt the Sampling-Importance-Resampling\n",
            "technique. Experiments on various domains and model scales demonstrate the\n",
            "superiority of our method in metrics alignment with human texts and human\n",
            "evaluation over strong baselines.\n",
            "\n",
            "100. Title: Mixed-Type Tabular Data Synthesis with Score-based Diffusion in Latent Space\n",
            "   Abstract: We study the memorization power of feedforward ReLU neural networks. We show\n",
            "that such networks can memorize any $N$ points that satisfy a mild separability\n",
            "assumption using $\\tilde{O}\\left(\\sqrt{N}\\right)$ parameters. Known\n",
            "VC-dimension upper bounds imply that memorizing $N$ samples requires\n",
            "$\\Omega(\\sqrt{N})$ parameters, and hence our construction is optimal up to\n",
            "logarithmic factors. We also give a generalized construction for networks with\n",
            "depth bounded by $1 \\leq L \\leq \\sqrt{N}$, for memorizing $N$ samples using\n",
            "$\\tilde{O}(N/L)$ parameters. This bound is also optimal up to logarithmic\n",
            "factors. Our construction uses weights with large bit complexity. We prove that\n",
            "having such a large bit complexity is both necessary and sufficient for\n",
            "memorization with a sub-linear number of parameters.\n",
            "\n",
            "101. Title: Chameleon: Increasing Label-Only Membership Leakage with Adaptive Poisoning\n",
            "   Abstract: Rule learning is critical to improving knowledge graph (KG) reasoning due to\n",
            "their ability to provide logical and interpretable explanations. Recently,\n",
            "Graph Neural Networks (GNNs) with tail entity scoring achieve the\n",
            "state-of-the-art performance on KG reasoning. However, the theoretical\n",
            "understandings for these GNNs are either lacking or focusing on\n",
            "single-relational graphs, leaving what the kind of rules these GNNs can learn\n",
            "an open problem. We propose to fill the above gap in this paper. Specifically,\n",
            "GNNs with tail entity scoring are unified into a common framework. Then, we\n",
            "analyze their expressivity by formally describing the rule structures they can\n",
            "learn and theoretically demonstrating their superiority. These results further\n",
            "inspire us to propose a novel labeling strategy to learn more rules in KG\n",
            "reasoning. Experimental results are consistent with our theoretical findings\n",
            "and verify the effectiveness of our proposed method. The code is publicly\n",
            "available at https://github.com/LARS-research/Rule-learning-expressivity.\n",
            "\n",
            "102. Title: One For All: Towards Training One Graph Model For All Classification Tasks\n",
            "   Abstract: Recent advances in tabular data generation have greatly enhanced synthetic\n",
            "data quality. However, extending diffusion models to tabular data is\n",
            "challenging due to the intricately varied distributions and a blend of data\n",
            "types of tabular data. This paper introduces Tabsyn, a methodology that\n",
            "synthesizes tabular data by leveraging a diffusion model within a variational\n",
            "autoencoder (VAE) crafted latent space. The key advantages of the proposed\n",
            "Tabsyn include (1) Generality: the ability to handle a broad spectrum of data\n",
            "types by converting them into a single unified space and explicitly capture\n",
            "inter-column relations; (2) Quality: optimizing the distribution of latent\n",
            "embeddings to enhance the subsequent training of diffusion models, which helps\n",
            "generate high-quality synthetic data, (3) Speed: much fewer number of reverse\n",
            "steps and faster synthesis speed than existing diffusion-based methods.\n",
            "Extensive experiments on six datasets with five metrics demonstrate that Tabsyn\n",
            "outperforms existing methods. Specifically, it reduces the error rates by 86%\n",
            "and 67% for column-wise distribution and pair-wise column correlation\n",
            "estimations compared with the most competitive baselines.\n",
            "\n",
            "103. Title: The Devil is in the Object Boundary: Towards Annotation-free Instance Segmentation using Foundation Models\n",
            "   Abstract: We introduce Cosmos, a framework for object-centric world modeling that is\n",
            "designed for compositional generalization (CompGen), i.e., high performance on\n",
            "unseen input scenes obtained through the composition of known visual \"atoms.\"\n",
            "The central insight behind Cosmos is the use of a novel form of neurosymbolic\n",
            "grounding. Specifically, the framework introduces two new tools: (i)\n",
            "neurosymbolic scene encodings, which represent each entity in a scene using a\n",
            "real vector computed using a neural encoder, as well as a vector of composable\n",
            "symbols describing attributes of the entity, and (ii) a neurosymbolic attention\n",
            "mechanism that binds these entities to learned rules of interaction. Cosmos is\n",
            "end-to-end differentiable; also, unlike traditional neurosymbolic methods that\n",
            "require representations to be manually mapped to symbols, it computes an\n",
            "entity's symbolic attributes using vision-language foundation models. Through\n",
            "an evaluation that considers two different forms of CompGen on an established\n",
            "blocks-pushing domain, we show that the framework establishes a new\n",
            "state-of-the-art for CompGen in world modeling. Artifacts are available at:\n",
            "https://trishullab.github.io/cosmos-web/\n",
            "\n",
            "104. Title: Curiosity-driven Red-teaming for Large Language Models\n",
            "   Abstract: Designing a single model to address multiple tasks has been a long-standing\n",
            "objective in artificial intelligence. Recently, large language models have\n",
            "demonstrated exceptional capability in solving different tasks within the\n",
            "language domain. However, a unified model for various graph tasks remains\n",
            "underexplored, primarily due to the challenges unique to the graph learning\n",
            "domain. First, graph data from different areas carry distinct attributes and\n",
            "follow different distributions. Such discrepancy makes it hard to represent\n",
            "graphs in a single representation space. Second, tasks on graphs diversify into\n",
            "node, link, and graph tasks, requiring distinct embedding strategies. Finally,\n",
            "an appropriate graph prompting paradigm for in-context learning is unclear. We\n",
            "propose \\textbf{One for All (OFA)}, the first general framework that can use a\n",
            "single graph model to address the above challenges. Specifically, OFA proposes\n",
            "text-attributed graphs to unify different graph data by describing nodes and\n",
            "edges with natural language and uses language models to encode the diverse and\n",
            "possibly cross-domain text attributes to feature vectors in the same embedding\n",
            "space. Furthermore, OFA introduces the concept of nodes-of-interest to\n",
            "standardize different tasks with a single task representation. For in-context\n",
            "learning on graphs, OFA introduces a novel graph prompting paradigm that\n",
            "appends prompting substructures to the input graph, which enables it to address\n",
            "varied tasks without fine-tuning. We train the OFA model using graph data from\n",
            "multiple domains (including citation networks, molecular graphs, knowledge\n",
            "graphs, etc.) simultaneously and evaluate its ability in supervised, few-shot,\n",
            "and zero-shot learning scenarios. OFA performs well across different tasks,\n",
            "making it the first general-purpose across-domains classification model on\n",
            "graphs.\n",
            "\n",
            "105. Title: Chain-of-Table: Evolving Tables in the Reasoning Chain for Table Understanding\n",
            "   Abstract: In this paper, we propose SemanticAC, a semantics-assisted framework for\n",
            "Audio Classification to better leverage the semantic information. Unlike\n",
            "conventional audio classification methods that treat class labels as discrete\n",
            "vectors, we employ a language model to extract abundant semantics from labels\n",
            "and optimize the semantic consistency between audio signals and their labels.\n",
            "We verify that simple textual information from labels and advanced pretraining\n",
            "models enable more abundant semantic supervision for better performance.\n",
            "Specifically, we design a text encoder to capture the semantic information from\n",
            "the text extension of labels. Then we map the audio signals to align with the\n",
            "semantics of corresponding class labels via an audio encoder and a similarity\n",
            "calculation module so as to enforce the semantic consistency. Extensive\n",
            "experiments on two audio datasets, ESC-50 and US8K demonstrate that our\n",
            "proposed method consistently outperforms the compared audio classification\n",
            "methods.\n",
            "\n",
            "106. Title: Deep Reinforcement Learning for Modelling Protein Complexes\n",
            "   Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in\n",
            "natural language processing tasks, but their vulnerability to jailbreak attacks\n",
            "poses significant security risks. This survey paper presents a comprehensive\n",
            "analysis of recent advancements in attack strategies and defense mechanisms\n",
            "within the field of Large Language Model (LLM) red-teaming. We analyze various\n",
            "attack methods, including gradient-based optimization, reinforcement learning,\n",
            "and prompt engineering approaches. We discuss the implications of these attacks\n",
            "on LLM safety and the need for improved defense mechanisms. This work aims to\n",
            "provide a thorough understanding of the current landscape of red-teaming\n",
            "attacks and defenses on LLMs, enabling the development of more secure and\n",
            "reliable language models.\n",
            "\n",
            "107. Title: Weakly-supervised Audio Separation via Bi-modal Semantic Similarity\n",
            "   Abstract: Graph Neural Networks (GNNs) are popular machine learning methods for\n",
            "modeling graph data. A lot of GNNs perform well on homophily graphs while\n",
            "having unsatisfactory performance on heterophily graphs. Recently, some\n",
            "researchers turn their attention to designing GNNs for heterophily graphs by\n",
            "adjusting the message passing mechanism or enlarging the receptive field of the\n",
            "message passing. Different from existing works that mitigate the issues of\n",
            "heterophily from model design perspective, we propose to study heterophily\n",
            "graphs from an orthogonal perspective by rewiring the graph structure to reduce\n",
            "heterophily and making the traditional GNNs perform better. Through\n",
            "comprehensive empirical studies and analysis, we verify the potential of the\n",
            "rewiring methods. To fully exploit its potential, we propose a method named\n",
            "Deep Heterophily Graph Rewiring (DHGR) to rewire graphs by adding homophilic\n",
            "edges and pruning heterophilic edges. The detailed way of rewiring is\n",
            "determined by comparing the similarity of label/feature-distribution of node\n",
            "neighbors. Besides, we design a scalable implementation for DHGR to guarantee\n",
            "high efficiency. DHRG can be easily used as a plug-in module, i.e., a graph\n",
            "pre-processing step, for any GNNs, including both GNN for homophily and\n",
            "heterophily, to boost their performance on the node classification task. To the\n",
            "best of our knowledge, it is the first work studying graph rewiring for\n",
            "heterophily graphs. Extensive experiments on 11 public graph datasets\n",
            "demonstrate the superiority of our proposed methods.\n",
            "\n",
            "108. Title: DAM: Towards a Foundation Model for Forecasting\n",
            "   Abstract: AlphaFold can be used for both single-chain and multi-chain protein structure\n",
            "prediction, while the latter becomes extremely challenging as the number of\n",
            "chains increases. In this work, by taking each chain as a node and assembly\n",
            "actions as edges, we show that an acyclic undirected connected graph can be\n",
            "used to predict the structure of multi-chain protein complexes (a.k.a., protein\n",
            "complex modelling, PCM). However, there are still two challenges: 1) The huge\n",
            "combinatorial optimization space of $N^{N-2}$ ($N$ is the number of chains) for\n",
            "the PCM problem can easily lead to high computational cost. 2) The scales of\n",
            "protein complexes exhibit distribution shift due to variance in chain numbers,\n",
            "which calls for the generalization in modelling complexes of various scales. To\n",
            "address these challenges, we propose GAPN, a Generative Adversarial Policy\n",
            "Network powered by domain-specific rewards and adversarial loss through policy\n",
            "gradient for automatic PCM prediction. Specifically, GAPN learns to efficiently\n",
            "search through the immense assembly space and optimize the direct docking\n",
            "reward through policy gradient. Importantly, we design an adversarial reward\n",
            "function to enhance the receptive field of our model. In this way, GAPN will\n",
            "simultaneously focus on a specific batch of complexes and the global assembly\n",
            "rules learned from complexes with varied chain numbers. Empirically, we have\n",
            "achieved both significant accuracy (measured by RMSD and TM-Score) and\n",
            "efficiency improvements compared to leading PCM softwares.\n",
            "\n",
            "109. Title: Rayleigh Quotient Graph Neural Networks for Graph-level Anomaly Detection\n",
            "   Abstract: Offline reinforcement learning (RL) offers a promising direction for learning\n",
            "policies from pre-collected datasets without requiring further interactions\n",
            "with the environment. However, existing methods struggle to handle\n",
            "out-of-distribution (OOD) extrapolation errors, especially in sparse reward or\n",
            "scarce data settings. In this paper, we propose a novel training algorithm\n",
            "called Conservative Density Estimation (CDE), which addresses this challenge by\n",
            "explicitly imposing constraints on the state-action occupancy stationary\n",
            "distribution. CDE overcomes the limitations of existing approaches, such as the\n",
            "stationary distribution correction method, by addressing the support mismatch\n",
            "issue in marginal importance sampling. Our method achieves state-of-the-art\n",
            "performance on the D4RL benchmark. Notably, CDE consistently outperforms\n",
            "baselines in challenging tasks with sparse rewards or insufficient data,\n",
            "demonstrating the advantages of our approach in addressing the extrapolation\n",
            "error problem in offline RL.\n",
            "\n",
            "110. Title: Locality-Aware Graph Rewiring in GNNs\n",
            "   Abstract: Graph-level anomaly detection has gained significant attention as it finds\n",
            "applications in various domains, such as cancer diagnosis and enzyme\n",
            "prediction. However, existing methods fail to capture the spectral properties\n",
            "of graph anomalies, resulting in unexplainable framework design and\n",
            "unsatisfying performance. In this paper, we re-investigate the spectral\n",
            "differences between anomalous and normal graphs. Our main observation shows a\n",
            "significant disparity in the accumulated spectral energy between these two\n",
            "classes. Moreover, we prove that the accumulated spectral energy of the graph\n",
            "signal can be represented by its Rayleigh Quotient, indicating that the\n",
            "Rayleigh Quotient is a driving factor behind the anomalous properties of\n",
            "graphs. Motivated by this, we propose Rayleigh Quotient Graph Neural Network\n",
            "(RQGNN), the first spectral GNN that explores the inherent spectral features of\n",
            "anomalous graphs for graph-level anomaly detection. Specifically, we introduce\n",
            "a novel framework with two components: the Rayleigh Quotient learning component\n",
            "(RQL) and Chebyshev Wavelet GNN with RQ-pooling (CWGNN-RQ). RQL explicitly\n",
            "captures the Rayleigh Quotient of graphs and CWGNN-RQ implicitly explores the\n",
            "spectral space of graphs. Extensive experiments on 10 real-world datasets show\n",
            "that RQGNN outperforms the best rival by 6.74% in Macro-F1 score and 1.44% in\n",
            "AUC, demonstrating the effectiveness of our framework. Our code is available at\n",
            "https://github.com/xydong127/RQGNN.\n",
            "\n",
            "111. Title: Stable Neural Stochastic Differential Equations in Analyzing Irregular Time Series Data\n",
            "   Abstract: To protect the copyright of the 3D scene represented by the neural radiation\n",
            "field, the embedding and extraction of the neural radiation field watermark are\n",
            "considered as a pair of inverse problems of image transformations. A scheme for\n",
            "protecting the copyright of the neural radiation field is proposed using\n",
            "invertible neural network watermarking, which utilizes watermarking techniques\n",
            "for 2D images to achieve the protection of the 3D scene. The scheme embeds the\n",
            "watermark in the training image of the neural radiation field through the\n",
            "forward process in the invertible network and extracts the watermark from the\n",
            "image rendered by the neural radiation field using the inverse process to\n",
            "realize the copyright protection of both the neural radiation field and the 3D\n",
            "scene. Since the rendering process of the neural radiation field can cause the\n",
            "loss of watermark information, the scheme incorporates an image quality\n",
            "enhancement module, which utilizes a neural network to recover the rendered\n",
            "image and then extracts the watermark. The scheme embeds a watermark in each\n",
            "training image to train the neural radiation field and enables the extraction\n",
            "of watermark information from multiple viewpoints. Simulation experimental\n",
            "results demonstrate the effectiveness of the method.\n",
            "\n",
            "112. Title: Unveiling the Unseen: Identifiable Clusters in Trained Depthwise Convolutional Kernels\n",
            "   Abstract: 2-TBSG is a two-player game model which aims to find Nash equilibriums and is\n",
            "widely utilized in reinforced learning and AI. Inspired by the fact that the\n",
            "simplex method for solving the deterministic discounted Markov decision\n",
            "processes (MDPs) is strongly polynomial independent of the discounted factor,\n",
            "we are trying to answer an open problem whether there is a similar algorithm\n",
            "for 2-TBSG. We develop a simplex strategy iteration where one player updates\n",
            "its strategy with a simplex step while the other player finds an optimal\n",
            "counterstrategy in turn, and a modified simplex strategy iteration. Both of\n",
            "them belong to a class of geometrically converging algorithms. We establish the\n",
            "strongly polynomial property of these algorithms by considering a strategy\n",
            "combined from the current strategy and the equilibrium strategy. Moreover, we\n",
            "present a method to transform general 2-TBSGs into special 2-TBSGs where each\n",
            "state has exactly two actions.\n",
            "\n",
            "113. Title: Learning from Sparse Offline Datasets via Conservative Density Estimation\n",
            "   Abstract: Irregular sampling intervals and missing values in real-world time series\n",
            "data present challenges for conventional methods that assume consistent\n",
            "intervals and complete data. Neural Ordinary Differential Equations (Neural\n",
            "ODEs) offer an alternative approach, utilizing neural networks combined with\n",
            "ODE solvers to learn continuous latent representations through parameterized\n",
            "vector fields. Neural Stochastic Differential Equations (Neural SDEs) extend\n",
            "Neural ODEs by incorporating a diffusion term, although this addition is not\n",
            "trivial, particularly when addressing irregular intervals and missing values.\n",
            "Consequently, careful design of drift and diffusion functions is crucial for\n",
            "maintaining stability and enhancing performance, while incautious choices can\n",
            "result in adverse properties such as the absence of strong solutions,\n",
            "stochastic destabilization, or unstable Euler discretizations, significantly\n",
            "affecting Neural SDEs' performance. In this study, we propose three stable\n",
            "classes of Neural SDEs: Langevin-type SDE, Linear Noise SDE, and Geometric SDE.\n",
            "Then, we rigorously demonstrate their robustness in maintaining excellent\n",
            "performance under distribution shift, while effectively preventing overfitting.\n",
            "To assess the effectiveness of our approach, we conduct extensive experiments\n",
            "on four benchmark datasets for interpolation, forecasting, and classification\n",
            "tasks, and analyze the robustness of our methods with 30 public datasets under\n",
            "different missing rates. Our results demonstrate the efficacy of the proposed\n",
            "method in handling real-world irregular time series data.\n",
            "\n",
            "114. Title: Llemma: An Open Language Model for Mathematics\n",
            "   Abstract: Foundation models, pre-trained on a large amount of data have demonstrated\n",
            "impressive zero-shot capabilities in various downstream tasks. However, in\n",
            "object detection and instance segmentation, two fundamental computer vision\n",
            "tasks heavily reliant on extensive human annotations, foundation models such as\n",
            "SAM and DINO struggle to achieve satisfactory performance. In this study, we\n",
            "reveal that the devil is in the object boundary, \\textit{i.e.}, these\n",
            "foundation models fail to discern boundaries between individual objects. For\n",
            "the first time, we probe that CLIP, which has never accessed any instance-level\n",
            "annotations, can provide a highly beneficial and strong instance-level boundary\n",
            "prior in the clustering results of its particular intermediate layer. Following\n",
            "this surprising observation, we propose $\\textbf{Zip}$ which $\\textbf{Z}$ips up\n",
            "CL$\\textbf{ip}$ and SAM in a novel classification-first-then-discovery\n",
            "pipeline, enabling annotation-free, complex-scene-capable, open-vocabulary\n",
            "object detection and instance segmentation. Our Zip significantly boosts SAM's\n",
            "mask AP on COCO dataset by 12.5% and establishes state-of-the-art performance\n",
            "in various settings, including training-free, self-training, and\n",
            "label-efficient finetuning. Furthermore, annotation-free Zip even achieves\n",
            "comparable performance to the best-performing open-vocabulary object detecters\n",
            "using base annotations. Code is released at\n",
            "https://github.com/ChengShiest/Zip-Your-CLIP\n",
            "\n",
            "115. Title: NfgTransformer: Equivariant Representation Learning for Normal-form Games\n",
            "   Abstract: Recent advances in depthwise-separable convolutional neural networks\n",
            "(DS-CNNs) have led to novel architectures, that surpass the performance of\n",
            "classical CNNs, by a considerable scalability and accuracy margin. This paper\n",
            "reveals another striking property of DS-CNN architectures: discernible and\n",
            "explainable patterns emerge in their trained depthwise convolutional kernels in\n",
            "all layers. Through an extensive analysis of millions of trained filters, with\n",
            "different sizes and from various models, we employed unsupervised clustering\n",
            "with autoencoders, to categorize these filters. Astonishingly, the patterns\n",
            "converged into a few main clusters, each resembling the difference of Gaussian\n",
            "(DoG) functions, and their first and second-order derivatives. Notably, we were\n",
            "able to classify over 95\\% and 90\\% of the filters from state-of-the-art\n",
            "ConvNextV2 and ConvNeXt models, respectively. This finding is not merely a\n",
            "technological curiosity; it echoes the foundational models neuroscientists have\n",
            "long proposed for the vision systems of mammals. Our results thus deepen our\n",
            "understanding of the emergent properties of trained DS-CNNs and provide a\n",
            "bridge between artificial and biological visual processing systems. More\n",
            "broadly, they pave the way for more interpretable and biologically-inspired\n",
            "neural network designs in the future.\n",
            "\n",
            "116. Title: Going Beyond Neural Network Feature Similarity: The Network Feature Complexity and Its Interpretation Using Category Theory\n",
            "   Abstract: Indirect experiments provide a valuable framework for estimating treatment\n",
            "effects in situations where conducting randomized control trials (RCTs) is\n",
            "impractical or unethical. Unlike RCTs, indirect experiments estimate treatment\n",
            "effects by leveraging (conditional) instrumental variables, enabling estimation\n",
            "through encouragement and recommendation rather than strict treatment\n",
            "assignment. However, the sample efficiency of such estimators depends not only\n",
            "on the inherent variability in outcomes but also on the varying compliance\n",
            "levels of users with the instrumental variables and the choice of estimator\n",
            "being used, especially when dealing with numerous instrumental variables. While\n",
            "adaptive experiment design has a rich literature for direct experiments, in\n",
            "this paper we take the initial steps towards enhancing sample efficiency for\n",
            "indirect experiments by adaptively designing a data collection policy over\n",
            "instrumental variables. Our main contribution is a practical computational\n",
            "procedure that utilizes influence functions to search for an optimal data\n",
            "collection policy, minimizing the mean-squared error of the desired\n",
            "(non-linear) estimator. Through experiments conducted in various domains\n",
            "inspired by real-world applications, we showcase how our method can\n",
            "significantly improve the sample efficiency of indirect experiments.\n",
            "\n",
            "117. Title: Language Model Detectors Are Easily Optimized Against\n",
            "   Abstract: We introduce the concept of scalable neural network kernels (SNNKs), the\n",
            "replacements of regular feedforward layers (FFLs), capable of approximating the\n",
            "latter, but with favorable computational properties. SNNKs effectively\n",
            "disentangle the inputs from the parameters of the neural network in the FFL,\n",
            "only to connect them in the final computation via the dot-product kernel. They\n",
            "are also strictly more expressive, as allowing to model complicated\n",
            "relationships beyond the functions of the dot-products of parameter-input\n",
            "vectors. We also introduce the neural network bundling process that applies\n",
            "SNNKs to compactify deep neural network architectures, resulting in additional\n",
            "compression gains. In its extreme version, it leads to the fully bundled\n",
            "network whose optimal parameters can be expressed via explicit formulae for\n",
            "several loss functions (e.g. mean squared error), opening a possibility to\n",
            "bypass backpropagation. As a by-product of our analysis, we introduce the\n",
            "mechanism of the universal random features (or URFs), applied to instantiate\n",
            "several SNNK variants, and interesting on its own in the context of scalable\n",
            "kernel methods. We provide rigorous theoretical analysis of all these concepts\n",
            "as well as an extensive empirical evaluation, ranging from point-wise kernel\n",
            "estimation to Transformers' fine-tuning with novel adapter layers inspired by\n",
            "SNNKs. Our mechanism provides up to 5x reduction in the number of trainable\n",
            "parameters, while maintaining competitive accuracy.\n",
            "\n",
            "118. Title: The Hedgehog & the Porcupine: Expressive Linear Attentions with Softmax Mimicry\n",
            "   Abstract: The behavior of neural networks still remains opaque, and a recently widely\n",
            "noted phenomenon is that networks often achieve similar performance when\n",
            "initialized with different random parameters. This phenomenon has attracted\n",
            "significant attention in measuring the similarity between features learned by\n",
            "distinct networks. However, feature similarity could be vague in describing the\n",
            "same feature since equivalent features hardly exist. In this paper, we expand\n",
            "the concept of equivalent feature and provide the definition of what we call\n",
            "functionally equivalent features. These features produce equivalent output\n",
            "under certain transformations. Using this definition, we aim to derive a more\n",
            "intrinsic metric for the so-called feature complexity regarding the redundancy\n",
            "of features learned by a neural network at each layer. We offer a formal\n",
            "interpretation of our approach through the lens of category theory, a\n",
            "well-developed area in mathematics. To quantify the feature complexity, we\n",
            "further propose an efficient algorithm named Iterative Feature Merging. Our\n",
            "experimental results validate our ideas and theories from various perspectives.\n",
            "We empirically demonstrate that the functionally equivalence widely exists\n",
            "among different features learned by the same neural network and we could reduce\n",
            "the number of parameters of the network without affecting the performance.The\n",
            "IFM shows great potential as a data-agnostic model prune method. We have also\n",
            "drawn several interesting empirical findings regarding the defined feature\n",
            "complexity.\n",
            "\n",
            "119. Title: Diffusion-TS: Interpretable Diffusion for General Time Series Generation\n",
            "   Abstract: Realistic simulation is critical for applications ranging from robotics to\n",
            "animation. Traditional analytic simulators sometimes struggle to capture\n",
            "sufficiently realistic simulation which can lead to problems including the well\n",
            "known \"sim-to-real\" gap in robotics. Learned simulators have emerged as an\n",
            "alternative for better capturing real-world physical dynamics, but require\n",
            "access to privileged ground truth physics information such as precise object\n",
            "geometry or particle tracks. Here we propose a method for learning simulators\n",
            "directly from observations. Visual Particle Dynamics (VPD) jointly learns a\n",
            "latent particle-based representation of 3D scenes, a neural simulator of the\n",
            "latent particle dynamics, and a renderer that can produce images of the scene\n",
            "from arbitrary views. VPD learns end to end from posed RGB-D videos and does\n",
            "not require access to privileged information. Unlike existing 2D video\n",
            "prediction models, we show that VPD's 3D structure enables scene editing and\n",
            "long-term predictions. These results pave the way for downstream applications\n",
            "ranging from video editing to robotic planning.\n",
            "\n",
            "120. Title: Scalable Neural Network Kernels\n",
            "   Abstract: Linear attentions have shown potential for improving Transformer efficiency,\n",
            "reducing attention's quadratic complexity to linear in sequence length. This\n",
            "holds exciting promise for (1) training linear Transformers from scratch, (2)\n",
            "\"finetuned-conversion\" of task-specific Transformers into linear versions that\n",
            "recover task performance, and (3) \"pretrained-conversion\" of Transformers such\n",
            "as large language models into linear versions finetunable on downstream tasks.\n",
            "However, linear attentions often underperform standard softmax attention in\n",
            "quality. To close this performance gap, we find prior linear attentions lack\n",
            "key properties of softmax attention tied to good performance: low-entropy (or\n",
            "\"spiky\") weights and dot-product monotonicity. We further observe surprisingly\n",
            "simple feature maps that retain these properties and match softmax performance,\n",
            "but are inefficient to compute in linear attention. We thus propose Hedgehog, a\n",
            "learnable linear attention that retains the spiky and monotonic properties of\n",
            "softmax attention while maintaining linear complexity. Hedgehog uses simple\n",
            "trainable MLPs to produce attention weights mimicking softmax attention.\n",
            "Experiments show Hedgehog recovers over 99% of standard Transformer quality in\n",
            "train-from-scratch and finetuned-conversion settings, outperforming prior\n",
            "linear attentions up to 6 perplexity points on WikiText-103 with causal GPTs,\n",
            "and up to 8.7 GLUE score points on finetuned bidirectional BERTs. Hedgehog also\n",
            "enables pretrained-conversion. Converting a pretrained GPT-2 into a linear\n",
            "attention variant achieves state-of-the-art 16.7 perplexity on WikiText-103 for\n",
            "125M subquadratic decoder models. We finally turn a pretrained Llama-2 7B into\n",
            "a viable linear attention Llama. With low-rank adaptation, Hedgehog-Llama2 7B\n",
            "achieves 28.1 higher ROUGE-1 points over the base standard attention model,\n",
            "where prior linear attentions lead to 16.5 point drops.\n",
            "\n",
            "121. Title: Pessimistic Nonlinear Least-Squares Value Iteration for Offline Reinforcement Learning\n",
            "   Abstract: Prediction sets capture uncertainty by predicting sets of labels rather than\n",
            "individual labels, enabling downstream decisions to conservatively account for\n",
            "all plausible outcomes. Conformal inference algorithms construct prediction\n",
            "sets guaranteed to contain the true label with high probability. These\n",
            "guarantees fail to hold in the face of distribution shift, which is precisely\n",
            "when reliable uncertainty quantification can be most useful. We propose a novel\n",
            "algorithm for constructing prediction sets with PAC guarantees in the label\n",
            "shift setting. This method estimates the predicted probabilities of the classes\n",
            "in a target domain, as well as the confusion matrix, then propagates\n",
            "uncertainty in these estimates through a Gaussian elimination algorithm to\n",
            "compute confidence intervals for importance weights. Finally, it uses these\n",
            "intervals to construct prediction sets. We evaluate our approach on five\n",
            "datasets: the CIFAR-10, ChestX-Ray and Entity-13 image datasets, the tabular\n",
            "CDC Heart dataset, and the AGNews text dataset. Our algorithm satisfies the PAC\n",
            "guarantee while producing smaller, more informative, prediction sets compared\n",
            "to several baselines.\n",
            "\n",
            "122. Title: DREAM: Dual Structured Exploration with Mixup for Open-set Graph Domain Adaption\n",
            "   Abstract: Unsupervised domain translation (UDT) aims to find functions that convert\n",
            "samples from one domain (e.g., sketches) to another domain (e.g., photos)\n",
            "without changing the high-level semantic meaning (also referred to as\n",
            "``content''). The translation functions are often sought by probability\n",
            "distribution matching of the transformed source domain and target domain.\n",
            "CycleGAN stands as arguably the most representative approach among this line of\n",
            "work. However, it was noticed in the literature that CycleGAN and variants\n",
            "could fail to identify the desired translation functions and produce\n",
            "content-misaligned translations. This limitation arises due to the presence of\n",
            "multiple translation functions -- referred to as ``measure-preserving\n",
            "automorphism\" (MPA) -- in the solution space of the learning criteria. Despite\n",
            "awareness of such identifiability issues, solutions have remained elusive. This\n",
            "study delves into the core identifiability inquiry and introduces an MPA\n",
            "elimination theory. Our analysis shows that MPA is unlikely to exist, if\n",
            "multiple pairs of diverse cross-domain conditional distributions are matched by\n",
            "the learning function. Our theory leads to a UDT learner using distribution\n",
            "matching over auxiliary variable-induced subsets of the domains -- other than\n",
            "over the entire data domains as in the classical approaches. The proposed\n",
            "framework is the first to rigorously establish translation identifiability\n",
            "under reasonable UDT settings, to our best knowledge. Experiments corroborate\n",
            "with our theoretical claims.\n",
            "\n",
            "123. Title: Learning Delays in Spiking Neural Networks using Dilated Convolutions with Learnable Spacings\n",
            "   Abstract: Selection bias in recommender system arises from the recommendation process\n",
            "of system filtering and the interactive process of user selection. Many\n",
            "previous studies have focused on addressing selection bias to achieve unbiased\n",
            "learning of the prediction model, but ignore the fact that potential outcomes\n",
            "for a given user-item pair may vary with the treatments assigned to other\n",
            "user-item pairs, named neighborhood effect. To fill the gap, this paper\n",
            "formally formulates the neighborhood effect as an interference problem from the\n",
            "perspective of causal inference and introduces a treatment representation to\n",
            "capture the neighborhood effect. On this basis, we propose a novel ideal loss\n",
            "that can be used to deal with selection bias in the presence of neighborhood\n",
            "effect. We further develop two new estimators for estimating the proposed ideal\n",
            "loss. We theoretically establish the connection between the proposed and\n",
            "previous debiasing methods ignoring the neighborhood effect, showing that the\n",
            "proposed methods can achieve unbiased learning when both selection bias and\n",
            "neighborhood effect are present, while the existing methods are biased.\n",
            "Extensive semi-synthetic and real-world experiments are conducted to\n",
            "demonstrate the effectiveness of the proposed methods.\n",
            "\n",
            "124. Title: Learning 3D Particle-based Simulators from RGB-D Videos\n",
            "   Abstract: Despite the widespread availability of LLMs, there remains a substantial gap\n",
            "in their capabilities and availability across diverse languages. One approach\n",
            "to address these issues has been to take an existing pre-trained LLM and\n",
            "continue to train it on new languages. While prior works have experimented with\n",
            "language adaptation, many questions around best practices and methodology have\n",
            "not been covered. In this paper, we present a comprehensive investigation into\n",
            "the adaptation of LLMs to new languages. Our study covers the key components in\n",
            "this process, including vocabulary extension, direct preference optimization\n",
            "and the data scarcity problem for human alignment in low-resource languages. We\n",
            "scale these experiments across 9 languages and 2 parameter scales (7B and 70B).\n",
            "We compare our models against Llama 2, Aya-101, XGLM, BLOOM and existing\n",
            "language experts, outperforming all prior published baselines. Additionally,\n",
            "all evaluation code and checkpoints are made public to facilitate future\n",
            "research.\n",
            "\n",
            "125. Title: PAC Prediction Sets Under Label Shift\n",
            "   Abstract: Offline reinforcement learning (RL), where the agent aims to learn the\n",
            "optimal policy based on the data collected by a behavior policy, has attracted\n",
            "increasing attention in recent years. While offline RL with linear function\n",
            "approximation has been extensively studied with optimal results achieved under\n",
            "certain assumptions, many works shift their interest to offline RL with\n",
            "non-linear function approximation. However, limited works on offline RL with\n",
            "non-linear function approximation have instance-dependent regret guarantees. In\n",
            "this paper, we propose an oracle-efficient algorithm, dubbed Pessimistic\n",
            "Nonlinear Least-Square Value Iteration (PNLSVI), for offline RL with non-linear\n",
            "function approximation. Our algorithmic design comprises three innovative\n",
            "components: (1) a variance-based weighted regression scheme that can be applied\n",
            "to a wide range of function classes, (2) a subroutine for variance estimation,\n",
            "and (3) a planning phase that utilizes a pessimistic value iteration approach.\n",
            "Our algorithm enjoys a regret bound that has a tight dependency on the function\n",
            "class complexity and achieves minimax optimal instance-dependent regret when\n",
            "specialized to linear function approximation. Our work extends the previous\n",
            "instance-dependent results within simpler function classes, such as linear and\n",
            "differentiable function to a more general framework.\n",
            "\n",
            "126. Title: Space and time continuous physics simulation from partial observations\n",
            "   Abstract: Spiking Neural Networks (SNNs) are a promising research direction for\n",
            "building power-efficient information processing systems, especially for\n",
            "temporal tasks such as speech recognition. In SNNs, delays refer to the time\n",
            "needed for one spike to travel from one neuron to another. These delays matter\n",
            "because they influence the spike arrival times, and it is well-known that\n",
            "spiking neurons respond more strongly to coincident input spikes. More\n",
            "formally, it has been shown theoretically that plastic delays greatly increase\n",
            "the expressivity in SNNs. Yet, efficient algorithms to learn these delays have\n",
            "been lacking. Here, we propose a new discrete-time algorithm that addresses\n",
            "this issue in deep feedforward SNNs using backpropagation, in an offline\n",
            "manner. To simulate delays between consecutive layers, we use 1D convolutions\n",
            "across time. The kernels contain only a few non-zero weights - one per synapse\n",
            "- whose positions correspond to the delays. These positions are learned\n",
            "together with the weights using the recently proposed Dilated Convolution with\n",
            "Learnable Spacings (DCLS). We evaluated our method on three datasets: the\n",
            "Spiking Heidelberg Dataset (SHD), the Spiking Speech Commands (SSC) and its\n",
            "non-spiking version Google Speech Commands v0.02 (GSC) benchmarks, which\n",
            "require detecting temporal patterns. We used feedforward SNNs with two or three\n",
            "hidden fully connected layers, and vanilla leaky integrate-and-fire neurons. We\n",
            "showed that fixed random delays help and that learning them helps even more.\n",
            "Furthermore, our method outperformed the state-of-the-art in the three datasets\n",
            "without using recurrent connections and with substantially fewer parameters.\n",
            "Our work demonstrates the potential of delay learning in developing accurate\n",
            "and precise models for temporal data processing. Our code is based on PyTorch /\n",
            "SpikingJelly and available at: https://github.com/Thvnvtos/SNN-delays\n",
            "\n",
            "127. Title: Beyond Vanilla Variational Autoencoders: Detecting Posterior Collapse in Conditional and Hierarchical Variational Autoencoders\n",
            "   Abstract: The future of machine learning lies in moving data collection along with\n",
            "training to the edge. Federated Learning, for short FL, has been recently\n",
            "proposed to achieve this goal. The principle of this approach is to aggregate\n",
            "models learned over a large number of distributed clients, i.e.,\n",
            "resource-constrained mobile devices that collect data from their environment,\n",
            "to obtain a new more general model. The latter is subsequently redistributed to\n",
            "clients for further training. A key feature that distinguishes federated\n",
            "learning from data-center-based distributed training is the inherent\n",
            "heterogeneity. In this work, we introduce and analyse a novel aggregation\n",
            "framework that allows for formalizing and tackling computational heterogeneity\n",
            "in federated optimization, in terms of both heterogeneous data and local\n",
            "updates. Proposed aggregation algorithms are extensively analyzed from a\n",
            "theoretical, and an experimental prospective.\n",
            "\n",
            "128. Title: Habitat 3.0: A Co-Habitat for Humans, Avatars, and Robots\n",
            "   Abstract: Modern techniques for physical simulations rely on numerical schemes and\n",
            "mesh-refinement methods to address trade-offs between precision and complexity,\n",
            "but these handcrafted solutions are tedious and require high computational\n",
            "power. Data-driven methods based on large-scale machine learning promise high\n",
            "adaptivity by integrating long-range dependencies more directly and\n",
            "efficiently. In this work, we focus on fluid dynamics and address the\n",
            "shortcomings of a large part of the literature, which are based on fixed\n",
            "support for computations and predictions in the form of regular or irregular\n",
            "grids. We propose a novel setup to perform predictions in a continuous spatial\n",
            "and temporal domain while being trained on sparse observations. We formulate\n",
            "the task as a double observation problem and propose a solution with two\n",
            "interlinked dynamical systems defined on, respectively, the sparse positions\n",
            "and the continuous domain, which allows to forecast and interpolate a solution\n",
            "from the initial condition. Our practical implementation involves recurrent\n",
            "GNNs and a spatio-temporal attention observer capable of interpolating the\n",
            "solution at arbitrary locations. Our model not only generalizes to new initial\n",
            "conditions (as standard auto-regressive models do) but also performs evaluation\n",
            "at arbitrary space and time locations. We evaluate on three standard datasets\n",
            "in fluid dynamics and compare to strong baselines, which are outperformed both\n",
            "in classical settings and in the extended new task requiring continuous\n",
            "predictions.\n",
            "\n",
            "129. Title: Be Aware of the Neighborhood Effect: Modeling Selection Bias under Interference\n",
            "   Abstract: The posterior collapse phenomenon in variational autoencoder (VAE), where the\n",
            "variational posterior distribution closely matches the prior distribution, can\n",
            "hinder the quality of the learned latent variables. As a consequence of\n",
            "posterior collapse, the latent variables extracted by the encoder in VAE\n",
            "preserve less information from the input data and thus fail to produce\n",
            "meaningful representations as input to the reconstruction process in the\n",
            "decoder. While this phenomenon has been an actively addressed topic related to\n",
            "VAE performance, the theory for posterior collapse remains underdeveloped,\n",
            "especially beyond the standard VAE. In this work, we advance the theoretical\n",
            "understanding of posterior collapse to two important and prevalent yet less\n",
            "studied classes of VAE: conditional VAE and hierarchical VAE. Specifically, via\n",
            "a non-trivial theoretical analysis of linear conditional VAE and hierarchical\n",
            "VAE with two levels of latent, we prove that the cause of posterior collapses\n",
            "in these models includes the correlation between the input and output of the\n",
            "conditional VAE and the effect of learnable encoder variance in the\n",
            "hierarchical VAE. We empirically validate our theoretical findings for linear\n",
            "conditional and hierarchical VAE and demonstrate that these results are also\n",
            "predictive for non-linear cases with extensive experiments.\n",
            "\n",
            "130. Title: Towards Identifiable Unsupervised Domain Translation: A Diversified Distribution Matching Approach\n",
            "   Abstract: We present Habitat 3.0: a simulation platform for studying collaborative\n",
            "human-robot tasks in home environments. Habitat 3.0 offers contributions across\n",
            "three dimensions: (1) Accurate humanoid simulation: addressing challenges in\n",
            "modeling complex deformable bodies and diversity in appearance and motion, all\n",
            "while ensuring high simulation speed. (2) Human-in-the-loop infrastructure:\n",
            "enabling real human interaction with simulated robots via mouse/keyboard or a\n",
            "VR interface, facilitating evaluation of robot policies with human input. (3)\n",
            "Collaborative tasks: studying two collaborative tasks, Social Navigation and\n",
            "Social Rearrangement. Social Navigation investigates a robot's ability to\n",
            "locate and follow humanoid avatars in unseen environments, whereas Social\n",
            "Rearrangement addresses collaboration between a humanoid and robot while\n",
            "rearranging a scene. These contributions allow us to study end-to-end learned\n",
            "and heuristic baselines for human-robot collaboration in-depth, as well as\n",
            "evaluate them with humans in the loop. Our experiments demonstrate that learned\n",
            "robot policies lead to efficient task completion when collaborating with unseen\n",
            "humanoid agents and human partners that might exhibit behaviors that the robot\n",
            "has not seen before. Additionally, we observe emergent behaviors during\n",
            "collaborative task execution, such as the robot yielding space when obstructing\n",
            "a humanoid agent, thereby allowing the effective completion of the task by the\n",
            "humanoid agent. Furthermore, our experiments using the human-in-the-loop tool\n",
            "demonstrate that our automated evaluation with humanoids can provide an\n",
            "indication of the relative ordering of different policies when evaluated with\n",
            "real human collaborators. Habitat 3.0 unlocks interesting new features in\n",
            "simulators for Embodied AI, and we hope it paves the way for a new frontier of\n",
            "embodied human-AI interaction capabilities.\n",
            "\n",
            "131. Title: How to Catch an AI Liar: Lie Detection in Black-Box LLMs by Asking Unrelated Questions\n",
            "   Abstract: Contrastive Language-Image Pre-training (CLIP) is an approach that has\n",
            "advanced research and applications in computer vision, fueling modern\n",
            "recognition systems and generative models. We believe that the main ingredient\n",
            "to the success of CLIP is its data and not the model architecture or\n",
            "pre-training objective. However, CLIP only provides very limited information\n",
            "about its data and how it has been collected, leading to works that aim to\n",
            "reproduce CLIP's data by filtering with its model parameters. In this work, we\n",
            "intend to reveal CLIP's data curation approach and in our pursuit of making it\n",
            "open to the community introduce Metadata-Curated Language-Image Pre-training\n",
            "(MetaCLIP). MetaCLIP takes a raw data pool and metadata (derived from CLIP's\n",
            "concepts) and yields a balanced subset over the metadata distribution. Our\n",
            "experimental study rigorously isolates the model and training settings,\n",
            "concentrating solely on data. MetaCLIP applied to CommonCrawl with 400M\n",
            "image-text data pairs outperforms CLIP's data on multiple standard benchmarks.\n",
            "In zero-shot ImageNet classification, MetaCLIP achieves 70.8% accuracy,\n",
            "surpassing CLIP's 68.3% on ViT-B models. Scaling to 1B data, while maintaining\n",
            "the same training budget, attains 72.4%. Our observations hold across various\n",
            "model sizes, exemplified by ViT-H achieving 80.5%, without any\n",
            "bells-and-whistles. Curation code and training data distribution on metadata is\n",
            "made available at https://github.com/facebookresearch/MetaCLIP.\n",
            "\n",
            "132. Title: Demystifying CLIP Data\n",
            "   Abstract: Large language models (LLMs) can \"lie\", which we define as outputting false\n",
            "statements despite \"knowing\" the truth in a demonstrable sense. LLMs might\n",
            "\"lie\", for example, when instructed to output misinformation. Here, we develop\n",
            "a simple lie detector that requires neither access to the LLM's activations\n",
            "(black-box) nor ground-truth knowledge of the fact in question. The detector\n",
            "works by asking a predefined set of unrelated follow-up questions after a\n",
            "suspected lie, and feeding the LLM's yes/no answers into a logistic regression\n",
            "classifier. Despite its simplicity, this lie detector is highly accurate and\n",
            "surprisingly general. When trained on examples from a single setting --\n",
            "prompting GPT-3.5 to lie about factual questions -- the detector generalises\n",
            "out-of-distribution to (1) other LLM architectures, (2) LLMs fine-tuned to lie,\n",
            "(3) sycophantic lies, and (4) lies emerging in real-life scenarios such as\n",
            "sales. These results indicate that LLMs have distinctive lie-related\n",
            "behavioural patterns, consistent across architectures and contexts, which could\n",
            "enable general-purpose lie detection.\n",
            "\n",
            "133. Title: Image2Sentence based Asymmetrical Zero-shot Composed Image Retrieval\n",
            "   Abstract: The task of composed image retrieval (CIR) aims to retrieve images based on\n",
            "the query image and the text describing the users' intent. Existing methods\n",
            "have made great progress with the advanced large vision-language (VL) model in\n",
            "CIR task, however, they generally suffer from two main issues: lack of labeled\n",
            "triplets for model training and difficulty of deployment on resource-restricted\n",
            "environments when deploying the large vision-language model. To tackle the\n",
            "above problems, we propose Image2Sentence based Asymmetric zero-shot composed\n",
            "image retrieval (ISA), which takes advantage of the VL model and only relies on\n",
            "unlabeled images for composition learning. In the framework, we propose a new\n",
            "adaptive token learner that maps an image to a sentence in the word embedding\n",
            "space of VL model. The sentence adaptively captures discriminative visual\n",
            "information and is further integrated with the text modifier. An asymmetric\n",
            "structure is devised for flexible deployment, in which the lightweight model is\n",
            "adopted for the query side while the large VL model is deployed on the gallery\n",
            "side. The global contrastive distillation and the local alignment\n",
            "regularization are adopted for the alignment between the light model and the VL\n",
            "model for CIR task. Our experiments demonstrate that the proposed ISA could\n",
            "better cope with the real retrieval scenarios and further improve retrieval\n",
            "accuracy and efficiency.\n",
            "\n",
            "134. Title: Interpreting CLIP's Image Representation via Text-Based Decomposition\n",
            "   Abstract: In this paper, we propose a new data poisoning attack and apply it to deep\n",
            "reinforcement learning agents. Our attack centers on what we call\n",
            "in-distribution triggers, which are triggers native to the data distributions\n",
            "the model will be trained on and deployed in. We outline a simple procedure for\n",
            "embedding these, and other, triggers in deep reinforcement learning agents\n",
            "following a multi-task learning paradigm, and demonstrate in three common\n",
            "reinforcement learning environments. We believe that this work has important\n",
            "implications for the security of deep learning models.\n",
            "\n",
            "135. Title: Physics-Regulated Deep Reinforcement Learning: Invariant Embeddings\n",
            "   Abstract: We investigate the CLIP image encoder by analyzing how individual model\n",
            "components affect the final representation. We decompose the image\n",
            "representation as a sum across individual image patches, model layers, and\n",
            "attention heads, and use CLIP's text representation to interpret the summands.\n",
            "Interpreting the attention heads, we characterize each head's role by\n",
            "automatically finding text representations that span its output space, which\n",
            "reveals property-specific roles for many heads (e.g. location or shape). Next,\n",
            "interpreting the image patches, we uncover an emergent spatial localization\n",
            "within CLIP. Finally, we use this understanding to remove spurious features\n",
            "from CLIP and to create a strong zero-shot image segmenter. Our results\n",
            "indicate that a scalable understanding of transformer models is attainable and\n",
            "can be used to repair and improve models.\n",
            "\n",
            "136. Title: The Effective Horizon Explains Deep RL Performance in Stochastic Environments\n",
            "   Abstract: Reinforcement learning (RL) theory has largely focused on proving minimax\n",
            "sample complexity bounds. These require strategic exploration algorithms that\n",
            "use relatively limited function classes for representing the policy or value\n",
            "function. Our goal is to explain why deep RL algorithms often perform well in\n",
            "practice, despite using random exploration and much more expressive function\n",
            "classes like neural networks. Our work arrives at an explanation by showing\n",
            "that many stochastic MDPs can be solved by performing only a few steps of value\n",
            "iteration on the random policy's Q function and then acting greedily. When this\n",
            "is true, we find that it is possible to separate the exploration and learning\n",
            "components of RL, making it much easier to analyze. We introduce a new RL\n",
            "algorithm, SQIRL, that iteratively learns a near-optimal policy by exploring\n",
            "randomly to collect rollouts and then performing a limited number of steps of\n",
            "fitted-Q iteration over those rollouts. Any regression algorithm that satisfies\n",
            "basic in-distribution generalization properties can be used in SQIRL to\n",
            "efficiently solve common MDPs. This can explain why deep RL works, since it is\n",
            "empirically established that neural networks generalize well in-distribution.\n",
            "Furthermore, SQIRL explains why random exploration works well in practice. We\n",
            "leverage SQIRL to derive instance-dependent sample complexity bounds for RL\n",
            "that are exponential only in an \"effective horizon\" of lookahead and on the\n",
            "complexity of the class used for function approximation. Empirically, we also\n",
            "find that SQIRL performance strongly correlates with PPO and DQN performance in\n",
            "a variety of stochastic environments, supporting that our theoretical analysis\n",
            "is predictive of practical performance. Our code and data are available at\n",
            "https://github.com/cassidylaidlaw/effective-horizon.\n",
            "\n",
            "137. Title: Multiscale Positive-Unlabeled Detection of AI-Generated Texts\n",
            "   Abstract: The Braess paradox is a counter-intuitive phenomenon whereby adding roads to\n",
            "a network results in higher travel time at equilibrium. In this paper we\n",
            "present an algorithm to detect the occurrence of this paradox in real-world\n",
            "networks with the help of an improved graph representation accounting for\n",
            "queues. The addition of queues to the network representation enables a closer\n",
            "match with real data. Moreover, we search for routes causing this phenomenon\n",
            "(\"Braess routes\") rather than links, and advocate removing such routes\n",
            "virtually from navigation systems so that the associated links can continue to\n",
            "serve other routes. Our algorithm relies on a convex optimization problem\n",
            "utilizing Beckmann potentials for road links as well as queues, and results in\n",
            "a route reconfiguration with reduced delay. We assume the availability of\n",
            "historical data to build the optimization model. We also assume the existence\n",
            "of a centralized navigation system to manage the routing options and remove the\n",
            "Braess routes. The theoretical solution demonstrates up to 12% delay reduction\n",
            "in a network from Montgomery County, Maryland. We validate the improvement with\n",
            "simulations.\n",
            "\n",
            "138. Title: PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization\n",
            "   Abstract: Recent releases of Large Language Models (LLMs), e.g. ChatGPT, are\n",
            "astonishing at generating human-like texts, but they may impact the\n",
            "authenticity of texts. Previous works proposed methods to detect these\n",
            "AI-generated texts, including simple ML classifiers, pretrained-model-based\n",
            "zero-shot methods, and finetuned language classification models. However,\n",
            "mainstream detectors always fail on short texts, like SMSes, Tweets, and\n",
            "reviews. In this paper, a Multiscale Positive-Unlabeled (MPU) training\n",
            "framework is proposed to address the difficulty of short-text detection without\n",
            "sacrificing long-texts. Firstly, we acknowledge the human-resemblance property\n",
            "of short machine texts, and rephrase AI text detection as a partial\n",
            "Positive-Unlabeled (PU) problem by regarding these short machine texts as\n",
            "partially ``unlabeled\". Then in this PU context, we propose the\n",
            "length-sensitive Multiscale PU Loss, where a recurrent model in abstraction is\n",
            "used to estimate positive priors of scale-variant corpora. Additionally, we\n",
            "introduce a Text Multiscaling module to enrich training corpora. Experiments\n",
            "show that our MPU method augments detection performance on long AI-generated\n",
            "texts, and significantly improves short-text detection of language model\n",
            "detectors. Language Models trained with MPU could outcompete existing detectors\n",
            "on various short-text and long-text detection benchmarks. The codes are\n",
            "available at\n",
            "https://github.com/mindspore-lab/mindone/tree/master/examples/detect_chatgpt\n",
            "and https://github.com/YuchuanTian/AIGC_text_detector.\n",
            "\n",
            "139. Title: TEDDY: Trimming Edges with Degree-based Discrimination Strategy\n",
            "   Abstract: Implicit processes (IPs) are a generalization of Gaussian processes (GPs).\n",
            "IPs may lack a closed-form expression but are easy to sample from. Examples\n",
            "include, among others, Bayesian neural networks or neural samplers. IPs can be\n",
            "used as priors over functions, resulting in flexible models with\n",
            "well-calibrated prediction uncertainty estimates. Methods based on IPs usually\n",
            "carry out function-space approximate inference, which overcomes some of the\n",
            "difficulties of parameter-space approximate inference. Nevertheless, the\n",
            "approximations employed often limit the expressiveness of the final model,\n",
            "resulting, e.g., in a Gaussian predictive distribution, which can be\n",
            "restrictive. We propose here a multi-layer generalization of IPs called the\n",
            "Deep Variational Implicit process (DVIP). This generalization is similar to\n",
            "that of deep GPs over GPs, but it is more flexible due to the use of IPs as the\n",
            "prior distribution over the latent functions. We describe a scalable\n",
            "variational inference algorithm for training DVIP and show that it outperforms\n",
            "previous IP-based methods and also deep GPs. We support these claims via\n",
            "extensive regression and classification experiments. We also evaluate DVIP on\n",
            "large datasets with up to several million data instances to illustrate its good\n",
            "scalability and performance.\n",
            "\n",
            "140. Title: Learning Adaptive Multiresolution Transforms via Meta-Framelet-based Graph Convolutional Network\n",
            "   Abstract: Graph Neural Networks (GNNs) have recently caught great attention and\n",
            "achieved significant progress in graph-level applications. In this paper, we\n",
            "propose a framework for graph neural networks with multiresolution Haar-like\n",
            "wavelets, or MathNet, with interrelated convolution and pooling strategies. The\n",
            "underlying method takes graphs in different structures as input and assembles\n",
            "consistent graph representations for readout layers, which then accomplishes\n",
            "label prediction. To achieve this, the multiresolution graph representations\n",
            "are first constructed and fed into graph convolutional layers for processing.\n",
            "The hierarchical graph pooling layers are then involved to downsample graph\n",
            "resolution while simultaneously remove redundancy within graph signals. The\n",
            "whole workflow could be formed with a multi-level graph analysis, which not\n",
            "only helps embed the intrinsic topological information of each graph into the\n",
            "GNN, but also supports fast computation of forward and adjoint graph\n",
            "transforms. We show by extensive experiments that the proposed framework\n",
            "obtains notable accuracy gains on graph classification and regression tasks\n",
            "with performance stability. The proposed MathNet outperforms various existing\n",
            "GNN models, especially on big data sets.\n",
            "\n",
            "141. Title: ControlVideo: Training-free Controllable Text-to-video Generation\n",
            "   Abstract: While large language models (LLMs) often adopt finetuning to unlock their\n",
            "capabilities for downstream applications, our understanding on the inductive\n",
            "biases (especially the scaling properties) of different finetuning methods is\n",
            "still limited. To fill this gap, we conduct systematic experiments studying\n",
            "whether and how different scaling factors, including LLM model size,\n",
            "pretraining data size, new finetuning parameter size and finetuning data size,\n",
            "affect the finetuning performance. We consider two types of finetuning --\n",
            "full-model tuning (FMT) and parameter efficient tuning (PET, including prompt\n",
            "tuning and LoRA), and explore their scaling behaviors in the data-limited\n",
            "regime where the LLM model size substantially outweighs the finetuning data\n",
            "size. Based on two sets of pretrained bilingual LLMs from 1B to 16B and\n",
            "experiments on bilingual machine translation and multilingual summarization\n",
            "benchmarks, we find that 1) LLM finetuning follows a powerbased multiplicative\n",
            "joint scaling law between finetuning data size and each other scaling factor;\n",
            "2) LLM finetuning benefits more from LLM model scaling than pretraining data\n",
            "scaling, and PET parameter scaling is generally ineffective; and 3) the optimal\n",
            "finetuning method is highly task- and finetuning data-dependent. We hope our\n",
            "findings could shed light on understanding, selecting and developing LLM\n",
            "finetuning methods.\n",
            "\n",
            "142. Title: Structured Video-Language Modeling with Temporal Grouping and Spatial Grounding\n",
            "   Abstract: Convolutional Neural Networks (CNNs) are deployed in more and more\n",
            "classification systems, but adversarial samples can be maliciously crafted to\n",
            "trick them, and are becoming a real threat. There have been various proposals\n",
            "to improve CNNs' adversarial robustness but these all suffer performance\n",
            "penalties or other limitations. In this paper, we provide a new approach in the\n",
            "form of a certifiable adversarial detection scheme, the Certifiable Taboo Trap\n",
            "(CTT). The system can provide certifiable guarantees of detection of\n",
            "adversarial inputs for certain $l_{\\infty}$ sizes on a reasonable assumption,\n",
            "namely that the training data have the same distribution as the test data. We\n",
            "develop and evaluate several versions of CTT with a range of defense\n",
            "capabilities, training overheads and certifiability on adversarial samples.\n",
            "Against adversaries with various $l_p$ norms, CTT outperforms existing defense\n",
            "methods that focus purely on improving network robustness. We show that CTT has\n",
            "small false positive rates on clean test data, minimal compute overheads when\n",
            "deployed, and can support complex security policies.\n",
            "\n",
            "143. Title: Contextual Bandits with Online Neural Regression\n",
            "   Abstract: Recent works have shown a reduction from contextual bandits to online\n",
            "regression under a realizability assumption [Foster and Rakhlin, 2020, Foster\n",
            "and Krishnamurthy, 2021]. In this work, we investigate the use of neural\n",
            "networks for such online regression and associated Neural Contextual Bandits\n",
            "(NeuCBs). Using existing results for wide networks, one can readily show a\n",
            "${\\mathcal{O}}(\\sqrt{T})$ regret for online regression with square loss, which\n",
            "via the reduction implies a ${\\mathcal{O}}(\\sqrt{K} T^{3/4})$ regret for\n",
            "NeuCBs. Departing from this standard approach, we first show a\n",
            "$\\mathcal{O}(\\log T)$ regret for online regression with almost convex losses\n",
            "that satisfy QG (Quadratic Growth) condition, a generalization of the PL\n",
            "(Polyak-\\L ojasiewicz) condition, and that have a unique minima. Although not\n",
            "directly applicable to wide networks since they do not have unique minima, we\n",
            "show that adding a suitable small random perturbation to the network\n",
            "predictions surprisingly makes the loss satisfy QG with unique minima. Based on\n",
            "such a perturbed prediction, we show a ${\\mathcal{O}}(\\log T)$ regret for\n",
            "online regression with both squared loss and KL loss, and subsequently convert\n",
            "these respectively to $\\tilde{\\mathcal{O}}(\\sqrt{KT})$ and\n",
            "$\\tilde{\\mathcal{O}}(\\sqrt{KL^*} + K)$ regret for NeuCB, where $L^*$ is the\n",
            "loss of the best policy. Separately, we also show that existing regret bounds\n",
            "for NeuCBs are $\\Omega(T)$ or assume i.i.d. contexts, unlike this work.\n",
            "Finally, our experimental results on various datasets demonstrate that our\n",
            "algorithms, especially the one based on KL loss, persistently outperform\n",
            "existing algorithms.\n",
            "\n",
            "144. Title: MiniLLM: Knowledge Distillation of Large Language Models\n",
            "   Abstract: State-of-the-art visual localization approaches generally rely on a first\n",
            "image retrieval step whose role is crucial. Yet, retrieval often struggles when\n",
            "facing varying conditions, due to e.g. weather or time of day, with dramatic\n",
            "consequences on the visual localization accuracy. In this paper, we improve\n",
            "this retrieval step and tailor it to the final localization task. Among the\n",
            "several changes we advocate for, we propose to synthesize variants of the\n",
            "training set images, obtained from generative text-to-image models, in order to\n",
            "automatically expand the training set towards a number of nameable variations\n",
            "that particularly hurt visual localization. After expanding the training set,\n",
            "we propose a training approach that leverages the specificities and the\n",
            "underlying geometry of this mix of real and synthetic images. We experimentally\n",
            "show that those changes translate into large improvements for the most\n",
            "challenging visual localization datasets. Project page:\n",
            "https://europe.naverlabs.com/ret4loc\n",
            "\n",
            "145. Title: Towards Robust Offline Reinforcement Learning under Diverse Data Corruption\n",
            "   Abstract: Existing video-language pre-training methods primarily focus on\n",
            "instance-level alignment between video clips and captions via global\n",
            "contrastive learning but neglect rich fine-grained local information in both\n",
            "videos and text, which is of importance to downstream tasks requiring temporal\n",
            "localization and semantic reasoning. A powerful model is expected to be capable\n",
            "of capturing region-object correspondences and recognizing scene changes in a\n",
            "video clip, reflecting spatial and temporal granularity, respectively. To\n",
            "strengthen model's understanding into such fine-grained details, we propose a\n",
            "simple yet effective video-language modeling framework, S-ViLM, by exploiting\n",
            "the intrinsic structures of these two modalities. It includes two novel\n",
            "designs, inter-clip spatial grounding and intra-clip temporal grouping, to\n",
            "promote learning region-object alignment and temporal-aware features,\n",
            "simultaneously. Comprehensive evaluations demonstrate that S-ViLM performs\n",
            "favorably against existing approaches in learning more expressive\n",
            "representations. Specifically, S-ViLM surpasses the state-of-the-art methods\n",
            "substantially on four representative downstream tasks, covering text-video\n",
            "retrieval, video question answering, video action recognition, and temporal\n",
            "action localization.\n",
            "\n",
            "146. Title: INViTE: INterpret and Control Vision-Language Models with Text Explanations\n",
            "   Abstract: Offline reinforcement learning (RL) presents a promising approach for\n",
            "learning reinforced policies from offline datasets without the need for costly\n",
            "or unsafe interactions with the environment. However, datasets collected by\n",
            "humans in real-world environments are often noisy and may even be maliciously\n",
            "corrupted, which can significantly degrade the performance of offline RL. In\n",
            "this work, we first investigate the performance of current offline RL\n",
            "algorithms under comprehensive data corruption, including states, actions,\n",
            "rewards, and dynamics. Our extensive experiments reveal that implicit\n",
            "Q-learning (IQL) demonstrates remarkable resilience to data corruption among\n",
            "various offline RL algorithms. Furthermore, we conduct both empirical and\n",
            "theoretical analyses to understand IQL's robust performance, identifying its\n",
            "supervised policy learning scheme as the key factor. Despite its relative\n",
            "robustness, IQL still suffers from heavy-tail targets of Q functions under\n",
            "dynamics corruption. To tackle this challenge, we draw inspiration from robust\n",
            "statistics to employ the Huber loss to handle the heavy-tailedness and utilize\n",
            "quantile estimators to balance penalization for corrupted data and learning\n",
            "stability. By incorporating these simple yet effective modifications into IQL,\n",
            "we propose a more robust offline RL approach named Robust IQL (RIQL). Extensive\n",
            "experiments demonstrate that RIQL exhibits highly robust performance when\n",
            "subjected to diverse data corruption scenarios.\n",
            "\n",
            "147. Title: Some Fundamental Aspects about Lipschitz Continuity of Neural Networks\n",
            "   Abstract: In recent years, advances in the large-scale pretraining of language and\n",
            "text-to-image models have revolutionized the field of machine learning. Yet,\n",
            "integrating these two modalities into a single, robust model capable of\n",
            "generating seamless multimodal outputs remains a significant challenge. To\n",
            "address this gap, we present the Joint Autoregressive Mixture (JAM) framework,\n",
            "a modular approach that systematically fuses existing text and image generation\n",
            "models. We also introduce a specialized, data-efficient instruction-tuning\n",
            "strategy, tailored for mixed-modal generation tasks. Our final instruct-tuned\n",
            "model demonstrates unparalleled performance in generating high-quality\n",
            "multimodal outputs and represents the first model explicitly designed for this\n",
            "purpose.\n",
            "\n",
            "148. Title: Jointly Training Large Autoregressive Multimodal Models\n",
            "   Abstract: Lipschitz continuity is a crucial functional property of any predictive\n",
            "model, that naturally governs its robustness, generalisation, as well as\n",
            "adversarial vulnerability. Contrary to other works that focus on obtaining\n",
            "tighter bounds and developing different practical strategies to enforce certain\n",
            "Lipschitz properties, we aim to thoroughly examine and characterise the\n",
            "Lipschitz behaviour of Neural Networks. Thus, we carry out an empirical\n",
            "investigation in a range of different settings (namely, architectures,\n",
            "datasets, label noise, and more) by exhausting the limits of the simplest and\n",
            "the most general lower and upper bounds. As a highlight of this investigation,\n",
            "we showcase a remarkable fidelity of the lower Lipschitz bound, identify a\n",
            "striking Double Descent trend in both upper and lower bounds to the Lipschitz\n",
            "and explain the intriguing effects of label noise on function smoothness and\n",
            "generalisation.\n",
            "\n",
            "149. Title: Time-Efficient Reinforcement Learning with Stochastic Stateful Policies\n",
            "   Abstract: Transformer models have achieved remarkable results in a wide range of\n",
            "applications. However, their scalability is hampered by the quadratic time and\n",
            "memory complexity of the self-attention mechanism concerning the sequence\n",
            "length. This limitation poses a substantial obstacle when dealing with long\n",
            "documents or high-resolution images. In this work, we study the self-attention\n",
            "mechanism by analyzing the distribution of the attention matrix and its\n",
            "concentration ability. Furthermore, we propose instruments to measure these\n",
            "quantities and introduce a novel self-attention mechanism, Linear Log-Normal\n",
            "Attention, designed to emulate the distribution and concentration behavior of\n",
            "the original self-attention. Our experimental results on popular natural\n",
            "language benchmarks reveal that our proposed Linear Log-Normal Attention\n",
            "outperforms other linearized attention alternatives, offering a promising\n",
            "avenue for enhancing the scalability of transformer models.\n",
            "\n",
            "150. Title: Linear Log-Normal Attention with Unbiased Concentration\n",
            "   Abstract: Stateful policies play an important role in reinforcement learning, such as\n",
            "handling partially observable environments, enhancing robustness, or imposing\n",
            "an inductive bias directly into the policy structure. The conventional method\n",
            "for training stateful policies is Backpropagation Through Time (BPTT), which\n",
            "comes with significant drawbacks, such as slow training due to sequential\n",
            "gradient propagation and the occurrence of vanishing or exploding gradients.\n",
            "The gradient is often truncated to address these issues, resulting in a biased\n",
            "policy update. We present a novel approach for training stateful policies by\n",
            "decomposing the latter into a stochastic internal state kernel and a stateless\n",
            "policy, jointly optimized by following the stateful policy gradient. We\n",
            "introduce different versions of the stateful policy gradient theorem, enabling\n",
            "us to easily instantiate stateful variants of popular reinforcement learning\n",
            "and imitation learning algorithms. Furthermore, we provide a theoretical\n",
            "analysis of our new gradient estimator and compare it with BPTT. We evaluate\n",
            "our approach on complex continuous control tasks, e.g., humanoid locomotion,\n",
            "and demonstrate that our gradient estimator scales effectively with task\n",
            "complexity while offering a faster and simpler alternative to BPTT.\n",
            "\n",
            "151. Title: Goodhart's Law in Reinforcement Learning\n",
            "   Abstract: Implementing a reward function that perfectly captures a complex task in the\n",
            "real world is impractical. As a result, it is often appropriate to think of the\n",
            "reward function as a proxy for the true objective rather than as its\n",
            "definition. We study this phenomenon through the lens of Goodhart's law, which\n",
            "predicts that increasing optimisation of an imperfect proxy beyond some\n",
            "critical point decreases performance on the true objective. First, we propose a\n",
            "way to quantify the magnitude of this effect and show empirically that\n",
            "optimising an imperfect proxy reward often leads to the behaviour predicted by\n",
            "Goodhart's law for a wide range of environments and reward functions. We then\n",
            "provide a geometric explanation for why Goodhart's law occurs in Markov\n",
            "decision processes. We use these theoretical insights to propose an optimal\n",
            "early stopping method that provably avoids the aforementioned pitfall and\n",
            "derive theoretical regret bounds for this method. Moreover, we derive a\n",
            "training method that maximises worst-case reward, for the setting where there\n",
            "is uncertainty about the true reward function. Finally, we evaluate our early\n",
            "stopping method experimentally. Our results support a foundation for a\n",
            "theoretically-principled study of reinforcement learning under reward\n",
            "misspecification.\n",
            "\n",
            "152. Title: Whittle Index with Multiple Actions and State Constraint for Inventory Management\n",
            "   Abstract: The angular synchronization problem aims to accurately estimate (up to a\n",
            "constant additive phase) a set of unknown angles $\\theta_1, \\dots,\n",
            "\\theta_n\\in[0, 2\\pi)$ from $m$ noisy measurements of their offsets\n",
            "$\\theta_i-\\theta_j \\;\\mbox{mod} \\; 2\\pi.$ Applications include, for example,\n",
            "sensor network localization, phase retrieval, and distributed clock\n",
            "synchronization. An extension of the problem to the heterogeneous setting\n",
            "(dubbed $k$-synchronization) is to estimate $k$ groups of angles\n",
            "simultaneously, given noisy observations (with unknown group assignment) from\n",
            "each group. Existing methods for angular synchronization usually perform poorly\n",
            "in high-noise regimes, which are common in applications. In this paper, we\n",
            "leverage neural networks for the angular synchronization problem, and its\n",
            "heterogeneous extension, by proposing GNNSync, a theoretically-grounded\n",
            "end-to-end trainable framework using directed graph neural networks. In\n",
            "addition, new loss functions are devised to encode synchronization objectives.\n",
            "Experimental results on extensive data sets demonstrate that GNNSync attains\n",
            "competitive, and often superior, performance against a comprehensive set of\n",
            "baselines for the angular synchronization problem and its extension, validating\n",
            "the robustness of GNNSync even at high noise levels.\n",
            "\n",
            "153. Title: Robust Angular Synchronization via Directed Graph Neural Networks\n",
            "   Abstract: We address the problem of user association in a dense millimeter wave\n",
            "(mmWave) network, in which each arriving user brings a file containing a random\n",
            "number of packets and each time slot is divided into multiple mini-slots. This\n",
            "problem is an instance of the restless multi-armed bandit problem, and is\n",
            "provably hard to solve. Using a technique introduced by Whittle, we relax the\n",
            "hard per-stage constraint that each arriving user must be associated with\n",
            "exactly one mmWave base station (mBS) to a long-term constraint and then use\n",
            "the Lagrangian multiplier technique to convert the problem into an\n",
            "unconstrained problem. This decouples the process governing the system into\n",
            "separate Markov Decision Processes at different mBSs. We prove that the problem\n",
            "is Whittle indexable, present a scheme for computing the Whittle indices of\n",
            "different mBSs, and propose an association scheme under which, each arriving\n",
            "user is associated with the mBS with the smallest value of the Whittle index.\n",
            "Using extensive simulations, we show that the proposed Whittle index based\n",
            "scheme outperforms several user association schemes proposed in prior work in\n",
            "terms of various performance metrics such as average cost, delay, throughput,\n",
            "and Jain's fairness index.\n",
            "\n",
            "154. Title: Learning to Relax: Setting Solver Parameters Across a Sequence of Linear System Instances\n",
            "   Abstract: It is now possible to reconstruct dynamic human motion and shape from a\n",
            "sparse set of cameras using Neural Radiance Fields (NeRF) driven by an\n",
            "underlying skeleton. However, a challenge remains to model the deformation of\n",
            "cloth and skin in relation to skeleton pose. Unlike existing avatar models that\n",
            "are learned implicitly or rely on a proxy surface, our approach is motivated by\n",
            "the observation that different poses necessitate unique frequency assignments.\n",
            "Neglecting this distinction yields noisy artifacts in smooth areas or blurs\n",
            "fine-grained texture and shape details in sharp regions. We develop a\n",
            "two-branch neural network that is adaptive and explicit in the frequency\n",
            "domain. The first branch is a graph neural network that models correlations\n",
            "among body parts locally, taking skeleton pose as input. The second branch\n",
            "combines these correlation features to a set of global frequencies and then\n",
            "modulates the feature encoding. Our experiments demonstrate that our network\n",
            "outperforms state-of-the-art methods in terms of preserving details and\n",
            "generalization capabilities.\n",
            "\n",
            "155. Title: Scale-Adaptive Diffusion Model for Complex Sketch Synthesis\n",
            "   Abstract: Solving a linear system $Ax=b$ is a fundamental scientific computing\n",
            "primitive for which numerous solvers and preconditioners have been developed.\n",
            "These come with parameters whose optimal values depend on the system being\n",
            "solved and are often impossible or too expensive to identify; thus in practice\n",
            "sub-optimal heuristics are used. We consider the common setting in which many\n",
            "related linear systems need to be solved, e.g. during a single numerical\n",
            "simulation. In this scenario, can we sequentially choose parameters that attain\n",
            "a near-optimal overall number of iterations, without extra matrix computations?\n",
            "We answer in the affirmative for Successive Over-Relaxation (SOR), a standard\n",
            "solver whose parameter $\\omega$ has a strong impact on its runtime. For this\n",
            "method, we prove that a bandit online learning algorithm--using only the number\n",
            "of iterations as feedback--can select parameters for a sequence of instances\n",
            "such that the overall cost approaches that of the best fixed $\\omega$ as the\n",
            "sequence length increases. Furthermore, when given additional structural\n",
            "information, we show that a contextual bandit method asymptotically achieves\n",
            "the performance of the instance-optimal policy, which selects the best $\\omega$\n",
            "for each instance. Our work provides the first learning-theoretic treatment of\n",
            "high-precision linear system solvers and the first end-to-end guarantees for\n",
            "data-driven scientific computing, demonstrating theoretically the potential to\n",
            "speed up numerical methods using well-understood learning algorithms.\n",
            "\n",
            "156. Title: NeurRev: Train Better Sparse Neural Network Practically via Neuron Revitalization\n",
            "   Abstract: The recent explosion in the capabilities of large language models has led to\n",
            "a wave of interest in how best to prompt a model to perform a given task. While\n",
            "it may be tempting to simply choose a prompt based on average performance on a\n",
            "validation set, this can lead to a deployment where unexpectedly poor responses\n",
            "are generated, especially for the worst-off users. To mitigate this prospect,\n",
            "we propose Prompt Risk Control, a lightweight framework for selecting a prompt\n",
            "based on rigorous upper bounds on families of informative risk measures. We\n",
            "offer methods for producing bounds on a diverse set of metrics, including\n",
            "quantities that measure worst-case responses and disparities in generation\n",
            "quality across the population of users. In addition, we extend the underlying\n",
            "statistical bounding techniques to accommodate the possibility of distribution\n",
            "shifts in deployment. Experiments on applications such as open-ended chat,\n",
            "medical question summarization, and code generation highlight how such a\n",
            "framework can foster responsible deployment by reducing the risk of the worst\n",
            "outcomes.\n",
            "\n",
            "157. Title: In-Context Learning Dynamics with Random Binary Sequences\n",
            "   Abstract: This paper presents a novel free-hand sketch synthesis approach addressing\n",
            "explicit abstraction control in class-conditional and photo-to-sketch\n",
            "synthesis. Abstraction is a vital aspect of sketches, as it defines the\n",
            "fundamental distinction between a sketch and an image. Previous works relied on\n",
            "implicit control to achieve different levels of abstraction, leading to\n",
            "inaccurate control and synthesized sketches deviating from human sketches. To\n",
            "resolve this challenge, we propose two novel abstraction control mechanisms,\n",
            "state embeddings and the stroke token, integrated into a transformer-based\n",
            "latent diffusion model (LDM). These mechanisms explicitly provide the required\n",
            "amount of points or strokes to the model, enabling accurate point-level and\n",
            "stroke-level control in synthesized sketches while preserving recognizability.\n",
            "Outperforming state-of-the-art approaches, our method effectively generates\n",
            "diverse, non-rigid and human-like sketches. The proposed approach enables\n",
            "coherent sketch synthesis and excels in representing human habits with desired\n",
            "abstraction levels, highlighting the potential of sketch synthesis for\n",
            "real-world applications.\n",
            "\n",
            "158. Title: Compressed Context Memory for Online Language Model Interaction\n",
            "   Abstract: In-context learning provides a new perspective for multi-task modeling for\n",
            "vision and NLP. Under this setting, the model can perceive tasks from prompts\n",
            "and accomplish them without any extra task-specific head predictions or model\n",
            "fine-tuning. However, Skeleton sequence modeling via in-context learning\n",
            "remains unexplored. Directly applying existing in-context models from other\n",
            "areas onto skeleton sequences fails due to the inter-frame and cross-task pose\n",
            "similarity that makes it outstandingly hard to perceive the task correctly from\n",
            "a subtle context. To address this challenge, we propose Skeleton-in-Context\n",
            "(SiC), an effective framework for in-context skeleton sequence modeling. Our\n",
            "SiC is able to handle multiple skeleton-based tasks simultaneously after a\n",
            "single training process and accomplish each task from context according to the\n",
            "given prompt. It can further generalize to new, unseen tasks according to\n",
            "customized prompts. To facilitate context perception, we additionally propose a\n",
            "task-unified prompt, which adaptively learns tasks of different natures, such\n",
            "as partial joint-level generation, sequence-level prediction, or 2D-to-3D\n",
            "motion prediction. We conduct extensive experiments to evaluate the\n",
            "effectiveness of our SiC on multiple tasks, including motion prediction, pose\n",
            "estimation, joint completion, and future pose estimation. We also evaluate its\n",
            "generalization capability on unseen tasks such as motion-in-between. These\n",
            "experiments show that our model achieves state-of-the-art multi-task\n",
            "performance and even outperforms single-task methods on certain tasks.\n",
            "\n",
            "159. Title: DreamClean: Restoring Clean Image Using Deep Diffusion Prior\n",
            "   Abstract: This paper presents a context key/value compression method for Transformer\n",
            "language models in online scenarios, where the context continually expands. As\n",
            "the context lengthens, the attention process demands increasing memory and\n",
            "computations, which in turn reduces the throughput of the language model. To\n",
            "address this challenge, we propose a compressed context memory system that\n",
            "continually compresses the accumulating attention key/value pairs into a\n",
            "compact memory space, facilitating language model inference in a limited memory\n",
            "space of computing environments. Our compression process involves integrating a\n",
            "lightweight conditional LoRA into the language model's forward pass during\n",
            "inference, without the need for fine-tuning the model's entire set of weights.\n",
            "We achieve efficient training by modeling the recursive compression process as\n",
            "a single parallelized forward computation. Through evaluations on conversation,\n",
            "personalization, and multi-task learning, we demonstrate that our approach\n",
            "achieves the performance level of a full context model with $5\\times$ smaller\n",
            "context memory size. We further demonstrate the applicability of our approach\n",
            "in a streaming setting with an unlimited context length, outperforming the\n",
            "sliding window approach. Codes are available at\n",
            "https://github.com/snu-mllab/context-memory.\n",
            "\n",
            "160. Title: Enhancing Tail Performance in Extreme Classifiers by Label Variance Reduction\n",
            "   Abstract: Posterior sampling has been shown to be a powerful Bayesian approach for\n",
            "solving imaging inverse problems. The recent plug-and-play unadjusted Langevin\n",
            "algorithm (PnP-ULA) has emerged as a promising method for Monte Carlo sampling\n",
            "and minimum mean squared error (MMSE) estimation by combining physical\n",
            "measurement models with deep-learning priors specified using image denoisers.\n",
            "However, the intricate relationship between the sampling distribution of\n",
            "PnP-ULA and the mismatched data-fidelity and denoiser has not been\n",
            "theoretically analyzed. We address this gap by proposing a posterior-L2\n",
            "pseudometric and using it to quantify an explicit error bound for PnP-ULA under\n",
            "mismatched posterior distribution. We numerically validate our theory on\n",
            "several inverse problems such as sampling from Gaussian mixture models and\n",
            "image deblurring. Our results suggest that the sensitivity of the sampling\n",
            "distribution of PnP-ULA to a mismatch in the measurement model and the denoiser\n",
            "can be precisely characterized.\n",
            "\n",
            "161. Title: Two-timescale Extragradient for Finding Local Minimax Points\n",
            "   Abstract: Detecting offensive language on social media is an important task. The\n",
            "ICWSM-2020 Data Challenge Task 2 is aimed at identifying offensive content\n",
            "using a crowd-sourced dataset containing 100k labelled tweets. The dataset,\n",
            "however, suffers from class imbalance, where certain labels are extremely rare\n",
            "compared with other classes (e.g, the hateful class is only 5% of the data). In\n",
            "this work, we present Dager (Data Augmenter), a generation-based data\n",
            "augmentation method, that improves the performance of classification on\n",
            "imbalanced and low-resource data such as the offensive language dataset. Dager\n",
            "extracts the lexical features of a given class, and uses these features to\n",
            "guide the generation of a conditional generator built on GPT-2. The generated\n",
            "text can then be added to the training set as augmentation data. We show that\n",
            "applying Dager can increase the F1 score of the data challenge by 11% when we\n",
            "use 1% of the whole dataset for training (using BERT for classification);\n",
            "moreover, the generated data also preserves the original labels very well. We\n",
            "test Dager on four different classifiers (BERT, CNN, Bi-LSTM with attention,\n",
            "and Transformer), observing universal improvement on the detection, indicating\n",
            "our method is effective and classifier-agnostic.\n",
            "\n",
            "162. Title: Personalize Segment Anything Model with One Shot\n",
            "   Abstract: Minimax problems are notoriously challenging to optimize. However, we present\n",
            "that the two-timescale extragradient method can be a viable solution. By\n",
            "utilizing dynamical systems theory, we show that it converges to points that\n",
            "satisfy the second-order necessary condition of local minimax points, under\n",
            "mild conditions that the two-timescale gradient descent ascent fails to work.\n",
            "This work provably improves upon all previous results on finding local minimax\n",
            "points, by eliminating a crucial assumption that the Hessian with respect to\n",
            "the maximization variable is nondegenerate.\n",
            "\n",
            "163. Title: Graphical Multioutput Gaussian Process with Attention\n",
            "   Abstract: Driven by large-data pre-training, Segment Anything Model (SAM) has been\n",
            "demonstrated as a powerful and promptable framework, revolutionizing the\n",
            "segmentation models. Despite the generality, customizing SAM for specific\n",
            "visual concepts without man-powered prompting is under explored, e.g.,\n",
            "automatically segmenting your pet dog in different images. In this paper, we\n",
            "propose a training-free Personalization approach for SAM, termed as PerSAM.\n",
            "Given only a single image with a reference mask, PerSAM first localizes the\n",
            "target concept by a location prior, and segments it within other images or\n",
            "videos via three techniques: target-guided attention, target-semantic\n",
            "prompting, and cascaded post-refinement. In this way, we effectively adapt SAM\n",
            "for private use without any training. To further alleviate the mask ambiguity,\n",
            "we present an efficient one-shot fine-tuning variant, PerSAM-F. Freezing the\n",
            "entire SAM, we introduce two learnable weights for multi-scale masks, only\n",
            "training 2 parameters within 10 seconds for improved performance. To\n",
            "demonstrate our efficacy, we construct a new segmentation dataset, PerSeg, for\n",
            "personalized evaluation, and test our methods on video object segmentation with\n",
            "competitive performance. Besides, our approach can also enhance DreamBooth to\n",
            "personalize Stable Diffusion for text-to-image generation, which discards the\n",
            "background disturbance for better target appearance learning. Code is released\n",
            "at https://github.com/ZrrSkywalker/Personalize-SAM\n",
            "\n",
            "164. Title: NoiseDiffusion: Correcting Noise for Image Interpolation with Diffusion Models beyond Spherical Linear Interpolation\n",
            "   Abstract: Adversarial training improves the robustness of neural networks against\n",
            "adversarial attacks, albeit at the expense of the trade-off between standard\n",
            "and robust generalization. To unveil the underlying factors driving this\n",
            "phenomenon, we examine the layer-wise learning capabilities of neural networks\n",
            "during the transition from a standard to an adversarial setting. Our empirical\n",
            "findings demonstrate that selectively updating specific layers while preserving\n",
            "others can substantially enhance the network's learning capacity. We therefore\n",
            "propose CURE, a novel training framework that leverages a gradient prominence\n",
            "criterion to perform selective conservation, updating, and revision of weights.\n",
            "Importantly, CURE is designed to be dataset- and architecture-agnostic,\n",
            "ensuring its applicability across various scenarios. It effectively tackles\n",
            "both memorization and overfitting issues, thus enhancing the trade-off between\n",
            "robustness and generalization and additionally, this training approach also\n",
            "aids in mitigating \"robust overfitting\". Furthermore, our study provides\n",
            "valuable insights into the mechanisms of selective adversarial training and\n",
            "offers a promising avenue for future research.\n",
            "\n",
            "165. Title: Dynamic Discounted Counterfactual Regret Minimization\n",
            "   Abstract: We propose a Deep Reinforcement Learning (Deep RL) algorithm for solving the\n",
            "online 3D bin packing problem for an arbitrary number of bins and any bin size.\n",
            "The focus is on producing decisions that can be physically implemented by a\n",
            "robotic loading arm, a laboratory prototype used for testing the concept. The\n",
            "problem considered in this paper is novel in two ways. First, unlike the\n",
            "traditional 3D bin packing problem, we assume that the entire set of objects to\n",
            "be packed is not known a priori. Instead, a fixed number of upcoming objects is\n",
            "visible to the loading system, and they must be loaded in the order of arrival.\n",
            "Second, the goal is not to move objects from one point to another via a\n",
            "feasible path, but to find a location and orientation for each object that\n",
            "maximises the overall packing efficiency of the bin(s). Finally, the learnt\n",
            "model is designed to work with problem instances of arbitrary size without\n",
            "retraining. Simulation results show that the RL-based method outperforms\n",
            "state-of-the-art online bin packing heuristics in terms of empirical\n",
            "competitive ratio and volume efficiency.\n",
            "\n",
            "166. Title: LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models\n",
            "   Abstract: Language models have shown promise in various tasks but can be affected by\n",
            "undesired data during training, fine-tuning, or alignment. For example, if some\n",
            "unsafe conversations are wrongly annotated as safe ones, the model fine-tuned\n",
            "on these samples may be harmful. Therefore, the correctness of annotations,\n",
            "i.e., the credibility of the dataset, is important. This study focuses on the\n",
            "credibility of real-world datasets, including the popular benchmarks Jigsaw\n",
            "Civil Comments, Anthropic Harmless & Red Team, PKU BeaverTails & SafeRLHF, that\n",
            "can be used for training a harmless language model. Given the cost and\n",
            "difficulty of cleaning these datasets by humans, we introduce a systematic\n",
            "framework for evaluating the credibility of datasets, identifying label errors,\n",
            "and evaluating the influence of noisy labels in the curated language data,\n",
            "specifically focusing on unsafe comments and conversation classification. With\n",
            "the framework, we find and fix an average of 6.16% label errors in 11 datasets\n",
            "constructed from the above benchmarks. The data credibility and downstream\n",
            "learning performance can be remarkably improved by directly fixing label\n",
            "errors, indicating the significance of cleaning existing real-world datasets.\n",
            "We provide an open-source tool, Docta, for data cleaning at\n",
            "https://github.com/Docta-ai/docta.\n",
            "\n",
            "167. Title: IceFormer: Accelerated Inference with Long-Sequence Transformers on CPUs\n",
            "   Abstract: Federated Domain Adaptation (FDA) describes the federated learning (FL)\n",
            "setting where source clients and a server work collaboratively to improve the\n",
            "performance of a target client where limited data is available. The domain\n",
            "shift between the source and target domains, coupled with limited data of the\n",
            "target client, makes FDA a challenging problem, e.g., common techniques such as\n",
            "federated averaging and fine-tuning fail due to domain shift and data scarcity.\n",
            "To theoretically understand the problem, we introduce new metrics that\n",
            "characterize the FDA setting and a theoretical framework with novel theorems\n",
            "for analyzing the performance of server aggregation rules. Further, we propose\n",
            "a novel lightweight aggregation rule, Federated Gradient Projection\n",
            "($\\texttt{FedGP}$), which significantly improves the target performance with\n",
            "domain shift and data scarcity. Moreover, our theory suggests an\n",
            "$\\textit{auto-weighting scheme}$ that finds the optimal combinations of the\n",
            "source and target gradients. This scheme improves both $\\texttt{FedGP}$ and a\n",
            "simpler heuristic aggregation rule. Extensive experiments verify the\n",
            "theoretical insights and illustrate the effectiveness of the proposed methods\n",
            "in practice.\n",
            "\n",
            "168. Title: Local Search GFlowNets\n",
            "   Abstract: We introduce Graphical Quadratic Algebra (GQA), a string diagrammatic\n",
            "calculus extending the language of Graphical Affine Algebra with a new\n",
            "generator characterised by invariance under rotation matrices. We show that GQA\n",
            "is a sound and complete axiomatisation for three different models: quadratic\n",
            "relations, which are a compositional formalism for least-squares problems,\n",
            "Gaussian stochastic processes, and Gaussian stochastic processes extended with\n",
            "non-determinisms. The equational theory of GQA sheds light on the connections\n",
            "between these perspectives, giving an algebraic interpretation to the interplay\n",
            "of stochastic behaviour, relational behaviour, non-determinism, and\n",
            "conditioning. As applications, we discuss various case studies, including\n",
            "linear regression, probabilistic programming, and electrical circuits with\n",
            "realistic (noisy) components.\n",
            "\n",
            "169. Title: Learning to solve Class-Constrained Bin Packing Problems via Encoder-Decoder Model\n",
            "   Abstract: Counterfactual regret minimization (CFR) is a family of algorithms for\n",
            "effectively solving imperfect-information games. It decomposes the total regret\n",
            "into counterfactual regrets, utilizing local regret minimization algorithms,\n",
            "such as Regret Matching (RM) or RM+, to minimize them. Recent research\n",
            "establishes a connection between Online Mirror Descent (OMD) and RM+, paving\n",
            "the way for an optimistic variant PRM+ and its extension PCFR+. However, PCFR+\n",
            "assigns uniform weights for each iteration when determining regrets, leading to\n",
            "substantial regrets when facing dominated actions. This work explores\n",
            "minimizing weighted counterfactual regret with optimistic OMD, resulting in a\n",
            "novel CFR variant PDCFR+. It integrates PCFR+ and Discounted CFR (DCFR) in a\n",
            "principled manner, swiftly mitigating negative effects of dominated actions and\n",
            "consistently leveraging predictions to accelerate convergence. Theoretical\n",
            "analyses prove that PDCFR+ converges to a Nash equilibrium, particularly under\n",
            "distinct weighting schemes for regrets and average strategies. Experimental\n",
            "results demonstrate PDCFR+'s fast convergence in common imperfect-information\n",
            "games. The code is available at https://github.com/rpSebastian/PDCFRPlus.\n",
            "\n",
            "170. Title: STanHop: Sparse Tandem Hopfield Model for Memory-Enhanced Time Series Prediction\n",
            "   Abstract: Generative Flow Networks (GFlowNets) are amortized sampling methods that\n",
            "learn a distribution over discrete objects proportional to their rewards.\n",
            "GFlowNets exhibit a remarkable ability to generate diverse samples, yet\n",
            "occasionally struggle to consistently produce samples with high rewards due to\n",
            "over-exploration on wide sample space. This paper proposes to train GFlowNets\n",
            "with local search, which focuses on exploiting high-rewarded sample space to\n",
            "resolve this issue. Our main idea is to explore the local neighborhood via\n",
            "backtracking and reconstruction guided by backward and forward policies,\n",
            "respectively. This allows biasing the samples toward high-reward solutions,\n",
            "which is not possible for a typical GFlowNet solution generation scheme, which\n",
            "uses the forward policy to generate the solution from scratch. Extensive\n",
            "experiments demonstrate a remarkable performance improvement in several\n",
            "biochemical tasks. Source code is available:\n",
            "\\url{https://github.com/dbsxodud-11/ls_gfn}.\n",
            "\n",
            "171. Title: Large Language Model Cascades with Mixture of Thought Representations for Cost-Efficient Reasoning\n",
            "   Abstract: Sparse Mixture-of-Experts (MoE) is a neural architecture design that can be\n",
            "utilized to add learnable parameters to Large Language Models (LLMs) without\n",
            "increasing inference cost. Instruction tuning is a technique for training LLMs\n",
            "to follow instructions. We advocate combining these two approaches, as we find\n",
            "that MoE models benefit more from instruction tuning than dense models. In\n",
            "particular, we conduct empirical studies across three experimental setups: (i)\n",
            "Direct finetuning on individual downstream tasks devoid of instruction tuning;\n",
            "(ii) Instructiontuning followed by in-context few-shot or zero-shot\n",
            "generalization on downstream tasks; and (iii) Instruction tuning supplemented\n",
            "by further finetuning on individual downstream tasks. In the first scenario,\n",
            "MoE models overall underperform dense models of identical computational\n",
            "capacity. This narrative, however, dramatically changes with the introduction\n",
            "of instruction tuning (second and third scenario), used independently or in\n",
            "conjunction with task-specific finetuning. Our most powerful model,\n",
            "FLAN-MOE-32B, surpasses the performance of FLAN-PALM-62B on four benchmark\n",
            "tasks, while using only a third of the FLOPs. The advancements embodied\n",
            "byFLAN-MOE inspire a reevaluation of the design principles of large-scale,\n",
            "high-performance language models in the framework of task-agnostic learning.\n",
            "\n",
            "172. Title: A Semantic Invariant Robust Watermark for Large Language Models\n",
            "   Abstract: Large language models (LLMs) such as GPT-4 have exhibited remarkable\n",
            "performance in a variety of tasks, but this strong performance often comes with\n",
            "the high expense of using paid API services. In this paper, we are motivated to\n",
            "study building an LLM cascade to save the cost of using LLMs, particularly for\n",
            "performing reasoning (e.g., mathematical, causal) tasks. Our cascade pipeline\n",
            "follows the intuition that simpler questions can be addressed by a weaker but\n",
            "more affordable LLM, whereas only the challenging questions necessitate the\n",
            "stronger and more expensive LLM. To realize this decision-making, we consider\n",
            "the \"answer consistency\" of the weaker LLM as a signal of the question\n",
            "difficulty and propose several methods for the answer sampling and consistency\n",
            "checking, including one leveraging a mixture of two thought representations\n",
            "(i.e., Chain-of-Thought and Program-of-Thought). Through experiments on six\n",
            "reasoning benchmark datasets, with GPT-3.5-turbo and GPT-4 being the weaker and\n",
            "stronger LLMs, respectively, we demonstrate that our proposed LLM cascades can\n",
            "achieve performance comparable to using solely the stronger LLM but require\n",
            "only 40% of its cost.\n",
            "\n",
            "173. Title: Towards Diverse Behaviors: A Benchmark for Imitation Learning with Human Demonstrations\n",
            "   Abstract: Invariance learning methods aim to learn invariant features in the hope that\n",
            "they generalize under distributional shifts. Although many tasks are naturally\n",
            "characterized by continuous domains, current invariance learning techniques\n",
            "generally assume categorically indexed domains. For example, auto-scaling in\n",
            "cloud computing often needs a CPU utilization prediction model that generalizes\n",
            "across different times (e.g., time of a day and date of a year), where `time'\n",
            "is a continuous domain index. In this paper, we start by theoretically showing\n",
            "that existing invariance learning methods can fail for continuous domain\n",
            "problems. Specifically, the naive solution of splitting continuous domains into\n",
            "discrete ones ignores the underlying relationship among domains, and therefore\n",
            "potentially leads to suboptimal performance. To address this challenge, we then\n",
            "propose Continuous Invariance Learning (CIL), which extracts invariant features\n",
            "across continuously indexed domains. CIL is a novel adversarial procedure that\n",
            "measures and controls the conditional independence between the labels and\n",
            "continuous domain indices given the extracted features. Our theoretical\n",
            "analysis demonstrates the superiority of CIL over existing invariance learning\n",
            "methods. Empirical results on both synthetic and real-world datasets (including\n",
            "data collected from production systems) show that CIL consistently outperforms\n",
            "strong baselines among all the tasks.\n",
            "\n",
            "174. Title: ImplicitSLIM and How it Improves Embedding-based Collaborative Filtering\n",
            "   Abstract: Imitation learning with human data has demonstrated remarkable success in\n",
            "teaching robots in a wide range of skills. However, the inherent diversity in\n",
            "human behavior leads to the emergence of multi-modal data distributions,\n",
            "thereby presenting a formidable challenge for existing imitation learning\n",
            "algorithms. Quantifying a model's capacity to capture and replicate this\n",
            "diversity effectively is still an open problem. In this work, we introduce\n",
            "simulation benchmark environments and the corresponding Datasets with Diverse\n",
            "human Demonstrations for Imitation Learning (D3IL), designed explicitly to\n",
            "evaluate a model's ability to learn multi-modal behavior. Our environments are\n",
            "designed to involve multiple sub-tasks that need to be solved, consider\n",
            "manipulation of multiple objects which increases the diversity of the behavior\n",
            "and can only be solved by policies that rely on closed loop sensory feedback.\n",
            "Other available datasets are missing at least one of these challenging\n",
            "properties. To address the challenge of diversity quantification, we introduce\n",
            "tractable metrics that provide valuable insights into a model's ability to\n",
            "acquire and reproduce diverse behaviors. These metrics offer a practical means\n",
            "to assess the robustness and versatility of imitation learning algorithms.\n",
            "Furthermore, we conduct a thorough evaluation of state-of-the-art methods on\n",
            "the proposed task suite. This evaluation serves as a benchmark for assessing\n",
            "their capability to learn diverse behaviors. Our findings shed light on the\n",
            "effectiveness of these methods in tackling the intricate problem of capturing\n",
            "and generalizing multi-modal human behaviors, offering a valuable reference for\n",
            "the design of future imitation learning algorithms.\n",
            "\n",
            "175. Title: Chain of Hindsight aligns Language Models with Feedback\n",
            "   Abstract: We study online reinforcement learning in linear Markov decision processes\n",
            "with adversarial losses and bandit feedback, without prior knowledge on\n",
            "transitions or access to simulators. We introduce two algorithms that achieve\n",
            "improved regret performance compared to existing approaches. The first\n",
            "algorithm, although computationally inefficient, ensures a regret of\n",
            "$\\widetilde{\\mathcal{O}}\\left(\\sqrt{K}\\right)$, where $K$ is the number of\n",
            "episodes. This is the first result with the optimal $K$ dependence in the\n",
            "considered setting. The second algorithm, which is based on the policy\n",
            "optimization framework, guarantees a regret of\n",
            "$\\widetilde{\\mathcal{O}}\\left(K^{\\frac{3}{4}} \\right)$ and is computationally\n",
            "efficient. Both our results significantly improve over the state-of-the-art: a\n",
            "computationally inefficient algorithm by Kong et al. [2023] with\n",
            "$\\widetilde{\\mathcal{O}}\\left(K^{\\frac{4}{5}}+poly\\left(\\frac{1}{\\lambda_{\\min}}\\right)\n",
            "\\right)$ regret, for some problem-dependent constant $\\lambda_{\\min}$ that can\n",
            "be arbitrarily close to zero, and a computationally efficient algorithm by\n",
            "Sherman et al. [2023b] with $\\widetilde{\\mathcal{O}}\\left(K^{\\frac{6}{7}}\n",
            "\\right)$ regret.\n",
            "\n",
            "176. Title: Towards Optimal Regret in Adversarial Linear MDPs with Bandit Feedback\n",
            "   Abstract: Machine learning models have demonstrated substantial performance\n",
            "enhancements over non-learned alternatives in various fundamental data\n",
            "management operations, including indexing (locating items in an array),\n",
            "cardinality estimation (estimating the number of matching records in a\n",
            "database), and range-sum estimation (estimating aggregate attribute values for\n",
            "query-matched records). However, real-world systems frequently favor less\n",
            "efficient non-learned methods due to their ability to offer (worst-case) error\n",
            "guarantees - an aspect where learned approaches often fall short. The primary\n",
            "objective of these guarantees is to ensure system reliability, ensuring that\n",
            "the chosen approach consistently delivers the desired level of accuracy across\n",
            "all databases. In this paper, we embark on the first theoretical study of such\n",
            "guarantees for learned methods, presenting the necessary conditions for such\n",
            "guarantees to hold when using machine learning to perform indexing, cardinality\n",
            "estimation and range-sum estimation. Specifically, we present the first known\n",
            "lower bounds on the model size required to achieve the desired accuracy for\n",
            "these three key database operations. Our results bound the required model size\n",
            "for given average and worst-case errors in performing database operations,\n",
            "serving as the first theoretical guidelines governing how model size must\n",
            "change based on data size to be able to guarantee an accuracy level. More\n",
            "broadly, our established guarantees pave the way for the broader adoption and\n",
            "integration of learned models into real-world systems.\n",
            "\n",
            "177. Title: Continuous Invariance Learning\n",
            "   Abstract: We present ImplicitSLIM, a novel unsupervised learning approach for sparse\n",
            "high-dimensional data, with applications to collaborative filtering. Sparse\n",
            "linear methods (SLIM) and their variations show outstanding performance, but\n",
            "they are memory-intensive and hard to scale. ImplicitSLIM improves\n",
            "embedding-based models by extracting embeddings from SLIM-like models in a\n",
            "computationally cheap and memory-efficient way, without explicit learning of\n",
            "heavy SLIM-like models. We show that ImplicitSLIM improves performance and\n",
            "speeds up convergence for both state of the art and classical collaborative\n",
            "filtering methods. The source code for ImplicitSLIM, related models, and\n",
            "applications is available at https://github.com/ilya-shenbin/ImplicitSLIM.\n",
            "\n",
            "178. Title: Mirage: Model-agnostic Graph Distillation for Graph Classification\n",
            "   Abstract: Watermark algorithms for large language models (LLMs) have achieved extremely\n",
            "high accuracy in detecting text generated by LLMs. Such algorithms typically\n",
            "involve adding extra watermark logits to the LLM's logits at each generation\n",
            "step. However, prior algorithms face a trade-off between attack robustness and\n",
            "security robustness. This is because the watermark logits for a token are\n",
            "determined by a certain number of preceding tokens; a small number leads to low\n",
            "security robustness, while a large number results in insufficient attack\n",
            "robustness. In this work, we propose a semantic invariant watermarking method\n",
            "for LLMs that provides both attack robustness and security robustness. The\n",
            "watermark logits in our work are determined by the semantics of all preceding\n",
            "tokens. Specifically, we utilize another embedding LLM to generate semantic\n",
            "embeddings for all preceding tokens, and then these semantic embeddings are\n",
            "transformed into the watermark logits through our trained watermark model.\n",
            "Subsequent analyses and experiments demonstrated the attack robustness of our\n",
            "method in semantically invariant settings: synonym substitution and text\n",
            "paraphrasing settings. Finally, we also show that our watermark possesses\n",
            "adequate security robustness. Our code and data are available at\n",
            "\\href{https://github.com/THU-BPM/Robust_Watermark}{https://github.com/THU-BPM/Robust\\_Watermark}.\n",
            "Additionally, our algorithm could also be accessed through MarkLLM\n",
            "\\citep{pan2024markllm} \\footnote{https://github.com/THU-BPM/MarkLLM}.\n",
            "\n",
            "179. Title: A Characterization Theorem for Equivariant Networks with Point-wise Activations\n",
            "   Abstract: We investigate a challenging task of nighttime optical flow, which suffers\n",
            "from weakened texture and amplified noise. These degradations weaken\n",
            "discriminative visual features, thus causing invalid motion feature matching.\n",
            "Typically, existing methods employ domain adaptation to transfer knowledge from\n",
            "auxiliary domain to nighttime domain in either input visual space or output\n",
            "motion space. However, this direct adaptation is ineffective, since there\n",
            "exists a large domain gap due to the intrinsic heterogeneous nature of the\n",
            "feature representations between auxiliary and nighttime domains. To overcome\n",
            "this issue, we explore a common-latent space as the intermediate bridge to\n",
            "reinforce the feature alignment between auxiliary and nighttime domains. In\n",
            "this work, we exploit two auxiliary daytime and event domains, and propose a\n",
            "novel common appearance-boundary adaptation framework for nighttime optical\n",
            "flow. In appearance adaptation, we employ the intrinsic image decomposition to\n",
            "embed the auxiliary daytime image and the nighttime image into a\n",
            "reflectance-aligned common space. We discover that motion distributions of the\n",
            "two reflectance maps are very similar, benefiting us to consistently transfer\n",
            "motion appearance knowledge from daytime to nighttime domain. In boundary\n",
            "adaptation, we theoretically derive the motion correlation formula between\n",
            "nighttime image and accumulated events within a spatiotemporal gradient-aligned\n",
            "common space. We figure out that the correlation of the two spatiotemporal\n",
            "gradient maps shares significant discrepancy, benefitting us to contrastively\n",
            "transfer boundary knowledge from event to nighttime domain. Moreover,\n",
            "appearance adaptation and boundary adaptation are complementary to each other,\n",
            "since they could jointly transfer global motion and local boundary knowledge to\n",
            "the nighttime domain.\n",
            "\n",
            "180. Title: Real3D-Portrait: One-shot Realistic 3D Talking Portrait Synthesis\n",
            "   Abstract: Learning from human preferences is important for language models to match\n",
            "human needs and to align with human and social values. Prior works have\n",
            "achieved remarkable successes by learning from human feedback to understand and\n",
            "follow instructions. Nonetheless, these methods are either founded on\n",
            "hand-picked model generations that are favored by human annotators, rendering\n",
            "them inefficient in terms of data utilization and challenging to apply in\n",
            "general, or they depend on reinforcement learning, which often suffers from\n",
            "imperfect reward functions and relies on extremely challenging optimizations.\n",
            "In this work, we propose a novel technique, Chain of Hindsight, that is easy to\n",
            "optimize and can learn from any form of feedback, regardless of its polarity.\n",
            "Our idea is inspired by how humans learn from extensive feedback presented in\n",
            "the form of languages. We convert all types of feedback into sequences of\n",
            "sentences, which are then used to fine-tune the model, allowing us to take\n",
            "advantage of the language comprehension capabilities of language models. We\n",
            "condition the model on a sequence of model generations paired with feedback. By\n",
            "doing so, the model is trained to generate outputs based on feedback, while\n",
            "learning to identify and correct negative attributes or errors. Applying our\n",
            "method to large language models, we observed that Chain of Hindsight\n",
            "significantly surpasses previous methods in aligning language models with human\n",
            "preferences. We report significant improvements on summarization and dialogue\n",
            "benchmarks, with our approach markedly preferred in human evaluations.\n",
            "\n",
            "181. Title: SLiMe: Segment Like Me\n",
            "   Abstract: Equivariant neural networks have shown improved performance, expressiveness\n",
            "and sample complexity on symmetrical domains. But for some specific symmetries,\n",
            "representations, and choice of coordinates, the most common point-wise\n",
            "activations, such as ReLU, are not equivariant, hence they cannot be employed\n",
            "in the design of equivariant neural networks. The theorem we present in this\n",
            "paper describes all possible combinations of finite-dimensional\n",
            "representations, choice of coordinates and point-wise activations to obtain an\n",
            "exactly equivariant layer, generalizing and strengthening existing\n",
            "characterizations. Notable cases of practical relevance are discussed as\n",
            "corollaries. Indeed, we prove that rotation-equivariant networks can only be\n",
            "invariant, as it happens for any network which is equivariant with respect to\n",
            "connected compact groups. Then, we discuss implications of our findings when\n",
            "applied to important instances of exactly equivariant networks. First, we\n",
            "completely characterize permutation equivariant networks such as Invariant\n",
            "Graph Networks with point-wise nonlinearities and their geometric counterparts,\n",
            "highlighting a plethora of models whose expressive power and performance are\n",
            "still unknown. Second, we show that feature spaces of disentangled steerable\n",
            "convolutional neural networks are trivial representations.\n",
            "\n",
            "182. Title: STREAM: Spatio-TempoRal Evaluation and Analysis Metric for Video Generative Models\n",
            "   Abstract: Powerful artificial intelligence systems are often used in settings where\n",
            "they must interact with agents that are computationally much weaker, for\n",
            "example when they work alongside humans or operate in complex environments\n",
            "where some tasks are handled by algorithms, heuristics, or other entities of\n",
            "varying computational power. For AI agents to successfully interact in these\n",
            "settings, however, achieving superhuman performance alone is not sufficient;\n",
            "they also need to account for suboptimal actions or idiosyncratic style from\n",
            "their less-skilled counterparts. We propose a formal evaluation framework for\n",
            "assessing the compatibility of near-optimal AI with interaction partners who\n",
            "may have much lower levels of skill; we use popular collaborative chess\n",
            "variants as model systems to study and develop AI agents that can successfully\n",
            "interact with lower-skill entities. Traditional chess engines designed to\n",
            "output near-optimal moves prove to be inadequate partners when paired with\n",
            "engines of various lower skill levels in this domain, as they are not designed\n",
            "to consider the presence of other agents. We contribute three methodologies to\n",
            "explicitly create skill-compatible AI agents in complex decision-making\n",
            "settings, and two chess game frameworks designed to foster collaboration\n",
            "between powerful AI agents and less-skilled partners. On these frameworks, our\n",
            "agents outperform state-of-the-art chess AI (based on AlphaZero) despite being\n",
            "weaker in conventional chess, demonstrating that skill-compatibility is a\n",
            "tangible trait that is qualitatively and measurably distinct from raw\n",
            "performance. Our evaluations further explore and clarify the mechanisms by\n",
            "which our agents achieve skill-compatibility.\n",
            "\n",
            "183. Title: AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models\n",
            "   Abstract: Exemplar-Free Class Incremental Learning (EFCIL) aims to learn from a\n",
            "sequence of tasks without having access to previous task data. In this paper,\n",
            "we consider the challenging Cold Start scenario in which insufficient data is\n",
            "available in the first task to learn a high-quality backbone. This is\n",
            "especially challenging for EFCIL since it requires high plasticity, which\n",
            "results in feature drift which is difficult to compensate for in the\n",
            "exemplar-free setting. To address this problem, we propose a simple and\n",
            "effective approach that consolidates feature representations by regularizing\n",
            "drift in directions highly relevant to previous tasks and employs prototypes to\n",
            "reduce task-recency bias. Our method, called Elastic Feature Consolidation\n",
            "(EFC), exploits a tractable second-order approximation of feature drift based\n",
            "on an Empirical Feature Matrix (EFM). The EFM induces a pseudo-metric in\n",
            "feature space which we use to regularize feature drift in important directions\n",
            "and to update Gaussian prototypes used in a novel asymmetric cross entropy loss\n",
            "which effectively balances prototype rehearsal with data from new tasks.\n",
            "Experimental results on CIFAR-100, Tiny-ImageNet, ImageNet-Subset and\n",
            "ImageNet-1K demonstrate that Elastic Feature Consolidation is better able to\n",
            "learn new tasks by maintaining model plasticity and significantly outperform\n",
            "the state-of-the-art.\n",
            "\n",
            "184. Title: A Foundation Model for Error Correction Codes\n",
            "   Abstract: Large foundation models are becoming ubiquitous, but training them from\n",
            "scratch is prohibitively expensive. Thus, efficiently adapting these powerful\n",
            "models to downstream tasks is increasingly important. In this paper, we study a\n",
            "principled finetuning paradigm -- Orthogonal Finetuning (OFT) -- for downstream\n",
            "task adaptation. Despite demonstrating good generalizability, OFT still uses a\n",
            "fairly large number of trainable parameters due to the high dimensionality of\n",
            "orthogonal matrices. To address this, we start by examining OFT from an\n",
            "information transmission perspective, and then identify a few key desiderata\n",
            "that enable better parameter-efficiency. Inspired by how the Cooley-Tukey fast\n",
            "Fourier transform algorithm enables efficient information transmission, we\n",
            "propose an efficient orthogonal parameterization using butterfly structures. We\n",
            "apply this parameterization to OFT, creating a novel parameter-efficient\n",
            "finetuning method, called Orthogonal Butterfly (BOFT). By subsuming OFT as a\n",
            "special case, BOFT introduces a generalized orthogonal finetuning framework.\n",
            "Finally, we conduct an extensive empirical study of adapting large vision\n",
            "transformers, large language models, and text-to-image diffusion models to\n",
            "various downstream tasks in vision and language.\n",
            "\n",
            "185. Title: Parameter-Efficient Orthogonal Finetuning via Butterfly Factorization\n",
            "   Abstract: Topological quantum computing has recently proven itself to be a powerful\n",
            "computational model when constructing viable architectures for large scale\n",
            "computation. The topological model is constructed from the foundation of a\n",
            "error correction code, required to correct for inevitable hardware faults that\n",
            "will exist for a large scale quantum device. It is also a measurement based\n",
            "model of quantum computation, meaning that the quantum hardware is responsible\n",
            "only for the construction of a large, computationally universal quantum state.\n",
            "This quantum state is then strategically consumed, allowing for the realisation\n",
            "of a fully error corrected quantum algorithm. The number of physical qubits\n",
            "needed by the quantum hardware and the amount of time required to implement an\n",
            "algorithm is dictated by the manner in which this universal quantum state is\n",
            "consumed. In this paper we examine the problem of algorithmic optimisation in\n",
            "the topological lattice and introduce the required elements that will be needed\n",
            "when designing a classical software package to compile and implement a large\n",
            "scale algorithm on a topological quantum computer.\n",
            "\n",
            "186. Title: Data-independent Module-aware Pruning for Hierarchical Vision Transformers\n",
            "   Abstract: In learning an embodied agent executing daily tasks via language directives,\n",
            "the literature largely assumes that the agent learns all training data at the\n",
            "beginning. We argue that such a learning scenario is less realistic since a\n",
            "robotic agent is supposed to learn the world continuously as it explores and\n",
            "perceives it. To take a step towards a more realistic embodied agent learning\n",
            "scenario, we propose two continual learning setups for embodied agents;\n",
            "learning new behaviors (Behavior Incremental Learning, Behavior-IL) and new\n",
            "environments (Environment Incremental Learning, Environment-IL) For the tasks,\n",
            "previous 'data prior' based continual learning methods maintain logits for the\n",
            "past tasks. However, the stored information is often insufficiently learned\n",
            "information and requires task boundary information, which might not always be\n",
            "available. Here, we propose to update them based on confidence scores without\n",
            "task boundary information during training (i.e., task-free) in a moving average\n",
            "fashion, named Confidence-Aware Moving Average (CAMA). In the proposed\n",
            "Behavior-IL and Environment-IL setups, our simple CAMA outperforms prior state\n",
            "of the art in our empirical validations by noticeable margins. The project page\n",
            "including codes is https://github.com/snumprlab/cl-alfred.\n",
            "\n",
            "187. Title: Adversarial Adaptive Sampling: Unify PINN and Optimal Transport for the Approximation of PDEs\n",
            "   Abstract: Hierarchical vision transformers (ViTs) have two advantages over conventional\n",
            "ViTs. First, hierarchical ViTs achieve linear computational complexity with\n",
            "respect to image size by local self-attention. Second, hierarchical ViTs create\n",
            "hierarchical feature maps by merging image patches in deeper layers for dense\n",
            "prediction. However, existing pruning methods ignore the unique properties of\n",
            "hierarchical ViTs and use the magnitude value as the weight importance. This\n",
            "approach leads to two main drawbacks. First, the \"local\" attention weights are\n",
            "compared at a \"global\" level, which may cause some \"locally\" important weights\n",
            "to be pruned due to their relatively small magnitude \"globally\". The second\n",
            "issue with magnitude pruning is that it fails to consider the distinct weight\n",
            "distributions of the network, which are essential for extracting coarse to\n",
            "fine-grained features at various hierarchical levels.\n",
            "  To solve the aforementioned issues, we have developed a Data-independent\n",
            "Module-Aware Pruning method (DIMAP) to compress hierarchical ViTs. To ensure\n",
            "that \"local\" attention weights at different hierarchical levels are compared\n",
            "fairly in terms of their contribution, we treat them as a module and examine\n",
            "their contribution by analyzing their information distortion. Furthermore, we\n",
            "introduce a novel weight metric that is solely based on weights and does not\n",
            "require input images, thereby eliminating the dependence on the patch merging\n",
            "process. Our method validates its usefulness and strengths on Swin Transformers\n",
            "of different sizes on ImageNet-1k classification. Notably, the top-5 accuracy\n",
            "drop is only 0.07% when we remove 52.5% FLOPs and 52.7% parameters of Swin-B.\n",
            "When we reduce 33.2% FLOPs and 33.2% parameters of Swin-S, we can even achieve\n",
            "a 0.8% higher relative top-5 accuracy than the original model. Code is\n",
            "available at: https://github.com/he-y/Data-independent-Module-Aware-Pruning\n",
            "\n",
            "188. Title: Removing Biases from Molecular Representations via Information Maximization\n",
            "   Abstract: High-throughput drug screening -- using cell imaging or gene expression\n",
            "measurements as readouts of drug effect -- is a critical tool in biotechnology\n",
            "to assess and understand the relationship between the chemical structure and\n",
            "biological activity of a drug. Since large-scale screens have to be divided\n",
            "into multiple experiments, a key difficulty is dealing with batch effects,\n",
            "which can introduce systematic errors and non-biological associations in the\n",
            "data. We propose InfoCORE, an Information maximization approach for COnfounder\n",
            "REmoval, to effectively deal with batch effects and obtain refined molecular\n",
            "representations. InfoCORE establishes a variational lower bound on the\n",
            "conditional mutual information of the latent representations given a batch\n",
            "identifier. It adaptively reweighs samples to equalize their implied batch\n",
            "distribution. Extensive experiments on drug screening data reveal InfoCORE's\n",
            "superior performance in a multitude of tasks including molecular property\n",
            "prediction and molecule-phenotype retrieval. Additionally, we show results for\n",
            "how InfoCORE offers a versatile framework and resolves general distribution\n",
            "shifts and issues of data fairness by minimizing correlation with spurious\n",
            "features or removing sensitive attributes. The code is available at\n",
            "https://github.com/uhlerlab/InfoCORE.\n",
            "\n",
            "189. Title: BooookScore: A systematic exploration of book-length summarization in the era of LLMs\n",
            "   Abstract: Physics-informed neural networks (PINNs) provide a deep learning framework\n",
            "for numerically solving partial differential equations (PDEs), and have been\n",
            "widely used in a variety of PDE problems. However, there still remain some\n",
            "challenges in the application of PINNs: 1) the mechanism of PINNs is unsuitable\n",
            "(at least cannot be directly applied) to exploiting a small size of (usually\n",
            "very few) extra informative samples to refine the networks; and 2) the\n",
            "efficiency of training PINNs often becomes low for some complicated PDEs. In\n",
            "this paper, we propose the generative adversarial physics-informed neural\n",
            "network (GA-PINN), which integrates the generative adversarial (GA) mechanism\n",
            "with the structure of PINNs, to improve the performance of PINNs by exploiting\n",
            "only a small size of exact solutions to the PDEs. Inspired from the weighting\n",
            "strategy of the Adaboost method, we then introduce a point-weighting (PW)\n",
            "method to improve the training efficiency of PINNs, where the weight of each\n",
            "sample point is adaptively updated at each training iteration. The numerical\n",
            "experiments show that GA-PINNs outperform PINNs in many well-known PDEs and the\n",
            "PW method also improves the efficiency of training PINNs and GA-PINNs.\n",
            "\n",
            "190. Title: Human Feedback is not Gold Standard\n",
            "   Abstract: Learning structured representations of the visual world in terms of objects\n",
            "promises to significantly improve the generalization abilities of current\n",
            "machine learning models. While recent efforts to this end have shown promising\n",
            "empirical progress, a theoretical account of when unsupervised object-centric\n",
            "representation learning is possible is still lacking. Consequently,\n",
            "understanding the reasons for the success of existing object-centric methods as\n",
            "well as designing new theoretically grounded methods remains challenging. In\n",
            "the present work, we analyze when object-centric representations can provably\n",
            "be learned without supervision. To this end, we first introduce two assumptions\n",
            "on the generative process for scenes comprised of several objects, which we\n",
            "call compositionality and irreducibility. Under this generative process, we\n",
            "prove that the ground-truth object representations can be identified by an\n",
            "invertible and compositional inference model, even in the presence of\n",
            "dependencies between objects. We empirically validate our results through\n",
            "experiments on synthetic data. Finally, we provide evidence that our theory\n",
            "holds predictive power for existing object-centric models by showing a close\n",
            "correspondence between models' compositionality and invertibility and their\n",
            "empirical identifiability.\n",
            "\n",
            "191. Title: You Only Query Once: An Efficient Label-Only Membership Inference Attack\n",
            "   Abstract: Recommender systems have been successfully applied in many applications.\n",
            "Nonetheless, recent studies demonstrate that recommender systems are vulnerable\n",
            "to membership inference attacks (MIAs), leading to the leakage of users'\n",
            "membership privacy. However, existing MIAs relying on shadow training suffer a\n",
            "large performance drop when the attacker lacks knowledge of the training data\n",
            "distribution and the model architecture of the target recommender system. To\n",
            "better understand the privacy risks of recommender systems, we propose\n",
            "shadow-free MIAs that directly leverage a user's recommendations for membership\n",
            "inference. Without shadow training, the proposed attack can conduct MIAs\n",
            "efficiently and effectively under a practice scenario where the attacker is\n",
            "given only black-box access to the target recommender system. The proposed\n",
            "attack leverages an intuition that the recommender system personalizes a user's\n",
            "recommendations if his historical interactions are used by it. Thus, an\n",
            "attacker can infer membership privacy by determining whether the\n",
            "recommendations are more similar to the interactions or the general popular\n",
            "items. We conduct extensive experiments on benchmark datasets across various\n",
            "recommender systems. Remarkably, our attack achieves far better attack accuracy\n",
            "with low false positive rates than baselines while with a much lower\n",
            "computational cost.\n",
            "\n",
            "192. Title: Flag Aggregator: Scalable Distributed Training under Failures and Augmented Losses using Convex Optimization\n",
            "   Abstract: Generative molecular design has moved from proof-of-concept to real-world\n",
            "applicability, as marked by the surge in very recent papers reporting\n",
            "experimental validation. Key challenges in explainability and sample efficiency\n",
            "present opportunities to enhance generative design to directly optimize\n",
            "expensive high-fidelity oracles and provide actionable insights to domain\n",
            "experts. Here, we propose Beam Enumeration to exhaustively enumerate the most\n",
            "probable sub-sequences from language-based molecular generative models and show\n",
            "that molecular substructures can be extracted. When coupled with reinforcement\n",
            "learning, extracted substructures become meaningful, providing a source of\n",
            "explainability and improving sample efficiency through self-conditioned\n",
            "generation. Beam Enumeration is generally applicable to any language-based\n",
            "molecular generative model and notably further improves the performance of the\n",
            "recently reported Augmented Memory algorithm, which achieved the new\n",
            "state-of-the-art on the Practical Molecular Optimization benchmark for sample\n",
            "efficiency. The combined algorithm generates more high reward molecules and\n",
            "faster, given a fixed oracle budget. Beam Enumeration shows that improvements\n",
            "to explainability and sample efficiency for molecular design can be made\n",
            "synergistic.\n",
            "\n",
            "193. Title: Can Sensitive Information Be Deleted From LLMs? Objectives for Defending Against Extraction Attacks\n",
            "   Abstract: Reinforcement learning (RL) has achieved phenomenal success in various\n",
            "domains. However, its data-driven nature also introduces new vulnerabilities\n",
            "that can be exploited by malicious opponents. Recent work shows that a\n",
            "well-trained RL agent can be easily manipulated by strategically perturbing its\n",
            "state observations at the test stage. Existing solutions either introduce a\n",
            "regularization term to improve the smoothness of the trained policy against\n",
            "perturbations or alternatively train the agent's policy and the attacker's\n",
            "policy. However, the former does not provide sufficient protection against\n",
            "strong attacks, while the latter is computationally prohibitive for large\n",
            "environments. In this work, we propose a new robust RL algorithm for deriving a\n",
            "pessimistic policy to safeguard against an agent's uncertainty about true\n",
            "states. This approach is further enhanced with belief state inference and\n",
            "diffusion-based state purification to reduce uncertainty. Empirical results\n",
            "show that our approach obtains superb performance under strong attacks and has\n",
            "a comparable training overhead with regularization-based methods. Our code is\n",
            "available at https://github.com/SliencerX/Belief-enriched-robust-Q-learning.\n",
            "\n",
            "194. Title: Belief-Enriched Pessimistic Q-Learning against Adversarial State Perturbations\n",
            "   Abstract: Human feedback has become the de facto standard for evaluating the\n",
            "performance of Large Language Models, and is increasingly being used as a\n",
            "training objective. However, it is not clear which properties of a generated\n",
            "output this single `preference' score captures. We hypothesise that preference\n",
            "scores are subjective and open to undesirable biases. We critically analyse the\n",
            "use of human feedback for both training and evaluation, to verify whether it\n",
            "fully captures a range of crucial error criteria. We find that while preference\n",
            "scores have fairly good coverage, they under-represent important aspects like\n",
            "factuality. We further hypothesise that both preference scores and error\n",
            "annotation may be affected by confounders, and leverage instruction-tuned\n",
            "models to generate outputs that vary along two possible confounding dimensions:\n",
            "assertiveness and complexity. We find that the assertiveness of an output skews\n",
            "the perceived rate of factuality errors, indicating that human annotations are\n",
            "not a fully reliable evaluation metric or training objective. Finally, we offer\n",
            "preliminary evidence that using human feedback as a training objective\n",
            "disproportionately increases the assertiveness of model outputs. We encourage\n",
            "future work to carefully consider whether preference scores are well aligned\n",
            "with the desired objective.\n",
            "\n",
            "195. Title: Proper Laplacian Representation Learning\n",
            "   Abstract: The ability to learn good representations of states is essential for solving\n",
            "large reinforcement learning problems, where exploration, generalization, and\n",
            "transfer are particularly challenging. The Laplacian representation is a\n",
            "promising approach to address these problems by inducing informative state\n",
            "encoding and intrinsic rewards for temporally-extended action discovery and\n",
            "reward shaping. To obtain the Laplacian representation one needs to compute the\n",
            "eigensystem of the graph Laplacian, which is often approximated through\n",
            "optimization objectives compatible with deep learning approaches. These\n",
            "approximations, however, depend on hyperparameters that are impossible to tune\n",
            "efficiently, converge to arbitrary rotations of the desired eigenvectors, and\n",
            "are unable to accurately recover the corresponding eigenvalues. In this paper\n",
            "we introduce a theoretically sound objective and corresponding optimization\n",
            "algorithm for approximating the Laplacian representation. Our approach\n",
            "naturally recovers both the true eigenvectors and eigenvalues while eliminating\n",
            "the hyperparameter dependence of previous approximations. We provide\n",
            "theoretical guarantees for our method and we show that those results translate\n",
            "empirically into robust learning across multiple environments.\n",
            "\n",
            "196. Title: EQA-MX: Embodied Question Answering using Multimodal Expression\n",
            "   Abstract: Modern ML applications increasingly rely on complex deep learning models and\n",
            "large datasets. There has been an exponential growth in the amount of\n",
            "computation needed to train the largest models. Therefore, to scale computation\n",
            "and data, these models are inevitably trained in a distributed manner in\n",
            "clusters of nodes, and their updates are aggregated before being applied to the\n",
            "model. However, a distributed setup is prone to Byzantine failures of\n",
            "individual nodes, components, and software. With data augmentation added to\n",
            "these settings, there is a critical need for robust and efficient aggregation\n",
            "systems. We define the quality of workers as reconstruction ratios $\\in (0,1]$,\n",
            "and formulate aggregation as a Maximum Likelihood Estimation procedure using\n",
            "Beta densities. We show that the Regularized form of log-likelihood wrt\n",
            "subspace can be approximately solved using iterative least squares solver, and\n",
            "provide convergence guarantees using recent Convex Optimization landscape\n",
            "results. Our empirical findings demonstrate that our approach significantly\n",
            "enhances the robustness of state-of-the-art Byzantine resilient aggregators. We\n",
            "evaluate our method in a distributed setup with a parameter server, and show\n",
            "simultaneous improvements in communication efficiency and accuracy across\n",
            "various tasks. The code is publicly available at\n",
            "https://github.com/hamidralmasi/FlagAggregator\n",
            "\n",
            "197. Title: Continuous-Multiple Image Outpainting in One-Step via Positional Query and A Diffusion-based Approach\n",
            "   Abstract: Graph neural networks (GNNs) have been widely used to predict properties and\n",
            "heuristics of mixed-integer linear programs (MILPs) and hence accelerate MILP\n",
            "solvers. This paper investigates the capacity of GNNs to represent strong\n",
            "branching (SB), the most effective yet computationally expensive heuristic\n",
            "employed in the branch-and-bound algorithm. In the literature, message-passing\n",
            "GNN (MP-GNN), as the simplest GNN structure, is frequently used as a fast\n",
            "approximation of SB and we find that not all MILPs's SB can be represented with\n",
            "MP-GNN. We precisely define a class of \"MP-tractable\" MILPs for which MP-GNNs\n",
            "can accurately approximate SB scores. Particularly, we establish a universal\n",
            "approximation theorem: for any data distribution over the MP-tractable class,\n",
            "there always exists an MP-GNN that can approximate the SB score with\n",
            "arbitrarily high accuracy and arbitrarily high probability, which lays a\n",
            "theoretical foundation of the existing works on imitating SB with MP-GNN. For\n",
            "MILPs without the MP-tractability, unfortunately, a similar result is\n",
            "impossible, which can be illustrated by two MILP instances with different SB\n",
            "scores that cannot be distinguished by any MP-GNN, regardless of the number of\n",
            "parameters. Recognizing this, we explore another GNN structure called the\n",
            "second-order folklore GNN (2-FGNN) that overcomes this limitation, and the\n",
            "aforementioned universal approximation theorem can be extended to the entire\n",
            "MILP space using 2-FGNN, regardless of the MP-tractability. A small-scale\n",
            "numerical experiment is conducted to directly validate our theoretical\n",
            "findings.\n",
            "\n",
            "198. Title: TimeMixer: Decomposable Multiscale Mixing for Time Series Forecasting\n",
            "   Abstract: Pretrained language models sometimes possess knowledge that we do not wish\n",
            "them to, including memorized personal information and knowledge that could be\n",
            "used to harm people. They can also output toxic or harmful text. To mitigate\n",
            "these safety and informational issues, we propose an attack-and-defense\n",
            "framework for studying the task of deleting sensitive information directly from\n",
            "model weights. We study direct edits to model weights because (1) this approach\n",
            "should guarantee that particular deleted information is never extracted by\n",
            "future prompt attacks, and (2) it should protect against whitebox attacks,\n",
            "which is necessary for making claims about safety/privacy in a setting where\n",
            "publicly available model weights could be used to elicit sensitive information.\n",
            "Our threat model assumes that an attack succeeds if the answer to a sensitive\n",
            "question is located among a set of B generated candidates, based on scenarios\n",
            "where the information would be insecure if the answer is among B candidates.\n",
            "Experimentally, we show that even state-of-the-art model editing methods such\n",
            "as ROME struggle to truly delete factual information from models like GPT-J, as\n",
            "our whitebox and blackbox attacks can recover \"deleted\" information from an\n",
            "edited model 38% of the time. These attacks leverage two key observations: (1)\n",
            "that traces of deleted information can be found in intermediate model hidden\n",
            "states, and (2) that applying an editing method for one question may not delete\n",
            "information across rephrased versions of the question. Finally, we provide new\n",
            "defense methods that protect against some extraction attacks, but we do not\n",
            "find a single universally effective defense method. Our results suggest that\n",
            "truly deleting sensitive information is a tractable but difficult problem,\n",
            "since even relatively low attack success rates have potentially severe societal\n",
            "implications for real-world deployment of language models.\n",
            "\n",
            "199. Title: Heterogeneous Personalized Federated Learning by Local-Global Updates Mixing via Convergence Rate\n",
            "   Abstract: Image outpainting aims to generate the content of an input sub-image beyond\n",
            "its original boundaries. It is an important task in content generation yet\n",
            "remains an open problem for generative models. This paper pushes the technical\n",
            "frontier of image outpainting in two directions that have not been resolved in\n",
            "literature: 1) outpainting with arbitrary and continuous multiples (without\n",
            "restriction), and 2) outpainting in a single step (even for large expansion\n",
            "multiples). Moreover, we develop a method that does not depend on a pre-trained\n",
            "backbone network, which is in contrast commonly required by the previous SOTA\n",
            "outpainting methods. The arbitrary multiple outpainting is achieved by\n",
            "utilizing randomly cropped views from the same image during training to capture\n",
            "arbitrary relative positional information. Specifically, by feeding one view\n",
            "and positional embeddings as queries, we can reconstruct another view. At\n",
            "inference, we generate images with arbitrary expansion multiples by inputting\n",
            "an anchor image and its corresponding positional embeddings. The one-step\n",
            "outpainting ability here is particularly noteworthy in contrast to previous\n",
            "methods that need to be performed for $N$ times to obtain a final multiple\n",
            "which is $N$ times of its basic and fixed multiple. We evaluate the proposed\n",
            "approach (called PQDiff as we adopt a diffusion-based generator as our\n",
            "embodiment, under our proposed \\textbf{P}ositional \\textbf{Q}uery scheme) on\n",
            "public benchmarks, demonstrating its superior performance over state-of-the-art\n",
            "approaches. Specifically, PQDiff achieves state-of-the-art FID scores on the\n",
            "Scenery (\\textbf{21.512}), Building Facades (\\textbf{25.310}), and WikiArts\n",
            "(\\textbf{36.212}) datasets. Furthermore, under the 2.25x, 5x and 11.7x\n",
            "outpainting settings, PQDiff only takes \\textbf{40.6\\%}, \\textbf{20.3\\%} and\n",
            "\\textbf{10.2\\%} of the time of the benchmark state-of-the-art (SOTA) method.\n",
            "\n",
            "200. Title: Free from Bellman Completeness: Trajectory Stitching via Model-based Return-conditioned Supervised Learning\n",
            "   Abstract: Statistical data heterogeneity is a significant barrier to convergence in\n",
            "federated learning (FL). While prior work has advanced heterogeneous FL through\n",
            "better optimization objectives, these methods fall short when there is extreme\n",
            "data heterogeneity among collaborating participants. We hypothesize that\n",
            "convergence under extreme data heterogeneity is primarily hindered due to the\n",
            "aggregation of conflicting updates from the participants in the initial\n",
            "collaboration rounds. To overcome this problem, we propose a warmup phase where\n",
            "each participant learns a personalized mask and updates only a subnetwork of\n",
            "the full model. This personalized warmup allows the participants to focus\n",
            "initially on learning specific subnetworks tailored to the heterogeneity of\n",
            "their data. After the warmup phase, the participants revert to standard\n",
            "federated optimization, where all parameters are communicated. We empirically\n",
            "demonstrate that the proposed personalized warmup via subnetworks (FedPeWS)\n",
            "approach improves accuracy and convergence speed over standard federated\n",
            "optimization methods.\n",
            "\n",
            "201. Title: FreeDyG: Frequency Enhanced Continuous-Time Dynamic Graph Model for Link Prediction\n",
            "   Abstract: Off-policy dynamic programming (DP) techniques such as $Q$-learning have\n",
            "proven to be important in sequential decision-making problems. In the presence\n",
            "of function approximation, however, these techniques often diverge due to the\n",
            "absence of Bellman completeness in the function classes considered, a crucial\n",
            "condition for the success of DP-based methods. In this paper, we show how\n",
            "off-policy learning techniques based on return-conditioned supervised learning\n",
            "(RCSL) are able to circumvent these challenges of Bellman completeness,\n",
            "converging under significantly more relaxed assumptions inherited from\n",
            "supervised learning. We prove there exists a natural environment in which if\n",
            "one uses two-layer multilayer perceptron as the function approximator, the\n",
            "layer width needs to grow linearly with the state space size to satisfy Bellman\n",
            "completeness while a constant layer width is enough for RCSL. These findings\n",
            "take a step towards explaining the superior empirical performance of RCSL\n",
            "methods compared to DP-based methods in environments with near-optimal\n",
            "datasets. Furthermore, in order to learn from sub-optimal datasets, we propose\n",
            "a simple framework called MBRCSL, granting RCSL methods the ability of dynamic\n",
            "programming to stitch together segments from distinct trajectories. MBRCSL\n",
            "leverages learned dynamics models and forward sampling to accomplish trajectory\n",
            "stitching while avoiding the need for Bellman completeness that plagues all\n",
            "dynamic programming algorithms. We propose both theoretical analysis and\n",
            "experimental evaluation to back these claims, outperforming state-of-the-art\n",
            "model-free and model-based offline RL algorithms across several simulated\n",
            "robotics problems.\n",
            "\n",
            "202. Title: CompA: Addressing the Gap in Compositional Reasoning in Audio-Language Models\n",
            "   Abstract: The information bottleneck (IB) approach is popular to improve the\n",
            "generalization, robustness and explainability of deep neural networks.\n",
            "Essentially, it aims to find a minimum sufficient representation $\\mathbf{t}$\n",
            "by striking a trade-off between a compression term $I(\\mathbf{x};\\mathbf{t})$\n",
            "and a prediction term $I(y;\\mathbf{t})$, where $I(\\cdot;\\cdot)$ refers to the\n",
            "mutual information (MI). MI is for the IB for the most part expressed in terms\n",
            "of the Kullback-Leibler (KL) divergence, which in the regression case\n",
            "corresponds to prediction based on mean squared error (MSE) loss with Gaussian\n",
            "assumption and compression approximated by variational inference. In this\n",
            "paper, we study the IB principle for the regression problem and develop a new\n",
            "way to parameterize the IB with deep neural networks by exploiting favorable\n",
            "properties of the Cauchy-Schwarz (CS) divergence. By doing so, we move away\n",
            "from MSE-based regression and ease estimation by avoiding variational\n",
            "approximations or distributional assumptions. We investigate the improved\n",
            "generalization ability of our proposed CS-IB and demonstrate strong adversarial\n",
            "robustness guarantees. We demonstrate its superior performance on six\n",
            "real-world regression tasks over other popular deep IB approaches. We\n",
            "additionally observe that the solutions discovered by CS-IB always achieve the\n",
            "best trade-off between prediction accuracy and compression ratio in the\n",
            "information plane. The code is available at\n",
            "\\url{https://github.com/SJYuCNEL/Cauchy-Schwarz-Information-Bottleneck}.\n",
            "\n",
            "203. Title: CPPO: Continual Learning for Reinforcement Learning with Human Feedback\n",
            "   Abstract: Recent breakthroughs in diffusion models have exhibited exceptional\n",
            "image-generation capabilities. However, studies show that some outputs are\n",
            "merely replications of training data. Such replications present potential legal\n",
            "challenges for model owners, especially when the generated content contains\n",
            "proprietary information. In this work, we introduce a straightforward yet\n",
            "effective method for detecting memorized prompts by inspecting the magnitude of\n",
            "text-conditional predictions. Our proposed method seamlessly integrates without\n",
            "disrupting sampling algorithms, and delivers high accuracy even at the first\n",
            "generation step, with a single generation per prompt. Building on our detection\n",
            "strategy, we unveil an explainable approach that shows the contribution of\n",
            "individual words or tokens to memorization. This offers an interactive medium\n",
            "for users to adjust their prompts. Moreover, we propose two strategies i.e., to\n",
            "mitigate memorization by leveraging the magnitude of text-conditional\n",
            "predictions, either through minimization during inference or filtering during\n",
            "training. These proposed strategies effectively counteract memorization while\n",
            "maintaining high-generation quality. Code is available at\n",
            "https://github.com/YuxinWenRick/diffusion_memorization.\n",
            "\n",
            "204. Title: Local Graph Clustering with Noisy Labels\n",
            "   Abstract: Large discrete action spaces (LDAS) remain a central challenge in\n",
            "reinforcement learning. Existing solution approaches can handle unstructured\n",
            "LDAS with up to a few million actions. However, many real-world applications in\n",
            "logistics, production, and transportation systems have combinatorial action\n",
            "spaces, whose size grows well beyond millions of actions, even on small\n",
            "instances. Fortunately, such action spaces exhibit structure, e.g., equally\n",
            "spaced discrete resource units. With this work, we focus on handling structured\n",
            "LDAS (SLDAS) with sizes that cannot be handled by current benchmarks: we\n",
            "propose Dynamic Neighborhood Construction (DNC), a novel exploitation paradigm\n",
            "for SLDAS. We present a scalable neighborhood exploration heuristic that\n",
            "utilizes this paradigm and efficiently explores the discrete neighborhood\n",
            "around the continuous proxy action in structured action spaces with up to\n",
            "$10^{73}$ actions. We demonstrate the performance of our method by benchmarking\n",
            "it against three state-of-the-art approaches designed for large discrete action\n",
            "spaces across two distinct environments. Our results show that DNC matches or\n",
            "outperforms state-of-the-art approaches while being computationally more\n",
            "efficient. Furthermore, our method scales to action spaces that so far remained\n",
            "computationally intractable for existing methodologies.\n",
            "\n",
            "205. Title: Language-Interfaced Tabular Oversampling via Progressive Imputation and Self-Authentication\n",
            "   Abstract: The growing interest in machine learning problems over graphs with additional\n",
            "node information such as texts, images, or labels has popularized methods that\n",
            "require the costly operation of processing the entire graph. Yet, little effort\n",
            "has been made to the development of fast local methods (i.e. without accessing\n",
            "the entire graph) that extract useful information from such data. To that end,\n",
            "we propose a study of local graph clustering using noisy node labels as a proxy\n",
            "for additional node information. In this setting, nodes receive initial binary\n",
            "labels based on cluster affiliation: 1 if they belong to the target cluster and\n",
            "0 otherwise. Subsequently, a fraction of these labels is flipped. We\n",
            "investigate the benefits of incorporating noisy labels for local graph\n",
            "clustering. By constructing a weighted graph with such labels, we study the\n",
            "performance of graph diffusion-based local clustering method on both the\n",
            "original and the weighted graphs. From a theoretical perspective, we consider\n",
            "recovering an unknown target cluster with a single seed node in a random graph\n",
            "with independent noisy node labels. We provide sufficient conditions on the\n",
            "label noise under which, with high probability, using diffusion in the weighted\n",
            "graph yields a more accurate recovery of the target cluster. This approach\n",
            "proves more effective than using the given labels alone or using diffusion in\n",
            "the label-free original graph. Empirically, we show that reliable node labels\n",
            "can be obtained with just a few samples from an attributed graph. Moreover,\n",
            "utilizing these labels via diffusion in the weighted graph leads to\n",
            "significantly better local clustering performance across several real-world\n",
            "datasets, improving F1 scores by up to 13%.\n",
            "\n",
            "206. Title: Learning with a Mole: Transferable latent spatial representations for navigation without reconstruction\n",
            "   Abstract: Missing data imputation (MDI) is crucial when dealing with tabular datasets\n",
            "across various domains. Autoencoders can be trained to reconstruct missing\n",
            "values, and graph autoencoders (GAE) can additionally consider similar patterns\n",
            "in the dataset when imputing new values for a given instance. However,\n",
            "previously proposed GAEs suffer from scalability issues, requiring the user to\n",
            "define a similarity metric among patterns to build the graph connectivity\n",
            "beforehand. In this paper, we leverage recent progress in latent graph\n",
            "imputation to propose a novel EdGe Generation Graph AutoEncoder (EGG-GAE) for\n",
            "missing data imputation that overcomes these two drawbacks. EGG-GAE works on\n",
            "randomly sampled mini-batches of the input data (hence scaling to larger\n",
            "datasets), and it automatically infers the best connectivity across the\n",
            "mini-batch for each architecture layer. We also experiment with several\n",
            "extensions, including an ensemble strategy for inference and the inclusion of\n",
            "what we call prototype nodes, obtaining significant improvements, both in terms\n",
            "of imputation error and final downstream accuracy, across multiple benchmarks\n",
            "and baselines.\n",
            "\n",
            "207. Title: CoBIT: A Contrastive Bi-directional Image-Text Generation Model\n",
            "   Abstract: In this paper, we propose a policy gradient method for confounded partially\n",
            "observable Markov decision processes (POMDPs) with continuous state and\n",
            "observation spaces in the offline setting. We first establish a novel\n",
            "identification result to non-parametrically estimate any history-dependent\n",
            "policy gradient under POMDPs using the offline data. The identification enables\n",
            "us to solve a sequence of conditional moment restrictions and adopt the min-max\n",
            "learning procedure with general function approximation for estimating the\n",
            "policy gradient. We then provide a finite-sample non-asymptotic bound for\n",
            "estimating the gradient uniformly over a pre-specified policy class in terms of\n",
            "the sample size, length of horizon, concentratability coefficient and the\n",
            "measure of ill-posedness in solving the conditional moment restrictions.\n",
            "Lastly, by deploying the proposed gradient estimation in the gradient ascent\n",
            "algorithm, we show the global convergence of the proposed algorithm in finding\n",
            "the history-dependent optimal policy under some technical conditions. To the\n",
            "best of our knowledge, this is the first work studying the policy gradient\n",
            "method for POMDPs under the offline setting.\n",
            "\n",
            "208. Title: Fast-ELECTRA for Efficient Pre-training\n",
            "   Abstract: Federated Learning is an emerging learning paradigm that allows training\n",
            "models from samples distributed across a large network of clients while\n",
            "respecting privacy and communication restrictions. Despite its success,\n",
            "federated learning faces several challenges related to its decentralized\n",
            "nature. In this work, we develop a novel algorithmic procedure with theoretical\n",
            "speedup guarantees that simultaneously handles two of these hurdles, namely (i)\n",
            "data heterogeneity, i.e., data distributions can vary substantially across\n",
            "clients, and (ii) system heterogeneity, i.e., the computational power of the\n",
            "clients could differ significantly. Our method relies on ideas from\n",
            "representation learning theory to find a global common representation using all\n",
            "clients' data and learn a user-specific set of parameters leading to a\n",
            "personalized solution for each client. Furthermore, our method mitigates the\n",
            "effects of stragglers by adaptively selecting clients based on their\n",
            "computational characteristics and statistical significance, thus achieving, for\n",
            "the first time, near optimal sample complexity and provable logarithmic\n",
            "speedup. Experimental results support our theoretical findings showing the\n",
            "superiority of our method over alternative personalized federated schemes in\n",
            "system and data heterogeneous environments.\n",
            "\n",
            "209. Title: MgNO: Efficient Parameterization of Linear Operators via Multigrid\n",
            "   Abstract: Agents navigating in 3D environments require some form of memory, which\n",
            "should hold a compact and actionable representation of the history of\n",
            "observations useful for decision taking and planning. In most end-to-end\n",
            "learning approaches the representation is latent and usually does not have a\n",
            "clearly defined interpretation, whereas classical robotics addresses this with\n",
            "scene reconstruction resulting in some form of map, usually estimated with\n",
            "geometry and sensor models and/or learning. In this work we propose to learn an\n",
            "actionable representation of the scene independently of the targeted downstream\n",
            "task and without explicitly optimizing reconstruction. The learned\n",
            "representation is optimized by a blind auxiliary agent trained to navigate with\n",
            "it on multiple short sub episodes branching out from a waypoint and, most\n",
            "importantly, without any direct visual observation. We argue and show that the\n",
            "blindness property is important and forces the (trained) latent representation\n",
            "to be the only means for planning. With probing experiments we show that the\n",
            "learned representation optimizes navigability and not reconstruction. On\n",
            "downstream tasks we show that it is robust to changes in distribution, in\n",
            "particular the sim2real gap, which we evaluate with a real physical robot in a\n",
            "real office building, significantly improving performance.\n",
            "\n",
            "210. Title: R&B: Region and Boundary Aware Zero-shot Grounded Text-to-image Generation\n",
            "   Abstract: Pre-training on graph neural networks (GNNs) aims to learn transferable\n",
            "knowledge for downstream tasks with unlabeled data, and it has recently become\n",
            "an active research area. The success of graph pre-training models is often\n",
            "attributed to the massive amount of input data. In this paper, however, we\n",
            "identify the curse of big data phenomenon in graph pre-training: more training\n",
            "data do not necessarily lead to better downstream performance. Motivated by\n",
            "this observation, we propose a better-with-less framework for graph\n",
            "pre-training: fewer, but carefully chosen data are fed into a GNN model to\n",
            "enhance pre-training. The proposed pre-training pipeline is called the\n",
            "data-active graph pre-training (APT) framework, and is composed of a graph\n",
            "selector and a pre-training model. The graph selector chooses the most\n",
            "representative and instructive data points based on the inherent properties of\n",
            "graphs as well as predictive uncertainty. The proposed predictive uncertainty,\n",
            "as feedback from the pre-training model, measures the confidence level of the\n",
            "model in the data. When fed with the chosen data, on the other hand, the\n",
            "pre-training model grasps an initial understanding of the new, unseen data, and\n",
            "at the same time attempts to remember the knowledge learned from previous data.\n",
            "Therefore, the integration and interaction between these two components form a\n",
            "unified framework (APT), in which graph pre-training is performed in a\n",
            "progressive and iterative way. Experiment results show that the proposed APT is\n",
            "able to obtain an efficient pre-training model with fewer training data and\n",
            "better downstream performance.\n",
            "\n",
            "211. Title: Context is Environment\n",
            "   Abstract: Class incremental learning (CIL) is a challenging setting of continual\n",
            "learning, which learns a series of tasks sequentially. Each task consists of a\n",
            "set of unique classes. The key feature of CIL is that no task identifier (or\n",
            "task-id) is provided at test time. Predicting the task-id for each test sample\n",
            "is a challenging problem. An emerging theory-guided approach (called TIL+OOD)\n",
            "is to train a task-specific model for each task in a shared network for all\n",
            "tasks based on a task-incremental learning (TIL) method to deal with\n",
            "catastrophic forgetting. The model for each task is an out-of-distribution\n",
            "(OOD) detector rather than a conventional classifier. The OOD detector can\n",
            "perform both within-task (in-distribution (IND)) class prediction and OOD\n",
            "detection. The OOD detection capability is the key to task-id prediction during\n",
            "inference. However, this paper argues that using a traditional OOD detector for\n",
            "task-id prediction is sub-optimal because additional information (e.g., the\n",
            "replay data and the learned tasks) available in CIL can be exploited to design\n",
            "a better and principled method for task-id prediction. We call the new method\n",
            "TPL (Task-id Prediction based on Likelihood Ratio). TPL markedly outperforms\n",
            "strong CIL baselines and has negligible catastrophic forgetting. The code of\n",
            "TPL is publicly available at https://github.com/linhaowei1/TPL.\n",
            "\n",
            "212. Title: OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models\n",
            "   Abstract: Link prediction, a fundamental task on graphs, has proven indispensable in\n",
            "various applications, e.g., friend recommendation, protein analysis, and drug\n",
            "interaction prediction. However, since datasets span a multitude of domains,\n",
            "they could have distinct underlying mechanisms of link formation. Evidence in\n",
            "existing literature underscores the absence of a universally best algorithm\n",
            "suitable for all datasets. In this paper, we endeavor to explore principles of\n",
            "link prediction across diverse datasets from a data-centric perspective. We\n",
            "recognize three fundamental factors critical to link prediction: local\n",
            "structural proximity, global structural proximity, and feature proximity. We\n",
            "then unearth relationships among those factors where (i) global structural\n",
            "proximity only shows effectiveness when local structural proximity is\n",
            "deficient. (ii) The incompatibility can be found between feature and structural\n",
            "proximity. Such incompatibility leads to GNNs for Link Prediction (GNN4LP)\n",
            "consistently underperforming on edges where the feature proximity factor\n",
            "dominates. Inspired by these new insights from a data perspective, we offer\n",
            "practical instruction for GNN4LP model design and guidelines for selecting\n",
            "appropriate benchmark datasets for more comprehensive evaluations.\n",
            "\n",
            "213. Title: Prometheus: Inducing Fine-Grained Evaluation Capability in Language Models\n",
            "   Abstract: Two lines of work are taking the central stage in AI research. On the one\n",
            "hand, the community is making increasing efforts to build models that discard\n",
            "spurious correlations and generalize better in novel test environments.\n",
            "Unfortunately, the bitter lesson so far is that no proposal convincingly\n",
            "outperforms a simple empirical risk minimization baseline. On the other hand,\n",
            "large language models (LLMs) have erupted as algorithms able to learn\n",
            "in-context, generalizing on-the-fly to eclectic contextual circumstances that\n",
            "users enforce by means of prompting. In this paper, we argue that context is\n",
            "environment, and posit that in-context learning holds the key to better domain\n",
            "generalization. Via extensive theory and experiments, we show that paying\n",
            "attention to context$\\unicode{x2013}\\unicode{x2013}$unlabeled examples as they\n",
            "arrive$\\unicode{x2013}\\unicode{x2013}$allows our proposed In-Context Risk\n",
            "Minimization (ICRM) algorithm to zoom-in on the test environment risk\n",
            "minimizer, leading to significant out-of-distribution performance improvements.\n",
            "From all of this, two messages are worth taking home. Researchers in domain\n",
            "generalization should consider environment as context, and harness the adaptive\n",
            "power of in-context learning. Researchers in LLMs should consider context as\n",
            "environment, to better structure data towards generalization.\n",
            "\n",
            "214. Title: Amortized Network Intervention to Steer the Excitatory Point Processes\n",
            "   Abstract: In this paper, we proposed a novel pipeline for image-level classification in\n",
            "the hyperspectral images. By doing this, we show that the discriminative\n",
            "spectral information at image-level features lead to significantly improved\n",
            "performance in a face recognition task. We also explored the potential of\n",
            "traditional feature descriptors in the hyperspectral images. From our\n",
            "evaluations, we observe that SIFT features outperform the state-of-the-art\n",
            "hyperspectral face recognition methods, and also the other descriptors. With\n",
            "the increasing deployment of hyperspectral sensors in a multitude of\n",
            "applications, we believe that our approach can effectively exploit the spectral\n",
            "information in hyperspectral images, thus beneficial to more accurate\n",
            "classification.\n",
            "\n",
            "215. Title: Neural Auto-designer for Enhanced Quantum Kernels\n",
            "   Abstract: Excitatory point processes (i.e., event flows) occurring over dynamic graphs\n",
            "(i.e., evolving topologies) provide a fine-grained model to capture how\n",
            "discrete events may spread over time and space. How to effectively steer the\n",
            "event flows by modifying the dynamic graph structures presents an interesting\n",
            "problem, motivated by curbing the spread of infectious diseases through\n",
            "strategically locking down cities to mitigating traffic congestion via traffic\n",
            "light optimization. To address the intricacies of planning and overcome the\n",
            "high dimensionality inherent to such decision-making problems, we design an\n",
            "Amortized Network Interventions (ANI) framework, allowing for the pooling of\n",
            "optimal policies from history and other contexts while ensuring a permutation\n",
            "equivalent property. This property enables efficient knowledge transfer and\n",
            "sharing across diverse contexts. Each task is solved by an H-step lookahead\n",
            "model-based reinforcement learning, where neural ODEs are introduced to model\n",
            "the dynamics of the excitatory point processes. Instead of simulating rollouts\n",
            "from the dynamics model, we derive an analytical mean-field approximation for\n",
            "the event flows given the dynamics, making the online planning more efficiently\n",
            "solvable. We empirically illustrate that this ANI approach substantially\n",
            "enhances policy learning for unseen dynamics and exhibits promising outcomes in\n",
            "steering event flows through network intervention using synthetic and real\n",
            "COVID datasets.\n",
            "\n",
            "216. Title: Poisoned Forgery Face: Towards Backdoor Attacks on Face Forgery Detection\n",
            "   Abstract: Quantum kernels hold great promise for offering computational advantages over\n",
            "classical learners, with the effectiveness of these kernels closely tied to the\n",
            "design of the quantum feature map. However, the challenge of designing\n",
            "effective quantum feature maps for real-world datasets, particularly in the\n",
            "absence of sufficient prior information, remains a significant obstacle. In\n",
            "this study, we present a data-driven approach that automates the design of\n",
            "problem-specific quantum feature maps. Our approach leverages feature-selection\n",
            "techniques to handle high-dimensional data on near-term quantum machines with\n",
            "limited qubits, and incorporates a deep neural predictor to efficiently\n",
            "evaluate the performance of various candidate quantum kernels. Through\n",
            "extensive numerical simulations on different datasets, we demonstrate the\n",
            "superiority of our proposal over prior methods, especially for the capability\n",
            "of eliminating the kernel concentration issue and identifying the feature map\n",
            "with prediction advantages. Our work not only unlocks the potential of quantum\n",
            "kernels for enhancing real-world tasks but also highlights the substantial role\n",
            "of deep learning in advancing quantum machine learning.\n",
            "\n",
            "217. Title: Finite Scalar Quantization: VQ-VAE Made Simple\n",
            "   Abstract: The proliferation of face forgery techniques has raised significant concerns\n",
            "within society, thereby motivating the development of face forgery detection\n",
            "methods. These methods aim to distinguish forged faces from genuine ones and\n",
            "have proven effective in practical applications. However, this paper introduces\n",
            "a novel and previously unrecognized threat in face forgery detection scenarios\n",
            "caused by backdoor attack. By embedding backdoors into models and incorporating\n",
            "specific trigger patterns into the input, attackers can deceive detectors into\n",
            "producing erroneous predictions for forged faces. To achieve this goal, this\n",
            "paper proposes \\emph{Poisoned Forgery Face} framework, which enables\n",
            "clean-label backdoor attacks on face forgery detectors. Our approach involves\n",
            "constructing a scalable trigger generator and utilizing a novel convolving\n",
            "process to generate translation-sensitive trigger patterns. Moreover, we employ\n",
            "a relative embedding method based on landmark-based regions to enhance the\n",
            "stealthiness of the poisoned samples. Consequently, detectors trained on our\n",
            "poisoned samples are embedded with backdoors. Notably, our approach surpasses\n",
            "SoTA backdoor baselines with a significant improvement in attack success rate\n",
            "(+16.39\\% BD-AUC) and reduction in visibility (-12.65\\% $L_\\infty$).\n",
            "Furthermore, our attack exhibits promising performance against backdoor\n",
            "defenses. We anticipate that this paper will draw greater attention to the\n",
            "potential threats posed by backdoor attacks in face forgery detection\n",
            "scenarios. Our codes will be made available at\n",
            "\\url{https://github.com/JWLiang007/PFF}\n",
            "\n",
            "218. Title: Combining Axes Preconditioners through Kronecker Approximation for Deep Learning\n",
            "   Abstract: We propose to replace vector quantization (VQ) in the latent representation\n",
            "of VQ-VAEs with a simple scheme termed finite scalar quantization (FSQ), where\n",
            "we project the VAE representation down to a few dimensions (typically less than\n",
            "10). Each dimension is quantized to a small set of fixed values, leading to an\n",
            "(implicit) codebook given by the product of these sets. By appropriately\n",
            "choosing the number of dimensions and values each dimension can take, we obtain\n",
            "the same codebook size as in VQ. On top of such discrete representations, we\n",
            "can train the same models that have been trained on VQ-VAE representations. For\n",
            "example, autoregressive and masked transformer models for image generation,\n",
            "multimodal generation, and dense prediction computer vision tasks. Concretely,\n",
            "we employ FSQ with MaskGIT for image generation, and with UViM for depth\n",
            "estimation, colorization, and panoptic segmentation. Despite the much simpler\n",
            "design of FSQ, we obtain competitive performance in all these tasks. We\n",
            "emphasize that FSQ does not suffer from codebook collapse and does not need the\n",
            "complex machinery employed in VQ (commitment losses, codebook reseeding, code\n",
            "splitting, entropy penalties, etc.) to learn expressive discrete\n",
            "representations.\n",
            "\n",
            "219. Title: DiffEnc: Variational Diffusion with a Learned Encoder\n",
            "   Abstract: We present latent combinational game design -- an approach for generating\n",
            "playable games that blend a given set of games in a desired combination using\n",
            "deep generative latent variable models. We use Gaussian Mixture Variational\n",
            "Autoencoders (GMVAEs) which model the VAE latent space via a mixture of\n",
            "Gaussian components. Through supervised training, each component encodes levels\n",
            "from one game and lets us define blended games as linear combinations of these\n",
            "components. This enables generating new games that blend the input games as\n",
            "well as controlling the relative proportions of each game in the blend. We also\n",
            "extend prior blending work using conditional VAEs and compare against the GMVAE\n",
            "and additionally introduce a hybrid conditional GMVAE (CGMVAE) architecture\n",
            "which lets us generate whole blended levels and layouts. Results show that\n",
            "these approaches can generate playable games that blend the input games in\n",
            "specified combinations. We use both platformers and dungeon-based games to\n",
            "demonstrate our results.\n",
            "\n",
            "220. Title: Learning Nash Equilibria in Rank-1 Games\n",
            "   Abstract: Recent works have empirically analyzed in-context learning and shown that\n",
            "transformers trained on synthetic linear regression tasks can learn to\n",
            "implement ridge regression, which is the Bayes-optimal predictor, given\n",
            "sufficient capacity [Aky\\\"urek et al., 2023], while one-layer transformers with\n",
            "linear self-attention and no MLP layer will learn to implement one step of\n",
            "gradient descent (GD) on a least-squares linear regression objective [von\n",
            "Oswald et al., 2022]. However, the theory behind these observations remains\n",
            "poorly understood. We theoretically study transformers with a single layer of\n",
            "linear self-attention, trained on synthetic noisy linear regression data.\n",
            "First, we mathematically show that when the covariates are drawn from a\n",
            "standard Gaussian distribution, the one-layer transformer which minimizes the\n",
            "pre-training loss will implement a single step of GD on the least-squares\n",
            "linear regression objective. Then, we find that changing the distribution of\n",
            "the covariates and weight vector to a non-isotropic Gaussian distribution has a\n",
            "strong impact on the learned algorithm: the global minimizer of the\n",
            "pre-training loss now implements a single step of $\\textit{pre-conditioned}$\n",
            "GD. However, if only the distribution of the responses is changed, then this\n",
            "does not have a large effect on the learned algorithm: even when the response\n",
            "comes from a more general family of $\\textit{nonlinear}$ functions, the global\n",
            "minimizer of the pre-training loss still implements a single step of GD on a\n",
            "least-squares linear regression objective.\n",
            "\n",
            "221. Title: What does automatic differentiation compute for neural networks?\n",
            "   Abstract: Parameter-efficient fine-tuning for pre-trained Vision Transformers aims to\n",
            "adeptly tailor a model to downstream tasks by learning a minimal set of new\n",
            "adaptation parameters while preserving the frozen majority of pre-trained\n",
            "parameters. Striking a balance between retaining the generalizable\n",
            "representation capacity of the pre-trained model and acquiring task-specific\n",
            "features poses a key challenge. Currently, there is a lack of focus on guiding\n",
            "this delicate trade-off. In this study, we approach the problem from the\n",
            "perspective of Singular Value Decomposition (SVD) of pre-trained parameter\n",
            "matrices, providing insights into the tuning dynamics of existing methods.\n",
            "Building upon this understanding, we propose a Residual-based Low-Rank\n",
            "Rescaling (RLRR) fine-tuning strategy. This strategy not only enhances\n",
            "flexibility in parameter tuning but also ensures that new parameters do not\n",
            "deviate excessively from the pre-trained model through a residual design.\n",
            "Extensive experiments demonstrate that our method achieves competitive\n",
            "performance across various downstream image classification tasks, all while\n",
            "maintaining comparable new parameters. We believe this work takes a step\n",
            "forward in offering a unified perspective for interpreting existing methods and\n",
            "serves as motivation for the development of new approaches that move closer to\n",
            "effectively considering the crucial trade-off mentioned above. Our code is\n",
            "available at\n",
            "\\href{https://github.com/zstarN70/RLRR.git}{https://github.com/zstarN70/RLRR.git}.\n",
            "\n",
            "222. Title: MUSTARD: Mastering Uniform Synthesis of Theorem and Proof Data\n",
            "   Abstract: Given a rank-1 bimatrix game (A,B), i.e., where rank(A+B)=1, we construct a\n",
            "suitable linear subspace of the rank-1 game space and show that this subspace\n",
            "is homeomorphic to its Nash equilibrium correspondence. Using this\n",
            "homeomorphism, we give the first polynomial time algorithm for computing an\n",
            "exact Nash equilibrium of a rank-1 bimatrix game. This settles an open question\n",
            "posed in Kannan and Theobald (SODA 2007) and Theobald (2007). In addition, we\n",
            "give a novel algorithm to enumerate all the Nash equilibria of a rank-1 game\n",
            "and show that a similar technique may also be applied for finding a Nash\n",
            "equilibrium of any bimatrix game. This technique also proves the existence,\n",
            "oddness and the index theorem of Nash equilibria in a bimatrix game. Further,\n",
            "we extend the rank-1 homeomorphism result to a fixed rank game space, and give\n",
            "a fixed point formulation on $[0,1]^k$ for solving a rank-k game. The\n",
            "homeomorphism and the fixed point formulation are piece-wise linear and\n",
            "considerably simpler than the classical constructions.\n",
            "\n",
            "223. Title: Learning Flexible Body Collision Dynamics with Hierarchical Contact Mesh Transformer\n",
            "   Abstract: TextCNN, the convolutional neural network for text, is a useful deep learning\n",
            "algorithm for sentence classification tasks such as sentiment analysis and\n",
            "question classification. However, neural networks have long been known as black\n",
            "boxes because interpreting them is a challenging task. Researchers have\n",
            "developed several tools to understand a CNN for image classification by deep\n",
            "visualization, but research about deep TextCNNs is still insufficient. In this\n",
            "paper, we are trying to understand what a TextCNN learns on two classical NLP\n",
            "datasets. Our work focuses on functions of different convolutional kernels and\n",
            "correlations between convolutional kernels.\n",
            "\n",
            "224. Title: On gauge freedom, conservativity and intrinsic dimensionality estimation in diffusion models\n",
            "   Abstract: Recently, many mesh-based graph neural network (GNN) models have been\n",
            "proposed for modeling complex high-dimensional physical systems. Remarkable\n",
            "achievements have been made in significantly reducing the solving time compared\n",
            "to traditional numerical solvers. These methods are typically designed to i)\n",
            "reduce the computational cost in solving physical dynamics and/or ii) propose\n",
            "techniques to enhance the solution accuracy in fluid and rigid body dynamics.\n",
            "However, it remains under-explored whether they are effective in addressing the\n",
            "challenges of flexible body dynamics, where instantaneous collisions occur\n",
            "within a very short timeframe. In this paper, we present Hierarchical Contact\n",
            "Mesh Transformer (HCMT), which uses hierarchical mesh structures and can learn\n",
            "long-range dependencies (occurred by collisions) among spatially distant\n",
            "positions of a body -- two close positions in a higher-level mesh correspond to\n",
            "two distant positions in a lower-level mesh. HCMT enables long-range\n",
            "interactions, and the hierarchical mesh structure quickly propagates collision\n",
            "effects to faraway positions. To this end, it consists of a contact mesh\n",
            "Transformer and a hierarchical mesh Transformer (CMT and HMT, respectively).\n",
            "Lastly, we propose a flexible body dynamics dataset, consisting of trajectories\n",
            "that reflect experimental settings frequently used in the display industry for\n",
            "product designs. We also compare the performance of several baselines using\n",
            "well-known benchmark datasets. Our results show that HCMT provides significant\n",
            "performance improvements over existing methods. Our code is available at\n",
            "https://github.com/yuyudeep/hcmt.\n",
            "\n",
            "225. Title: SPDER: Semiperiodic Damping-Enabled Object Representation\n",
            "   Abstract: Robust locomotion control depends on accurate state estimations. However, the\n",
            "sensors of most legged robots can only provide partial and noisy observations,\n",
            "making the estimation particularly challenging, especially for external states\n",
            "like terrain frictions and elevation maps. Inspired by the classical Internal\n",
            "Model Control principle, we consider these external states as disturbances and\n",
            "introduce Hybrid Internal Model (HIM) to estimate them according to the\n",
            "response of the robot. The response, which we refer to as the hybrid internal\n",
            "embedding, contains the robot's explicit velocity and implicit stability\n",
            "representation, corresponding to two primary goals for locomotion tasks:\n",
            "explicitly tracking velocity and implicitly maintaining stability. We use\n",
            "contrastive learning to optimize the embedding to be close to the robot's\n",
            "successor state, in which the response is naturally embedded. HIM has several\n",
            "appealing benefits: It only needs the robot's proprioceptions, i.e., those from\n",
            "joint encoders and IMU as observations. It innovatively maintains consistent\n",
            "observations between simulation reference and reality that avoids information\n",
            "loss in mimicking learning. It exploits batch-level information that is more\n",
            "robust to noises and keeps better sample efficiency. It only requires 1 hour of\n",
            "training on an RTX 4090 to enable a quadruped robot to traverse any terrain\n",
            "under any disturbances. A wealth of real-world experiments demonstrates its\n",
            "agility, even in high-difficulty tasks and cases never occurred during the\n",
            "training process, revealing remarkable open-world generalizability.\n",
            "\n",
            "226. Title: Hybrid Internal Model: Learning Agile Legged Locomotion with Simulated Robot Response\n",
            "   Abstract: Diffusion models are generative models that have recently demonstrated\n",
            "impressive performances in terms of sampling quality and density estimation in\n",
            "high dimensions. They rely on a forward continuous diffusion process and a\n",
            "backward continuous denoising process, which can be described by a\n",
            "time-dependent vector field and is used as a generative model. In the original\n",
            "formulation of the diffusion model, this vector field is assumed to be the\n",
            "score function (i.e. it is the gradient of the log-probability at a given time\n",
            "in the diffusion process). Curiously, on the practical side, most studies on\n",
            "diffusion models implement this vector field as a neural network function and\n",
            "do not constrain it be the gradient of some energy function (that is, most\n",
            "studies do not constrain the vector field to be conservative). Even though some\n",
            "studies investigated empirically whether such a constraint will lead to a\n",
            "performance gain, they lead to contradicting results and failed to provide\n",
            "analytical results. Here, we provide three analytical results regarding the\n",
            "extent of the modeling freedom of this vector field. {Firstly, we propose a\n",
            "novel decomposition of vector fields into a conservative component and an\n",
            "orthogonal component which satisfies a given (gauge) freedom. Secondly, from\n",
            "this orthogonal decomposition, we show that exact density estimation and exact\n",
            "sampling is achieved when the conservative component is exactly equals to the\n",
            "true score and therefore conservativity is neither necessary nor sufficient to\n",
            "obtain exact density estimation and exact sampling. Finally, we show that when\n",
            "it comes to inferring local information of the data manifold, constraining the\n",
            "vector field to be conservative is desirable.\n",
            "\n",
            "227. Title: Learning Decentralized Partially Observable Mean Field Control for Artificial Collective Behavior\n",
            "   Abstract: Recent reinforcement learning (RL) methods have achieved success in various\n",
            "domains. However, multi-agent RL (MARL) remains a challenge in terms of\n",
            "decentralization, partial observability and scalability to many agents.\n",
            "Meanwhile, collective behavior requires resolution of the aforementioned\n",
            "challenges, and remains of importance to many state-of-the-art applications\n",
            "such as active matter physics, self-organizing systems, opinion dynamics, and\n",
            "biological or robotic swarms. Here, MARL via mean field control (MFC) offers a\n",
            "potential solution to scalability, but fails to consider decentralized and\n",
            "partially observable systems. In this paper, we enable decentralized behavior\n",
            "of agents under partial information by proposing novel models for decentralized\n",
            "partially observable MFC (Dec-POMFC), a broad class of problems with\n",
            "permutation-invariant agents allowing for reduction to tractable single-agent\n",
            "Markov decision processes (MDP) with single-agent RL solution. We provide\n",
            "rigorous theoretical results, including a dynamic programming principle,\n",
            "together with optimality guarantees for Dec-POMFC solutions applied to finite\n",
            "swarms of interest. Algorithmically, we propose Dec-POMFC-based policy gradient\n",
            "methods for MARL via centralized training and decentralized execution, together\n",
            "with policy gradient approximation guarantees. In addition, we improve upon\n",
            "state-of-the-art histogram-based MFC by kernel methods, which is of separate\n",
            "interest also for fully observable MFC. We evaluate numerically on\n",
            "representative collective behavior tasks such as adapted Kuramoto and Vicsek\n",
            "swarming models, being on par with state-of-the-art MARL. Overall, our\n",
            "framework takes a step towards RL-based engineering of artificial collective\n",
            "behavior via MFC.\n",
            "\n",
            "228. Title: Multi-granularity Correspondence Learning from Long-term Noisy Videos\n",
            "   Abstract: We present a novel podcast recommender system deployed at industrial scale.\n",
            "This system successfully optimizes personal listening journeys that unfold over\n",
            "months for hundreds of millions of listeners. In deviating from the pervasive\n",
            "industry practice of optimizing machine learning algorithms for short-term\n",
            "proxy metrics, the system substantially improves long-term performance in A/B\n",
            "tests. The paper offers insights into how our methods cope with attribution,\n",
            "coordination, and measurement challenges that usually hinder such long-term\n",
            "optimization. To contextualize these practical insights within a broader\n",
            "academic framework, we turn to reinforcement learning (RL). Using the language\n",
            "of RL, we formulate a comprehensive model of users' recurring relationships\n",
            "with a recommender system. Then, within this model, we identify our approach as\n",
            "a policy improvement update to a component of the existing recommender system,\n",
            "enhanced by tailored modeling of value functions and user-state\n",
            "representations. Illustrative offline experiments suggest this specialized\n",
            "modeling reduces data requirements by as much as a factor of 120,000 compared\n",
            "to black-box approaches.\n",
            "\n",
            "229. Title: A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis\n",
            "   Abstract: Neural field methods have seen great progress in various long-standing tasks\n",
            "in computer vision and computer graphics, including novel view synthesis and\n",
            "geometry reconstruction. As existing neural field methods try to predict some\n",
            "coordinate-based continuous target values, such as RGB for Neural Radiance\n",
            "Field (NeRF), all of these methods are regression models and are optimized by\n",
            "some regression loss. However, are regression models really better than\n",
            "classification models for neural field methods? In this work, we try to visit\n",
            "this very fundamental but overlooked question for neural fields from a machine\n",
            "learning perspective. We successfully propose a novel Neural Field Classifier\n",
            "(NFC) framework which formulates existing neural field methods as\n",
            "classification tasks rather than regression tasks. The proposed NFC can easily\n",
            "transform arbitrary Neural Field Regressor (NFR) into its classification\n",
            "variant via employing a novel Target Encoding module and optimizing a\n",
            "classification loss. By encoding a continuous regression target into a\n",
            "high-dimensional discrete encoding, we naturally formulate a multi-label\n",
            "classification task. Extensive experiments demonstrate the impressive\n",
            "effectiveness of NFC at the nearly free extra computational costs. Moreover,\n",
            "NFC also shows robustness to sparse inputs, corrupted images, and dynamic\n",
            "scenes.\n",
            "\n",
            "230. Title: Neural Field Classifiers via Target Encoding and Classification Loss\n",
            "   Abstract: We argue that the theory and practice of diffusion-based generative models\n",
            "are currently unnecessarily convoluted and seek to remedy the situation by\n",
            "presenting a design space that clearly separates the concrete design choices.\n",
            "This lets us identify several changes to both the sampling and training\n",
            "processes, as well as preconditioning of the score networks. Together, our\n",
            "improvements yield new state-of-the-art FID of 1.79 for CIFAR-10 in a\n",
            "class-conditional setting and 1.97 in an unconditional setting, with much\n",
            "faster sampling (35 network evaluations per image) than prior designs. To\n",
            "further demonstrate their modular nature, we show that our design changes\n",
            "dramatically improve both the efficiency and quality obtainable with\n",
            "pre-trained score networks from previous work, including improving the FID of a\n",
            "previously trained ImageNet-64 model from 2.07 to near-SOTA 1.55, and after\n",
            "re-training with our proposed improvements to a new SOTA of 1.36.\n",
            "\n",
            "231. Title: Towards Understanding Factual Knowledge of Large Language Models\n",
            "   Abstract: Pre-trained large language models (LLMs) have recently achieved better\n",
            "generalization and sample efficiency in autonomous web automation. However, the\n",
            "performance on real-world websites has still suffered from (1) open domainness,\n",
            "(2) limited context length, and (3) lack of inductive bias on HTML. We\n",
            "introduce WebAgent, an LLM-driven agent that learns from self-experience to\n",
            "complete tasks on real websites following natural language instructions.\n",
            "WebAgent plans ahead by decomposing instructions into canonical\n",
            "sub-instructions, summarizes long HTML documents into task-relevant snippets,\n",
            "and acts on websites via Python programs generated from those. We design\n",
            "WebAgent with Flan-U-PaLM, for grounded code generation, and HTML-T5, new\n",
            "pre-trained LLMs for long HTML documents using local and global attention\n",
            "mechanisms and a mixture of long-span denoising objectives, for planning and\n",
            "summarization. We empirically demonstrate that our modular recipe improves the\n",
            "success on real websites by over 50%, and that HTML-T5 is the best model to\n",
            "solve various HTML understanding tasks; achieving 18.7% higher success rate\n",
            "than the prior method on MiniWoB web automation benchmark, and SoTA performance\n",
            "on Mind2Web, an offline task planning evaluation.\n",
            "\n",
            "232. Title: On Double Descent in Reinforcement Learning with LSTD and Random Features\n",
            "   Abstract: In CRYPTO'19, Gohr proposed a new cryptanalysis strategy using machine\n",
            "learning algorithms. Combining the differential-neural distinguisher with a\n",
            "differential path and integrating the advanced key recovery procedure, Gohr\n",
            "achieved a 12-round key recovery attack on Speck32/64. Chen and Yu improved\n",
            "prediction accuracy of differential-neural distinguisher considering derived\n",
            "features from multiple-ciphertext pairs instead of single-ciphertext pairs. By\n",
            "modifying the kernel size of initial convolutional layer to capture more\n",
            "dimensional information, the prediction accuracy of differential-neural\n",
            "distinguisher can be improved for for three reduced symmetric ciphers. For DES,\n",
            "we improve the prediction accuracy of (5-6)-round differential-neural\n",
            "distinguisher and train a new 7-round differential-neural distinguisher. For\n",
            "Chaskey, we improve the prediction accuracy of (3-4)-round differential-neural\n",
            "distinguisher. For PRESENT, we improve the prediction accuracy of (6-7)-round\n",
            "differential-neural distinguisher. The source codes are available in\n",
            "https://drive.google.com/drive/folders/1i0RciZlGZsEpCyW-wQAy7zzJeOLJNWqL?usp=sharing.\n",
            "\n",
            "233. Title: Leave-one-out Distinguishability in Machine Learning\n",
            "   Abstract: Large language models (LLMs) are a promising venue for natural language\n",
            "understanding and generation tasks. However, current LLMs are far from\n",
            "reliable: they are prone to generate non-factual information and, more\n",
            "crucially, to contradict themselves when prompted to reason about beliefs of\n",
            "the world. These problems are currently addressed with large scale fine-tuning\n",
            "or by delegating consistent reasoning to external tools. In this work, we\n",
            "strive for a middle ground and introduce a training objective based on\n",
            "principled probabilistic reasoning that teaches a LLM to be consistent with\n",
            "external knowledge in the form of a set of facts and rules. Fine-tuning with\n",
            "our loss on a limited set of facts enables our LLMs to be more logically\n",
            "consistent than previous baselines and allows them to extrapolate to unseen but\n",
            "semantically similar factual knowledge more systematically.\n",
            "\n",
            "234. Title: Most discriminative stimuli for functional cell type clustering\n",
            "   Abstract: Innovations like protein diffusion have enabled significant progress in de\n",
            "novo protein design, which is a vital topic in life science. These methods\n",
            "typically depend on protein structure encoders to model residue backbone\n",
            "frames, where atoms do not exist. Most prior encoders rely on atom-wise\n",
            "features, such as angles and distances between atoms, which are not available\n",
            "in this context. Thus far, only several simple encoders, such as IPA, have been\n",
            "proposed for this scenario, exposing the frame modeling as a bottleneck. In\n",
            "this work, we proffer the Vector Field Network (VFN), which enables network\n",
            "layers to perform learnable vector computations between coordinates of\n",
            "frame-anchored virtual atoms, thus achieving a higher capability for modeling\n",
            "frames. The vector computation operates in a manner similar to a linear layer,\n",
            "with each input channel receiving 3D virtual atom coordinates instead of scalar\n",
            "values. The multiple feature vectors output by the vector computation are then\n",
            "used to update the residue representations and virtual atom coordinates via\n",
            "attention aggregation. Remarkably, VFN also excels in modeling both frames and\n",
            "atoms, as the real atoms can be treated as the virtual atoms for modeling,\n",
            "positioning VFN as a potential universal encoder. In protein diffusion (frame\n",
            "modeling), VFN exhibits an impressive performance advantage over IPA, excelling\n",
            "in terms of both designability (67.04% vs. 53.58%) and diversity (66.54% vs.\n",
            "51.98%). In inverse folding (frame and atom modeling), VFN outperforms the\n",
            "previous SoTA model, PiFold (54.7% vs. 51.66%), on sequence recovery rate. We\n",
            "also propose a method of equipping VFN with the ESM model, which significantly\n",
            "surpasses the previous ESM-based SoTA (62.67% vs. 55.65%), LM-Design, by a\n",
            "substantial margin.\n",
            "\n",
            "235. Title: TOSS: High-quality Text-guided Novel View Synthesis from a Single Image\n",
            "   Abstract: Identifying cell types and understanding their functional properties is\n",
            "crucial for unraveling the mechanisms underlying perception and cognition. In\n",
            "the retina, functional types can be identified by carefully selected stimuli,\n",
            "but this requires expert domain knowledge and biases the procedure towards\n",
            "previously known cell types. In the visual cortex, it is still unknown what\n",
            "functional types exist and how to identify them. Thus, for unbiased\n",
            "identification of the functional cell types in retina and visual cortex, new\n",
            "approaches are needed. Here we propose an optimization-based clustering\n",
            "approach using deep predictive models to obtain functional clusters of neurons\n",
            "using Most Discriminative Stimuli (MDS). Our approach alternates between\n",
            "stimulus optimization with cluster reassignment akin to an\n",
            "expectation-maximization algorithm. The algorithm recovers functional clusters\n",
            "in mouse retina, marmoset retina and macaque visual area V4. This demonstrates\n",
            "that our approach can successfully find discriminative stimuli across species,\n",
            "stages of the visual system and recording techniques. The resulting most\n",
            "discriminative stimuli can be used to assign functional cell types fast and on\n",
            "the fly, without the need to train complex predictive models or show a large\n",
            "natural scene dataset, paving the way for experiments that were previously\n",
            "limited by experimental time. Crucially, MDS are interpretable: they visualize\n",
            "the distinctive stimulus patterns that most unambiguously identify a specific\n",
            "type of neuron.\n",
            "\n",
            "236. Title: Bootstrapping Variational Information Pursuit with Large Language and Vision Models for Interpretable Image Classification\n",
            "   Abstract: In this work we present an approach for generating alternative text (or\n",
            "alt-text) descriptions for images shared on social media, specifically Twitter.\n",
            "More than just a special case of image captioning, alt-text is both more\n",
            "literally descriptive and context-specific. Also critically, images posted to\n",
            "Twitter are often accompanied by user-written text that despite not necessarily\n",
            "describing the image may provide useful context that if properly leveraged can\n",
            "be informative. We address this task with a multimodal model that conditions on\n",
            "both textual information from the associated social media post as well as\n",
            "visual signal from the image, and demonstrate that the utility of these two\n",
            "information sources stacks. We put forward a new dataset of 371k images paired\n",
            "with alt-text and tweets scraped from Twitter and evaluate on it across a\n",
            "variety of automated metrics as well as human evaluation. We show that our\n",
            "approach of conditioning on both tweet text and visual information\n",
            "significantly outperforms prior work, by more than 2x on BLEU@4.\n",
            "\n",
            "237. Title: AGILE3D: Attention Guided Interactive Multi-object 3D Segmentation\n",
            "   Abstract: Temporal Difference (TD) algorithms are widely used in Deep Reinforcement\n",
            "Learning (RL). Their performance is heavily influenced by the size of the\n",
            "neural network. While in supervised learning, the regime of\n",
            "over-parameterization and its benefits are well understood, the situation in RL\n",
            "is much less clear. In this paper, we present a theoretical analysis of the\n",
            "influence of network size and $l_2$-regularization on performance. We identify\n",
            "the ratio between the number of parameters and the number of visited states as\n",
            "a crucial factor and define over-parameterization as the regime when it is\n",
            "larger than one. Furthermore, we observe a double descent phenomenon, i.e., a\n",
            "sudden drop in performance around the parameter/state ratio of one. Leveraging\n",
            "random features and the lazy training regime, we study the regularized\n",
            "Least-Square Temporal Difference (LSTD) algorithm in an asymptotic regime, as\n",
            "both the number of parameters and states go to infinity, maintaining a constant\n",
            "ratio. We derive deterministic limits of both the empirical and the true\n",
            "Mean-Squared Bellman Error (MSBE) that feature correction terms responsible for\n",
            "the double descent. Correction terms vanish when the $l_2$-regularization is\n",
            "increased or the number of unvisited states goes to zero. Numerical experiments\n",
            "with synthetic and small real-world environments closely match the theoretical\n",
            "predictions.\n",
            "\n",
            "238. Title: Machine Unlearning for Image-to-Image Generative Models\n",
            "   Abstract: Bayesian optimization (BO) has established itself as a leading strategy for\n",
            "efficiently optimizing expensive-to-evaluate functions. Existing BO methods\n",
            "mostly rely on Gaussian process (GP) surrogate models and are not applicable to\n",
            "(doubly-stochastic) Gaussian Cox processes, where the observation process is\n",
            "modulated by a latent intensity function modeled as a GP. In this paper, we\n",
            "propose a novel maximum a posteriori inference of Gaussian Cox processes. It\n",
            "leverages the Laplace approximation and change of kernel technique to transform\n",
            "the problem into a new reproducing kernel Hilbert space, where it becomes more\n",
            "tractable computationally. It enables us to obtain both a functional posterior\n",
            "of the latent intensity function and the covariance of the posterior, thus\n",
            "extending existing works that often focus on specific link functions or\n",
            "estimating the posterior mean. Using the result, we propose a BO framework\n",
            "based on the Gaussian Cox process model and further develop a Nystr\\\"om\n",
            "approximation for efficient computation. Extensive evaluations on various\n",
            "synthetic and real-world datasets demonstrate significant improvement over\n",
            "state-of-the-art inference solutions for Gaussian Cox processes, as well as\n",
            "effective BO with a wide range of acquisition functions designed through the\n",
            "underlying Gaussian Cox process model.\n",
            "\n",
            "239. Title: Bayesian Optimization through Gaussian Cox Process Models for Spatio-temporal Data\n",
            "   Abstract: The generation of molecules with desired properties has become increasingly\n",
            "popular, revolutionizing the way scientists design molecular structures and\n",
            "providing valuable support for chemical and drug design. However, despite the\n",
            "potential of language models in molecule generation, they face challenges such\n",
            "as generating syntactically or chemically flawed molecules, having narrow\n",
            "domain focus, and struggling to create diverse and feasible molecules due to\n",
            "limited annotated data or external molecular databases. To tackle these\n",
            "challenges, we introduce MolGen, a pre-trained molecular language model\n",
            "tailored specifically for molecule generation. Through the reconstruction of\n",
            "over 100 million molecular SELFIES, MolGen internalizes structural and\n",
            "grammatical insights. This is further enhanced by domain-agnostic molecular\n",
            "prefix tuning, fostering robust knowledge transfer across diverse domains.\n",
            "Importantly, our chemical feedback paradigm steers the model away from\n",
            "molecular hallucinations, ensuring alignment between the model's estimated\n",
            "probabilities and real-world chemical preferences. Extensive experiments on\n",
            "well-known benchmarks underscore MolGen's optimization capabilities in\n",
            "properties such as penalized logP, QED, and molecular docking. Additional\n",
            "analyses confirm its proficiency in accurately capturing molecule\n",
            "distributions, discerning intricate structural patterns, and efficiently\n",
            "exploring the chemical space. Code is available at\n",
            "https://github.com/zjunlp/MolGen.\n",
            "\n",
            "240. Title: On the Learnability of Watermarks for Language Models\n",
            "   Abstract: Once users have shared their data online, it is generally difficult for them\n",
            "to revoke access and ask for the data to be deleted. Machine learning (ML)\n",
            "exacerbates this problem because any model trained with said data may have\n",
            "memorized it, putting users at risk of a successful privacy attack exposing\n",
            "their information. Yet, having models unlearn is notoriously difficult. We\n",
            "introduce SISA training, a framework that expedites the unlearning process by\n",
            "strategically limiting the influence of a data point in the training procedure.\n",
            "While our framework is applicable to any learning algorithm, it is designed to\n",
            "achieve the largest improvements for stateful algorithms like stochastic\n",
            "gradient descent for deep neural networks. SISA training reduces the\n",
            "computational overhead associated with unlearning, even in the worst-case\n",
            "setting where unlearning requests are made uniformly across the training set.\n",
            "In some cases, the service provider may have a prior on the distribution of\n",
            "unlearning requests that will be issued by users. We may take this prior into\n",
            "account to partition and order data accordingly, and further decrease overhead\n",
            "from unlearning. Our evaluation spans several datasets from different domains,\n",
            "with corresponding motivations for unlearning. Under no distributional\n",
            "assumptions, for simple learning tasks, we observe that SISA training improves\n",
            "time to unlearn points from the Purchase dataset by 4.63x, and 2.45x for the\n",
            "SVHN dataset, over retraining from scratch. SISA training also provides a\n",
            "speed-up of 1.36x in retraining for complex learning tasks such as ImageNet\n",
            "classification; aided by transfer learning, this results in a small degradation\n",
            "in accuracy. Our work contributes to practical data governance in machine\n",
            "unlearning.\n",
            "\n",
            "241. Title: Time Fairness in Online Knapsack Problems\n",
            "   Abstract: We are interested in enabling visual planning for complex long-horizon tasks\n",
            "in the space of generated videos and language, leveraging recent advances in\n",
            "large generative models pretrained on Internet-scale data. To this end, we\n",
            "present video language planning (VLP), an algorithm that consists of a tree\n",
            "search procedure, where we train (i) vision-language models to serve as both\n",
            "policies and value functions, and (ii) text-to-video models as dynamics models.\n",
            "VLP takes as input a long-horizon task instruction and current image\n",
            "observation, and outputs a long video plan that provides detailed multimodal\n",
            "(video and language) specifications that describe how to complete the final\n",
            "task. VLP scales with increasing computation budget where more computation time\n",
            "results in improved video plans, and is able to synthesize long-horizon video\n",
            "plans across different robotics domains: from multi-object rearrangement, to\n",
            "multi-camera bi-arm dexterous manipulation. Generated video plans can be\n",
            "translated into real robot actions via goal-conditioned policies, conditioned\n",
            "on each intermediate frame of the generated video. Experiments show that VLP\n",
            "substantially improves long-horizon task success rates compared to prior\n",
            "methods on both simulated and real robots (across 3 hardware platforms).\n",
            "\n",
            "242. Title: DataInf: Efficiently Estimating Data Influence in LoRA-tuned LLMs and Diffusion Models\n",
            "   Abstract: Language models' (LMs) proficiency in handling deterministic symbolic\n",
            "reasoning and rule-based tasks remains limited due to their dependency implicit\n",
            "learning on textual data. To endow LMs with genuine rule comprehension\n",
            "abilities, we propose \"Neural Comprehension\" - a framework that synergistically\n",
            "integrates compiled neural networks (CoNNs) into the standard transformer\n",
            "architecture. CoNNs are neural modules designed to explicitly encode rules\n",
            "through artificially generated attention weights. By incorporating CoNN\n",
            "modules, the Neural Comprehension framework enables LMs to accurately and\n",
            "robustly execute rule-intensive symbolic tasks. Extensive experiments\n",
            "demonstrate the superiority of our approach over existing techniques in terms\n",
            "of length generalization, efficiency, and interpretability for symbolic\n",
            "operations. Furthermore, it can be applied to LMs across different model\n",
            "scales, outperforming tool-calling methods in arithmetic reasoning tasks while\n",
            "maintaining superior inference efficiency. Our work highlights the potential of\n",
            "seamlessly unifying explicit rule learning via CoNNs and implicit pattern\n",
            "learning in LMs, paving the way for true symbolic comprehension capabilities.\n",
            "\n",
            "243. Title: Mastering Symbolic Operations: Augmenting Language Models with Compiled Neural Networks\n",
            "   Abstract: To draw scientifically meaningful conclusions and build reliable models of\n",
            "quantitative phenomena, cause and effect must be taken into consideration\n",
            "(either implicitly or explicitly). This is particularly challenging when the\n",
            "measurements are not from controlled experimental (interventional) settings,\n",
            "since cause and effect can be obscured by spurious, indirect influences. Modern\n",
            "predictive techniques from machine learning are capable of capturing\n",
            "high-dimensional, nonlinear relationships between variables while relying on\n",
            "few parametric or probabilistic model assumptions. However, since these\n",
            "techniques are associational, applied to observational data they are prone to\n",
            "picking up spurious influences from non-experimental (observational) data,\n",
            "making their predictions unreliable. Techniques from causal inference, such as\n",
            "probabilistic causal diagrams and do-calculus, provide powerful (nonparametric)\n",
            "tools for drawing causal inferences from such observational data. However,\n",
            "these techniques are often incompatible with modern, nonparametric machine\n",
            "learning algorithms since they typically require explicit probabilistic models.\n",
            "Here, we develop causal bootstrapping for augmenting classical nonparametric\n",
            "bootstrap resampling with information on the causal relationship between\n",
            "variables. This makes it possible to resample observational data such that, if\n",
            "it is possible to identify an interventional relationship from that data, new\n",
            "data representing that relationship can be simulated from the original\n",
            "observational data. In this way, we can use modern machine learning algorithms\n",
            "unaltered to make statistically powerful, yet causally-robust, predictions. We\n",
            "develop several causal bootstrapping algorithms for drawing interventional\n",
            "inferences from observational data, for classification and regression problems,\n",
            "and demonstrate, using synthetic and real-world examples, the value of this\n",
            "approach.\n",
            "\n",
            "244. Title: Video Language Planning\n",
            "   Abstract: The online knapsack problem is a classic problem in the field of online\n",
            "algorithms. Its canonical version asks how to pack items of different values\n",
            "and weights arriving online into a capacity-limited knapsack so as to maximize\n",
            "the total value of the admitted items. Although optimal competitive algorithms\n",
            "are known for this problem, they may be fundamentally unfair, i.e., individual\n",
            "items may be treated inequitably in different ways. We formalize a\n",
            "practically-relevant notion of time fairness which effectively models a trade\n",
            "off between static and dynamic pricing in a motivating application such as\n",
            "cloud resource allocation, and show that existing algorithms perform poorly\n",
            "under this metric. We propose a parameterized deterministic algorithm where the\n",
            "parameter precisely captures the Pareto-optimal trade-off between fairness\n",
            "(static pricing) and competitiveness (dynamic pricing). We show that\n",
            "randomization is theoretically powerful enough to be simultaneously competitive\n",
            "and fair; however, it does not work well in experiments. To further improve the\n",
            "trade-off between fairness and competitiveness, we develop a nearly-optimal\n",
            "learning-augmented algorithm which is fair, consistent, and robust\n",
            "(competitive), showing substantial performance improvements in numerical\n",
            "experiments.\n",
            "\n",
            "245. Title: Domain-Agnostic Molecular Generation with Chemical Feedback\n",
            "   Abstract: We study risk-sensitive Reinforcement Learning (RL), where we aim to maximize\n",
            "the Conditional Value at Risk (CVaR) with a fixed risk tolerance $\\tau$. Prior\n",
            "theoretical work studying risk-sensitive RL focuses on the tabular Markov\n",
            "Decision Processes (MDPs) setting. To extend CVaR RL to settings where state\n",
            "space is large, function approximation must be deployed. We study CVaR RL in\n",
            "low-rank MDPs with nonlinear function approximation. Low-rank MDPs assume the\n",
            "underlying transition kernel admits a low-rank decomposition, but unlike prior\n",
            "linear models, low-rank MDPs do not assume the feature or state-action\n",
            "representation is known. We propose a novel Upper Confidence Bound (UCB)\n",
            "bonus-driven algorithm to carefully balance the interplay between exploration,\n",
            "exploitation, and representation learning in CVaR RL. We prove that our\n",
            "algorithm achieves a sample complexity of $\\tilde{O}\\left(\\frac{H^7 A^2\n",
            "d^4}{\\tau^2 \\epsilon^2}\\right)$ to yield an $\\epsilon$-optimal CVaR, where $H$\n",
            "is the length of each episode, $A$ is the capacity of action space, and $d$ is\n",
            "the dimension of representations. Computational-wise, we design a novel\n",
            "discretized Least-Squares Value Iteration (LSVI) algorithm for the CVaR\n",
            "objective as the planning oracle and show that we can find the near-optimal\n",
            "policy in a polynomial running time with a Maximum Likelihood Estimation\n",
            "oracle. To our knowledge, this is the first provably efficient CVaR RL\n",
            "algorithm in low-rank MDPs.\n",
            "\n",
            "246. Title: Entropy is not Enough for Test-Time Adaptation: From the Perspective of Disentangled Factors\n",
            "   Abstract: Watermarking of language model outputs enables statistical detection of\n",
            "model-generated text, which can mitigate harms and misuses of language models.\n",
            "Existing watermarking strategies operate by altering the decoder of an existing\n",
            "language model. In this paper, we ask whether language models can directly\n",
            "learn to generate watermarked text, which would have significant implications\n",
            "for the real-world deployment of watermarks. First, learned watermarks could be\n",
            "used to build open models that naturally generate watermarked text, enabling\n",
            "watermarking for open models, where users can control the decoding procedure.\n",
            "Second, if watermarking is used to determine the provenance of generated text,\n",
            "an adversary can hurt the reputation of a victim model by spoofing its\n",
            "watermark and generating damaging watermarked text. To investigate the\n",
            "learnability of watermarks, we propose watermark distillation, which trains a\n",
            "student model to behave like a teacher model that uses decoding-based\n",
            "watermarking. We test our approach on three decoding-based watermarking\n",
            "strategies and various hyperparameter settings, finding that models can learn\n",
            "to generate watermarked text with high detectability. We also find limitations\n",
            "to learnability, including the loss of watermarking capabilities under\n",
            "fine-tuning on normal text and high sample complexity when learning\n",
            "low-distortion watermarks.\n",
            "\n",
            "247. Title: Leveraging Generative Models for Unsupervised Alignment of Neural Time Series Data\n",
            "   Abstract: Koopman representations aim to learn features of nonlinear dynamical systems\n",
            "(NLDS) which lead to linear dynamics in the latent space. Theoretically, such\n",
            "features can be used to simplify many problems in modeling and control of NLDS.\n",
            "In this work we study autoencoder formulations of this problem, and different\n",
            "ways they can be used to model dynamics, specifically for future state\n",
            "prediction over long horizons. We discover several limitations of predicting\n",
            "future states in the latent space and propose an inference-time mechanism,\n",
            "which we refer to as Periodic Reencoding, for faithfully capturing long term\n",
            "dynamics. We justify this method both analytically and empirically via\n",
            "experiments in low and high dimensional NLDS.\n",
            "\n",
            "248. Title: Semantic Flow: Learning Semantic Fields of Dynamic Scenes from Monocular Videos\n",
            "   Abstract: In this work, we pioneer Semantic Flow, a neural semantic representation of\n",
            "dynamic scenes from monocular videos. In contrast to previous NeRF methods that\n",
            "reconstruct dynamic scenes from the colors and volume densities of individual\n",
            "points, Semantic Flow learns semantics from continuous flows that contain rich\n",
            "3D motion information. As there is 2D-to-3D ambiguity problem in the viewing\n",
            "direction when extracting 3D flow features from 2D video frames, we consider\n",
            "the volume densities as opacity priors that describe the contributions of flow\n",
            "features to the semantics on the frames. More specifically, we first learn a\n",
            "flow network to predict flows in the dynamic scene, and propose a flow feature\n",
            "aggregation module to extract flow features from video frames. Then, we propose\n",
            "a flow attention module to extract motion information from flow features, which\n",
            "is followed by a semantic network to output semantic logits of flows. We\n",
            "integrate the logits with volume densities in the viewing direction to\n",
            "supervise the flow features with semantic labels on video frames. Experimental\n",
            "results show that our model is able to learn from multiple dynamic scenes and\n",
            "supports a series of new tasks such as instance-level scene editing, semantic\n",
            "completions, dynamic scene tracking and semantic adaption on novel scenes.\n",
            "Codes are available at https://github.com/tianfr/Semantic-Flow/.\n",
            "\n",
            "249. Title: Dirichlet-based Per-Sample Weighting by Transition Matrix for Noisy Label Learning\n",
            "   Abstract: Noisy multi-label learning has garnered increasing attention due to the\n",
            "challenges posed by collecting large-scale accurate labels, making noisy labels\n",
            "a more practical alternative. Motivated by noisy multi-class learning, the\n",
            "introduction of transition matrices can help model multi-label noise and enable\n",
            "the development of statistically consistent algorithms for noisy multi-label\n",
            "learning. However, estimating multi-label noise transition matrices remains a\n",
            "challenging task, as most existing estimators in noisy multi-class learning\n",
            "rely on anchor points and accurate fitting of noisy class posteriors, which is\n",
            "hard to satisfy in noisy multi-label learning. In this paper, we address this\n",
            "problem by first investigating the identifiability of class-dependent\n",
            "transition matrices in noisy multi-label learning. Building upon the\n",
            "identifiability results, we propose a novel estimator that leverages label\n",
            "correlations without the need for anchor points or precise fitting of noisy\n",
            "class posteriors. Specifically, we first estimate the occurrence probability of\n",
            "two noisy labels to capture noisy label correlations. Subsequently, we employ\n",
            "sample selection techniques to extract information implying clean label\n",
            "correlations, which are then used to estimate the occurrence probability of one\n",
            "noisy label when a certain clean label appears. By exploiting the mismatches in\n",
            "label correlations implied by these occurrence probabilities, we demonstrate\n",
            "that the transition matrix becomes identifiable and can be acquired by solving\n",
            "a bilinear decomposition problem. Theoretically, we establish an estimation\n",
            "error bound for our multi-label transition matrix estimator and derive a\n",
            "generalization error bound for our statistically consistent algorithm.\n",
            "Empirically, we validate the effectiveness of our estimator in estimating\n",
            "multi-label noise transition matrices, leading to excellent classification\n",
            "performance.\n",
            "\n",
            "250. Title: Hyper Evidential Deep Learning to Quantify Composite Classification Uncertainty\n",
            "   Abstract: Federated learning (FL) has been widely deployed to enable machine learning\n",
            "training on sensitive data across distributed devices. However, the\n",
            "decentralized learning paradigm and heterogeneity of FL further extend the\n",
            "attack surface for backdoor attacks. Existing FL attack and defense\n",
            "methodologies typically focus on the whole model. None of them recognizes the\n",
            "existence of backdoor-critical (BC) layers-a small subset of layers that\n",
            "dominate the model vulnerabilities. Attacking the BC layers achieves equivalent\n",
            "effects as attacking the whole model but at a far smaller chance of being\n",
            "detected by state-of-the-art (SOTA) defenses. This paper proposes a general\n",
            "in-situ approach that identifies and verifies BC layers from the perspective of\n",
            "attackers. Based on the identified BC layers, we carefully craft a new backdoor\n",
            "attack methodology that adaptively seeks a fundamental balance between\n",
            "attacking effects and stealthiness under various defense strategies. Extensive\n",
            "experiments show that our BC layer-aware backdoor attacks can successfully\n",
            "backdoor FL under seven SOTA defenses with only 10% malicious clients and\n",
            "outperform the latest backdoor attack methods.\n",
            "\n",
            "251. Title: SpeechTokenizer: Unified Speech Tokenizer for Speech Language Models\n",
            "   Abstract: Deep neural networks (DNNs) have been shown to perform well on exclusive,\n",
            "multi-class classification tasks. However, when different classes have similar\n",
            "visual features, it becomes challenging for human annotators to differentiate\n",
            "them. This scenario necessitates the use of composite class labels. In this\n",
            "paper, we propose a novel framework called Hyper-Evidential Neural Network\n",
            "(HENN) that explicitly models predictive uncertainty due to composite class\n",
            "labels in training data in the context of the belief theory called Subjective\n",
            "Logic (SL). By placing a grouped Dirichlet distribution on the class\n",
            "probabilities, we treat predictions of a neural network as parameters of\n",
            "hyper-subjective opinions and learn the network that collects both single and\n",
            "composite evidence leading to these hyper-opinions by a deterministic DNN from\n",
            "data. We introduce a new uncertainty type called vagueness originally designed\n",
            "for hyper-opinions in SL to quantify composite classification uncertainty for\n",
            "DNNs. Our results demonstrate that HENN outperforms its state-of-the-art\n",
            "counterparts based on four image datasets. The code and datasets are available\n",
            "at: https://github.com/Hugo101/HyperEvidentialNN.\n",
            "\n",
            "252. Title: Backdoor Federated Learning by Poisoning Backdoor-Critical Layers\n",
            "   Abstract: While backpropagation (BP) is the mainstream approach for gradient\n",
            "computation in neural network training, its heavy reliance on the chain rule of\n",
            "differentiation constrains the designing flexibility of network architecture\n",
            "and training pipelines. We avoid the recursive computation in BP and develop a\n",
            "unified likelihood ratio (ULR) method for gradient estimation with just one\n",
            "forward propagation. Not only can ULR be extended to train a wide variety of\n",
            "neural network architectures, but the computation flow in BP can also be\n",
            "rearranged by ULR for better device adaptation. Moreover, we propose several\n",
            "variance reduction techniques to further accelerate the training process. Our\n",
            "experiments offer numerical results across diverse aspects, including various\n",
            "neural network training scenarios, computation flow rearrangement, and\n",
            "fine-tuning of pre-trained models. All findings demonstrate that ULR\n",
            "effectively enhances the flexibility of neural network training by permitting\n",
            "localized module training without compromising the global objective and\n",
            "significantly boosts the network robustness.\n",
            "\n",
            "253. Title: One Forward is Enough for Neural Network Training via Likelihood Ratio Method\n",
            "   Abstract: Temporal Graph Neural Networks (TGNN) have the ability to capture both the\n",
            "graph topology and dynamic dependencies of interactions within a graph over\n",
            "time. There has been a growing need to explain the predictions of TGNN models\n",
            "due to the difficulty in identifying how past events influence their\n",
            "predictions. Since the explanation model for a static graph cannot be readily\n",
            "applied to temporal graphs due to its inability to capture temporal\n",
            "dependencies, recent studies proposed explanation models for temporal graphs.\n",
            "However, existing explanation models for temporal graphs rely on post-hoc\n",
            "explanations, requiring separate models for prediction and explanation, which\n",
            "is limited in two aspects: efficiency and accuracy of explanation. In this\n",
            "work, we propose a novel built-in explanation framework for temporal graphs,\n",
            "called Self-Explainable Temporal Graph Networks based on Graph Information\n",
            "Bottleneck (TGIB). TGIB provides explanations for event occurrences by\n",
            "introducing stochasticity in each temporal event based on the Information\n",
            "Bottleneck theory. Experimental results demonstrate the superiority of TGIB in\n",
            "terms of both the link prediction performance and explainability compared to\n",
            "state-of-the-art methods. This is the first work that simultaneously performs\n",
            "prediction and explanation for temporal graphs in an end-to-end manner.\n",
            "\n",
            "254. Title: Coeditor: Leveraging Repo-level Diffs for Code Auto-editing\n",
            "   Abstract: We propose a fully probabilistic formulation of the notion of mechanistic\n",
            "interaction (interaction in some fundamental mechanistic sense) between the\n",
            "effects of putative (possibly continuous) causal factors A and B on a binary\n",
            "outcome variable Y indicating 'survival' vs 'failure'. We define mechanistic\n",
            "interaction in terms of departure from a generalized 'noisy OR' model, under\n",
            "which the multiplicative causal effect of A (resp., B) on the probability of\n",
            "failure cannot be enhanced by manipulating B (resp., A). We present conditions\n",
            "under which mechanistic interaction in the above sense can be assessed via\n",
            "simple tests on excess risk or superadditivity, in a possibly retrospective\n",
            "regime of observation. These conditions are defined in terms of generalized\n",
            "conditional independence relationships (generalised because they may involve\n",
            "non-stochastic 'regime indicators') that can often be checked on a graphical\n",
            "representation of the problem. Inference about mechanistic interaction between\n",
            "direct, or path-specific, causal effects can be accommodated in the proposed\n",
            "framework. The method is illustrated with the aid of a study in experimental\n",
            "psychology.\n",
            "\n",
            "255. Title: Generalization in diffusion models arises from geometry-adaptive harmonic representations\n",
            "   Abstract: The quality of Machine Learning (ML) models strongly depends on the input\n",
            "data, as such Feature Engineering (FE) is often required in ML. In addition,\n",
            "with the proliferation of ML-powered systems, especially in critical contexts,\n",
            "the need for interpretability and explainability becomes increasingly\n",
            "important. Since manual FE is time-consuming and requires case specific\n",
            "knowledge, we propose KRAFT, an AutoFE framework that leverages a knowledge\n",
            "graph to guide the generation of interpretable features. Our hybrid AI approach\n",
            "combines a neural generator to transform raw features through a series of\n",
            "transformations and a knowledge-based reasoner to evaluate features\n",
            "interpretability using Description Logics (DL). The generator is trained\n",
            "through Deep Reinforcement Learning (DRL) to maximize the prediction accuracy\n",
            "and the interpretability of the generated features. Extensive experiments on\n",
            "real datasets demonstrate that KRAFT significantly improves accuracy while\n",
            "ensuring a high level of interpretability.\n",
            "\n",
            "256. Title: OpenChat: Advancing Open-source Language Models with Mixed-Quality Data\n",
            "   Abstract: In classical physics, entropy quantifies the randomness of large systems,\n",
            "where the complete specification of the state, though possible in theory, is\n",
            "not possible in practice. In quantum physics, despite its inherently\n",
            "probabilistic nature, the concept of entropy has been elusive. The von Neumann\n",
            "entropy, currently adopted in quantum information and computing, models only\n",
            "the randomness associated with unknown specifications of a state and is zero\n",
            "for pure quantum states, and thus cannot quantify the inherent randomness of\n",
            "its observables. Our goal is to provide such quantification.\n",
            "  This paper focuses on the quantification of the observed spin values\n",
            "associated with a pure quantum state, given an axis $z$. To this end, we define\n",
            "a spin entropy, which is not zero for pure states, and its minimum is $\\ln\n",
            "2\\pi$, reflecting the uncertainty principle for the spin observables. We study\n",
            "the spin entropy for single massive particles with spin $1/2$ and spin 1,\n",
            "photons, and two fermions in entangled and disentangled states. The spin\n",
            "entropy attains local minima for Bell states, which are pure entangled states\n",
            "of two fermions, and local maxima for disentangled states. The spin entropy may\n",
            "be useful for developing robust quantum computational processes.\n",
            "\n",
            "257. Title: On the hardness of learning under symmetries\n",
            "   Abstract: Shared-account Sequential Recommendation (SSR) aims to provide personalized\n",
            "recommendations for accounts shared by multiple users with varying sequential\n",
            "preferences. Previous studies on SSR struggle to capture the fine-grained\n",
            "associations between interactions and different latent users within the shared\n",
            "account's hybrid sequences. Moreover, most existing SSR methods (e.g.,\n",
            "RNN-based or GCN-based methods) have quadratic computational complexities,\n",
            "hindering the deployment of SSRs on resource-constrained devices. To this end,\n",
            "we propose a Lightweight Graph Capsule Convolutional Network with subspace\n",
            "alignment for shared-account sequential recommendation, named LightGC$^2$N.\n",
            "Specifically, we devise a lightweight graph capsule convolutional network. It\n",
            "facilitates the fine-grained matching between interactions and latent users by\n",
            "attentively propagating messages on the capsule graphs. Besides, we present an\n",
            "efficient subspace alignment method. This method refines the sequence\n",
            "representations and then aligns them with the finely clustered preferences of\n",
            "latent users. The experimental results on four real-world datasets indicate\n",
            "that LightGC$^2$N outperforms nine state-of-the-art methods in accuracy and\n",
            "efficiency.\n",
            "\n",
            "258. Title: GAIA: Zero-shot Talking Avatar Generation\n",
            "   Abstract: Deep neural networks (DNNs) trained for image denoising are able to generate\n",
            "high-quality samples with score-based reverse diffusion algorithms. These\n",
            "impressive capabilities seem to imply an escape from the curse of\n",
            "dimensionality, but recent reports of memorization of the training set raise\n",
            "the question of whether these networks are learning the \"true\" continuous\n",
            "density of the data. Here, we show that two DNNs trained on non-overlapping\n",
            "subsets of a dataset learn nearly the same score function, and thus the same\n",
            "density, when the number of training images is large enough. In this regime of\n",
            "strong generalization, diffusion-generated images are distinct from the\n",
            "training set, and are of high visual quality, suggesting that the inductive\n",
            "biases of the DNNs are well-aligned with the data density. We analyze the\n",
            "learned denoising functions and show that the inductive biases give rise to a\n",
            "shrinkage operation in a basis adapted to the underlying image. Examination of\n",
            "these bases reveals oscillating harmonic structures along contours and in\n",
            "homogeneous regions. We demonstrate that trained denoisers are inductively\n",
            "biased towards these geometry-adaptive harmonic bases since they arise not only\n",
            "when the network is trained on photographic images, but also when it is trained\n",
            "on image classes supported on low-dimensional manifolds for which the harmonic\n",
            "basis is suboptimal. Finally, we show that when trained on regular image\n",
            "classes for which the optimal basis is known to be geometry-adaptive and\n",
            "harmonic, the denoising performance of the networks is near-optimal.\n",
            "\n",
            "259. Title: A differentiable brain simulator bridging brain simulation and brain-inspired computing\n",
            "   Abstract: Brain simulation builds dynamical models to mimic the structure and functions\n",
            "of the brain, while brain-inspired computing (BIC) develops intelligent systems\n",
            "by learning from the structure and functions of the brain. The two fields are\n",
            "intertwined and should share a common programming framework to facilitate each\n",
            "other's development. However, none of the existing software in the fields can\n",
            "achieve this goal, because traditional brain simulators lack differentiability\n",
            "for training, while existing deep learning (DL) frameworks fail to capture the\n",
            "biophysical realism and complexity of brain dynamics. In this paper, we\n",
            "introduce BrainPy, a differentiable brain simulator developed using JAX and\n",
            "XLA, with the aim of bridging the gap between brain simulation and BIC. BrainPy\n",
            "expands upon the functionalities of JAX, a powerful AI framework, by\n",
            "introducing complete capabilities for flexible, efficient, and scalable brain\n",
            "simulation. It offers a range of sparse and event-driven operators for\n",
            "efficient and scalable brain simulation, an abstraction for managing the\n",
            "intricacies of synaptic computations, a modular and flexible interface for\n",
            "constructing multi-scale brain models, and an object-oriented just-in-time\n",
            "compilation approach to handle the memory-intensive nature of brain dynamics.\n",
            "We showcase the efficiency and scalability of BrainPy on benchmark tasks,\n",
            "highlight its differentiable simulation for biologically plausible spiking\n",
            "models, and discuss its potential to support research at the intersection of\n",
            "brain simulation and BIC.\n",
            "\n",
            "260. Title: M3C: A Framework towards Convergent, Flexible, and Unsupervised Learning of Mixture Graph Matching and Clustering\n",
            "   Abstract: Instance-level image classification tasks have traditionally relied on\n",
            "single-instance labels to train models, e.g., few-shot learning and transfer\n",
            "learning. However, set-level coarse-grained labels that capture relationships\n",
            "among instances can provide richer information in real-world scenarios. In this\n",
            "paper, we present a novel approach to enhance instance-level image\n",
            "classification by leveraging set-level labels. We provide a theoretical\n",
            "analysis of the proposed method, including recognition conditions for fast\n",
            "excess risk rate, shedding light on the theoretical foundations of our\n",
            "approach. We conducted experiments on two distinct categories of datasets:\n",
            "natural image datasets and histopathology image datasets. Our experimental\n",
            "results demonstrate the effectiveness of our approach, showcasing improved\n",
            "classification performance compared to traditional single-instance label-based\n",
            "methods. Notably, our algorithm achieves 13% improvement in classification\n",
            "accuracy compared to the strongest baseline on the histopathology image\n",
            "classification benchmarks. Importantly, our experimental findings align with\n",
            "the theoretical analysis, reinforcing the robustness and reliability of our\n",
            "proposed method. This work bridges the gap between instance-level and set-level\n",
            "image classification, offering a promising avenue for advancing the\n",
            "capabilities of image classification models with set-level coarse-grained\n",
            "labels.\n",
            "\n",
            "261. Title: Protein-ligand binding representation learning from fine-grained interactions\n",
            "   Abstract: We study the problem of learning equivariant neural networks via gradient\n",
            "descent. The incorporation of known symmetries (\"equivariance\") into neural\n",
            "nets has empirically improved the performance of learning pipelines, in domains\n",
            "ranging from biology to computer vision. However, a rich yet separate line of\n",
            "learning theoretic research has demonstrated that actually learning shallow,\n",
            "fully-connected (i.e. non-symmetric) networks has exponential complexity in the\n",
            "correlational statistical query (CSQ) model, a framework encompassing gradient\n",
            "descent. In this work, we ask: are known problem symmetries sufficient to\n",
            "alleviate the fundamental hardness of learning neural nets with gradient\n",
            "descent? We answer this question in the negative. In particular, we give lower\n",
            "bounds for shallow graph neural networks, convolutional networks, invariant\n",
            "polynomials, and frame-averaged networks for permutation subgroups, which all\n",
            "scale either superpolynomially or exponentially in the relevant input\n",
            "dimension. Therefore, in spite of the significant inductive bias imparted via\n",
            "symmetry, actually learning the complete classes of functions represented by\n",
            "equivariant neural networks via gradient descent remains hard.\n",
            "\n",
            "262. Title: Unleashing the Power of Pre-trained Language Models for Offline Reinforcement Learning\n",
            "   Abstract: Fast changing states or volatile environments pose a significant challenge to\n",
            "online optimization, which needs to perform rapid adaptation under limited\n",
            "observation. In this paper, we give query and regret optimal bandit algorithms\n",
            "under the strict notion of strongly adaptive regret, which measures the maximum\n",
            "regret over any contiguous interval $I$. Due to its worst-case nature, there is\n",
            "an almost-linear $\\Omega(|I|^{1-\\epsilon})$ regret lower bound, when only one\n",
            "query per round is allowed [Daniely el al, ICML 2015]. Surprisingly, with just\n",
            "two queries per round, we give Strongly Adaptive Bandit Learner (StABL) that\n",
            "achieves $\\tilde{O}(\\sqrt{n|I|})$ adaptive regret for multi-armed bandits with\n",
            "$n$ arms. The bound is tight and cannot be improved in general. Our algorithm\n",
            "leverages a multiplicative update scheme of varying stepsizes and a carefully\n",
            "chosen observation distribution to control the variance. Furthermore, we extend\n",
            "our results and provide optimal algorithms in the bandit convex optimization\n",
            "setting. Finally, we empirically demonstrate the superior performance of our\n",
            "algorithms under volatile environments and for downstream tasks, such as\n",
            "algorithm selection for hyperparameter optimization.\n",
            "\n",
            "263. Title: Adaptive Regret for Bandits Made Possible: Two Queries Suffice\n",
            "   Abstract: In a backdoor attack, an adversary inserts maliciously constructed backdoor\n",
            "examples into a training set to make the resulting model vulnerable to\n",
            "manipulation. Defending against such attacks typically involves viewing these\n",
            "inserted examples as outliers in the training set and using techniques from\n",
            "robust statistics to detect and remove them.\n",
            "  In this work, we present a different approach to the backdoor attack problem.\n",
            "Specifically, we show that without structural information about the training\n",
            "data distribution, backdoor attacks are indistinguishable from\n",
            "naturally-occurring features in the data--and thus impossible to \"detect\" in a\n",
            "general sense. Then, guided by this observation, we revisit existing defenses\n",
            "against backdoor attacks and characterize the (often latent) assumptions they\n",
            "make and on which they depend. Finally, we explore an alternative perspective\n",
            "on backdoor attacks: one that assumes these attacks correspond to the strongest\n",
            "feature in the training data. Under this assumption (which we make formal) we\n",
            "develop a new primitive for detecting backdoor attacks. Our primitive naturally\n",
            "gives rise to a detection algorithm that comes with theoretical guarantees and\n",
            "is effective in practice.\n",
            "\n",
            "264. Title: Towards Cross Domain Generalization of Hamiltonian Representation via Meta Learning\n",
            "   Abstract: Residual neural networks are state-of-the-art deep learning models. Their\n",
            "continuous-depth analog, neural ordinary differential equations (ODEs), are\n",
            "also widely used. Despite their success, the link between the discrete and\n",
            "continuous models still lacks a solid mathematical foundation. In this article,\n",
            "we take a step in this direction by establishing an implicit regularization of\n",
            "deep residual networks towards neural ODEs, for nonlinear networks trained with\n",
            "gradient flow. We prove that if the network is initialized as a discretization\n",
            "of a neural ODE, then such a discretization holds throughout training. Our\n",
            "results are valid for a finite training time, and also as the training time\n",
            "tends to infinity provided that the network satisfies a Polyak-Lojasiewicz\n",
            "condition. Importantly, this condition holds for a family of residual networks\n",
            "where the residuals are two-layer perceptrons with an overparameterization in\n",
            "width that is only linear, and implies the convergence of gradient flow to a\n",
            "global minimum. Numerical experiments illustrate our results.\n",
            "\n",
            "265. Title: Enhancing Instance-Level Image Classification with Set-Level Labels\n",
            "   Abstract: Effective out-of-distribution (OOD) detection is crucial for reliable machine\n",
            "learning models, yet most current methods are limited in practical use due to\n",
            "requirements like access to training data or intervention in training. We\n",
            "present a novel method for detecting OOD data in Transformers based on\n",
            "transformation smoothness between intermediate layers of a network (BLOOD),\n",
            "which is applicable to pre-trained models without access to training data.\n",
            "BLOOD utilizes the tendency of between-layer representation transformations of\n",
            "in-distribution (ID) data to be smoother than the corresponding transformations\n",
            "of OOD data, a property that we also demonstrate empirically. We evaluate BLOOD\n",
            "on several text classification tasks with Transformer networks and demonstrate\n",
            "that it outperforms methods with comparable resource requirements. Our analysis\n",
            "also suggests that when learning simpler tasks, OOD data transformations\n",
            "maintain their original sharpness, whereas sharpness increases with more\n",
            "complex tasks.\n",
            "\n",
            "266. Title: Implicit regularization of deep residual networks towards neural ODEs\n",
            "   Abstract: Recent advances in deep learning for physics have focused on discovering\n",
            "shared representations of target systems by incorporating physics priors or\n",
            "inductive biases into neural networks. While effective, these methods are\n",
            "limited to the system domain, where the type of system remains consistent and\n",
            "thus cannot ensure the adaptation to new, or unseen physical systems governed\n",
            "by different laws. For instance, a neural network trained on a mass-spring\n",
            "system cannot guarantee accurate predictions for the behavior of a two-body\n",
            "system or any other system with different physical laws. In this work, we take\n",
            "a significant leap forward by targeting cross domain generalization within the\n",
            "field of Hamiltonian dynamics. We model our system with a graph neural network\n",
            "(GNN) and employ a meta learning algorithm to enable the model to gain\n",
            "experience over a distribution of systems and make it adapt to new physics. Our\n",
            "approach aims to learn a unified Hamiltonian representation that is\n",
            "generalizable across multiple system domains, thereby overcoming the\n",
            "limitations of system-specific models. We demonstrate that the meta-trained\n",
            "model captures the generalized Hamiltonian representation that is consistent\n",
            "across different physical domains. Overall, through the use of meta learning,\n",
            "we offer a framework that achieves cross domain generalization, providing a\n",
            "step towards a unified model for understanding a wide array of dynamical\n",
            "systems via deep learning.\n",
            "\n",
            "267. Title: Forward Learning of Graph Neural Networks\n",
            "   Abstract: Offline reinforcement learning (RL) aims to find a near-optimal policy using\n",
            "pre-collected datasets. In real-world scenarios, data collection could be\n",
            "costly and risky; therefore, offline RL becomes particularly challenging when\n",
            "the in-domain data is limited. Given recent advances in Large Language Models\n",
            "(LLMs) and their few-shot learning prowess, this paper introduces\n",
            "$\\textbf{La}$nguage Models for $\\textbf{Mo}$tion Control ($\\textbf{LaMo}$), a\n",
            "general framework based on Decision Transformers to effectively use pre-trained\n",
            "Language Models (LMs) for offline RL. Our framework highlights four crucial\n",
            "components: (1) Initializing Decision Transformers with sequentially\n",
            "pre-trained LMs, (2) employing the LoRA fine-tuning method, in contrast to\n",
            "full-weight fine-tuning, to combine the pre-trained knowledge from LMs and\n",
            "in-domain knowledge effectively, (3) using the non-linear MLP transformation\n",
            "instead of linear projections, to generate embeddings, and (4) integrating an\n",
            "auxiliary language prediction loss during fine-tuning to stabilize the LMs and\n",
            "retain their original abilities on languages. Empirical results indicate\n",
            "$\\textbf{LaMo}$ achieves excellent performance in sparse-reward tasks and\n",
            "closes the gap between value-based offline RL methods and decision transformers\n",
            "in dense-reward tasks. In particular, our method demonstrates superior\n",
            "performance in scenarios with limited data samples.\n",
            "\n",
            "268. Title: Rethinking Model Ensemble in Transfer-based Adversarial Attacks\n",
            "   Abstract: Locally Rotation Invariant (LRI) image analysis was shown to be fundamental\n",
            "in many applications and in particular in medical imaging where local\n",
            "structures of tissues occur at arbitrary rotations. LRI constituted the\n",
            "cornerstone of several breakthroughs in texture analysis, including Local\n",
            "Binary Patterns (LBP), Maximum Response 8 (MR8) and steerable filterbanks.\n",
            "Whereas globally rotation invariant Convolutional Neural Networks (CNN) were\n",
            "recently proposed, LRI was very little investigated in the context of deep\n",
            "learning. LRI designs allow learning filters accounting for all orientations,\n",
            "which enables a drastic reduction of trainable parameters and training data\n",
            "when compared to standard 3D CNNs. In this paper, we propose and compare\n",
            "several methods to obtain LRI CNNs with directional sensitivity. Two methods\n",
            "use orientation channels (responses to rotated kernels), either by explicitly\n",
            "rotating the kernels or using steerable filters. These orientation channels\n",
            "constitute a locally rotation equivariant representation of the data. Local\n",
            "pooling across orientations yields LRI image analysis. Steerable filters are\n",
            "used to achieve a fine and efficient sampling of 3D rotations as well as a\n",
            "reduction of trainable parameters and operations, thanks to a parametric\n",
            "representations involving solid Spherical Harmonics (SH), which are products of\n",
            "SH with associated learned radial profiles.Finally, we investigate a third\n",
            "strategy to obtain LRI based on rotational invariants calculated from responses\n",
            "to a learned set of solid SHs. The proposed methods are evaluated and compared\n",
            "to standard CNNs on 3D datasets including synthetic textured volumes composed\n",
            "of rotated patterns, and pulmonary nodule classification in CT. The results\n",
            "show the importance of LRI image analysis while resulting in a drastic\n",
            "reduction of trainable parameters, outperforming standard 3D CNNs trained with\n",
            "data augmentation.\n",
            "\n",
            "269. Title: Distinguished In Uniform: Self-Attention Vs. Virtual Nodes\n",
            "   Abstract: Recent advancements in robotics have enabled robots to navigate complex\n",
            "scenes or manipulate diverse objects independently. However, robots are still\n",
            "impotent in many household tasks requiring coordinated behaviors such as\n",
            "opening doors. The factorization of navigation and manipulation, while\n",
            "effective for some tasks, fails in scenarios requiring coordinated actions. To\n",
            "address this challenge, we introduce, HarmonicMM, an end-to-end learning method\n",
            "that optimizes both navigation and manipulation, showing notable improvement\n",
            "over existing techniques in everyday tasks. This approach is validated in\n",
            "simulated and real-world environments and adapts to novel unseen settings\n",
            "without additional tuning. Our contributions include a new benchmark for mobile\n",
            "manipulation and the successful deployment with only RGB visual observation in\n",
            "a real unseen apartment, demonstrating the potential for practical indoor robot\n",
            "deployment in daily life. More results are on our project site:\n",
            "https://rchalyang.github.io/HarmonicMM/\n",
            "\n",
            "270. Title: Risk Bounds of Accelerated SGD for Overparameterized Linear Regression\n",
            "   Abstract: Graph Transformers (GTs) such as SAN and GPS are graph processing models that\n",
            "combine Message-Passing GNNs (MPGNNs) with global Self-Attention. They were\n",
            "shown to be universal function approximators, with two reservations: 1. The\n",
            "initial node features must be augmented with certain positional encodings. 2.\n",
            "The approximation is non-uniform: Graphs of different sizes may require a\n",
            "different approximating network.\n",
            "  We first clarify that this form of universality is not unique to GTs: Using\n",
            "the same positional encodings, also pure MPGNNs and even 2-layer MLPs are\n",
            "non-uniform universal approximators. We then consider uniform expressivity: The\n",
            "target function is to be approximated by a single network for graphs of all\n",
            "sizes. There, we compare GTs to the more efficient MPGNN + Virtual Node\n",
            "architecture. The essential difference between the two model definitions is in\n",
            "their global computation method -- Self-Attention Vs Virtual Node. We prove\n",
            "that none of the models is a uniform-universal approximator, before proving our\n",
            "main result: Neither model's uniform expressivity subsumes the other's. We\n",
            "demonstrate the theory with experiments on synthetic data. We further augment\n",
            "our study with real-world datasets, observing mixed results which indicate no\n",
            "clear ranking in practice as well.\n",
            "\n",
            "271. Title: Rotation Has Two Sides: Evaluating Data Augmentation for Deep One-class Classification\n",
            "   Abstract: Accelerated stochastic gradient descent (ASGD) is a workhorse in deep\n",
            "learning and often achieves better generalization performance than SGD.\n",
            "However, existing optimization theory can only explain the faster convergence\n",
            "of ASGD, but cannot explain its better generalization. In this paper, we study\n",
            "the generalization of ASGD for overparameterized linear regression, which is\n",
            "possibly the simplest setting of learning with overparameterization. We\n",
            "establish an instance-dependent excess risk bound for ASGD within each\n",
            "eigen-subspace of the data covariance matrix. Our analysis shows that (i) ASGD\n",
            "outperforms SGD in the subspace of small eigenvalues, exhibiting a faster rate\n",
            "of exponential decay for bias error, while in the subspace of large\n",
            "eigenvalues, its bias error decays slower than SGD; and (ii) the variance error\n",
            "of ASGD is always larger than that of SGD. Our result suggests that ASGD can\n",
            "outperform SGD when the difference between the initialization and the true\n",
            "weight vector is mostly confined to the subspace of small eigenvalues.\n",
            "Additionally, when our analysis is specialized to linear regression in the\n",
            "strongly convex setting, it yields a tighter bound for bias error than the\n",
            "best-known result.\n",
            "\n",
            "272. Title: Large Language Models as Analogical Reasoners\n",
            "   Abstract: Analogical reasoning, particularly in multimodal contexts, is the foundation\n",
            "of human perception and creativity. Multimodal Large Language Model (MLLM) has\n",
            "recently sparked considerable discussion due to its emergent capabilities. In\n",
            "this paper, we delve into the multimodal analogical reasoning capability of\n",
            "MLLM. Specifically, we explore two facets: \\textit{MLLM as an explainer} and\n",
            "\\textit{MLLM as a predictor}. In \\textit{MLLM as an explainer}, we primarily\n",
            "focus on whether MLLM can deeply comprehend multimodal analogical reasoning\n",
            "problems. We propose a unified prompt template and a method for harnessing the\n",
            "comprehension capabilities of MLLM to augment existing models. In \\textit{MLLM\n",
            "as a predictor}, we aim to determine whether MLLM can directly solve multimodal\n",
            "analogical reasoning problems. The experiments show that our approach\n",
            "outperforms existing methods on popular datasets, providing preliminary\n",
            "evidence for the analogical reasoning capability of MLLM.\n",
            "\n",
            "273. Title: OMNI: Open-endedness via Models of human Notions of Interestingness\n",
            "   Abstract: We present the Graph Forward-Forward (GFF) algorithm, an extension of the\n",
            "Forward-Forward procedure to graphs, able to handle features distributed over a\n",
            "graph's nodes. This allows training graph neural networks with forward passes\n",
            "only, without backpropagation. Our method is agnostic to the message-passing\n",
            "scheme, and provides a more biologically plausible learning scheme than\n",
            "backpropagation, while also carrying computational advantages. With GFF, graph\n",
            "neural networks are trained greedily layer by layer, using both positive and\n",
            "negative samples. We run experiments on 11 standard graph property prediction\n",
            "tasks, showing how GFF provides an effective alternative to backpropagation for\n",
            "training graph neural networks. This shows in particular that this procedure is\n",
            "remarkably efficient in spite of combining the per-layer training with the\n",
            "locality of the processing in a GNN.\n",
            "\n",
            "274. Title: BatchPrompt: Accomplish more with less\n",
            "   Abstract: We introduce a family of symmetric convex bodies called generalized\n",
            "ellipsoids of degree $d$ (GE-$d$s), with ellipsoids corresponding to the case\n",
            "of $d=0$. Generalized ellipsoids (GEs) retain many geometric, algebraic, and\n",
            "algorithmic properties of ellipsoids. We show that the conditions that the\n",
            "parameters of a GE must satisfy can be checked in strongly polynomial time, and\n",
            "that one can search for GEs of a given degree by solving a semidefinite program\n",
            "whose size grows only linearly with dimension. We give an example of a GE which\n",
            "does not have a second-order cone representation, but show that every GE has a\n",
            "semidefinite representation whose size depends linearly on both its dimension\n",
            "and degree. In terms of expressiveness, we prove that for any integer $m\\geq\n",
            "2$, every symmetric full-dimensional polytope with $2m$ facets and every\n",
            "intersection of $m$ co-centered ellipsoids can be represented exactly as a\n",
            "GE-$d$ with $d \\leq 2m-3$. Using this result, we show that every symmetric\n",
            "convex body can be approximated arbitrarily well by a GE-$d$ and we quantify\n",
            "the quality of the approximation as a function of the degree $d$. Finally, we\n",
            "present applications of GEs to several areas, such as time-varying portfolio\n",
            "optimization, stability analysis of switched linear systems, robust-to-dynamics\n",
            "optimization, and robust polynomial regression.\n",
            "\n",
            "275. Title: How Well Do Supervised 3D Models Transfer to Medical Imaging Tasks?\n",
            "   Abstract: Vision tasks are characterized by the properties of locality and translation\n",
            "invariance. The superior performance of convolutional neural networks (CNNs) on\n",
            "these tasks is widely attributed to the inductive bias of locality and weight\n",
            "sharing baked into their architecture. Existing attempts to quantify the\n",
            "statistical benefits of these biases in CNNs over locally connected\n",
            "convolutional neural networks (LCNs) and fully connected neural networks (FCNs)\n",
            "fall into one of the following categories: either they disregard the optimizer\n",
            "and only provide uniform convergence upper bounds with no separating lower\n",
            "bounds, or they consider simplistic tasks that do not truly mirror the locality\n",
            "and translation invariance as found in real-world vision tasks. To address\n",
            "these deficiencies, we introduce the Dynamic Signal Distribution (DSD)\n",
            "classification task that models an image as consisting of $k$ patches, each of\n",
            "dimension $d$, and the label is determined by a $d$-sparse signal vector that\n",
            "can freely appear in any one of the $k$ patches. On this task, for any\n",
            "orthogonally equivariant algorithm like gradient descent, we prove that CNNs\n",
            "require $\\tilde{O}(k+d)$ samples, whereas LCNs require $\\Omega(kd)$ samples,\n",
            "establishing the statistical advantages of weight sharing in translation\n",
            "invariant tasks. Furthermore, LCNs need $\\tilde{O}(k(k+d))$ samples, compared\n",
            "to $\\Omega(k^2d)$ samples for FCNs, showcasing the benefits of locality in\n",
            "local tasks. Additionally, we develop information theoretic tools for analyzing\n",
            "randomized algorithms, which may be of interest for statistical research.\n",
            "\n",
            "276. Title: FairTune: Optimizing Parameter Efficient Fine Tuning for Fairness in Medical Image Analysis\n",
            "   Abstract: Data augmentation is critical to the empirical success of modern\n",
            "self-supervised representation learning, such as contrastive learning and\n",
            "masked language modeling. However, a theoretical understanding of the exact\n",
            "role of augmentation remains limited. Recent work has built the connection\n",
            "between self-supervised learning and the approximation of the top eigenspace of\n",
            "a graph Laplacian operator, suggesting that learning a linear probe atop such\n",
            "representation can be connected to RKHS regression. Building on this insight,\n",
            "this work delves into a statistical analysis of augmentation-based pretraining.\n",
            "Starting from the isometry property, a geometric characterization of the target\n",
            "function given by the augmentation, we disentangle the effects of the model and\n",
            "the augmentation, and prove two generalization bounds that are free of model\n",
            "complexity. Our first bound works for an arbitrary encoder, where the\n",
            "prediction error is decomposed as the sum of an estimation error incurred by\n",
            "fitting a linear probe with RKHS regression, and an approximation error\n",
            "entailed by RKHS approximation. Our second bound specifically addresses the\n",
            "case where the encoder is near-optimal, that is it approximates the top-d\n",
            "eigenspace of the RKHS induced by the augmentation. A key ingredient in our\n",
            "analysis is the augmentation complexity, which we use to quantitatively compare\n",
            "different augmentations and analyze their impact on downstream performance.\n",
            "\n",
            "277. Title: What Algorithms can Transformers Learn? A Study in Length Generalization\n",
            "   Abstract: We report the presence of a simple neural mechanism that represents an\n",
            "input-output function as a vector within autoregressive transformer language\n",
            "models (LMs). Using causal mediation analysis on a diverse range of\n",
            "in-context-learning (ICL) tasks, we find that a small number attention heads\n",
            "transport a compact representation of the demonstrated task, which we call a\n",
            "function vector (FV). FVs are robust to changes in context, i.e., they trigger\n",
            "execution of the task on inputs such as zero-shot and natural text settings\n",
            "that do not resemble the ICL contexts from which they are collected. We test\n",
            "FVs across a range of tasks, models, and layers and find strong causal effects\n",
            "across settings in middle layers. We investigate the internal structure of FVs\n",
            "and find while that they often contain information that encodes the output\n",
            "space of the function, this information alone is not sufficient to reconstruct\n",
            "an FV. Finally, we test semantic vector composition in FVs, and find that to\n",
            "some extent they can be summed to create vectors that trigger new complex\n",
            "tasks. Our findings show that compact, causal internal vector representations\n",
            "of function abstractions can be explicitly extracted from LLMs. Our code and\n",
            "data are available at https://functions.baulab.info.\n",
            "\n",
            "278. Title: Function Vectors in Large Language Models\n",
            "   Abstract: The creation of photorealistic virtual worlds requires the accurate modeling\n",
            "of 3D surface geometry for a wide range of objects. For this, meshes are\n",
            "appealing since they 1) enable fast physics-based rendering with realistic\n",
            "material and lighting, 2) support physical simulation, and 3) are\n",
            "memory-efficient for modern graphics pipelines. Recent work on reconstructing\n",
            "and statistically modeling 3D shape, however, has critiqued meshes as being\n",
            "topologically inflexible. To capture a wide range of object shapes, any 3D\n",
            "representation must be able to model solid, watertight, shapes as well as thin,\n",
            "open, surfaces. Recent work has focused on the former, and methods for\n",
            "reconstructing open surfaces do not support fast reconstruction with material\n",
            "and lighting or unconditional generative modelling. Inspired by the observation\n",
            "that open surfaces can be seen as islands floating on watertight surfaces, we\n",
            "parameterize open surfaces by defining a manifold signed distance field on\n",
            "watertight templates. With this parameterization, we further develop a\n",
            "grid-based and differentiable representation that parameterizes both watertight\n",
            "and non-watertight meshes of arbitrary topology. Our new representation, called\n",
            "Ghost-on-the-Shell (G-Shell), enables two important applications:\n",
            "differentiable rasterization-based reconstruction from multiview images and\n",
            "generative modelling of non-watertight meshes. We empirically demonstrate that\n",
            "G-Shell achieves state-of-the-art performance on non-watertight mesh\n",
            "reconstruction and generation tasks, while also performing effectively for\n",
            "watertight meshes.\n",
            "\n",
            "279. Title: Understanding Augmentation-based Self-Supervised Representation Learning via RKHS Approximation and Regression\n",
            "   Abstract: The variational quantum eigensolver (VQE) is one of the most representative\n",
            "quantum algorithms in the noisy intermediate-size quantum (NISQ) era, and is\n",
            "generally speculated to deliver one of the first quantum advantages for the\n",
            "ground-state simulations of some non-trivial Hamiltonians. However, short\n",
            "quantum coherence time and limited availability of quantum hardware resources\n",
            "in the NISQ hardware strongly restrain the capacity and expressiveness of VQEs.\n",
            "In this Letter, we introduce the variational quantum-neural hybrid eigensolver\n",
            "(VQNHE) in which the shallow-circuit quantum ansatz can be further enhanced by\n",
            "classical post-processing with neural networks. We show that VQNHE consistently\n",
            "and significantly outperforms VQE in simulating ground-state energies of\n",
            "quantum spins and molecules given the same amount of quantum resources. More\n",
            "importantly, we demonstrate that for arbitrary post-processing neural\n",
            "functions, VQNHE only incurs an polynomial overhead of processing time and\n",
            "represents the first scalable method to exponentially accelerate VQE with\n",
            "non-unitary post-processing that can be efficiently implemented in the NISQ\n",
            "era.\n",
            "\n",
            "280. Title: Symmetric Neural-Collapse Representations with Supervised Contrastive Loss: The Impact of ReLU and Batching\n",
            "   Abstract: Large language models exhibit surprising emergent generalization properties,\n",
            "yet also struggle on many simple reasoning tasks such as arithmetic and parity.\n",
            "This raises the question of if and when Transformer models can learn the true\n",
            "algorithm for solving a task. We study the scope of Transformers' abilities in\n",
            "the specific setting of length generalization on algorithmic tasks. Here, we\n",
            "propose a unifying framework to understand when and how Transformers can\n",
            "exhibit strong length generalization on a given task. Specifically, we leverage\n",
            "RASP (Weiss et al., 2021) -- a programming language designed for the\n",
            "computational model of a Transformer -- and introduce the RASP-Generalization\n",
            "Conjecture: Transformers tend to length generalize on a task if the task can be\n",
            "solved by a short RASP program which works for all input lengths. This simple\n",
            "conjecture remarkably captures most known instances of length generalization on\n",
            "algorithmic tasks. Moreover, we leverage our insights to drastically improve\n",
            "generalization performance on traditionally hard tasks (such as parity and\n",
            "addition). On the theoretical side, we give a simple example where the\n",
            "\"min-degree-interpolator\" model of learning from Abbe et al. (2023) does not\n",
            "correctly predict Transformers' out-of-distribution behavior, but our\n",
            "conjecture does. Overall, our work provides a novel perspective on the\n",
            "mechanisms of compositional generalization and the algorithmic capabilities of\n",
            "Transformers.\n",
            "\n",
            "281. Title: Learning Energy-Based Models by Cooperative Diffusion Recovery Likelihood\n",
            "   Abstract: Critical learning periods are periods early in development where temporary\n",
            "sensory deficits can have a permanent effect on behavior and learned\n",
            "representations. Despite the radical differences between biological and\n",
            "artificial networks, critical learning periods have been empirically observed\n",
            "in both systems. This suggests that critical periods may be fundamental to\n",
            "learning and not an accident of biology. Yet, why exactly critical periods\n",
            "emerge in deep networks is still an open question, and in particular it is\n",
            "unclear whether the critical periods observed in both systems depend on\n",
            "particular architectural or optimization details. To isolate the key underlying\n",
            "factors, we focus on deep linear network models, and show that, surprisingly,\n",
            "such networks also display much of the behavior seen in biology and artificial\n",
            "networks, while being amenable to analytical treatment. We show that critical\n",
            "periods depend on the depth of the model and structure of the data\n",
            "distribution. We also show analytically and in simulations that the learning of\n",
            "features is tied to competition between sources. Finally, we extend our\n",
            "analysis to multi-task learning to show that pre-training on certain tasks can\n",
            "damage the transfer performance on new tasks, and show how this depends on the\n",
            "relationship between tasks and the duration of the pre-training stage. To the\n",
            "best of our knowledge, our work provides the first analytically tractable model\n",
            "that sheds light into why critical learning periods emerge in biological and\n",
            "artificial networks.\n",
            "\n",
            "282. Title: P2Seg: Pointly-supervised Segmentation via Mutual Distillation\n",
            "   Abstract: Training energy-based models (EBMs) on high-dimensional data can be both\n",
            "challenging and time-consuming, and there exists a noticeable gap in sample\n",
            "quality between EBMs and other generative frameworks like GANs and diffusion\n",
            "models. To close this gap, inspired by the recent efforts of learning EBMs by\n",
            "maximizing diffusion recovery likelihood (DRL), we propose cooperative\n",
            "diffusion recovery likelihood (CDRL), an effective approach to tractably learn\n",
            "and sample from a series of EBMs defined on increasingly noisy versions of a\n",
            "dataset, paired with an initializer model for each EBM. At each noise level,\n",
            "the two models are jointly estimated within a cooperative training framework:\n",
            "samples from the initializer serve as starting points that are refined by a few\n",
            "MCMC sampling steps from the EBM. The EBM is then optimized by maximizing\n",
            "recovery likelihood, while the initializer model is optimized by learning from\n",
            "the difference between the refined samples and the initial samples. In\n",
            "addition, we made several practical designs for EBM training to further improve\n",
            "the sample quality. Combining these advances, our approach significantly boost\n",
            "the generation performance compared to existing EBM methods on CIFAR-10 and\n",
            "ImageNet datasets. We also demonstrate the effectiveness of our models for\n",
            "several downstream tasks, including classifier-free guided generation,\n",
            "compositional generation, image inpainting and out-of-distribution detection.\n",
            "\n",
            "283. Title: ToolChain*: Efficient Action Space Navigation in Large Language Models with A* Search\n",
            "   Abstract: We provide new lower bounds on the privacy guarantee of the multi-epoch\n",
            "Adaptive Batch Linear Queries (ABLQ) mechanism with shuffled batch sampling,\n",
            "demonstrating substantial gaps when compared to Poisson subsampling; prior\n",
            "analysis was limited to a single epoch. Since the privacy analysis of\n",
            "Differentially Private Stochastic Gradient Descent (DP-SGD) is obtained by\n",
            "analyzing the ABLQ mechanism, this brings into serious question the common\n",
            "practice of implementing shuffling-based DP-SGD, but reporting privacy\n",
            "parameters as if Poisson subsampling was used. To understand the impact of this\n",
            "gap on the utility of trained machine learning models, we introduce a practical\n",
            "approach to implement Poisson subsampling at scale using massively parallel\n",
            "computation, and efficiently train models with the same. We compare the utility\n",
            "of models trained with Poisson-subsampling-based DP-SGD, and the optimistic\n",
            "estimates of utility when using shuffling, via our new lower bounds on the\n",
            "privacy guarantee of ABLQ with shuffling.\n",
            "\n",
            "284. Title: Compressing LLMs: The Truth is Rarely Pure and Never Simple\n",
            "   Abstract: The pre-training and fine-tuning paradigm has become prominent in transfer\n",
            "learning. For example, if the model is pre-trained on ImageNet and then\n",
            "fine-tuned to PASCAL, it can significantly outperform that trained on PASCAL\n",
            "from scratch. While ImageNet pre-training has shown enormous success, it is\n",
            "formed in 2D, and the learned features are for classification tasks; when\n",
            "transferring to more diverse tasks, like 3D image segmentation, its performance\n",
            "is inevitably compromised due to the deviation from the original ImageNet\n",
            "context. A significant challenge lies in the lack of large, annotated 3D\n",
            "datasets rivaling the scale of ImageNet for model pre-training. To overcome\n",
            "this challenge, we make two contributions. Firstly, we construct AbdomenAtlas\n",
            "1.1 that comprises 9,262 three-dimensional computed tomography (CT) volumes\n",
            "with high-quality, per-voxel annotations of 25 anatomical structures and pseudo\n",
            "annotations of seven tumor types. Secondly, we develop a suite of models that\n",
            "are pre-trained on our AbdomenAtlas 1.1 for transfer learning. Our preliminary\n",
            "analyses indicate that the model trained only with 21 CT volumes, 672 masks,\n",
            "and 40 GPU hours has a transfer learning ability similar to the model trained\n",
            "with 5,050 (unlabeled) CT volumes and 1,152 GPU hours. More importantly, the\n",
            "transfer learning ability of supervised models can further scale up with larger\n",
            "annotated datasets, achieving significantly better performance than preexisting\n",
            "pre-trained models, irrespective of their pre-training methodologies or data\n",
            "sources. We hope this study can facilitate collective efforts in constructing\n",
            "larger 3D medical datasets and more releases of supervised pre-trained models.\n",
            "\n",
            "285. Title: Hybrid Directional Graph Neural Network for Molecules\n",
            "   Abstract: Pre-training is known to generate universal representations for downstream\n",
            "tasks in large-scale deep learning such as large language models. Existing\n",
            "literature, e.g., \\cite{kim2020adversarial}, empirically observe that the\n",
            "downstream tasks can inherit the adversarial robustness of the pre-trained\n",
            "model. We provide theoretical justifications for this robustness inheritance\n",
            "phenomenon. Our theoretical results reveal that feature purification plays an\n",
            "important role in connecting the adversarial robustness of the pre-trained\n",
            "model and the downstream tasks in two-layer neural networks. Specifically, we\n",
            "show that (i) with adversarial training, each hidden node tends to pick only\n",
            "one (or a few) feature; (ii) without adversarial training, the hidden nodes can\n",
            "be vulnerable to attacks. This observation is valid for both supervised\n",
            "pre-training and contrastive learning. With purified nodes, it turns out that\n",
            "clean training is enough to achieve adversarial robustness in downstream tasks.\n",
            "\n",
            "286. Title: A 2-Dimensional State Space Layer for Spatial Inductive Bias\n",
            "   Abstract: Despite their remarkable achievements, modern Large Language Models (LLMs)\n",
            "face exorbitant computational and memory footprints. Recently, several works\n",
            "have shown significant success in training-free and data-free compression\n",
            "(pruning and quantization) of LLMs that achieve 50 - 60% sparsity and reduce\n",
            "the bit width to 3 or 4 bits per weight, with negligible degradation of\n",
            "perplexity over the uncompressed baseline. As recent research efforts are\n",
            "focused on developing increasingly sophisticated compression methods, our work\n",
            "takes a step back and re-evaluates the effectiveness of existing SoTA\n",
            "compression methods, which rely on a fairly simple and widely questioned\n",
            "metric, perplexity (even for dense LLMs). We introduce Knowledge-Intensive\n",
            "Compressed LLM BenchmarK (LLM-KICK), a collection of carefully curated tasks to\n",
            "redefine the evaluation protocol for compressed LLMs, which have significant\n",
            "alignment with their dense counterparts and perplexity fail to capture subtle\n",
            "change in their true capabilities. LLM-KICK unveils many favorable merits and\n",
            "unfortunate plights of current SoTA compression methods: all pruning methods\n",
            "suffer significant performance degradation, sometimes at trivial sparsity\n",
            "ratios (e.g., 25-30%), and fail for N:M sparsity in knowledge-intensive tasks;\n",
            "current quantization methods are more successful than pruning; yet, pruned LLMs\n",
            "even at $\\geq 50$% sparsity are robust in-context retrieval and summarization\n",
            "systems; among others. LLM-KICK is designed to holistically access compressed\n",
            "LLMs' ability for language understanding, reasoning, generation, in-context\n",
            "retrieval, in-context summarization, etc. We hope our study can foster the\n",
            "development of better LLM compression methods. The reproduced codes are\n",
            "available at https://github.com/VITA-Group/llm-kick.\n",
            "\n",
            "287. Title: Equivariant Scalar Fields for Molecular Docking with Fast Fourier Transforms\n",
            "   Abstract: Continual Learning (CL) focuses on learning from dynamic and changing data\n",
            "distributions while retaining previously acquired knowledge. Various methods\n",
            "have been developed to address the challenge of catastrophic forgetting,\n",
            "including regularization-based, Bayesian-based, and memory-replay-based\n",
            "techniques. However, these methods lack a unified framework and common\n",
            "terminology for describing their approaches. This research aims to bridge this\n",
            "gap by introducing a comprehensive and overarching framework that encompasses\n",
            "and reconciles these existing methodologies. Notably, this new framework is\n",
            "capable of encompassing established CL approaches as special instances within a\n",
            "unified and general optimization objective. An intriguing finding is that\n",
            "despite their diverse origins, these methods share common mathematical\n",
            "structures. This observation highlights the compatibility of these seemingly\n",
            "distinct techniques, revealing their interconnectedness through a shared\n",
            "underlying optimization objective. Moreover, the proposed general framework\n",
            "introduces an innovative concept called refresh learning, specifically designed\n",
            "to enhance the CL performance. This novel approach draws inspiration from\n",
            "neuroscience, where the human brain often sheds outdated information to improve\n",
            "the retention of crucial knowledge and facilitate the acquisition of new\n",
            "information. In essence, refresh learning operates by initially unlearning\n",
            "current data and subsequently relearning it. It serves as a versatile plug-in\n",
            "that seamlessly integrates with existing CL methods, offering an adaptable and\n",
            "effective enhancement to the learning process. Extensive experiments on CL\n",
            "benchmarks and theoretical analysis demonstrate the effectiveness of the\n",
            "proposed refresh learning. Code is available at\n",
            "\\url{https://github.com/joey-wang123/CL-refresh-learning}.\n",
            "\n",
            "288. Title: LogicMP: A Neuro-symbolic Approach for Encoding First-order Logic Constraints\n",
            "   Abstract: Supervised contrastive loss (SCL) is a competitive and often superior\n",
            "alternative to the cross-entropy loss for classification. While prior studies\n",
            "have demonstrated that both losses yield symmetric training representations\n",
            "under balanced data, this symmetry breaks under class imbalances. This paper\n",
            "presents an intriguing discovery: the introduction of a ReLU activation at the\n",
            "final layer effectively restores the symmetry in SCL-learned representations.\n",
            "We arrive at this finding analytically, by establishing that the global\n",
            "minimizers of an unconstrained features model with SCL loss and entry-wise\n",
            "non-negativity constraints form an orthogonal frame. Extensive experiments\n",
            "conducted across various datasets, architectures, and imbalance scenarios\n",
            "corroborate our finding. Importantly, our experiments reveal that the inclusion\n",
            "of the ReLU activation restores symmetry without compromising test accuracy.\n",
            "This constitutes the first geometry characterization of SCL under imbalances.\n",
            "Additionally, our analysis and experiments underscore the pivotal role of batch\n",
            "selection strategies in representation geometry. By proving necessary and\n",
            "sufficient conditions for mini-batch choices that ensure invariant symmetric\n",
            "representations, we introduce batch-binding as an efficient strategy that\n",
            "guarantees these conditions hold.\n",
            "\n",
            "289. Title: LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset\n",
            "   Abstract: The fast growing capabilities of large-scale deep learning models, such as\n",
            "Bert, GPT and ViT, are revolutionizing the landscape of NLP, CV and many other\n",
            "domains. Training such models, however, poses an unprecedented demand for\n",
            "computing power, which incurs exponentially increasing energy cost and carbon\n",
            "dioxide emissions. It is thus critical to develop efficient training solutions\n",
            "to reduce the training costs. Motivated by a set of key observations of inter-\n",
            "and intra-layer similarities among feature maps and attentions that can be\n",
            "identified from typical training processes, we propose a multi-level framework\n",
            "for training acceleration. Specifically, the framework is based on three basic\n",
            "operators, Coalescing, De-coalescing and Interpolation, which can be\n",
            "orchestrated to build a multi-level training framework. The framework consists\n",
            "of a V-cycle training process, which progressively down- and up-scales the\n",
            "model size and projects the parameters between adjacent levels of models via\n",
            "coalescing and de-coalescing. The key idea is that a smaller model that can be\n",
            "trained for fast convergence and the trained parameters provides high-qualities\n",
            "intermediate solutions for the next level larger network. The interpolation\n",
            "operator is designed to break the symmetry of neurons incurred by de-coalescing\n",
            "for better convergence performance. Our experiments on transformer-based\n",
            "language models (e.g. Bert, GPT) as well as a vision model (e.g. DeiT) prove\n",
            "that the proposed framework reduces the computational cost by about 20% on\n",
            "training BERT/GPT-Base models and up to 51.6% on training the BERT-Large model\n",
            "while preserving the performance.\n",
            "\n",
            "290. Title: Demystifying Poisoning Backdoor Attacks from a Statistical Perspective\n",
            "   Abstract: A central objective in computer vision is to design models with appropriate\n",
            "2-D inductive bias. Desiderata for 2D inductive bias include two-dimensional\n",
            "position awareness, dynamic spatial locality, and translation and permutation\n",
            "invariance. To address these goals, we leverage an expressive variation of the\n",
            "multidimensional State Space Model (SSM). Our approach introduces efficient\n",
            "parameterization, accelerated computation, and a suitable normalization scheme.\n",
            "Empirically, we observe that incorporating our layer at the beginning of each\n",
            "transformer block of Vision Transformers (ViT) significantly enhances\n",
            "performance for multiple ViT backbones and across datasets. The new layer is\n",
            "effective even with a negligible amount of additional parameters and inference\n",
            "time. Ablation studies and visualizations demonstrate that the layer has a\n",
            "strong 2-D inductive bias. For example, vision transformers equipped with our\n",
            "layer exhibit effective performance even without positional encoding\n",
            "\n",
            "291. Title: FreeReg: Image-to-Point Cloud Registration Leveraging Pretrained Diffusion Models and Monocular Depth Estimators\n",
            "   Abstract: Molecular docking is critical to structure-based virtual screening, yet the\n",
            "throughput of such workflows is limited by the expensive optimization of\n",
            "scoring functions involved in most docking algorithms. We explore how machine\n",
            "learning can accelerate this process by learning a scoring function with a\n",
            "functional form that allows for more rapid optimization. Specifically, we\n",
            "define the scoring function to be the cross-correlation of multi-channel ligand\n",
            "and protein scalar fields parameterized by equivariant graph neural networks,\n",
            "enabling rapid optimization over rigid-body degrees of freedom with fast\n",
            "Fourier transforms. The runtime of our approach can be amortized at several\n",
            "levels of abstraction, and is particularly favorable for virtual screening\n",
            "settings with a common binding pocket. We benchmark our scoring functions on\n",
            "two simplified docking-related tasks: decoy pose scoring and rigid conformer\n",
            "docking. Our method attains similar but faster performance on crystal\n",
            "structures compared to the widely-used Vina and Gnina scoring functions, and is\n",
            "more robust on computationally predicted structures. Code is available at\n",
            "https://github.com/bjing2016/scalar-fields.\n",
            "\n",
            "292. Title: Accelerating Distributed Stochastic Optimization via Self-Repellent Random Walks\n",
            "   Abstract: The growing dependence on machine learning in real-world applications\n",
            "emphasizes the importance of understanding and ensuring its safety. Backdoor\n",
            "attacks pose a significant security risk due to their stealthy nature and\n",
            "potentially serious consequences. Such attacks involve embedding triggers\n",
            "within a learning model with the intention of causing malicious behavior when\n",
            "an active trigger is present while maintaining regular functionality without\n",
            "it. This paper evaluates the effectiveness of any backdoor attack incorporating\n",
            "a constant trigger, by establishing tight lower and upper boundaries for the\n",
            "performance of the compromised model on both clean and backdoor test data. The\n",
            "developed theory answers a series of fundamental but previously underexplored\n",
            "problems, including (1) what are the determining factors for a backdoor\n",
            "attack's success, (2) what is the direction of the most effective backdoor\n",
            "attack, and (3) when will a human-imperceptible trigger succeed. Our derived\n",
            "understanding applies to both discriminative and generative models. We also\n",
            "demonstrate the theory by conducting experiments using benchmark datasets and\n",
            "state-of-the-art backdoor attack scenarios.\n",
            "\n",
            "293. Title: Fine-tuning Multimodal LLMs to Follow Zero-shot Demonstrative Instructions\n",
            "   Abstract: Instruction tuning is a standard technique employed to align large language\n",
            "models to end tasks and user preferences after the initial pretraining phase.\n",
            "Recent research indicates the critical role of data engineering in instruction\n",
            "tuning -- when appropriately selected, only limited data is necessary to\n",
            "achieve superior performance. However, we still lack a principled understanding\n",
            "of what makes good instruction tuning data for alignment, and how we should\n",
            "select data automatically and effectively. In this work, we delve deeply into\n",
            "automatic data selection strategies for alignment. We start with controlled\n",
            "studies to measure data across three dimensions: complexity, quality, and\n",
            "diversity, along which we examine existing methods and introduce novel\n",
            "techniques for enhanced data measurement. Subsequently, we propose a simple\n",
            "strategy to select data samples based on the measurement. We present deita\n",
            "(short for Data-Efficient Instruction Tuning for Alignment), a series of models\n",
            "fine-tuned from LLaMA and Mistral models using data samples automatically\n",
            "selected with our proposed approach. Empirically, deita performs better or on\n",
            "par with the state-of-the-art open-source alignment models with only 6K SFT\n",
            "training data samples -- over 10x less than the data used in the baselines.\n",
            "When further trained with direct preference optimization (DPO),\n",
            "deita-Mistral-7B + DPO trained with 6K SFT and 10K DPO samples achieve 7.55\n",
            "MT-Bench and 90.06% AlpacaEval scores. We anticipate this work to provide tools\n",
            "on automatic data selection, facilitating data-efficient alignment. We release\n",
            "our models as well as the selected datasets for future researches to\n",
            "effectively align models more efficiently.\n",
            "\n",
            "294. Title: Manifold Diffusion Fields\n",
            "   Abstract: We study a family of distributed stochastic optimization algorithms where\n",
            "gradients are sampled by a token traversing a network of agents in random-walk\n",
            "fashion. Typically, these random-walks are chosen to be Markov chains that\n",
            "asymptotically sample from a desired target distribution, and play a critical\n",
            "role in the convergence of the optimization iterates. In this paper, we take a\n",
            "novel approach by replacing the standard linear Markovian token by one which\n",
            "follows a nonlinear Markov chain - namely the Self-Repellent Radom Walk (SRRW).\n",
            "Defined for any given 'base' Markov chain, the SRRW, parameterized by a\n",
            "positive scalar {\\alpha}, is less likely to transition to states that were\n",
            "highly visited in the past, thus the name. In the context of MCMC sampling on a\n",
            "graph, a recent breakthrough in Doshi et al. (2023) shows that the SRRW\n",
            "achieves O(1/{\\alpha}) decrease in the asymptotic variance for sampling. We\n",
            "propose the use of a 'generalized' version of the SRRW to drive token\n",
            "algorithms for distributed stochastic optimization in the form of stochastic\n",
            "approximation, termed SA-SRRW. We prove that the optimization iterate errors of\n",
            "the resulting SA-SRRW converge to zero almost surely and prove a central limit\n",
            "theorem, deriving the explicit form of the resulting asymptotic covariance\n",
            "matrix corresponding to iterate errors. This asymptotic covariance is always\n",
            "smaller than that of an algorithm driven by the base Markov chain and decreases\n",
            "at rate O(1/{\\alpha}^2) - the performance benefit of using SRRW thereby\n",
            "amplified in the stochastic optimization context. Empirical results support our\n",
            "theoretical findings.\n",
            "\n",
            "295. Title: AntGPT: Can Large Language Models Help Long-term Action Anticipation from Videos?\n",
            "   Abstract: We present Manifold Diffusion Fields (MDF), an approach that unlocks learning\n",
            "of diffusion models of data in general non-Euclidean geometries. Leveraging\n",
            "insights from spectral geometry analysis, we define an intrinsic coordinate\n",
            "system on the manifold via the eigen-functions of the Laplace-Beltrami\n",
            "Operator. MDF represents functions using an explicit parametrization formed by\n",
            "a set of multiple input-output pairs. Our approach allows to sample continuous\n",
            "functions on manifolds and is invariant with respect to rigid and isometric\n",
            "transformations of the manifold. In addition, we show that MDF generalizes to\n",
            "the case where the training set contains functions on different manifolds.\n",
            "Empirical results on multiple datasets and manifolds including challenging\n",
            "scientific problems like weather prediction or molecular conformation show that\n",
            "MDF can capture distributions of such functions with better diversity and\n",
            "fidelity than previous approaches.\n",
            "\n",
            "296. Title: Large Language Models as Optimizers\n",
            "   Abstract: Autoregressive models (ARMs) are widely regarded as the cornerstone of large\n",
            "language models (LLMs). We challenge this notion by introducing LLaDA, a\n",
            "diffusion model trained from scratch under the pre-training and supervised\n",
            "fine-tuning (SFT) paradigm. LLaDA models distributions through a forward data\n",
            "masking process and a reverse process, parameterized by a vanilla Transformer\n",
            "to predict masked tokens. By optimizing a likelihood bound, it provides a\n",
            "principled generative approach for probabilistic inference. Across extensive\n",
            "benchmarks, LLaDA demonstrates strong scalability, outperforming our\n",
            "self-constructed ARM baselines. Remarkably, LLaDA 8B is competitive with strong\n",
            "LLMs like LLaMA3 8B in in-context learning and, after SFT, exhibits impressive\n",
            "instruction-following abilities in case studies such as multi-turn dialogue.\n",
            "Moreover, LLaDA addresses the reversal curse, surpassing GPT-4o in a reversal\n",
            "poem completion task. Our findings establish diffusion models as a viable and\n",
            "promising alternative to ARMs, challenging the assumption that key LLM\n",
            "capabilities discussed above are inherently tied to ARMs. Project page and\n",
            "codes: https://ml-gsai.github.io/LLaDA-demo/.\n",
            "\n",
            "297. Title: An improved analysis of per-sample and per-update clipping in federated learning\n",
            "   Abstract: Conformal prediction is a powerful tool to generate uncertainty sets with\n",
            "guaranteed coverage using any predictive model, under the assumption that the\n",
            "training and test data are i.i.d.. Recently, it has been shown that adversarial\n",
            "examples are able to manipulate conformal methods to construct prediction sets\n",
            "with invalid coverage rates, as the i.i.d. assumption is violated. To address\n",
            "this issue, a recent work, Randomized Smoothed Conformal Prediction (RSCP), was\n",
            "first proposed to certify the robustness of conformal prediction methods to\n",
            "adversarial noise. However, RSCP has two major limitations: (i) its robustness\n",
            "guarantee is flawed when used in practice and (ii) it tends to produce large\n",
            "uncertainty sets. To address these limitations, we first propose a novel\n",
            "framework called RSCP+ to provide provable robustness guarantee in evaluation,\n",
            "which fixes the issues in the original RSCP method. Next, we propose two novel\n",
            "methods, Post-Training Transformation (PTT) and Robust Conformal Training\n",
            "(RCT), to effectively reduce prediction set size with little computation\n",
            "overhead. Experimental results in CIFAR10, CIFAR100, and ImageNet suggest the\n",
            "baseline method only yields trivial predictions including full label set, while\n",
            "our methods could boost the efficiency by up to $4.36\\times$, $5.46\\times$, and\n",
            "$16.9\\times$ respectively and provide practical robustness guarantee. Our codes\n",
            "are available at\n",
            "https://github.com/Trustworthy-ML-Lab/Provably-Robust-Conformal-Prediction.\n",
            "\n",
            "298. Title: PB-LLM: Partially Binarized Large Language Models\n",
            "   Abstract: Large language models (LLMs) and large visual language models (LVLMs) have\n",
            "been at the forefront of the artificial intelligence field, particularly for\n",
            "tasks like text generation, video captioning, and question-answering.\n",
            "Typically, it is more applicable to train these models on broader knowledge\n",
            "bases or datasets to increase generalizability, learn relationships between\n",
            "topics, and recognize patterns. Instead, we propose to provide instructional\n",
            "datasets specific to the task of each modality within a distinct domain and\n",
            "then fine-tune the parameters of the model using LORA. With our approach, we\n",
            "can eliminate all noise irrelevant to the given task while also ensuring that\n",
            "the model generates with enhanced precision. For this work, we use Video-LLaVA\n",
            "to generate recipes given cooking videos without transcripts. Video-LLaVA's\n",
            "multimodal architecture allows us to provide cooking images to its image\n",
            "encoder, cooking videos to its video encoder, and general cooking questions to\n",
            "its text encoder. Thus, we aim to remove all noise unrelated to cooking while\n",
            "improving our model's capabilities to generate specific ingredient lists and\n",
            "detailed instructions. As a result, our approach to fine-tuning Video-LLaVA\n",
            "leads to gains over the baseline Video-LLaVA by 2% on the YouCook2 dataset.\n",
            "While this may seem like a marginal increase, our model trains on an image\n",
            "instruction dataset 2.5% the size of Video-LLaVA's and a video instruction\n",
            "dataset 23.76% of Video-LLaVA's.\n",
            "\n",
            "299. Title: WildChat: 1M ChatGPT Interaction Logs in the Wild\n",
            "   Abstract: Recently, federated learning has raised increasing interest in the medical\n",
            "image analysis field due to its ability to aggregate multi-center data with\n",
            "privacy-preserving properties. A large amount of federated training schemes\n",
            "have been published, which we categorize into global (one final model),\n",
            "personalized (one model per institution) or hybrid (one model per cluster of\n",
            "institutions) methods. However, their applicability on the recently published\n",
            "Federated Brain Tumor Segmentation 2022 dataset has not been explored yet. We\n",
            "propose an extensive benchmark of federated learning algorithms from all three\n",
            "classes on this task. While standard FedAvg already performs very well, we show\n",
            "that some methods from each category can bring a slight performance improvement\n",
            "and potentially limit the final model(s) bias toward the predominant data\n",
            "distribution of the federation. Moreover, we provide a deeper understanding of\n",
            "the behaviour of federated learning on this task through alternative ways of\n",
            "distributing the pooled dataset among institutions, namely an Independent and\n",
            "Identical Distributed (IID) setup, and a limited data setup.\n",
            "\n",
            "300. Title: Out-Of-Domain Unlabeled Data Improves Generalization\n",
            "   Abstract: Crystal structures are characterized by atomic bases within a primitive unit\n",
            "cell that repeats along a regular lattice throughout 3D space. The periodic and\n",
            "infinite nature of crystals poses unique challenges for geometric graph\n",
            "representation learning. Specifically, constructing graphs that effectively\n",
            "capture the complete geometric information of crystals and handle chiral\n",
            "crystals remains an unsolved and challenging problem. In this paper, we\n",
            "introduce a novel approach that utilizes the periodic patterns of unit cells to\n",
            "establish the lattice-based representation for each atom, enabling efficient\n",
            "and expressive graph representations of crystals. Furthermore, we propose\n",
            "ComFormer, a SE(3) transformer designed specifically for crystalline materials.\n",
            "ComFormer includes two variants; namely, iComFormer that employs invariant\n",
            "geometric descriptors of Euclidean distances and angles, and eComFormer that\n",
            "utilizes equivariant vector representations. Experimental results demonstrate\n",
            "the state-of-the-art predictive accuracy of ComFormer variants on various tasks\n",
            "across three widely-used crystal benchmarks. Our code is publicly available as\n",
            "part of the AIRS library (https://github.com/divelab/AIRS).\n",
            "\n",
            "301. Title: Fast-DetectGPT: Efficient Zero-Shot Detection of Machine-Generated Text via Conditional Probability Curvature\n",
            "   Abstract: This paper investigates the sparse phase retrieval problem, which aims to\n",
            "recover a sparse signal from a system of quadratic measurements. In this work,\n",
            "we propose a novel non-convex algorithm, termed Gradient Hard Thresholding\n",
            "Pursuit (GraHTP), for sparse phase retrieval with complex sensing vectors.\n",
            "GraHTP is theoretically provable and exhibits high efficiency, achieving a\n",
            "quadratic convergence rate after a finite number of iterations, while\n",
            "maintaining low computational complexity per iteration. Numerical experiments\n",
            "further demonstrate GraHTP's superior performance compared to state-of-the-art\n",
            "algorithms.\n",
            "\n",
            "302. Title: Stochastic Modified Equations and Dynamics of Dropout Algorithm\n",
            "   Abstract: Semi-supervised learning (SSL) leverages both labeled and unlabeled data for\n",
            "training models when the labeled data is limited and the unlabeled data is\n",
            "vast. Frequently, the unlabeled data is more widely available than the labeled\n",
            "data, hence this data is used to improve the level of generalization of a model\n",
            "when the labeled data is scarce. However, in real-world settings unlabeled data\n",
            "might depict a different distribution than the labeled dataset distribution.\n",
            "This is known as distribution mismatch. Such problem generally occurs when the\n",
            "source of unlabeled data is different from the labeled data. For instance, in\n",
            "the medical imaging domain, when training a COVID-19 detector using chest X-ray\n",
            "images, different unlabeled datasets sampled from different hospitals might be\n",
            "used. In this work, we propose an automatic thresholding method to filter\n",
            "out-of-distribution data in the unlabeled dataset. We use the Mahalanobis\n",
            "distance between the labeled and unlabeled datasets using the feature space\n",
            "built by a pre-trained Image-net Feature Extractor (FE) to score each unlabeled\n",
            "observation. We test two simple automatic thresholding methods in the context\n",
            "of training a COVID-19 detector using chest X-ray images. The tested methods\n",
            "provide an automatic manner to define what unlabeled data to preserve when\n",
            "training a semi-supervised deep learning architecture.\n",
            "\n",
            "303. Title: Enhancing Human Experience in Human-Agent Collaboration: A Human-Centered Modeling Approach Based on Positive Human Gain\n",
            "   Abstract: The \"pre-training $\\rightarrow$ downstream adaptation\" presents both new\n",
            "opportunities and challenges for Continual Learning (CL). Although the recent\n",
            "state-of-the-art in CL is achieved through Parameter-Efficient-Tuning (PET)\n",
            "adaptation paradigm, only prompt has been explored, limiting its application to\n",
            "Transformers only. In this paper, we position prompting as one instantiation of\n",
            "PET, and propose a unified CL framework with general PET, dubbed as\n",
            "Learning-Accumulation-Ensemble (LAE). PET, e.g., using Adapter, LoRA, or\n",
            "Prefix, can adapt a pre-trained model to downstream tasks with fewer parameters\n",
            "and resources. Given a PET method, our LAE framework incorporates it for CL\n",
            "with three novel designs. 1) Learning: the pre-trained model adapts to the new\n",
            "task by tuning an online PET module, along with our adaptation speed\n",
            "calibration to align different PET modules, 2) Accumulation: the task-specific\n",
            "knowledge learned by the online PET module is accumulated into an offline PET\n",
            "module through momentum update, 3) Ensemble: During inference, we respectively\n",
            "construct two experts with online/offline PET modules (which are favored by the\n",
            "novel/historical tasks) for prediction ensemble. We show that LAE is compatible\n",
            "with a battery of PET methods and gains strong CL capability. For example, LAE\n",
            "with Adaptor PET surpasses the prior state-of-the-art by 1.3% and 3.6% in\n",
            "last-incremental accuracy on CIFAR100 and ImageNet-R datasets, respectively.\n",
            "Code is available at \\url{https://github.com/gqk/LAE}.\n",
            "\n",
            "304. Title: LLM-CXR: Instruction-Finetuned LLM for CXR Image Understanding and Generation\n",
            "   Abstract: Dropout is a widely utilized regularization technique in the training of\n",
            "neural networks, nevertheless, its underlying mechanism and its impact on\n",
            "achieving good generalization abilities remain poorly understood. In this work,\n",
            "we derive the stochastic modified equations for analyzing the dynamics of\n",
            "dropout, where its discrete iteration process is approximated by a class of\n",
            "stochastic differential equations. In order to investigate the underlying\n",
            "mechanism by which dropout facilitates the identification of flatter minima, we\n",
            "study the noise structure of the derived stochastic modified equation for\n",
            "dropout. By drawing upon the structural resemblance between the Hessian and\n",
            "covariance through several intuitive approximations, we empirically demonstrate\n",
            "the universal presence of the inverse variance-flatness relation and the\n",
            "Hessian-variance relation, throughout the training process of dropout. These\n",
            "theoretical and empirical findings make a substantial contribution to our\n",
            "understanding of the inherent tendency of dropout to locate flatter minima.\n",
            "\n",
            "305. Title: Solving Diffusion ODEs with Optimal Boundary Conditions for Better Image Super-Resolution\n",
            "   Abstract: Existing game AI research mainly focuses on enhancing agents' abilities to\n",
            "win games, but this does not inherently make humans have a better experience\n",
            "when collaborating with these agents. For example, agents may dominate the\n",
            "collaboration and exhibit unintended or detrimental behaviors, leading to poor\n",
            "experiences for their human partners. In other words, most game AI agents are\n",
            "modeled in a \"self-centered\" manner. In this paper, we propose a\n",
            "\"human-centered\" modeling scheme for collaborative agents that aims to enhance\n",
            "the experience of humans. Specifically, we model the experience of humans as\n",
            "the goals they expect to achieve during the task. We expect that agents should\n",
            "learn to enhance the extent to which humans achieve these goals while\n",
            "maintaining agents' original abilities (e.g., winning games). To achieve this,\n",
            "we propose the Reinforcement Learning from Human Gain (RLHG) approach. The RLHG\n",
            "approach introduces a \"baseline\", which corresponds to the extent to which\n",
            "humans primitively achieve their goals, and encourages agents to learn\n",
            "behaviors that can effectively enhance humans in achieving their goals better.\n",
            "We evaluate the RLHG agent in the popular Multi-player Online Battle Arena\n",
            "(MOBA) game, Honor of Kings, by conducting real-world human-agent tests. Both\n",
            "objective performance and subjective preference results show that the RLHG\n",
            "agent provides participants better gaming experience.\n",
            "\n",
            "306. Title: Benign Overfitting and Grokking in ReLU Networks for XOR Cluster Data\n",
            "   Abstract: In many machine learning systems that jointly learn from multiple modalities,\n",
            "a core research question is to understand the nature of multimodal\n",
            "interactions: how modalities combine to provide new task-relevant information\n",
            "that was not present in either alone. We study this challenge of interaction\n",
            "quantification in a semi-supervised setting with only labeled unimodal data and\n",
            "naturally co-occurring multimodal data (e.g., unlabeled images and captions,\n",
            "video and corresponding audio) but when labeling them is time-consuming. Using\n",
            "a precise information-theoretic definition of interactions, our key\n",
            "contribution is the derivation of lower and upper bounds to quantify the amount\n",
            "of multimodal interactions in this semi-supervised setting. We propose two\n",
            "lower bounds: one based on the shared information between modalities and the\n",
            "other based on disagreement between separately trained unimodal classifiers,\n",
            "and derive an upper bound through connections to approximate algorithms for\n",
            "min-entropy couplings. We validate these estimated bounds and show how they\n",
            "accurately track true interactions. Finally, we show how these theoretical\n",
            "results can be used to estimate multimodal model performance, guide data\n",
            "collection, and select appropriate multimodal models for various tasks.\n",
            "\n",
            "307. Title: PARL: A Unified Framework for Policy Alignment in Reinforcement Learning from Human Feedback\n",
            "   Abstract: A Marked Temporal Point Process (MTPP) is a stochastic process whose\n",
            "realization is a set of event-time data. MTPP is often used to understand\n",
            "complex dynamics of asynchronous temporal events such as money transaction,\n",
            "social media, healthcare, etc. Recent studies have utilized deep neural\n",
            "networks to capture complex temporal dependencies of events and generate\n",
            "embedding that aptly represent the observed events. While most previous studies\n",
            "focus on the inter-event dependencies and their representations, how individual\n",
            "events influence the overall dynamics over time has been under-explored. In\n",
            "this regime, we propose a Decoupled MTPP framework that disentangles\n",
            "characterization of a stochastic process into a set of evolving influences from\n",
            "different events. Our approach employs Neural Ordinary Differential Equations\n",
            "(Neural ODEs) to learn flexible continuous dynamics of these influences while\n",
            "simultaneously addressing multiple inference problems, such as density\n",
            "estimation and survival rate computation. We emphasize the significance of\n",
            "disentangling the influences by comparing our framework with state-of-the-art\n",
            "methods on real-life datasets, and provide analysis on the model behavior for\n",
            "potential applications.\n",
            "\n",
            "308. Title: Mitigating the Curse of Dimensionality for Certified Robustness via Dual Randomized Smoothing\n",
            "   Abstract: Large vision-language models (VLMs) such as GPT-4 have achieved exceptional\n",
            "performance across various multi-modal tasks. However, the deployment of VLMs\n",
            "necessitates substantial energy consumption and computational resources. Once\n",
            "attackers maliciously induce high energy consumption and latency time\n",
            "(energy-latency cost) during inference of VLMs, it will exhaust computational\n",
            "resources. In this paper, we explore this attack surface about availability of\n",
            "VLMs and aim to induce high energy-latency cost during inference of VLMs. We\n",
            "find that high energy-latency cost during inference of VLMs can be manipulated\n",
            "by maximizing the length of generated sequences. To this end, we propose\n",
            "verbose images, with the goal of crafting an imperceptible perturbation to\n",
            "induce VLMs to generate long sentences during inference. Concretely, we design\n",
            "three loss objectives. First, a loss is proposed to delay the occurrence of\n",
            "end-of-sequence (EOS) token, where EOS token is a signal for VLMs to stop\n",
            "generating further tokens. Moreover, an uncertainty loss and a token diversity\n",
            "loss are proposed to increase the uncertainty over each generated token and the\n",
            "diversity among all tokens of the whole generated sequence, respectively, which\n",
            "can break output dependency at token-level and sequence-level. Furthermore, a\n",
            "temporal weight adjustment algorithm is proposed, which can effectively balance\n",
            "these losses. Extensive experiments demonstrate that our verbose images can\n",
            "increase the length of generated sequences by 7.87 times and 8.56 times\n",
            "compared to original images on MS-COCO and ImageNet datasets, which presents\n",
            "potential challenges for various applications. Our code is available at\n",
            "https://github.com/KuofengGao/Verbose_Images.\n",
            "\n",
            "309. Title: Str2Str: A Score-based Framework for Zero-shot Protein Conformation Sampling\n",
            "   Abstract: Diffusion models, as a kind of powerful generative model, have given\n",
            "impressive results on image super-resolution (SR) tasks. However, due to the\n",
            "randomness introduced in the reverse process of diffusion models, the\n",
            "performances of diffusion-based SR models are fluctuating at every time of\n",
            "sampling, especially for samplers with few resampled steps. This inherent\n",
            "randomness of diffusion models results in ineffectiveness and instability,\n",
            "making it challenging for users to guarantee the quality of SR results.\n",
            "However, our work takes this randomness as an opportunity: fully analyzing and\n",
            "leveraging it leads to the construction of an effective plug-and-play sampling\n",
            "method that owns the potential to benefit a series of diffusion-based SR\n",
            "methods. More in detail, we propose to steadily sample high-quality SR images\n",
            "from pre-trained diffusion-based SR models by solving diffusion ordinary\n",
            "differential equations (diffusion ODEs) with optimal boundary conditions (BCs)\n",
            "and analyze the characteristics between the choices of BCs and their\n",
            "corresponding SR results. Our analysis shows the route to obtain an\n",
            "approximately optimal BC via an efficient exploration in the whole space. The\n",
            "quality of SR results sampled by the proposed method with fewer steps\n",
            "outperforms the quality of results sampled by current methods with randomness\n",
            "from the same pre-trained diffusion-based SR model, which means that our\n",
            "sampling method \"boosts\" current diffusion-based SR models without any\n",
            "additional training.\n",
            "\n",
            "310. Title: Jumanji: a Diverse Suite of Scalable Reinforcement Learning Environments in JAX\n",
            "   Abstract: Real-life applications of deep neural networks are hindered by their unsteady\n",
            "predictions when faced with noisy inputs and adversarial attacks. The certified\n",
            "radius in this context is a crucial indicator of the robustness of models.\n",
            "However how to design an efficient classifier with an associated certified\n",
            "radius? Randomized smoothing provides a promising framework by relying on noise\n",
            "injection into the inputs to obtain a smoothed and robust classifier. In this\n",
            "paper, we first show that the variance introduced by the Monte-Carlo sampling\n",
            "in the randomized smoothing procedure estimate closely interacts with two other\n",
            "important properties of the classifier, \\textit{i.e.} its Lipschitz constant\n",
            "and margin. More precisely, our work emphasizes the dual impact of the\n",
            "Lipschitz constant of the base classifier, on both the smoothed classifier and\n",
            "the empirical variance. To increase the certified robust radius, we introduce a\n",
            "different way to convert logits to probability vectors for the base classifier\n",
            "to leverage the variance-margin trade-off. We leverage the use of Bernstein's\n",
            "concentration inequality along with enhanced Lipschitz bounds for randomized\n",
            "smoothing. Experimental results show a significant improvement in certified\n",
            "accuracy compared to current state-of-the-art methods. Our novel certification\n",
            "procedure allows us to use pre-trained models with randomized smoothing,\n",
            "effectively improving the current certification radius in a zero-shot manner.\n",
            "\n",
            "311. Title: InfoBatch: Lossless Training Speed Up by Unbiased Dynamic Data Pruning\n",
            "   Abstract: Generative Artificial Intelligence (AI) technologies and large models are\n",
            "producing realistic outputs across various domains, such as images, text,\n",
            "speech, and music. Creating these advanced generative models requires\n",
            "significant resources, particularly large and high-quality datasets. To\n",
            "minimise training expenses, many algorithm developers use data created by the\n",
            "models themselves as a cost-effective training solution. However, not all\n",
            "synthetic data effectively improve model performance, necessitating a strategic\n",
            "balance in the use of real versus synthetic data to optimise outcomes.\n",
            "Currently, the previously well-controlled integration of real and synthetic\n",
            "data is becoming uncontrollable. The widespread and unregulated dissemination\n",
            "of synthetic data online leads to the contamination of datasets traditionally\n",
            "compiled through web scraping, now mixed with unlabeled synthetic data. This\n",
            "trend, known as the AI autophagy phenomenon, suggests a future where generative\n",
            "AI systems may increasingly consume their own outputs without discernment,\n",
            "raising concerns about model performance, reliability, and ethical\n",
            "implications. What will happen if generative AI continuously consumes itself\n",
            "without discernment? What measures can we take to mitigate the potential\n",
            "adverse effects? To address these research questions, this study examines the\n",
            "existing literature, delving into the consequences of AI autophagy, analyzing\n",
            "the associated risks, and exploring strategies to mitigate its impact. Our aim\n",
            "is to provide a comprehensive perspective on this phenomenon advocating for a\n",
            "balanced approach that promotes the sustainable development of generative AI\n",
            "technologies in the era of large models.\n",
            "\n",
            "312. Title: Graph-based Virtual Sensing from Sparse and Partial Multivariate Observations\n",
            "   Abstract: Double descent presents a counter-intuitive aspect within the machine\n",
            "learning domain, and researchers have observed its manifestation in various\n",
            "models and tasks. While some theoretical explanations have been proposed for\n",
            "this phenomenon in specific contexts, an accepted theory to account for its\n",
            "occurrence in deep learning remains yet to be established. In this study, we\n",
            "revisit the phenomenon of double descent and demonstrate that its occurrence is\n",
            "strongly influenced by the presence of noisy data. Through conducting a\n",
            "comprehensive analysis of the feature space of learned representations, we\n",
            "unveil that double descent arises in imperfect models trained with noisy data.\n",
            "We argue that double descent is a consequence of the model first learning the\n",
            "noisy data until interpolation and then adding implicit regularization via\n",
            "over-parameterization acquiring therefore capability to separate the\n",
            "information from the noise.\n",
            "\n",
            "313. Title: Unraveling the Enigma of Double Descent: An In-depth Analysis through the Lens of Learned Feature Space\n",
            "   Abstract: Heavy-ball momentum with decaying learning rates is widely used with SGD for\n",
            "optimizing deep learning models. In contrast to its empirical popularity, the\n",
            "understanding of its theoretical property is still quite limited, especially\n",
            "under the standard anisotropic gradient noise condition for quadratic\n",
            "regression problems. Although it is widely conjectured that heavy-ball momentum\n",
            "method can provide accelerated convergence and should work well in large batch\n",
            "settings, there is no rigorous theoretical analysis. In this paper, we fill\n",
            "this theoretical gap by establishing a non-asymptotic convergence bound for\n",
            "stochastic heavy-ball methods with step decay scheduler on quadratic\n",
            "objectives, under the anisotropic gradient noise condition. As a direct\n",
            "implication, we show that heavy-ball momentum can provide\n",
            "$\\tilde{\\mathcal{O}}(\\sqrt{\\kappa})$ accelerated convergence of the bias term\n",
            "of SGD while still achieving near-optimal convergence rate with respect to the\n",
            "stochastic variance term. The combined effect implies an overall convergence\n",
            "rate within log factors from the statistical minimax rate. This means SGD with\n",
            "heavy-ball momentum is useful in the large-batch settings such as distributed\n",
            "machine learning or federated learning, where a smaller number of iterations\n",
            "can significantly reduce the number of communication rounds, leading to\n",
            "acceleration in practice.\n",
            "\n",
            "314. Title: The Generative AI Paradox: “What It Can Create, It May Not Understand”\n",
            "   Abstract: Topic model evaluation, like evaluation of other unsupervised methods, can be\n",
            "contentious. However, the field has coalesced around automated estimates of\n",
            "topic coherence, which rely on the frequency of word co-occurrences in a\n",
            "reference corpus. Contemporary neural topic models surpass classical ones\n",
            "according to these metrics. At the same time, topic model evaluation suffers\n",
            "from a validation gap: automated coherence, developed for classical models, has\n",
            "not been validated using human experimentation for neural models. In addition,\n",
            "a meta-analysis of topic modeling literature reveals a substantial\n",
            "standardization gap in automated topic modeling benchmarks. To address the\n",
            "validation gap, we compare automated coherence with the two most widely\n",
            "accepted human judgment tasks: topic rating and word intrusion. To address the\n",
            "standardization gap, we systematically evaluate a dominant classical model and\n",
            "two state-of-the-art neural models on two commonly used datasets. Automated\n",
            "evaluations declare a winning model when corresponding human evaluations do\n",
            "not, calling into question the validity of fully automatic evaluations\n",
            "independent of human judgments.\n",
            "\n",
            "315. Title: SEAL: A Framework for Systematic Evaluation of Real-World Super-Resolution\n",
            "   Abstract: Vision Transformer (ViT) has emerged as a powerful architecture in the realm\n",
            "of modern computer vision. However, its application in certain imaging fields,\n",
            "such as microscopy and satellite imaging, presents unique challenges. In these\n",
            "domains, images often contain multiple channels, each carrying semantically\n",
            "distinct and independent information. Furthermore, the model must demonstrate\n",
            "robustness to sparsity in input channels, as they may not be densely available\n",
            "during training or testing. In this paper, we propose a modification to the ViT\n",
            "architecture that enhances reasoning across the input channels and introduce\n",
            "Hierarchical Channel Sampling (HCS) as an additional regularization technique\n",
            "to ensure robustness when only partial channels are presented during test time.\n",
            "Our proposed model, ChannelViT, constructs patch tokens independently from each\n",
            "input channel and utilizes a learnable channel embedding that is added to the\n",
            "patch tokens, similar to positional embeddings. We evaluate the performance of\n",
            "ChannelViT on ImageNet, JUMP-CP (microscopy cell imaging), and So2Sat\n",
            "(satellite imaging). Our results show that ChannelViT outperforms ViT on\n",
            "classification tasks and generalizes well, even when a subset of input channels\n",
            "is used during testing. Across our experiments, HCS proves to be a powerful\n",
            "regularizer, independent of the architecture employed, suggesting itself as a\n",
            "straightforward technique for robust ViT training. Lastly, we find that\n",
            "ChannelViT generalizes effectively even when there is limited access to all\n",
            "channels during training, highlighting its potential for multi-channel imaging\n",
            "under real-world conditions with sparse sensors. Our code is available at\n",
            "https://github.com/insitro/ChannelViT.\n",
            "\n",
            "316. Title: Energy-based Automated Model Evaluation\n",
            "   Abstract: Early stopping methods in deep learning face the challenge of balancing the\n",
            "volume of training and validation data, especially in the presence of label\n",
            "noise. Concretely, sparing more data for validation from training data would\n",
            "limit the performance of the learned model, yet insufficient validation data\n",
            "could result in a sub-optimal selection of the desired model. In this paper, we\n",
            "propose a novel early stopping method called Label Wave, which does not require\n",
            "validation data for selecting the desired model in the presence of label noise.\n",
            "It works by tracking the changes in the model's predictions on the training set\n",
            "during the training process, aiming to halt training before the model unduly\n",
            "fits mislabeled data. This method is empirically supported by our observation\n",
            "that minimum fluctuations in predictions typically occur at the training epoch\n",
            "before the model excessively fits mislabeled data. Through extensive\n",
            "experiments, we show both the effectiveness of the Label Wave method across\n",
            "various settings and its capability to enhance the performance of existing\n",
            "methods for learning with noisy labels.\n",
            "\n",
            "317. Title: Online Stabilization of Spiking Neural Networks\n",
            "   Abstract: Recent efforts in fine-tuning language models often rely on automatic data\n",
            "selection, commonly using Nearest Neighbors retrieval from large datasets.\n",
            "However, we theoretically show that this approach tends to select redundant\n",
            "data, limiting its effectiveness or even hurting performance. To address this,\n",
            "we introduce SIFT, a data selection algorithm designed to reduce uncertainty\n",
            "about the model's response given a prompt, which unifies ideas from retrieval\n",
            "and active learning. Whereas Nearest Neighbor retrieval typically fails in the\n",
            "presence of information duplication, SIFT accounts for information duplication\n",
            "and optimizes the overall information gain of the selected examples. We focus\n",
            "our evaluations on fine-tuning at test-time for prompt-specific language\n",
            "modeling on the Pile dataset, and show that SIFT consistently outperforms\n",
            "Nearest Neighbor retrieval, with minimal computational overhead. Moreover, we\n",
            "show that our uncertainty estimates can predict the performance gain of\n",
            "test-time fine-tuning, and use this to develop an adaptive algorithm that\n",
            "invests test-time compute proportional to realized performance gains. We\n",
            "provide the $\\texttt{activeft}$ (Active Fine-Tuning) library which can be used\n",
            "as a drop-in replacement for Nearest Neighbor retrieval.\n",
            "\n",
            "318. Title: Channel Vision Transformers: An Image Is Worth 1 x 16 x 16 Words\n",
            "   Abstract: Neural networks trained by gradient descent (GD) have exhibited a number of\n",
            "surprising generalization behaviors. First, they can achieve a perfect fit to\n",
            "noisy training data and still generalize near-optimally, showing that\n",
            "overfitting can sometimes be benign. Second, they can undergo a period of\n",
            "classical, harmful overfitting -- achieving a perfect fit to training data with\n",
            "near-random performance on test data -- before transitioning (\"grokking\") to\n",
            "near-optimal generalization later in training. In this work, we show that both\n",
            "of these phenomena provably occur in two-layer ReLU networks trained by GD on\n",
            "XOR cluster data where a constant fraction of the training labels are flipped.\n",
            "In this setting, we show that after the first step of GD, the network achieves\n",
            "100% training accuracy, perfectly fitting the noisy labels in the training\n",
            "data, but achieves near-random test accuracy. At a later training step, the\n",
            "network achieves near-optimal test accuracy while still fitting the random\n",
            "labels in the training data, exhibiting a \"grokking\" phenomenon. This provides\n",
            "the first theoretical result of benign overfitting in neural network\n",
            "classification when the data distribution is not linearly separable. Our proofs\n",
            "rely on analyzing the feature learning process under GD, which reveals that the\n",
            "network implements a non-generalizable linear classifier after one step and\n",
            "gradually learns generalizable features in later steps.\n",
            "\n",
            "319. Title: Test-Time Training on Nearest Neighbors for Large Language Models\n",
            "   Abstract: Randomized Smoothing (RS) has been proven a promising method for endowing an\n",
            "arbitrary image classifier with certified robustness. However, the substantial\n",
            "uncertainty inherent in the high-dimensional isotropic Gaussian noise imposes\n",
            "the curse of dimensionality on RS. Specifically, the upper bound of ${\\ell_2}$\n",
            "certified robustness radius provided by RS exhibits a diminishing trend with\n",
            "the expansion of the input dimension $d$, proportionally decreasing at a rate\n",
            "of $1/\\sqrt{d}$. This paper explores the feasibility of providing ${\\ell_2}$\n",
            "certified robustness for high-dimensional input through the utilization of dual\n",
            "smoothing in the lower-dimensional space. The proposed Dual Randomized\n",
            "Smoothing (DRS) down-samples the input image into two sub-images and smooths\n",
            "the two sub-images in lower dimensions. Theoretically, we prove that DRS\n",
            "guarantees a tight ${\\ell_2}$ certified robustness radius for the original\n",
            "input and reveal that DRS attains a superior upper bound on the ${\\ell_2}$\n",
            "robustness radius, which decreases proportionally at a rate of $(1/\\sqrt m +\n",
            "1/\\sqrt n )$ with $m+n=d$. Extensive experiments demonstrate the\n",
            "generalizability and effectiveness of DRS, which exhibits a notable capability\n",
            "to integrate with established methodologies, yielding substantial improvements\n",
            "in both accuracy and ${\\ell_2}$ certified robustness baselines of RS on the\n",
            "CIFAR-10 and ImageNet datasets. Code is available at\n",
            "https://github.com/xiasong0501/DRS.\n",
            "\n",
            "320. Title: Synergistic Patch Pruning for Vision Transformer: Unifying Intra- & Inter-Layer Patch Importance\n",
            "   Abstract: Virtual sensing techniques allow for inferring signals at new unmonitored\n",
            "locations by exploiting spatio-temporal measurements coming from physical\n",
            "sensors at different locations. However, as the sensor coverage becomes sparse\n",
            "due to costs or other constraints, physical proximity cannot be used to support\n",
            "interpolation. In this paper, we overcome this challenge by leveraging\n",
            "dependencies between the target variable and a set of correlated variables\n",
            "(covariates) that can frequently be associated with each location of interest.\n",
            "From this viewpoint, covariates provide partial observability, and the problem\n",
            "consists of inferring values for unobserved channels by exploiting observations\n",
            "at other locations to learn how such variables can correlate. We introduce a\n",
            "novel graph-based methodology to exploit such relationships and design a graph\n",
            "deep learning architecture, named GgNet, implementing the framework. The\n",
            "proposed approach relies on propagating information over a nested graph\n",
            "structure that is used to learn dependencies between variables as well as\n",
            "locations. GgNet is extensively evaluated under different virtual sensing\n",
            "scenarios, demonstrating higher reconstruction accuracy compared to the\n",
            "state-of-the-art.\n",
            "\n",
            "321. Title: AutoCast++: Enhancing World Event Prediction with Zero-shot Ranking-based Context Retrieval\n",
            "   Abstract: Video stabilization technique is essential for most hand-held captured videos\n",
            "due to high-frequency shakes. Several 2D-, 2.5D- and 3D-based stabilization\n",
            "techniques are well studied, but to our knowledge, no solutions based on deep\n",
            "neural networks had been proposed. The reason for this is mostly the shortage\n",
            "of training data, as well as the challenge of modeling the problem using neural\n",
            "networks. In this paper, we solve the video stabilization problem using a\n",
            "convolutional neural network (ConvNet). Instead of dealing with offline\n",
            "holistic camera path smoothing based on feature matching, we focus on\n",
            "low-latency real-time camera path smoothing without explicitly representing the\n",
            "camera path. Our network, called StabNet, learns a transformation for each\n",
            "input unsteady frame progressively along the time-line, while creating a more\n",
            "stable latent camera path. To train the network, we create a dataset of\n",
            "synchronized steady/unsteady video pairs via a well designed hand-held\n",
            "hardware. Experimental results shows that the proposed online method (without\n",
            "using future frames) performs comparatively to traditional offline video\n",
            "stabilization methods, while running about 30 times faster. Further, the\n",
            "proposed StabNet is able to handle night-time and blurry videos, where existing\n",
            "methods fail in robust feature matching.\n",
            "\n",
            "322. Title: Neural Atoms: Propagating Long-range Interaction in Molecular Graphs through Efficient Communication Channel\n",
            "   Abstract: Graph Neural Networks (GNNs) have been widely adopted for drug discovery with\n",
            "molecular graphs. Nevertheless, current GNNs mainly excel in leveraging\n",
            "short-range interactions (SRI) but struggle to capture long-range interactions\n",
            "(LRI), both of which are crucial for determining molecular properties. To\n",
            "tackle this issue, we propose a method to abstract the collective information\n",
            "of atomic groups into a few $\\textit{Neural Atoms}$ by implicitly projecting\n",
            "the atoms of a molecular. Specifically, we explicitly exchange the information\n",
            "among neural atoms and project them back to the atoms' representations as an\n",
            "enhancement. With this mechanism, neural atoms establish the communication\n",
            "channels among distant nodes, effectively reducing the interaction scope of\n",
            "arbitrary node pairs into a single hop. To provide an inspection of our method\n",
            "from a physical perspective, we reveal its connection to the traditional LRI\n",
            "calculation method, Ewald Summation. The Neural Atom can enhance GNNs to\n",
            "capture LRI by approximating the potential LRI of the molecular. We conduct\n",
            "extensive experiments on four long-range graph benchmarks, covering graph-level\n",
            "and link-level tasks on molecular graphs. We achieve up to a 27.32% and 38.27%\n",
            "improvement in the 2D and 3D scenarios, respectively. Empirically, our method\n",
            "can be equipped with an arbitrary GNN to help capture LRI. Code and datasets\n",
            "are publicly available in https://github.com/tmlr-group/NeuralAtom.\n",
            "\n",
            "323. Title: Fast Updating Truncated SVD for Representation Learning with Sparse Matrices\n",
            "   Abstract: There has been tremendous success in the field of graph neural networks\n",
            "(GNNs) as a result of the development of the message-passing (MP) layer, which\n",
            "updates the representation of a node by combining it with its neighbors to\n",
            "address variable-size and unordered graphs. Despite the fruitful progress of MP\n",
            "GNNs, their performance can suffer from over-smoothing, when node\n",
            "representations become too similar and even indistinguishable from one another.\n",
            "Furthermore, it has been reported that intrinsic graph structures are smoothed\n",
            "out as the GNN layer increases. Inspired by the edge-preserving bilateral\n",
            "filters used in image processing, we propose a new, adaptable, and powerful MP\n",
            "framework to prevent over-smoothing. Our bilateral-MP estimates a pairwise\n",
            "modular gradient by utilizing the class information of nodes, and further\n",
            "preserves the global graph structure by using the gradient when the aggregating\n",
            "function is applied. Our proposed scheme can be generalized to all ordinary MP\n",
            "GNNs. Experiments on five medium-size benchmark datasets using four\n",
            "state-of-the-art MP GNNs indicate that the bilateral-MP improves performance by\n",
            "alleviating over-smoothing. By inspecting quantitative measurements, we\n",
            "additionally validate the effectiveness of the proposed mechanism in preventing\n",
            "the over-smoothing issue.\n",
            "\n",
            "324. Title: Boosting Graph Anomaly Detection with Adaptive Message Passing\n",
            "   Abstract: Social network alignment has been an important research problem for social\n",
            "network analysis in recent years. With the identified shared users across\n",
            "networks, it will provide researchers with the opportunity to achieve a more\n",
            "comprehensive understanding of users' social activities both within and across\n",
            "networks. Social network alignment is a very difficult problem. Besides the\n",
            "challenges introduced by the network heterogeneity, the network alignment\n",
            "problem can be reduced to a combinatorial optimization problem with an\n",
            "extremely large search space. The learning effectiveness and efficiency of\n",
            "existing alignment models will be degraded significantly as the network size\n",
            "increases. In this paper, we will focus on studying the scalable heterogeneous\n",
            "social network alignment problem, and propose to address it with a novel\n",
            "two-stage network alignment model, namely \\textbf{S}calable\n",
            "\\textbf{H}eterogeneous \\textbf{N}etwork \\textbf{A}lignment (SHNA). Based on a\n",
            "group of intra- and inter-network meta diagrams, SHNA first partitions the\n",
            "social networks into a group of sub-networks synergistically. Via the partially\n",
            "known anchor links, SHNA will extract the partitioned sub-network\n",
            "correspondence relationships. Instead of aligning the complete input network,\n",
            "SHNA proposes to identify the anchor links between the matched sub-network\n",
            "pairs, while those between the unmatched sub-networks will be pruned to\n",
            "effectively shrink the search space. Extensive experiments have been done to\n",
            "compare SHNA with the state-of-the-art baseline methods on a real-world aligned\n",
            "social networks dataset. The experimental results have demonstrated both the\n",
            "effectiveness and efficiency of the {\\our} model in addressing the problem.\n",
            "\n",
            "325. Title: Internal Cross-layer Gradients for Extending Homogeneity to Heterogeneity in Federated Learning\n",
            "   Abstract: When operating in service of people, robots need to optimize rewards aligned\n",
            "with end-user preferences. Since robots will rely on raw perceptual inputs like\n",
            "RGB images, their rewards will inevitably use visual representations. Recently\n",
            "there has been excitement in using representations from pre-trained visual\n",
            "models, but key to making these work in robotics is fine-tuning, which is\n",
            "typically done via proxy tasks like dynamics prediction or enforcing temporal\n",
            "cycle-consistency. However, all these proxy tasks bypass the human's input on\n",
            "what matters to them, exacerbating spurious correlations and ultimately leading\n",
            "to robot behaviors that are misaligned with user preferences. In this work, we\n",
            "propose that robots should leverage human feedback to align their visual\n",
            "representations with the end-user and disentangle what matters for the task. We\n",
            "propose Representation-Aligned Preference-based Learning (RAPL), a method for\n",
            "solving the visual representation alignment problem and visual reward learning\n",
            "problem through the lens of preference-based learning and optimal transport.\n",
            "Across experiments in X-MAGICAL and in robotic manipulation, we find that\n",
            "RAPL's reward consistently generates preferred robot behaviors with high sample\n",
            "efficiency, and shows strong zero-shot generalization when the visual\n",
            "representation is learned from a different embodiment than the robot's.\n",
            "\n",
            "326. Title: Generative Learning for Financial Time Series with Irregular and Scale-Invariant Patterns\n",
            "   Abstract: Updating a truncated Singular Value Decomposition (SVD) is crucial in\n",
            "representation learning, especially when dealing with large-scale data matrices\n",
            "that continuously evolve in practical scenarios. Aligning SVD-based models with\n",
            "fast-paced updates becomes increasingly important. Existing methods for\n",
            "updating truncated SVDs employ Rayleigh-Ritz projection procedures, where\n",
            "projection matrices are augmented based on original singular vectors. However,\n",
            "these methods suffer from inefficiency due to the densification of the update\n",
            "matrix and the application of the projection to all singular vectors. To\n",
            "address these limitations, we introduce a novel method for dynamically\n",
            "approximating the truncated SVD of a sparse and temporally evolving matrix. Our\n",
            "approach leverages sparsity in the orthogonalization process of augmented\n",
            "matrices and utilizes an extended decomposition to independently store\n",
            "projections in the column space of singular vectors. Numerical experiments\n",
            "demonstrate a remarkable efficiency improvement of an order of magnitude\n",
            "compared to previous methods. Remarkably, this improvement is achieved while\n",
            "maintaining a comparable precision to existing approaches.\n",
            "\n",
            "327. Title: WizardLM: Empowering Large Pre-Trained Language Models to Follow Complex Instructions\n",
            "   Abstract: Idempotence is the stability of image codec to re-compression. At the first\n",
            "glance, it is unrelated to perceptual image compression. However, we find that\n",
            "theoretically: 1) Conditional generative model-based perceptual codec satisfies\n",
            "idempotence; 2) Unconditional generative model with idempotence constraint is\n",
            "equivalent to conditional generative codec. Based on this newfound equivalence,\n",
            "we propose a new paradigm of perceptual image codec by inverting unconditional\n",
            "generative model with idempotence constraints. Our codec is theoretically\n",
            "equivalent to conditional generative codec, and it does not require training\n",
            "new models. Instead, it only requires a pre-trained mean-square-error codec and\n",
            "unconditional generative model. Empirically, we show that our proposed approach\n",
            "outperforms state-of-the-art methods such as HiFiC and ILLM, in terms of\n",
            "Fr\\'echet Inception Distance (FID). The source code is provided in\n",
            "https://github.com/tongdaxu/Idempotence-and-Perceptual-Image-Compression.\n",
            "\n",
            "328. Title: Butterfly Effects of SGD Noise: Error Amplification in Behavior Cloning and Autoregression\n",
            "   Abstract: This paper captures irregularities in financial time series data,\n",
            "particularly stock prices, in the presence of COVID-19 shock. We conjectured\n",
            "that jumps and irregularities are embedded in stock data due to the pandemic\n",
            "shock, which brings forth irregular trends in the time series data. We put\n",
            "forward that efficient and robust forecasting methods are needed to predict\n",
            "stock closing prices in the presence of the pandemic shock. This piece of\n",
            "information is helpful to investors as far as confidence risk and return boost\n",
            "are concerned. Generative adversarial networks of a time series nature are used\n",
            "to provide new ways of modeling and learning the proper and suitable\n",
            "distribution for the financial time series data under complex setups. Ideally,\n",
            "these traditional models are liable to producing high forecasting errors, and\n",
            "they need to be more robust to capture dependency structures and other stylized\n",
            "facts like volatility in stock markets. The TimeGAN model is used, effectively\n",
            "dealing with this risk of poor forecasts. Using the DAX stock index from\n",
            "January 2010 to November 2022, we trained the LSTM, GRU, WGAN, and TimeGAN\n",
            "models as benchmarks and forecasting errors were noted, and our TimeGAN\n",
            "outperformed them all as indicated by a small forecasting error.\n",
            "\n",
            "329. Title: Error Feedback Reloaded: From Quadratic to Arithmetic Mean of Smoothness Constants\n",
            "   Abstract: Object detection is one of the major problems in computer vision, and has\n",
            "been extensively studied. Most of the existing detection works rely on\n",
            "labor-intensive supervision, such as ground truth bounding boxes of objects or\n",
            "at least image-level annotations. On the contrary, we propose an object\n",
            "detection method that does not require any form of human annotation on target\n",
            "tasks, by exploiting freely available web images. In order to facilitate\n",
            "effective knowledge transfer from web images, we introduce a multi-instance\n",
            "multi-label domain adaption learning framework with two key innovations. First\n",
            "of all, we propose an instance-level adversarial domain adaptation network with\n",
            "attention on foreground objects to transfer the object appearances from web\n",
            "domain to target domain. Second, to preserve the class-specific semantic\n",
            "structure of transferred object features, we propose a simultaneous transfer\n",
            "mechanism to transfer the supervision across domains through pseudo strong\n",
            "label generation. With our end-to-end framework that simultaneously learns a\n",
            "weakly supervised detector and transfers knowledge across domains, we achieved\n",
            "significant improvements over baseline methods on the benchmark datasets.\n",
            "\n",
            "330. Title: Transferring Labels to Solve Annotation Mismatches Across Object Detection Datasets\n",
            "   Abstract: Temporal abstraction and efficient planning pose significant challenges in\n",
            "offline reinforcement learning, mainly when dealing with domains that involve\n",
            "temporally extended tasks and delayed sparse rewards. Existing methods\n",
            "typically plan in the raw action space and can be inefficient and inflexible.\n",
            "Latent action spaces offer a more flexible paradigm, capturing only possible\n",
            "actions within the behavior policy support and decoupling the temporal\n",
            "structure between planning and modeling. However, current latent-action-based\n",
            "methods are limited to discrete spaces and require expensive planning. This\n",
            "paper presents a unified framework for continuous latent action space\n",
            "representation learning and planning by leveraging latent, score-based\n",
            "diffusion models. We establish the theoretical equivalence between planning in\n",
            "the latent action space and energy-guided sampling with a pretrained diffusion\n",
            "model and incorporate a novel sequence-level exact sampling method. Our\n",
            "proposed method, $\\texttt{LatentDiffuser}$, demonstrates competitive\n",
            "performance on low-dimensional locomotion control tasks and surpasses existing\n",
            "methods in higher-dimensional tasks.\n",
            "\n",
            "331. Title: Efficient Multi-agent Reinforcement Learning by Planning\n",
            "   Abstract: Error Feedback (EF) is a highly popular and immensely effective mechanism for\n",
            "fixing convergence issues which arise in distributed training methods (such as\n",
            "distributed GD or SGD) when these are enhanced with greedy communication\n",
            "compression techniques such as TopK. While EF was proposed almost a decade ago\n",
            "(Seide et al., 2014), and despite concentrated effort by the community to\n",
            "advance the theoretical understanding of this mechanism, there is still a lot\n",
            "to explore. In this work we study a modern form of error feedback called EF21\n",
            "(Richtarik et al., 2021) which offers the currently best-known theoretical\n",
            "guarantees, under the weakest assumptions, and also works well in practice. In\n",
            "particular, while the theoretical communication complexity of EF21 depends on\n",
            "the quadratic mean of certain smoothness parameters, we improve this dependence\n",
            "to their arithmetic mean, which is always smaller, and can be substantially\n",
            "smaller, especially in heterogeneous data regimes. We take the reader on a\n",
            "journey of our discovery process. Starting with the idea of applying EF21 to an\n",
            "equivalent reformulation of the underlying problem which (unfortunately)\n",
            "requires (often impractical) machine cloning, we continue to the discovery of a\n",
            "new weighted version of EF21 which can (fortunately) be executed without any\n",
            "cloning, and finally circle back to an improved analysis of the original EF21\n",
            "method. While this development applies to the simplest form of EF21, our\n",
            "approach naturally extends to more elaborate variants involving stochastic\n",
            "gradients and partial participation. Further, our technique improves the\n",
            "best-known theory of EF21 in the rare features regime (Richtarik et al., 2023).\n",
            "Finally, we validate our theoretical findings with suitable experiments.\n",
            "\n",
            "332. Title: LLCP: Learning Latent Causal Processes for Reasoning-based Video Question Answer\n",
            "   Abstract: We initiate the study of federated reinforcement learning under environmental\n",
            "heterogeneity by considering a policy evaluation problem. Our setup involves\n",
            "$N$ agents interacting with environments that share the same state and action\n",
            "space but differ in their reward functions and state transition kernels.\n",
            "Assuming agents can communicate via a central server, we ask: Does exchanging\n",
            "information expedite the process of evaluating a common policy? To answer this\n",
            "question, we provide the first comprehensive finite-time analysis of a\n",
            "federated temporal difference (TD) learning algorithm with linear function\n",
            "approximation, while accounting for Markovian sampling, heterogeneity in the\n",
            "agents' environments, and multiple local updates to save communication. Our\n",
            "analysis crucially relies on several novel ingredients: (i) deriving\n",
            "perturbation bounds on TD fixed points as a function of the heterogeneity in\n",
            "the agents' underlying Markov decision processes (MDPs); (ii) introducing a\n",
            "virtual MDP to closely approximate the dynamics of the federated TD algorithm;\n",
            "and (iii) using the virtual MDP to make explicit connections to federated\n",
            "optimization. Putting these pieces together, we rigorously prove that in a\n",
            "low-heterogeneity regime, exchanging model estimates leads to linear\n",
            "convergence speedups in the number of agents.\n",
            "\n",
            "333. Title: On Penalty Methods for Nonconvex Bilevel Optimization and First-Order Stochastic Approximation\n",
            "   Abstract: Lossless and near-lossless image compression is of paramount importance to\n",
            "professional users in many technical fields, such as medicine, remote sensing,\n",
            "precision engineering and scientific research. But despite rapidly growing\n",
            "research interests in learning-based image compression, no published method\n",
            "offers both lossless and near-lossless modes. In this paper, we propose a\n",
            "unified and powerful deep lossy plus residual (DLPR) coding framework for both\n",
            "lossless and near-lossless image compression. In the lossless mode, the DLPR\n",
            "coding system first performs lossy compression and then lossless coding of\n",
            "residuals. We solve the joint lossy and residual compression problem in the\n",
            "approach of VAEs, and add autoregressive context modeling of the residuals to\n",
            "enhance lossless compression performance. In the near-lossless mode, we\n",
            "quantize the original residuals to satisfy a given $\\ell_\\infty$ error bound,\n",
            "and propose a scalable near-lossless compression scheme that works for variable\n",
            "$\\ell_\\infty$ bounds instead of training multiple networks. To expedite the\n",
            "DLPR coding, we increase the degree of algorithm parallelization by a novel\n",
            "design of coding context, and accelerate the entropy coding with adaptive\n",
            "residual interval. Experimental results demonstrate that the DLPR coding system\n",
            "achieves both the state-of-the-art lossless and near-lossless image compression\n",
            "performance with competitive coding speed.\n",
            "\n",
            "334. Title: Idempotence and Perceptual Image Compression\n",
            "   Abstract: In this work, we study first-order algorithms for solving Bilevel\n",
            "Optimization (BO) where the objective functions are smooth but possibly\n",
            "nonconvex in both levels and the variables are restricted to closed convex\n",
            "sets. As a first step, we study the landscape of BO through the lens of penalty\n",
            "methods, in which the upper- and lower-level objectives are combined in a\n",
            "weighted sum with penalty parameter $\\sigma > 0$. In particular, we establish a\n",
            "strong connection between the penalty function and the hyper-objective by\n",
            "explicitly characterizing the conditions under which the values and derivatives\n",
            "of the two must be $O(\\sigma)$-close. A by-product of our analysis is the\n",
            "explicit formula for the gradient of hyper-objective when the lower-level\n",
            "problem has multiple solutions under minimal conditions, which could be of\n",
            "independent interest. Next, viewing the penalty formulation as\n",
            "$O(\\sigma)$-approximation of the original BO, we propose first-order algorithms\n",
            "that find an $\\epsilon$-stationary solution by optimizing the penalty\n",
            "formulation with $\\sigma = O(\\epsilon)$. When the perturbed lower-level problem\n",
            "uniformly satisfies the small-error proximal error-bound (EB) condition, we\n",
            "propose a first-order algorithm that converges to an $\\epsilon$-stationary\n",
            "point of the penalty function, using in total $O(\\epsilon^{-3})$ and\n",
            "$O(\\epsilon^{-7})$ accesses to first-order (stochastic) gradient oracles when\n",
            "the oracle is deterministic and oracles are noisy, respectively. Under an\n",
            "additional assumption on stochastic oracles, we show that the algorithm can be\n",
            "implemented in a fully {\\it single-loop} manner, i.e., with $O(1)$ samples per\n",
            "iteration, and achieves the improved oracle-complexity of $O(\\epsilon^{-3})$\n",
            "and $O(\\epsilon^{-5})$, respectively.\n",
            "\n",
            "335. Title: Optimistic Bayesian Optimization with Unknown Constraints\n",
            "   Abstract: Model merging aims to cheaply combine individual task-specific models into a\n",
            "single multitask model. In this work, we view past merging methods as\n",
            "leveraging different notions of a ''task parameter subspace'' in which models\n",
            "are matched before being merged. We connect the task parameter subspace of a\n",
            "given model to its loss landscape and formalize how this approach to model\n",
            "merging can be seen as solving a linear system of equations. While past work\n",
            "has generally been limited to linear systems that have a closed-form solution,\n",
            "we consider using the conjugate gradient method to find a solution. We show\n",
            "that using the conjugate gradient method can outperform closed-form solutions,\n",
            "enables merging via linear systems that are otherwise intractable to solve, and\n",
            "flexibly allows choosing from a wide variety of initializations and estimates\n",
            "for the ''task parameter subspace''. We ultimately demonstrate that our merging\n",
            "framework called ''Matching Models in their Task Parameter Subspace'' (MaTS)\n",
            "achieves state-of-the-art results in multitask and intermediate-task model\n",
            "merging. We release all of the code and checkpoints used in our work at\n",
            "https://github.com/r-three/mats.\n",
            "\n",
            "336. Title: Finite-State Autoregressive Entropy Coding for Efficient Learned Lossless Compression\n",
            "   Abstract: As LLMs become commonplace, machine-generated text has the potential to flood\n",
            "the internet with spam, social media bots, and valueless content. Watermarking\n",
            "is a simple and effective strategy for mitigating such harms by enabling the\n",
            "detection and documentation of LLM-generated text. Yet a crucial question\n",
            "remains: How reliable is watermarking in realistic settings in the wild? There,\n",
            "watermarked text may be modified to suit a user's needs, or entirely rewritten\n",
            "to avoid detection. We study the robustness of watermarked text after it is\n",
            "re-written by humans, paraphrased by a non-watermarked LLM, or mixed into a\n",
            "longer hand-written document. We find that watermarks remain detectable even\n",
            "after human and machine paraphrasing. While these attacks dilute the strength\n",
            "of the watermark, paraphrases are statistically likely to leak n-grams or even\n",
            "longer fragments of the original text, resulting in high-confidence detections\n",
            "when enough tokens are observed. For example, after strong human paraphrasing\n",
            "the watermark is detectable after observing 800 tokens on average, when setting\n",
            "a 1e-5 false positive rate. We also consider a range of new detection schemes\n",
            "that are sensitive to short spans of watermarked text embedded inside a large\n",
            "document, and we compare the robustness of watermarking to other kinds of\n",
            "detectors.\n",
            "\n",
            "337. Title: Model Merging by Uncertainty-Based Gradient Matching\n",
            "   Abstract: We consider online sequential decision problems where an agent must balance\n",
            "exploration and exploitation. We derive a set of Bayesian `optimistic' policies\n",
            "which, in the stochastic multi-armed bandit case, includes the Thompson\n",
            "sampling policy. We provide a new analysis showing that any algorithm producing\n",
            "policies in the optimistic set enjoys $\\tilde O(\\sqrt{AT})$ Bayesian regret for\n",
            "a problem with $A$ actions after $T$ rounds. We extend the regret analysis for\n",
            "optimistic policies to bilinear saddle-point problems which include zero-sum\n",
            "matrix games and constrained bandits as special cases. In this case we show\n",
            "that Thompson sampling can produce policies outside of the optimistic set and\n",
            "suffer linear regret in some instances. Finding a policy inside the optimistic\n",
            "set amounts to solving a convex optimization problem and we call the resulting\n",
            "algorithm `variational Bayesian optimistic sampling' (VBOS). The procedure\n",
            "works for any posteriors, \\ie, it does not require the posterior to have any\n",
            "special properties, such as log-concavity, unimodality, or smoothness. The\n",
            "variational view of the problem has many useful properties, including the\n",
            "ability to tune the exploration-exploitation tradeoff, add regularization,\n",
            "incorporate constraints, and linearly parameterize the policy.\n",
            "\n",
            "338. Title: SpaCE: The Spatial Confounding Environment\n",
            "   Abstract: Utilizing massive web-scale datasets has led to unprecedented performance\n",
            "gains in machine learning models, but also imposes outlandish compute\n",
            "requirements for their training. In order to improve training and data\n",
            "efficiency, we here push the limits of pruning large-scale multimodal datasets\n",
            "for training CLIP-style models. Today's most effective pruning method on\n",
            "ImageNet clusters data samples into separate concepts according to their\n",
            "embedding and prunes away the most prototypical samples. We scale this approach\n",
            "to LAION and improve it by noting that the pruning rate should be\n",
            "concept-specific and adapted to the complexity of the concept. Using a simple\n",
            "and intuitive complexity measure, we are able to reduce the training cost to a\n",
            "quarter of regular training. By filtering from the LAION dataset, we find that\n",
            "training on a smaller set of high-quality data can lead to higher performance\n",
            "with significantly lower training costs. More specifically, we are able to\n",
            "outperform the LAION-trained OpenCLIP-ViT-B32 model on ImageNet zero-shot\n",
            "accuracy by 1.1p.p. while only using 27.7% of the data and training compute.\n",
            "Despite a strong reduction in training cost, we also see improvements on\n",
            "ImageNet dist. shifts, retrieval tasks and VTAB. On the DataComp Medium\n",
            "benchmark, we achieve a new state-of-the-art\n",
            "Imagehttps://info.arxiv.org/help/prep#commentsNet zero-shot accuracy and a\n",
            "competitive average zero-shot accuracy on 38 evaluation tasks.\n",
            "\n",
            "339. Title: Plugin estimators for selective classification with out-of-distribution detection\n",
            "   Abstract: Real-world classifiers can benefit from the option of abstaining from\n",
            "predicting on samples where they have low confidence. Such abstention is\n",
            "particularly useful on samples which are close to the learned decision\n",
            "boundary, or which are outliers with respect to the training sample. These\n",
            "settings have been the subject of extensive but disjoint study in the selective\n",
            "classification (SC) and out-of-distribution (OOD) detection literature. Recent\n",
            "work on selective classification with OOD detection (SCOD) has argued for the\n",
            "unified study of these problems; however, the formal underpinnings of this\n",
            "problem are still nascent, and existing techniques are heuristic in nature. In\n",
            "this paper, we propose new plugin estimators for SCOD that are theoretically\n",
            "grounded, effective, and generalise existing approaches from the SC and OOD\n",
            "detection literature. In the course of our analysis, we formally explicate how\n",
            "na\\\"{i}ve use of existing SC and OOD detection baselines may be inadequate for\n",
            "SCOD. We empirically demonstrate that our approaches yields competitive SC and\n",
            "OOD detection performance compared to baselines from both literatures.\n",
            "\n",
            "340. Title: Boosting the Adversarial Robustness of Graph Neural Networks: An OOD Perspective\n",
            "   Abstract: This work studies training instabilities of behavior cloning with deep neural\n",
            "networks. We observe that minibatch SGD updates to the policy network during\n",
            "training result in sharp oscillations in long-horizon rewards, despite\n",
            "negligibly affecting the behavior cloning loss. We empirically disentangle the\n",
            "statistical and computational causes of these oscillations, and find them to\n",
            "stem from the chaotic propagation of minibatch SGD noise through unstable\n",
            "closed-loop dynamics. While SGD noise is benign in the single-step action\n",
            "prediction objective, it results in catastrophic error accumulation over long\n",
            "horizons, an effect we term gradient variance amplification (GVA). We show that\n",
            "many standard mitigation techniques do not alleviate GVA, but find an\n",
            "exponential moving average (EMA) of iterates to be surprisingly effective at\n",
            "doing so. We illustrate the generality of this phenomenon by showing the\n",
            "existence of GVA and its amelioration by EMA in both continuous control and\n",
            "autoregressive language generation. Finally, we provide theoretical vignettes\n",
            "that highlight the benefits of EMA in alleviating GVA and shed light on the\n",
            "extent to which classical convex models can help in understanding the benefits\n",
            "of iterate averaging in deep learning.\n",
            "\n",
            "341. Title: DreamCraft3D: Hierarchical 3D Generation with Bootstrapped Diffusion Prior\n",
            "   Abstract: In the context of adversarial robustness, a single model does not usually\n",
            "have enough power to defend against all possible adversarial attacks, and as a\n",
            "result, has sub-optimal robustness. Consequently, an emerging line of work has\n",
            "focused on learning an ensemble of neural networks to defend against\n",
            "adversarial attacks. In this work, we take a principled approach towards\n",
            "building robust ensembles. We view this problem from the perspective of\n",
            "margin-boosting and develop an algorithm for learning an ensemble with maximum\n",
            "margin. Through extensive empirical evaluation on benchmark datasets, we show\n",
            "that our algorithm not only outperforms existing ensembling techniques, but\n",
            "also large models trained in an end-to-end fashion. An important byproduct of\n",
            "our work is a margin-maximizing cross-entropy (MCE) loss, which is a better\n",
            "alternative to the standard cross-entropy (CE) loss. Empirically, we show that\n",
            "replacing the CE loss in state-of-the-art adversarial training techniques with\n",
            "our MCE loss leads to significant performance improvement.\n",
            "\n",
            "342. Title: On the Reliability of Watermarks for Large Language Models\n",
            "   Abstract: Federated learning (FL) inevitably confronts the challenge of system\n",
            "heterogeneity in practical scenarios. To enhance the capabilities of most\n",
            "model-homogeneous FL methods in handling system heterogeneity, we propose a\n",
            "training scheme that can extend their capabilities to cope with this challenge.\n",
            "In this paper, we commence our study with a detailed exploration of homogeneous\n",
            "and heterogeneous FL settings and discover three key observations: (1) a\n",
            "positive correlation between client performance and layer similarities, (2)\n",
            "higher similarities in the shallow layers in contrast to the deep layers, and\n",
            "(3) the smoother gradients distributions indicate the higher layer\n",
            "similarities. Building upon these observations, we propose InCo Aggregation\n",
            "that leverages internal cross-layer gradients, a mixture of gradients from\n",
            "shallow and deep layers within a server model, to augment the similarity in the\n",
            "deep layers without requiring additional communication between clients.\n",
            "Furthermore, our methods can be tailored to accommodate model-homogeneous FL\n",
            "methods such as FedAvg, FedProx, FedNova, Scaffold, and MOON, to expand their\n",
            "capabilities to handle the system heterogeneity. Copious experimental results\n",
            "validate the effectiveness of InCo Aggregation, spotlighting internal\n",
            "cross-layer gradients as a promising avenue to enhance the performance in\n",
            "heterogeneous FL.\n",
            "\n",
            "343. Title: Beyond Worst-case Attacks: Robust RL with Adaptive Defense via Non-dominated Policies\n",
            "   Abstract: When predictions support decisions they may influence the outcome they aim to\n",
            "predict. We call such predictions performative; the prediction influences the\n",
            "target. Performativity is a well-studied phenomenon in policy-making that has\n",
            "so far been neglected in supervised learning. When ignored, performativity\n",
            "surfaces as undesirable distribution shift, routinely addressed with\n",
            "retraining.\n",
            "  We develop a risk minimization framework for performative prediction bringing\n",
            "together concepts from statistics, game theory, and causality. A conceptual\n",
            "novelty is an equilibrium notion we call performative stability. Performative\n",
            "stability implies that the predictions are calibrated not against past\n",
            "outcomes, but against the future outcomes that manifest from acting on the\n",
            "prediction. Our main results are necessary and sufficient conditions for the\n",
            "convergence of retraining to a performatively stable point of nearly minimal\n",
            "loss.\n",
            "  In full generality, performative prediction strictly subsumes the setting\n",
            "known as strategic classification. We thus also give the first sufficient\n",
            "conditions for retraining to overcome strategic feedback effects.\n",
            "\n",
            "344. Title: The LLM Surgeon\n",
            "   Abstract: Image registration, the process of aligning two or more images, is the core\n",
            "technique of many (semi-)automatic medical image analysis tasks. Recent studies\n",
            "have shown that deep learning methods, notably convolutional neural networks\n",
            "(ConvNets), can be used for image registration. Thus far training of ConvNets\n",
            "for registration was supervised using predefined example registrations.\n",
            "However, obtaining example registrations is not trivial. To circumvent the need\n",
            "for predefined examples, and thereby to increase convenience of training\n",
            "ConvNets for image registration, we propose the Deep Learning Image\n",
            "Registration (DLIR) framework for \\textit{unsupervised} affine and deformable\n",
            "image registration. In the DLIR framework ConvNets are trained for image\n",
            "registration by exploiting image similarity analogous to conventional\n",
            "intensity-based image registration. After a ConvNet has been trained with the\n",
            "DLIR framework, it can be used to register pairs of unseen images in one shot.\n",
            "We propose flexible ConvNets designs for affine image registration and for\n",
            "deformable image registration. By stacking multiple of these ConvNets into a\n",
            "larger architecture, we are able to perform coarse-to-fine image registration.\n",
            "We show for registration of cardiac cine MRI and registration of chest CT that\n",
            "performance of the DLIR framework is comparable to conventional image\n",
            "registration while being several orders of magnitude faster.\n",
            "\n",
            "345. Title: Constrained Decoding for Cross-lingual Label Projection\n",
            "   Abstract: In light of the burgeoning success of reinforcement learning (RL) in diverse\n",
            "real-world applications, considerable focus has been directed towards ensuring\n",
            "RL policies are robust to adversarial attacks during test time. Current\n",
            "approaches largely revolve around solving a minimax problem to prepare for\n",
            "potential worst-case scenarios. While effective against strong attacks, these\n",
            "methods often compromise performance in the absence of attacks or the presence\n",
            "of only weak attacks. To address this, we study policy robustness under the\n",
            "well-accepted state-adversarial attack model, extending our focus beyond only\n",
            "worst-case attacks. We first formalize this task at test time as a regret\n",
            "minimization problem and establish its intrinsic hardness in achieving\n",
            "sublinear regret when the baseline policy is from a general continuous policy\n",
            "class, $\\Pi$. This finding prompts us to \\textit{refine} the baseline policy\n",
            "class $\\Pi$ prior to test time, aiming for efficient adaptation within a finite\n",
            "policy class $\\Tilde{\\Pi}$, which can resort to an adversarial bandit\n",
            "subroutine. In light of the importance of a small, finite $\\Tilde{\\Pi}$, we\n",
            "propose a novel training-time algorithm to iteratively discover\n",
            "\\textit{non-dominated policies}, forming a near-optimal and minimal\n",
            "$\\Tilde{\\Pi}$, thereby ensuring both robustness and test-time efficiency.\n",
            "Empirical validation on the Mujoco corroborates the superiority of our approach\n",
            "in terms of natural and robust performance, as well as adaptability to various\n",
            "attack scenarios.\n",
            "\n",
            "346. Title: Improving Domain Generalization with Domain Relations\n",
            "   Abstract: Robot-assisted surgical systems have demonstrated significant potential in\n",
            "enhancing surgical precision and minimizing human errors. However, existing\n",
            "systems cannot accommodate individual surgeons' unique preferences and\n",
            "requirements. Additionally, they primarily focus on general surgeries (e.g.,\n",
            "laparoscopy) and are unsuitable for highly precise microsurgeries, such as\n",
            "ophthalmic procedures. Thus, we propose an image-guided approach for\n",
            "surgeon-centered autonomous agents that can adapt to the individual surgeon's\n",
            "skill level and preferred surgical techniques during ophthalmic cataract\n",
            "surgery. Our approach trains reinforcement and imitation learning agents\n",
            "simultaneously using curriculum learning approaches guided by image data to\n",
            "perform all tasks of the incision phase of cataract surgery. By integrating the\n",
            "surgeon's actions and preferences into the training process, our approach\n",
            "enables the robot to implicitly learn and adapt to the individual surgeon's\n",
            "unique techniques through surgeon-in-the-loop demonstrations. This results in a\n",
            "more intuitive and personalized surgical experience for the surgeon while\n",
            "ensuring consistent performance for the autonomous robotic apprentice. We\n",
            "define and evaluate the effectiveness of our approach in a simulated\n",
            "environment using our proposed metrics and highlight the trade-off between a\n",
            "generic agent and a surgeon-centered adapted agent. Finally, our approach has\n",
            "the potential to extend to other ophthalmic and microsurgical procedures,\n",
            "opening the door to a new generation of surgeon-in-the-loop autonomous surgical\n",
            "robots. We provide an open-source simulation framework for future development\n",
            "and reproducibility at\n",
            "https://github.com/amrgomaaelhady/CataractAdaptSurgRobot.\n",
            "\n",
            "347. Title: Bounding the Expected Robustness of Graph Neural Networks Subject to Node Feature Attacks\n",
            "   Abstract: Zero-shot cross-lingual transfer utilizing multilingual LLMs has become a\n",
            "popular learning paradigm for low-resource languages with no labeled training\n",
            "data. However, for NLP tasks that involve fine-grained predictions on words and\n",
            "phrases, the performance of zero-shot cross-lingual transfer learning lags far\n",
            "behind supervised fine-tuning methods. Therefore, it is common to exploit\n",
            "translation and label projection to further improve the performance by (1)\n",
            "translating training data that is available in a high-resource language (e.g.,\n",
            "English) together with the gold labels into low-resource languages, and/or (2)\n",
            "translating test data in low-resource languages to a high-source language to\n",
            "run inference on, then projecting the predicted span-level labels back onto the\n",
            "original test data. However, state-of-the-art marker-based label projection\n",
            "methods suffer from translation quality degradation due to the extra label\n",
            "markers injected in the input to the translation model. In this work, we\n",
            "explore a new direction that leverages constrained decoding for label\n",
            "projection to overcome the aforementioned issues. Our new method not only can\n",
            "preserve the quality of translated texts but also has the versatility of being\n",
            "applicable to both translating training and translating test data strategies.\n",
            "This versatility is crucial as our experiments reveal that translating test\n",
            "data can lead to a considerable boost in performance compared to translating\n",
            "only training data. We evaluate on two cross-lingual transfer tasks, namely\n",
            "Named Entity Recognition and Event Argument Extraction, spanning 20 languages.\n",
            "The results demonstrate that our approach outperforms the state-of-the-art\n",
            "marker-based method by a large margin and also shows better performance than\n",
            "other label projection methods that rely on external word alignment.\n",
            "\n",
            "348. Title: MetaCoCo: A New Few-Shot Classification Benchmark with Spurious Correlation\n",
            "   Abstract: We propose Scalable Mechanistic Neural Network (S-MNN), an enhanced neural\n",
            "network framework designed for scientific machine learning applications\n",
            "involving long temporal sequences. By reformulating the original Mechanistic\n",
            "Neural Network (MNN) (Pervez et al., 2024), we reduce the computational time\n",
            "and space complexities from cubic and quadratic with respect to the sequence\n",
            "length, respectively, to linear. This significant improvement enables efficient\n",
            "modeling of long-term dynamics without sacrificing accuracy or\n",
            "interpretability. Extensive experiments demonstrate that S-MNN matches the\n",
            "original MNN in precision while substantially reducing computational resources.\n",
            "Consequently, S-MNN can drop-in replace the original MNN in applications,\n",
            "providing a practical and efficient tool for integrating mechanistic\n",
            "bottlenecks into neural network models of complex dynamical systems.\n",
            "\n",
            "349. Title: Scalable Monotonic Neural Networks\n",
            "   Abstract: We present a unified probabilistic formulation for diffusion-based image\n",
            "editing, where a latent variable is edited in a task-specific manner and\n",
            "generally deviates from the corresponding marginal distribution induced by the\n",
            "original stochastic or ordinary differential equation (SDE or ODE). Instead, it\n",
            "defines a corresponding SDE or ODE for editing. In the formulation, we prove\n",
            "that the Kullback-Leibler divergence between the marginal distributions of the\n",
            "two SDEs gradually decreases while that for the ODEs remains as the time\n",
            "approaches zero, which shows the promise of SDE in image editing. Inspired by\n",
            "it, we provide the SDE counterparts for widely used ODE baselines in various\n",
            "tasks including inpainting and image-to-image translation, where SDE shows a\n",
            "consistent and substantial improvement. Moreover, we propose SDE-Drag -- a\n",
            "simple yet effective method built upon the SDE formulation for point-based\n",
            "content dragging. We build a challenging benchmark (termed DragBench) with\n",
            "open-set natural, art, and AI-generated images for evaluation. A user study on\n",
            "DragBench indicates that SDE-Drag significantly outperforms our ODE baseline,\n",
            "existing diffusion-based methods, and the renowned DragGAN. Our results\n",
            "demonstrate the superiority and versatility of SDE in image editing and push\n",
            "the boundary of diffusion-based editing methods.\n",
            "\n",
            "350. Title: Robustifying State-space Models for Long Sequences via Approximate Diagonalization\n",
            "   Abstract: Distribution shift presents a significant challenge in machine learning,\n",
            "where models often underperform during the test stage when faced with a\n",
            "different distribution than the one they were trained on. This paper focuses on\n",
            "domain shifts, which occur when the model is applied to new domains that are\n",
            "different from the ones it was trained on, and propose a new approach called\n",
            "D$^3$G. Unlike previous methods that aim to learn a single model that is domain\n",
            "invariant, D$^3$G leverages domain similarities based on domain metadata to\n",
            "learn domain-specific models. Concretely, D$^3$G learns a set of\n",
            "training-domain-specific functions during the training stage and reweights them\n",
            "based on domain relations during the test stage. These domain relations can be\n",
            "directly obtained and learned from domain metadata. Under mild assumptions, we\n",
            "theoretically prove that using domain relations to reweight\n",
            "training-domain-specific functions achieves stronger out-of-domain\n",
            "generalization compared to the conventional averaging approach. Empirically, we\n",
            "evaluate the effectiveness of D$^3$G using real-world datasets for tasks such\n",
            "as temperature regression, land use classification, and molecule-protein\n",
            "binding affinity prediction. Our results show that D$^3$G consistently\n",
            "outperforms state-of-the-art methods.\n",
            "\n",
            "351. Title: BayesPrompt: Prompting Large-Scale Pre-Trained Language Models on Few-shot Inference via Debiased Domain Abstraction\n",
            "   Abstract: Graph Neural Networks (GNNs) have demonstrated state-of-the-art performance\n",
            "in various graph representation learning tasks. Recently, studies revealed\n",
            "their vulnerability to adversarial attacks. In this work, we theoretically\n",
            "define the concept of expected robustness in the context of attributed graphs\n",
            "and relate it to the classical definition of adversarial robustness in the\n",
            "graph representation learning literature. Our definition allows us to derive an\n",
            "upper bound of the expected robustness of Graph Convolutional Networks (GCNs)\n",
            "and Graph Isomorphism Networks subject to node feature attacks. Building on\n",
            "these findings, we connect the expected robustness of GNNs to the\n",
            "orthonormality of their weight matrices and consequently propose an\n",
            "attack-independent, more robust variant of the GCN, called the Graph\n",
            "Convolutional Orthonormal Robust Networks (GCORNs). We further introduce a\n",
            "probabilistic method to estimate the expected robustness, which allows us to\n",
            "evaluate the effectiveness of GCORN on several real-world datasets.\n",
            "Experimental experiments showed that GCORN outperforms available defense\n",
            "methods. Our code is publicly available at:\n",
            "\\href{https://github.com/Sennadir/GCORN}{https://github.com/Sennadir/GCORN}.\n",
            "\n",
            "352. Title: DeepSPF: Spherical SO(3)-Equivariant Patches for Scan-to-CAD Estimation\n",
            "   Abstract: State-space models (SSMs) have recently emerged as a framework for learning\n",
            "long-range sequence tasks. An example is the structured state-space sequence\n",
            "(S4) layer, which uses the diagonal-plus-low-rank structure of the HiPPO\n",
            "initialization framework. However, the complicated structure of the S4 layer\n",
            "poses challenges; and, in an effort to address these challenges, models such as\n",
            "S4D and S5 have considered a purely diagonal structure. This choice simplifies\n",
            "the implementation, improves computational efficiency, and allows channel\n",
            "communication. However, diagonalizing the HiPPO framework is itself an\n",
            "ill-posed problem. In this paper, we propose a general solution for this and\n",
            "related ill-posed diagonalization problems in machine learning. We introduce a\n",
            "generic, backward-stable \"perturb-then-diagonalize\" (PTD) methodology, which is\n",
            "based on the pseudospectral theory of non-normal operators, and which may be\n",
            "interpreted as the approximate diagonalization of the non-normal matrices\n",
            "defining SSMs. Based on this, we introduce the S4-PTD and S5-PTD models.\n",
            "Through theoretical analysis of the transfer functions of different\n",
            "initialization schemes, we demonstrate that the S4-PTD/S5-PTD initialization\n",
            "strongly converges to the HiPPO framework, while the S4D/S5 initialization only\n",
            "achieves weak convergences. As a result, our new models show resilience to\n",
            "Fourier-mode noise-perturbed inputs, a crucial property not achieved by the\n",
            "S4D/S5 models. In addition to improved robustness, our S5-PTD model averages\n",
            "87.6% accuracy on the Long-Range Arena benchmark, demonstrating that the PTD\n",
            "methodology helps to improve the accuracy of deep learning models.\n",
            "\n",
            "353. Title: Group Preference Optimization: Few-Shot Alignment of Large Language Models\n",
            "   Abstract: Geometric deep learning extends deep learning to incorporate information\n",
            "about the geometry and topology data, especially in complex domains like\n",
            "graphs. Despite the popularity of message passing in this field, it has\n",
            "limitations such as the need for graph rewiring, ambiguity in interpreting\n",
            "data, and over-smoothing. In this paper, we take a different approach, focusing\n",
            "on leveraging geometric information from simplicial complexes embedded in\n",
            "$\\mathbb{R}^n$ using node coordinates. We use differential k-forms in\n",
            "\\mathbb{R}^n to create representations of simplices, offering interpretability\n",
            "and geometric consistency without message passing. This approach also enables\n",
            "us to apply differential geometry tools and achieve universal approximation.\n",
            "Our method is efficient, versatile, and applicable to various input complexes,\n",
            "including graphs, simplicial complexes, and cell complexes. It outperforms\n",
            "existing message passing neural networks in harnessing information from\n",
            "geometrical graphs with node features serving as coordinates.\n",
            "\n",
            "354. Title: Causal Fairness under Unobserved Confounding: A Neural Sensitivity Framework\n",
            "   Abstract: Many applications of large language models (LLMs), ranging from chatbots to\n",
            "creative writing, require nuanced subjective judgments that can differ\n",
            "significantly across different groups. Existing alignment algorithms can be\n",
            "expensive to align for each group, requiring prohibitive amounts of\n",
            "group-specific preference data and computation for real-world use cases. We\n",
            "introduce Group Preference Optimization (GPO), an alignment framework that\n",
            "steers language models to preferences of individual groups in a few-shot\n",
            "manner. In GPO, we augment the base LLM with an independent transformer module\n",
            "trained to predict the preferences of a group for the LLM generations. For\n",
            "few-shot learning, we parameterize this module as an in-context autoregressive\n",
            "transformer and train it via meta-learning on several groups. We empirically\n",
            "validate the efficacy of GPO through rigorous evaluations using LLMs with\n",
            "varied sizes on three human opinion adaptation tasks. These tasks involve\n",
            "adapting to the preferences of US demographic groups, global countries, and\n",
            "individual users. Our results demonstrate that GPO not only aligns models more\n",
            "accurately but also requires fewer group-specific preferences, and less\n",
            "training and inference computing resources, outperforming existing strategies\n",
            "such as in-context steering and fine-tuning methods.\n",
            "\n",
            "355. Title: Point2SSM: Learning Morphological Variations of Anatomies from Point Clouds\n",
            "   Abstract: Fairness for machine learning predictions is widely required in practice for\n",
            "legal, ethical, and societal reasons. Existing work typically focuses on\n",
            "settings without unobserved confounding, even though unobserved confounding can\n",
            "lead to severe violations of causal fairness and, thus, unfair predictions. In\n",
            "this work, we analyze the sensitivity of causal fairness to unobserved\n",
            "confounding. Our contributions are three-fold. First, we derive bounds for\n",
            "causal fairness metrics under different sources of unobserved confounding. This\n",
            "enables practitioners to examine the sensitivity of their machine learning\n",
            "models to unobserved confounding in fairness-critical applications. Second, we\n",
            "propose a novel neural framework for learning fair predictions, which allows us\n",
            "to offer worst-case guarantees of the extent to which causal fairness can be\n",
            "violated due to unobserved confounding. Third, we demonstrate the effectiveness\n",
            "of our framework in a series of experiments, including a real-world case study\n",
            "about predicting prison sentences. To the best of our knowledge, ours is the\n",
            "first work to study causal fairness under unobserved confounding. To this end,\n",
            "our work is of direct practical value as a refutation strategy to ensure the\n",
            "fairness of predictions in high-stakes applications.\n",
            "\n",
            "356. Title: Decomposed Diffusion Sampler for Accelerating Large-Scale Inverse Problems\n",
            "   Abstract: Synthesizing realistic animations of humans, animals, and even imaginary\n",
            "creatures, has long been a goal for artists and computer graphics\n",
            "professionals. Compared to the imaging domain, which is rich with large\n",
            "available datasets, the number of data instances for the motion domain is\n",
            "limited, particularly for the animation of animals and exotic creatures (e.g.,\n",
            "dragons), which have unique skeletons and motion patterns. In this work, we\n",
            "present a Single Motion Diffusion Model, dubbed SinMDM, a model designed to\n",
            "learn the internal motifs of a single motion sequence with arbitrary topology\n",
            "and synthesize motions of arbitrary length that are faithful to them. We\n",
            "harness the power of diffusion models and present a denoising network\n",
            "explicitly designed for the task of learning from a single input motion. SinMDM\n",
            "is designed to be a lightweight architecture, which avoids overfitting by using\n",
            "a shallow network with local attention layers that narrow the receptive field\n",
            "and encourage motion diversity. SinMDM can be applied in various contexts,\n",
            "including spatial and temporal in-betweening, motion expansion, style transfer,\n",
            "and crowd animation. Our results show that SinMDM outperforms existing methods\n",
            "both in quality and time-space efficiency. Moreover, while current approaches\n",
            "require additional training for different applications, our work facilitates\n",
            "these applications at inference time. Our code and trained models are available\n",
            "at https://sinmdm.github.io/SinMDM-page.\n",
            "\n",
            "357. Title: Adversarial Imitation Learning via Boosting\n",
            "   Abstract: Adversarial imitation learning (AIL) has stood out as a dominant framework\n",
            "across various imitation learning (IL) applications, with Discriminator Actor\n",
            "Critic (DAC) (Kostrikov et al.,, 2019) demonstrating the effectiveness of\n",
            "off-policy learning algorithms in improving sample efficiency and scalability\n",
            "to higher-dimensional observations. Despite DAC's empirical success, the\n",
            "original AIL objective is on-policy and DAC's ad-hoc application of off-policy\n",
            "training does not guarantee successful imitation (Kostrikov et al., 2019;\n",
            "2020). Follow-up work such as ValueDICE (Kostrikov et al., 2020) tackles this\n",
            "issue by deriving a fully off-policy AIL objective. Instead in this work, we\n",
            "develop a novel and principled AIL algorithm via the framework of boosting.\n",
            "Like boosting, our new algorithm, AILBoost, maintains an ensemble of properly\n",
            "weighted weak learners (i.e., policies) and trains a discriminator that\n",
            "witnesses the maximum discrepancy between the distributions of the ensemble and\n",
            "the expert policy. We maintain a weighted replay buffer to represent the\n",
            "state-action distribution induced by the ensemble, allowing us to train\n",
            "discriminators using the entire data collected so far. In the weighted replay\n",
            "buffer, the contribution of the data from older policies are properly\n",
            "discounted with the weight computed based on the boosting framework.\n",
            "Empirically, we evaluate our algorithm on both controller state-based and\n",
            "pixel-based environments from the DeepMind Control Suite. AILBoost outperforms\n",
            "DAC on both types of environments, demonstrating the benefit of properly\n",
            "weighting replay buffer data for off-policy training. On state-based\n",
            "environments, DAC outperforms ValueDICE and IQ-Learn (Gary et al., 2021),\n",
            "achieving competitive performance with as little as one expert trajectory.\n",
            "\n",
            "358. Title: Parallelizing non-linear sequential models over the sequence length\n",
            "   Abstract: Krylov subspace, which is generated by multiplying a given vector by the\n",
            "matrix of a linear transformation and its successive powers, has been\n",
            "extensively studied in classical optimization literature to design algorithms\n",
            "that converge quickly for large linear inverse problems. For example, the\n",
            "conjugate gradient method (CG), one of the most popular Krylov subspace\n",
            "methods, is based on the idea of minimizing the residual error in the Krylov\n",
            "subspace. However, with the recent advancement of high-performance diffusion\n",
            "solvers for inverse problems, it is not clear how classical wisdom can be\n",
            "synergistically combined with modern diffusion models. In this study, we\n",
            "propose a novel and efficient diffusion sampling strategy that synergistically\n",
            "combines the diffusion sampling and Krylov subspace methods. Specifically, we\n",
            "prove that if the tangent space at a denoised sample by Tweedie's formula forms\n",
            "a Krylov subspace, then the CG initialized with the denoised data ensures the\n",
            "data consistency update to remain in the tangent space. This negates the need\n",
            "to compute the manifold-constrained gradient (MCG), leading to a more efficient\n",
            "diffusion sampling method. Our method is applicable regardless of the\n",
            "parametrization and setting (i.e., VE, VP). Notably, we achieve\n",
            "state-of-the-art reconstruction quality on challenging real-world medical\n",
            "inverse imaging problems, including multi-coil MRI reconstruction and 3D CT\n",
            "reconstruction. Moreover, our proposed method achieves more than 80 times\n",
            "faster inference time than the previous state-of-the-art method. Code is\n",
            "available at https://github.com/HJ-harry/DDS\n",
            "\n",
            "359. Title: Learning from Aggregate responses: Instance Level versus Bag Level Loss Functions\n",
            "   Abstract: Sequential models, such as Recurrent Neural Networks and Neural Ordinary\n",
            "Differential Equations, have long suffered from slow training due to their\n",
            "inherent sequential nature. For many years this bottleneck has persisted, as\n",
            "many thought sequential models could not be parallelized. We challenge this\n",
            "long-held belief with our parallel algorithm that accelerates GPU evaluation of\n",
            "sequential models by up to 3 orders of magnitude faster without compromising\n",
            "output accuracy. The algorithm does not need any special structure in the\n",
            "sequential models' architecture, making it applicable to a wide range of\n",
            "architectures. Using our method, training sequential models can be more than 10\n",
            "times faster than the common sequential method without any meaningful\n",
            "difference in the training results. Leveraging this accelerated training, we\n",
            "discovered the efficacy of the Gated Recurrent Unit in a long time series\n",
            "classification problem with 17k time samples. By overcoming the training\n",
            "bottleneck, our work serves as the first step to unlock the potential of\n",
            "non-linear sequential models for long sequence problems.\n",
            "\n",
            "360. Title: VBH-GNN: Variational Bayesian Heterogeneous Graph Neural Networks for Cross-subject Emotion Recognition\n",
            "   Abstract: Due to the rise of privacy concerns, in many practical applications the\n",
            "training data is aggregated before being shared with the learner, in order to\n",
            "protect privacy of users' sensitive responses. In an aggregate learning\n",
            "framework, the dataset is grouped into bags of samples, where each bag is\n",
            "available only with an aggregate response, providing a summary of individuals'\n",
            "responses in that bag. In this paper, we study two natural loss functions for\n",
            "learning from aggregate responses: bag-level loss and the instance-level loss.\n",
            "In the former, the model is learnt by minimizing a loss between aggregate\n",
            "responses and aggregate model predictions, while in the latter the model aims\n",
            "to fit individual predictions to the aggregate responses. In this work, we show\n",
            "that the instance-level loss can be perceived as a regularized form of the\n",
            "bag-level loss. This observation lets us compare the two approaches with\n",
            "respect to bias and variance of the resulting estimators, and introduce a novel\n",
            "interpolating estimator which combines the two approaches. For linear\n",
            "regression tasks, we provide a precise characterization of the risk of the\n",
            "interpolating estimator in an asymptotic regime where the size of the training\n",
            "set grows in proportion to the features dimension. Our analysis allows us to\n",
            "theoretically understand the effect of different factors, such as bag size on\n",
            "the model prediction risk. In addition, we propose a mechanism for\n",
            "differentially private learning from aggregate responses and derive the optimal\n",
            "bag size in terms of prediction risk-privacy trade-off. We also carry out\n",
            "thorough experiments to corroborate our theory and show the efficacy of the\n",
            "interpolating estimator.\n",
            "\n",
            "361. Title: Vision-by-Language for Training-Free Compositional Image Retrieval\n",
            "   Abstract: Fairness, especially group fairness, is an important consideration in the\n",
            "context of machine learning systems. The most commonly adopted group\n",
            "fairness-enhancing techniques are in-processing methods that rely on a mixture\n",
            "of a fairness objective (e.g., demographic parity) and a task-specific\n",
            "objective (e.g., cross-entropy) during the training process. However, when data\n",
            "arrives in an online fashion -- one instance at a time -- optimizing such\n",
            "fairness objectives poses several challenges. In particular, group fairness\n",
            "objectives are defined using expectations of predictions across different\n",
            "demographic groups. In the online setting, where the algorithm has access to a\n",
            "single instance at a time, estimating the group fairness objective requires\n",
            "additional storage and significantly more computation (e.g., forward/backward\n",
            "passes) than the task-specific objective at every time step. In this paper, we\n",
            "propose Aranyani, an ensemble of oblique decision trees, to make fair decisions\n",
            "in online settings. The hierarchical tree structure of Aranyani enables\n",
            "parameter isolation and allows us to efficiently compute the fairness gradients\n",
            "using aggregate statistics of previous decisions, eliminating the need for\n",
            "additional storage and forward/backward passes. We also present an efficient\n",
            "framework to train Aranyani and theoretically analyze several of its\n",
            "properties. We conduct empirical evaluations on 5 publicly available benchmarks\n",
            "(including vision and language datasets) to show that Aranyani achieves a\n",
            "better accuracy-fairness trade-off compared to baseline approaches.\n",
            "\n",
            "362. Title: Flow to Better: Offline Preference-based Reinforcement Learning via Preferred Trajectory Generation\n",
            "   Abstract: In the real world, where information is abundant and diverse across different\n",
            "modalities, understanding and utilizing various data types to improve retrieval\n",
            "systems is a key focus of research. Multimodal composite retrieval integrates\n",
            "diverse modalities such as text, image and audio, etc. to provide more\n",
            "accurate, personalized, and contextually relevant results. To facilitate a\n",
            "deeper understanding of this promising direction, this survey explores\n",
            "multimodal composite editing and retrieval in depth, covering image-text\n",
            "composite editing, image-text composite retrieval, and other multimodal\n",
            "composite retrieval. In this survey, we systematically organize the application\n",
            "scenarios, methods, benchmarks, experiments, and future directions. Multimodal\n",
            "learning is a hot topic in large model era, and have also witnessed some\n",
            "surveys in multimodal learning and vision-language models with transformers\n",
            "published in the PAMI journal. To the best of our knowledge, this survey is the\n",
            "first comprehensive review of the literature on multimodal composite retrieval,\n",
            "which is a timely complement of multimodal fusion to existing reviews. To help\n",
            "readers' quickly track this field, we build the project page for this survey,\n",
            "which can be found at\n",
            "https://github.com/fuxianghuang1/Multimodal-Composite-Editing-and-Retrieval.\n",
            "\n",
            "363. Title: Revisiting Data Augmentation in Deep Reinforcement Learning\n",
            "   Abstract: We study the named entity recognition (NER) problem under the extremely weak\n",
            "supervision (XWS) setting, where only one example entity per type is given in a\n",
            "context-free way. While one can see that XWS is lighter than one-shot in terms\n",
            "of the amount of supervision, we propose a novel method X-NER that can\n",
            "outperform the state-of-the-art one-shot NER methods. We first mine entity\n",
            "spans that are similar to the example entities from an unlabelled training\n",
            "corpus. Instead of utilizing entity span representations from language models,\n",
            "we find it more effective to compare the context distributions before and after\n",
            "the span is replaced by the entity example. We then leverage the top-ranked\n",
            "spans as pseudo-labels to train an NER tagger. Extensive experiments and\n",
            "analyses on 4 NER datasets show the superior end-to-end NER performance of\n",
            "X-NER, outperforming the state-of-the-art few-shot methods with 1-shot\n",
            "supervision and ChatGPT annotations significantly. Finally, our X-NER possesses\n",
            "several notable properties, such as inheriting the cross-lingual abilities of\n",
            "the underlying language models.\n",
            "\n",
            "364. Title: Prompt Gradient Projection for Continual Learning\n",
            "   Abstract: Generative models based on dynamical transport of measure, such as diffusion\n",
            "models, flow matching models, and stochastic interpolants, learn an ordinary or\n",
            "stochastic differential equation whose trajectories push initial conditions\n",
            "from a known base distribution onto the target. While training is cheap,\n",
            "samples are generated via simulation, which is more expensive than one-step\n",
            "models like GANs. To close this gap, we introduce flow map matching -- an\n",
            "algorithm that learns the two-time flow map of an underlying ordinary\n",
            "differential equation. The approach leads to an efficient few-step generative\n",
            "model whose step count can be chosen a-posteriori to smoothly trade off\n",
            "accuracy for computational expense. Leveraging the stochastic interpolant\n",
            "framework, we introduce losses for both direct training of flow maps and\n",
            "distillation from pre-trained (or otherwise known) velocity fields.\n",
            "Theoretically, we show that our approach unifies many existing few-step\n",
            "generative models, including consistency models, consistency trajectory models,\n",
            "progressive distillation, and neural operator approaches, which can be obtained\n",
            "as particular cases of our formalism. With experiments on CIFAR-10 and ImageNet\n",
            "32x32, we show that flow map matching leads to high-quality samples with\n",
            "significantly reduced sampling cost compared to diffusion or stochastic\n",
            "interpolant methods.\n",
            "\n",
            "365. Title: AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors\n",
            "   Abstract: The development of person search techniques has been greatly promoted in\n",
            "recent years for its superior practicality and challenging goals. Despite their\n",
            "significant progress, existing person search models still lack the ability to\n",
            "continually learn from increaseing real-world data and adaptively process input\n",
            "from different domains. To this end, this work introduces the continual person\n",
            "search task that sequentially learns on multiple domains and then performs\n",
            "person search on all seen domains. This requires balancing the stability and\n",
            "plasticity of the model to continually learn new knowledge without catastrophic\n",
            "forgetting. For this, we propose a Prompt-based Continual Person Search (PoPS)\n",
            "model in this paper. First, we design a compositional person search transformer\n",
            "to construct an effective pre-trained transformer without exhaustive\n",
            "pre-training from scratch on large-scale person search data. This serves as the\n",
            "fundamental for prompt-based continual learning. On top of that, we design a\n",
            "domain incremental prompt pool with a diverse attribute matching module. For\n",
            "each domain, we independently learn a set of prompts to encode the\n",
            "domain-oriented knowledge. Meanwhile, we jointly learn a group of diverse\n",
            "attribute projections and prototype embeddings to capture discriminative domain\n",
            "attributes. By matching an input image with the learned attributes across\n",
            "domains, the learned prompts can be properly selected for model inference.\n",
            "Extensive experiments are conducted to validate the proposed method for\n",
            "continual person search. The source code is available at\n",
            "https://github.com/PatrickZad/PoPS.\n",
            "\n",
            "366. Title: ResFields: Residual Neural Fields for Spatiotemporal Signals\n",
            "   Abstract: Various data augmentation techniques have been recently proposed in\n",
            "image-based deep reinforcement learning (DRL). Although they empirically\n",
            "demonstrate the effectiveness of data augmentation for improving sample\n",
            "efficiency or generalization, which technique should be preferred is not always\n",
            "clear. To tackle this question, we analyze existing methods to better\n",
            "understand them and to uncover how they are connected. Notably, by expressing\n",
            "the variance of the Q-targets and that of the empirical actor/critic losses of\n",
            "these methods, we can analyze the effects of their different components and\n",
            "compare them. We furthermore formulate an explanation about how these methods\n",
            "may be affected by choosing different data augmentation transformations in\n",
            "calculating the target Q-values. This analysis suggests recommendations on how\n",
            "to exploit data augmentation in a more principled way. In addition, we include\n",
            "a regularization term called tangent prop, previously proposed in computer\n",
            "vision, but whose adaptation to DRL is novel to the best of our knowledge. We\n",
            "evaluate our proposition and validate our analysis in several domains. Compared\n",
            "to different relevant baselines, we demonstrate that it achieves\n",
            "state-of-the-art performance in most environments and shows higher sample\n",
            "efficiency and better generalization ability in some complex environments.\n",
            "\n",
            "367. Title: Parametric Augmentation for Time Series Contrastive Learning\n",
            "   Abstract: Despite the recent advances in large-scale diffusion models, little progress\n",
            "has been made on the layout-to-image (L2I) synthesis task. Current L2I models\n",
            "either suffer from poor editability via text or weak alignment between the\n",
            "generated image and the input layout. This limits their usability in practice.\n",
            "To mitigate this, we propose to integrate adversarial supervision into the\n",
            "conventional training pipeline of L2I diffusion models (ALDM). Specifically, we\n",
            "employ a segmentation-based discriminator which provides explicit feedback to\n",
            "the diffusion generator on the pixel-level alignment between the denoised image\n",
            "and the input layout. To encourage consistent adherence to the input layout\n",
            "over the sampling steps, we further introduce the multistep unrolling strategy.\n",
            "Instead of looking at a single timestep, we unroll a few steps recursively to\n",
            "imitate the inference process, and ask the discriminator to assess the\n",
            "alignment of denoised images with the layout over a certain time window. Our\n",
            "experiments show that ALDM enables layout faithfulness of the generated images,\n",
            "while allowing broad editability via text prompts. Moreover, we showcase its\n",
            "usefulness for practical applications: by synthesizing target distribution\n",
            "samples via text control, we improve domain generalization of semantic\n",
            "segmentation models by a large margin (~12 mIoU points).\n",
            "\n",
            "368. Title: Adversarial Supervision Makes Layout-to-Image Diffusion Models Thrive\n",
            "   Abstract: Modern techniques like contrastive learning have been effectively used in\n",
            "many areas, including computer vision, natural language processing, and\n",
            "graph-structured data. Creating positive examples that assist the model in\n",
            "learning robust and discriminative representations is a crucial stage in\n",
            "contrastive learning approaches. Usually, preset human intuition directs the\n",
            "selection of relevant data augmentations. Due to patterns that are easily\n",
            "recognized by humans, this rule of thumb works well in the vision and language\n",
            "domains. However, it is impractical to visually inspect the temporal structures\n",
            "in time series. The diversity of time series augmentations at both the dataset\n",
            "and instance levels makes it difficult to choose meaningful augmentations on\n",
            "the fly. In this study, we address this gap by analyzing time series data\n",
            "augmentation using information theory and summarizing the most commonly adopted\n",
            "augmentations in a unified format. We then propose a contrastive learning\n",
            "framework with parametric augmentation, AutoTCL, which can be adaptively\n",
            "employed to support time series representation learning. The proposed approach\n",
            "is encoder-agnostic, allowing it to be seamlessly integrated with different\n",
            "backbone encoders. Experiments on univariate forecasting tasks demonstrate the\n",
            "highly competitive results of our method, with an average 6.5\\% reduction in\n",
            "MSE and 4.7\\% in MAE over the leading baselines. In classification tasks,\n",
            "AutoTCL achieves a $1.2\\%$ increase in average accuracy.\n",
            "\n",
            "369. Title: Minimax optimality of convolutional neural networks for infinite dimensional input-output problems and separation from kernel methods\n",
            "   Abstract: In any given machine learning problem, there may be many models that could\n",
            "explain the data almost equally well. However, most learning algorithms return\n",
            "only one of these models, leaving practitioners with no practical way to\n",
            "explore alternative models that might have desirable properties beyond what\n",
            "could be expressed within a loss function. The Rashomon set is the set of these\n",
            "all almost-optimal models. Rashomon sets can be extremely complicated,\n",
            "particularly for highly nonlinear function classes that allow complex\n",
            "interaction terms, such as decision trees. We provide the first technique for\n",
            "completely enumerating the Rashomon set for sparse decision trees; in fact, our\n",
            "work provides the first complete enumeration of any Rashomon set for a\n",
            "non-trivial problem with a highly nonlinear discrete function class. This\n",
            "allows the user an unprecedented level of control over model choice among all\n",
            "models that are approximately equally good. We represent the Rashomon set in a\n",
            "specialized data structure that supports efficient querying and sampling. We\n",
            "show three applications of the Rashomon set: 1) it can be used to study\n",
            "variable importance for the set of almost-optimal trees (as opposed to a single\n",
            "tree), 2) the Rashomon set for accuracy enables enumeration of the Rashomon\n",
            "sets for balanced accuracy and F1-score, and 3) the Rashomon set for a full\n",
            "dataset can be used to produce Rashomon sets constructed with only subsets of\n",
            "the data set. Thus, we are able to examine Rashomon sets across problems with a\n",
            "new lens, enabling users to choose models rather than be at the mercy of an\n",
            "algorithm that produces only a single model.\n",
            "\n",
            "370. Title: Invariance-based Learning of Latent Dynamics\n",
            "   Abstract: We develop an approach for estimating models described via conditional moment\n",
            "restrictions, with a prototypical application being non-parametric instrumental\n",
            "variable regression. We introduce a min-max criterion function, under which the\n",
            "estimation problem can be thought of as solving a zero-sum game between a\n",
            "modeler who is optimizing over the hypothesis space of the target model and an\n",
            "adversary who identifies violating moments over a test function space. We\n",
            "analyze the statistical estimation rate of the resulting estimator for\n",
            "arbitrary hypothesis spaces, with respect to an appropriate analogue of the\n",
            "mean squared error metric, for ill-posed inverse problems. We show that when\n",
            "the minimax criterion is regularized with a second moment penalty on the test\n",
            "function and the test function space is sufficiently rich, then the estimation\n",
            "rate scales with the critical radius of the hypothesis and test function\n",
            "spaces, a quantity which typically gives tight fast rates. Our main result\n",
            "follows from a novel localized Rademacher analysis of statistical learning\n",
            "problems defined via minimax objectives. We provide applications of our main\n",
            "results for several hypothesis spaces used in practice such as: reproducing\n",
            "kernel Hilbert spaces, high dimensional sparse linear functions, spaces defined\n",
            "via shape constraints, ensemble estimators such as random forests, and neural\n",
            "networks. For each of these applications we provide computationally efficient\n",
            "optimization methods for solving the corresponding minimax problem (e.g.\n",
            "stochastic first-order heuristics for neural networks). In several\n",
            "applications, we show how our modified mean squared error rate, combined with\n",
            "conditions that bound the ill-posedness of the inverse problem, lead to mean\n",
            "squared error rates. We conclude with an extensive experimental analysis of the\n",
            "proposed methods.\n",
            "\n",
            "371. Title: Open-ended VQA benchmarking of Vision-Language models by exploiting Classification datasets and their semantic hierarchy\n",
            "   Abstract: Estimating camera poses is a fundamental task for 3D reconstruction and\n",
            "remains challenging given sparsely sampled views (<10). In contrast to existing\n",
            "approaches that pursue top-down prediction of global parametrizations of camera\n",
            "extrinsics, we propose a distributed representation of camera pose that treats\n",
            "a camera as a bundle of rays. This representation allows for a tight coupling\n",
            "with spatial image features improving pose precision. We observe that this\n",
            "representation is naturally suited for set-level transformers and develop a\n",
            "regression-based approach that maps image patches to corresponding rays. To\n",
            "capture the inherent uncertainties in sparse-view pose inference, we adapt this\n",
            "approach to learn a denoising diffusion model which allows us to sample\n",
            "plausible modes while improving performance. Our proposed methods, both\n",
            "regression- and diffusion-based, demonstrate state-of-the-art performance on\n",
            "camera pose estimation on CO3D while generalizing to unseen object categories\n",
            "and in-the-wild captures.\n",
            "\n",
            "372. Title: Is This the Subspace You Are Looking for? An Interpretability Illusion for Subspace Activation Patching\n",
            "   Abstract: Latent dynamical models are commonly used to learn the distribution of a\n",
            "latent dynamical process that represents a sequence of noisy data samples.\n",
            "However, producing samples from such models with high fidelity is challenging\n",
            "due to the complexity and variability of latent and observation dynamics.\n",
            "Recent advances in diffusion-based generative models, such as DDPM and NCSN,\n",
            "have shown promising alternatives to state-of-the-art latent generative models,\n",
            "such as Neural ODEs, RNNs, and Normalizing flow networks, for generating\n",
            "high-quality sequential samples from a prior distribution. However, their\n",
            "application in modeling sequential data with latent dynamical models is yet to\n",
            "be explored. Here, we propose a novel latent variable model named latent\n",
            "dynamical implicit diffusion processes (LDIDPs), which utilizes implicit\n",
            "diffusion processes to sample from dynamical latent processes and generate\n",
            "sequential observation samples accordingly. We tested LDIDPs on synthetic and\n",
            "simulated neural decoding problems. We demonstrate that LDIDPs can accurately\n",
            "learn the dynamics over latent dimensions. Furthermore, the implicit sampling\n",
            "method allows for the computationally efficient generation of high-quality\n",
            "sequential data samples from the latent and observation spaces.\n",
            "\n",
            "373. Title: Understanding Convergence and Generalization in Federated Learning through Feature Learning Theory\n",
            "   Abstract: The evaluation of text-generative vision-language models is a challenging yet\n",
            "crucial endeavor. By addressing the limitations of existing Visual Question\n",
            "Answering (VQA) benchmarks and proposing innovative evaluation methodologies,\n",
            "our research seeks to advance our understanding of these models' capabilities.\n",
            "We propose a novel VQA benchmark based on well-known visual classification\n",
            "datasets which allows a granular evaluation of text-generative vision-language\n",
            "models and their comparison with discriminative vision-language models. To\n",
            "improve the assessment of coarse answers on fine-grained classification tasks,\n",
            "we suggest using the semantic hierarchy of the label space to ask automatically\n",
            "generated follow-up questions about the ground-truth category. Finally, we\n",
            "compare traditional NLP and LLM-based metrics for the problem of evaluating\n",
            "model predictions given ground-truth answers. We perform a human evaluation\n",
            "study upon which we base our decision on the final metric. We apply our\n",
            "benchmark to a suite of vision-language models and show a detailed comparison\n",
            "of their abilities on object, action, and attribute classification. Our\n",
            "contributions aim to lay the foundation for more precise and meaningful\n",
            "assessments, facilitating targeted progress in the exciting field of\n",
            "vision-language modeling.\n",
            "\n",
            "374. Title: L2MAC: Large Language Model Automatic Computer for Extensive Code Generation\n",
            "   Abstract: The rapidly developing Large Vision Language Models (LVLMs) have shown\n",
            "notable capabilities on a range of multi-modal tasks, but still face the\n",
            "hallucination phenomena where the generated texts do not align with the given\n",
            "contexts, significantly restricting the usages of LVLMs. Most previous work\n",
            "detects and mitigates hallucination at the coarse-grained level or requires\n",
            "expensive annotation (e.g., labeling by proprietary models or human experts).\n",
            "To address these issues, we propose detecting and mitigating hallucinations in\n",
            "LVLMs via fine-grained AI feedback. The basic idea is that we generate a\n",
            "small-size sentence-level hallucination annotation dataset by proprietary\n",
            "models, whereby we train a hallucination detection model which can perform\n",
            "sentence-level hallucination detection, covering primary hallucination types\n",
            "(i.e., object, attribute, and relationship). Then, we propose a\n",
            "detect-then-rewrite pipeline to automatically construct preference dataset for\n",
            "training hallucination mitigating model. Furthermore, we propose\n",
            "differentiating the severity of hallucinations, and introducing a Hallucination\n",
            "Severity-Aware Direct Preference Optimization (HSA-DPO) for mitigating\n",
            "hallucination in LVLMs by incorporating the severity of hallucinations into\n",
            "preference learning. Extensive experiments demonstrate the effectiveness of our\n",
            "method.\n",
            "\n",
            "375. Title: Self-contradictory Hallucinations of Large Language Models: Evaluation, Detection and Mitigation\n",
            "   Abstract: Batch normalization (BN) is a technique to normalize activations in\n",
            "intermediate layers of deep neural networks. Its tendency to improve accuracy\n",
            "and speed up training have established BN as a favorite technique in deep\n",
            "learning. Yet, despite its enormous success, there remains little consensus on\n",
            "the exact reason and mechanism behind these improvements. In this paper we take\n",
            "a step towards a better understanding of BN, following an empirical approach.\n",
            "We conduct several experiments, and show that BN primarily enables training\n",
            "with larger learning rates, which is the cause for faster convergence and\n",
            "better generalization. For networks without BN we demonstrate how large\n",
            "gradient updates can result in diverging loss and activations growing\n",
            "uncontrollably with network depth, which limits possible learning rates. BN\n",
            "avoids this problem by constantly correcting activations to be zero-mean and of\n",
            "unit standard deviation, which enables larger gradient steps, yields faster\n",
            "convergence and may help bypass sharp local minima. We further show various\n",
            "ways in which gradients and activations of deep unnormalized networks are\n",
            "ill-behaved. We contrast our results against recent findings in random matrix\n",
            "theory, shedding new light on classical initialization schemes and their\n",
            "consequences.\n",
            "\n",
            "376. Title: Building Cooperative Embodied Agents Modularly with Large Language Models\n",
            "   Abstract: Mechanistic interpretability aims to understand model behaviors in terms of\n",
            "specific, interpretable features, often hypothesized to manifest as\n",
            "low-dimensional subspaces of activations. Specifically, recent studies have\n",
            "explored subspace interventions (such as activation patching) as a way to\n",
            "simultaneously manipulate model behavior and attribute the features behind it\n",
            "to given subspaces.\n",
            "  In this work, we demonstrate that these two aims diverge, potentially leading\n",
            "to an illusory sense of interpretability. Counterintuitively, even if a\n",
            "subspace intervention makes the model's output behave as if the value of a\n",
            "feature was changed, this effect may be achieved by activating a dormant\n",
            "parallel pathway leveraging another subspace that is causally disconnected from\n",
            "model outputs. We demonstrate this phenomenon in a distilled mathematical\n",
            "example, in two real-world domains (the indirect object identification task and\n",
            "factual recall), and present evidence for its prevalence in practice. In the\n",
            "context of factual recall, we further show a link to rank-1 fact editing,\n",
            "providing a mechanistic explanation for previous work observing an\n",
            "inconsistency between fact editing performance and fact localization.\n",
            "  However, this does not imply that activation patching of subspaces is\n",
            "intrinsically unfit for interpretability. To contextualize our findings, we\n",
            "also show what a success case looks like in a task (indirect object\n",
            "identification) where prior manual circuit analysis informs an understanding of\n",
            "the location of a feature. We explore the additional evidence needed to argue\n",
            "that a patched subspace is faithful.\n",
            "\n",
            "377. Title: An Emulator for Fine-tuning Large Language Models using Small Language Models\n",
            "   Abstract: In this work, we address challenging multi-agent cooperation problems with\n",
            "decentralized control, raw sensory observations, costly communication, and\n",
            "multi-objective tasks instantiated in various embodied environments. While\n",
            "previous research either presupposes a cost-free communication channel or\n",
            "relies on a centralized controller with shared observations, we harness the\n",
            "commonsense knowledge, reasoning ability, language comprehension, and text\n",
            "generation prowess of LLMs and seamlessly incorporate them into a\n",
            "cognitive-inspired modular framework that integrates with perception, memory,\n",
            "and execution. Thus building a Cooperative Embodied Language Agent CoELA, who\n",
            "can plan, communicate, and cooperate with others to accomplish long-horizon\n",
            "tasks efficiently. Our experiments on C-WAH and TDW-MAT demonstrate that CoELA\n",
            "driven by GPT-4 can surpass strong planning-based methods and exhibit emergent\n",
            "effective communication. Though current Open LMs like LLAMA-2 still\n",
            "underperform, we fine-tune a CoELA with data collected with our agents and show\n",
            "how they can achieve promising performance. We also conducted a user study for\n",
            "human-agent interaction and discovered that CoELA communicating in natural\n",
            "language can earn more trust and cooperate more effectively with humans. Our\n",
            "research underscores the potential of LLMs for future research in multi-agent\n",
            "cooperation. Videos can be found on the project website\n",
            "https://vis-www.cs.umass.edu/Co-LLM-Agents/.\n",
            "\n",
            "378. Title: ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving\n",
            "   Abstract: We need to look at our shoelaces as we first learn to tie them but having\n",
            "mastered this skill, can do it from touch alone. We call this phenomenon\n",
            "\"sensory scaffolding\": observation streams that are not needed by a master\n",
            "might yet aid a novice learner. We consider such sensory scaffolding setups for\n",
            "training artificial agents. For example, a robot arm may need to be deployed\n",
            "with just a low-cost, robust, general-purpose camera; yet its performance may\n",
            "improve by having privileged training-time-only access to informative albeit\n",
            "expensive and unwieldy motion capture rigs or fragile tactile sensors. For\n",
            "these settings, we propose \"Scaffolder\", a reinforcement learning approach\n",
            "which effectively exploits privileged sensing in critics, world models, reward\n",
            "estimators, and other such auxiliary components that are only used at training\n",
            "time, to improve the target policy. For evaluating sensory scaffolding agents,\n",
            "we design a new \"S3\" suite of ten diverse simulated robotic tasks that explore\n",
            "a wide range of practical sensor setups. Agents must use privileged camera\n",
            "sensing to train blind hurdlers, privileged active visual perception to help\n",
            "robot arms overcome visual occlusions, privileged touch sensors to train robot\n",
            "hands, and more. Scaffolder easily outperforms relevant prior baselines and\n",
            "frequently performs comparably even to policies that have test-time access to\n",
            "the privileged sensors. Website: https://penn-pal-lab.github.io/scaffolder/\n",
            "\n",
            "379. Title: Privileged Sensing Scaffolds Reinforcement Learning\n",
            "   Abstract: The field of imbalanced self-supervised learning, especially in the context\n",
            "of tabular data, has not been extensively studied. Existing research has\n",
            "predominantly focused on image datasets. This paper aims to fill this gap by\n",
            "examining the specific challenges posed by data imbalance in self-supervised\n",
            "learning in the domain of tabular data, with a primary focus on autoencoders.\n",
            "Autoencoders are widely employed for learning and constructing a new\n",
            "representation of a dataset, particularly for dimensionality reduction. They\n",
            "are also often used for generative model learning, as seen in variational\n",
            "autoencoders. When dealing with mixed tabular data, qualitative variables are\n",
            "often encoded using a one-hot encoder with a standard loss function (MSE or\n",
            "Cross Entropy). In this paper, we analyze the drawbacks of this approach,\n",
            "especially when categorical variables are imbalanced. We propose a novel metric\n",
            "to balance learning: a Multi-Supervised Balanced MSE. This approach reduces the\n",
            "reconstruction error by balancing the influence of variables. Finally, we\n",
            "empirically demonstrate that this new metric, compared to the standard MSE: i)\n",
            "outperforms when the dataset is imbalanced, especially when the learning\n",
            "process is insufficient, and ii) provides similar results in the opposite case.\n",
            "\n",
            "380. Title: Self-supervised Representation Learning from Random Data Projectors\n",
            "   Abstract: Widely used language models (LMs) are typically built by scaling up a\n",
            "two-stage training pipeline: a pre-training stage that uses a very large,\n",
            "diverse dataset of text and a fine-tuning (sometimes, 'alignment') stage that\n",
            "uses targeted examples or other specifications of desired behaviors. While it\n",
            "has been hypothesized that knowledge and skills come from pre-training, and\n",
            "fine-tuning mostly filters this knowledge and skillset, this intuition has not\n",
            "been extensively tested. To aid in doing so, we introduce a novel technique for\n",
            "decoupling the knowledge and skills gained in these two stages, enabling a\n",
            "direct answer to the question, \"What would happen if we combined the knowledge\n",
            "learned by a large model during pre-training with the knowledge learned by a\n",
            "small model during fine-tuning (or vice versa)?\" Using an RL-based framework\n",
            "derived from recent developments in learning from human preferences, we\n",
            "introduce emulated fine-tuning (EFT), a principled and practical method for\n",
            "sampling from a distribution that approximates (or 'emulates') the result of\n",
            "pre-training and fine-tuning at different scales. Our experiments with EFT show\n",
            "that scaling up fine-tuning tends to improve helpfulness, while scaling up\n",
            "pre-training tends to improve factuality. Beyond decoupling scale, we show that\n",
            "EFT enables test-time adjustment of competing behavioral traits like\n",
            "helpfulness and harmlessness without additional training. Finally, a special\n",
            "case of emulated fine-tuning, which we call LM up-scaling, avoids\n",
            "resource-intensive fine-tuning of large pre-trained models by ensembling them\n",
            "with small fine-tuned models, essentially emulating the result of fine-tuning\n",
            "the large pre-trained model. Up-scaling consistently improves helpfulness and\n",
            "factuality of instruction-following models in the Llama, Llama-2, and Falcon\n",
            "families, without additional hyperparameters or training.\n",
            "\n",
            "381. Title: Interpreting Robustness Proofs of Deep Neural Networks\n",
            "   Abstract: Developing autonomous agents that can interact with changing environments is\n",
            "an open challenge in machine learning. Robustness is particularly important in\n",
            "these settings as agents are often fit offline on expert demonstrations but\n",
            "deployed online where they must generalize to the closed feedback loop within\n",
            "the environment. In this work, we explore the application of recurrent neural\n",
            "networks to tasks of this nature and understand how a parameterization of their\n",
            "recurrent connectivity influences robustness in closed-loop settings.\n",
            "Specifically, we represent the recurrent connectivity as a function of rank and\n",
            "sparsity and show both theoretically and empirically that modulating these two\n",
            "variables has desirable effects on network dynamics. The proposed low-rank,\n",
            "sparse connectivity induces an interpretable prior on the network that proves\n",
            "to be most amenable for a class of models known as closed-form continuous-time\n",
            "neural networks (CfCs). We find that CfCs with fewer parameters can outperform\n",
            "their full-rank, fully-connected counterparts in the online setting under\n",
            "distribution shift. This yields memory-efficient and robust agents while\n",
            "opening a new perspective on how we can modulate network dynamics through\n",
            "connectivity.\n",
            "\n",
            "382. Title: Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning\n",
            "   Abstract: In recent years numerous methods have been developed to formally verify the\n",
            "robustness of deep neural networks (DNNs). Though the proposed techniques are\n",
            "effective in providing mathematical guarantees about the DNNs behavior, it is\n",
            "not clear whether the proofs generated by these methods are\n",
            "human-interpretable. In this paper, we bridge this gap by developing new\n",
            "concepts, algorithms, and representations to generate human understandable\n",
            "interpretations of the proofs. Leveraging the proposed method, we show that the\n",
            "robustness proofs of standard DNNs rely on spurious input features, while the\n",
            "proofs of DNNs trained to be provably robust filter out even the semantically\n",
            "meaningful features. The proofs for the DNNs combining adversarial and provably\n",
            "robust training are the most effective at selectively filtering out spurious\n",
            "features as well as relying on human-understandable input features.\n",
            "\n",
            "383. Title: Illusory Attacks: Information-theoretic detectability matters in adversarial attacks\n",
            "   Abstract: Can a pre-trained generator be adapted to the hybrid of multiple target\n",
            "domains and generate images with integrated attributes of them? In this work,\n",
            "we introduce a new task -- Few-shot Hybrid Domain Adaptation (HDA). Given a\n",
            "source generator and several target domains, HDA aims to acquire an adapted\n",
            "generator that preserves the integrated attributes of all target domains,\n",
            "without overriding the source domain's characteristics. Compared with Domain\n",
            "Adaptation (DA), HDA offers greater flexibility and versatility to adapt\n",
            "generators to more composite and expansive domains. Simultaneously, HDA also\n",
            "presents more challenges than DA as we have access only to images from\n",
            "individual target domains and lack authentic images from the hybrid domain. To\n",
            "address this issue, we introduce a discriminator-free framework that directly\n",
            "encodes different domains' images into well-separable subspaces. To achieve\n",
            "HDA, we propose a novel directional subspace loss comprised of a distance loss\n",
            "and a direction loss. Concretely, the distance loss blends the attributes of\n",
            "all target domains by reducing the distances from generated images to all\n",
            "target subspaces. The direction loss preserves the characteristics from the\n",
            "source domain by guiding the adaptation along the perpendicular to subspaces.\n",
            "Experiments show that our method can obtain numerous domain-specific attributes\n",
            "in a single adapted generator, which surpasses the baseline methods in semantic\n",
            "similarity, image fidelity, and cross-domain consistency.\n",
            "\n",
            "384. Title: Sparse Autoencoders Find Highly Interpretable Features in Language Models\n",
            "   Abstract: Given a set of $K$ probability densities, we consider the multimarginal\n",
            "generative modeling problem of learning a joint distribution that recovers\n",
            "these densities as marginals. The structure of this joint distribution should\n",
            "identify multi-way correspondences among the prescribed marginals. We formalize\n",
            "an approach to this task within a generalization of the stochastic interpolant\n",
            "framework, leading to efficient learning algorithms built upon dynamical\n",
            "transport of measure. Our generative models are defined by velocity and score\n",
            "fields that can be characterized as the minimizers of simple quadratic\n",
            "objectives, and they are defined on a simplex that generalizes the time\n",
            "variable in the usual dynamical transport framework. The resulting transport on\n",
            "the simplex is influenced by all marginals, and we show that multi-way\n",
            "correspondences can be extracted. The identification of such correspondences\n",
            "has applications to style transfer, algorithmic fairness, and data\n",
            "decorruption. In addition, the multimarginal perspective enables an efficient\n",
            "algorithm for reducing the dynamical transport cost in the ordinary\n",
            "two-marginal setting. We demonstrate these capacities with several numerical\n",
            "examples.\n",
            "\n",
            "385. Title: GTMGC: Using Graph Transformer to Predict Molecule’s Ground-State Conformation\n",
            "   Abstract: The Mixture of Experts (MoE) is a widely known neural architecture where an\n",
            "ensemble of specialized sub-models optimizes overall performance with a\n",
            "constant computational cost. However, conventional MoEs pose challenges at\n",
            "scale due to the need to store all experts in memory. In this paper, we push\n",
            "MoE to the limit. We propose extremely parameter-efficient MoE by uniquely\n",
            "combining MoE architecture with lightweight experts.Our MoE architecture\n",
            "outperforms standard parameter-efficient fine-tuning (PEFT) methods and is on\n",
            "par with full fine-tuning by only updating the lightweight experts -- less than\n",
            "1% of an 11B parameters model. Furthermore, our method generalizes to unseen\n",
            "tasks as it does not depend on any prior task knowledge. Our research\n",
            "underscores the versatility of the mixture of experts architecture, showcasing\n",
            "its ability to deliver robust performance even when subjected to rigorous\n",
            "parameter constraints. Our code used in all the experiments is publicly\n",
            "available here: https://github.com/for-ai/parameter-efficient-moe.\n",
            "\n",
            "386. Title: LOQA: Learning with Opponent Q-Learning Awareness\n",
            "   Abstract: Autonomous agents deployed in the real world need to be robust against\n",
            "adversarial attacks on sensory inputs. Robustifying agent policies requires\n",
            "anticipating the strongest attacks possible. We demonstrate that existing\n",
            "observation-space attacks on reinforcement learning agents have a common\n",
            "weakness: while effective, their lack of information-theoretic detectability\n",
            "constraints makes them detectable using automated means or human inspection.\n",
            "Detectability is undesirable to adversaries as it may trigger security\n",
            "escalations. We introduce {\\epsilon}-illusory, a novel form of adversarial\n",
            "attack on sequential decision-makers that is both effective and of\n",
            "{\\epsilon}-bounded statistical detectability. We propose a novel dual ascent\n",
            "algorithm to learn such attacks end-to-end. Compared to existing attacks, we\n",
            "empirically find {\\epsilon}-illusory to be significantly harder to detect with\n",
            "automated methods, and a small study with human participants (IRB approval\n",
            "under reference R84123/RE001) suggests they are similarly harder to detect for\n",
            "humans. Our findings suggest the need for better anomaly detectors, as well as\n",
            "effective hardware- and system-level defenses. The project website can be found\n",
            "at https://tinyurl.com/illusory-attacks.\n",
            "\n",
            "387. Title: Attention-based Iterative Decomposition for Tensor Product Representation\n",
            "   Abstract: There is a recently discovered and intriguing phenomenon called Neural\n",
            "Collapse: at the terminal phase of training a deep neural network for\n",
            "classification, the within-class penultimate feature means and the associated\n",
            "classifier vectors of all flat classes collapse to the vertices of a simplex\n",
            "Equiangular Tight Frame (ETF). Recent work has tried to exploit this phenomenon\n",
            "by fixing the related classifier weights to a pre-computed ETF to induce neural\n",
            "collapse and maximize the separation of the learned features when training with\n",
            "imbalanced data. In this work, we propose to fix the linear classifier of a\n",
            "deep neural network to a Hierarchy-Aware Frame (HAFrame), instead of an ETF,\n",
            "and use a cosine similarity-based auxiliary loss to learn hierarchy-aware\n",
            "penultimate features that collapse to the HAFrame. We demonstrate that our\n",
            "approach reduces the mistake severity of the model's predictions while\n",
            "maintaining its top-1 accuracy on several datasets of varying scales with\n",
            "hierarchies of heights ranging from 3 to 12. Code:\n",
            "https://github.com/ltong1130ztr/HAFrame\n",
            "\n",
            "388. Title: Few-shot Hybrid Domain Adaptation of Image Generator\n",
            "   Abstract: Despite the growing popularity of explainable and interpretable machine\n",
            "learning, there is still surprisingly limited work on inherently interpretable\n",
            "clustering methods. Recently, there has been a surge of interest in explaining\n",
            "the classic k-means algorithm, leading to efficient algorithms that approximate\n",
            "k-means clusters using axis-aligned decision trees. However, interpretable\n",
            "variants of k-means have limited applicability in practice, where more flexible\n",
            "clustering methods are often needed to obtain useful partitions of the data. In\n",
            "this work, we investigate interpretable kernel clustering, and propose\n",
            "algorithms that construct decision trees to approximate the partitions induced\n",
            "by kernel k-means, a nonlinear extension of k-means. We further build on\n",
            "previous work on explainable k-means and demonstrate how a suitable choice of\n",
            "features allows preserving interpretability without sacrificing approximation\n",
            "guarantees on the interpretable model.\n",
            "\n",
            "389. Title: Multimarginal Generative Modeling with Stochastic Interpolants\n",
            "   Abstract: Sparse autoencoders provide a promising unsupervised approach for extracting\n",
            "interpretable features from a language model by reconstructing activations from\n",
            "a sparse bottleneck layer. Since language models learn many concepts,\n",
            "autoencoders need to be very large to recover all relevant features. However,\n",
            "studying the properties of autoencoder scaling is difficult due to the need to\n",
            "balance reconstruction and sparsity objectives and the presence of dead\n",
            "latents. We propose using k-sparse autoencoders [Makhzani and Frey, 2013] to\n",
            "directly control sparsity, simplifying tuning and improving the\n",
            "reconstruction-sparsity frontier. Additionally, we find modifications that\n",
            "result in few dead latents, even at the largest scales we tried. Using these\n",
            "techniques, we find clean scaling laws with respect to autoencoder size and\n",
            "sparsity. We also introduce several new metrics for evaluating feature quality\n",
            "based on the recovery of hypothesized features, the explainability of\n",
            "activation patterns, and the sparsity of downstream effects. These metrics all\n",
            "generally improve with autoencoder size. To demonstrate the scalability of our\n",
            "approach, we train a 16 million latent autoencoder on GPT-4 activations for 40\n",
            "billion tokens. We release training code and autoencoders for open-source\n",
            "models, as well as a visualizer.\n",
            "\n",
            "390. Title: QLLM: Accurate and Efficient Low-Bitwidth Quantization for Large Language Models\n",
            "   Abstract: In this paper we review basic and emerging models and associated algorithms\n",
            "for large-scale tensor networks, especially Tensor Train (TT) decompositions\n",
            "using novel mathematical and graphical representations. We discus the concept\n",
            "of tensorization (i.e., creating very high-order tensors from lower-order\n",
            "original data) and super compression of data achieved via quantized tensor\n",
            "train (QTT) networks. The purpose of a tensorization and quantization is to\n",
            "achieve, via low-rank tensor approximations \"super\" compression, and\n",
            "meaningful, compact representation of structured data. The main objective of\n",
            "this paper is to show how tensor networks can be used to solve a wide class of\n",
            "big data optimization problems (that are far from tractable by classical\n",
            "numerical methods) by applying tensorization and performing all operations\n",
            "using relatively small size matrices and tensors and applying iteratively\n",
            "optimized and approximative tensor contractions.\n",
            "  Keywords: Tensor networks, tensor train (TT) decompositions, matrix product\n",
            "states (MPS), matrix product operators (MPO), basic tensor operations,\n",
            "tensorization, distributed representation od data optimization problems for\n",
            "very large-scale problems: generalized eigenvalue decomposition (GEVD),\n",
            "PCA/SVD, canonical correlation analysis (CCA).\n",
            "\n",
            "391. Title: Post-hoc bias scoring is optimal for fair classification\n",
            "   Abstract: Multi-label classification is an essential task utilized in a wide variety of\n",
            "real-world applications. Multi-label zero-shot learning is a method for\n",
            "classifying images into multiple unseen categories for which no training data\n",
            "is available, while in general zero-shot situations, the test set may include\n",
            "observed classes. The CLIP-Decoder is a novel method based on the\n",
            "state-of-the-art ML-Decoder attention-based head. We introduce multi-modal\n",
            "representation learning in CLIP-Decoder, utilizing the text encoder to extract\n",
            "text features and the image encoder for image feature extraction. Furthermore,\n",
            "we minimize semantic mismatch by aligning image and word embeddings in the same\n",
            "dimension and comparing their respective representations using a combined loss,\n",
            "which comprises classification loss and CLIP loss. This strategy outperforms\n",
            "other methods and we achieve cutting-edge results on zero-shot multilabel\n",
            "classification tasks using CLIP-Decoder. Our method achieves an absolute\n",
            "increase of 3.9% in performance compared to existing methods for zero-shot\n",
            "learning multi-label classification tasks. Additionally, in the generalized\n",
            "zero-shot learning multi-label classification task, our method shows an\n",
            "impressive increase of almost 2.3%.\n",
            "\n",
            "392. Title: SEINE: Short-to-Long Video Diffusion Model for Generative Transition and Prediction\n",
            "   Abstract: Diffusion models are powerful generative models that map noise to data using\n",
            "stochastic processes. However, for many applications such as image editing, the\n",
            "model input comes from a distribution that is not random noise. As such,\n",
            "diffusion models must rely on cumbersome methods like guidance or projected\n",
            "sampling to incorporate this information in the generative process. In our\n",
            "work, we propose Denoising Diffusion Bridge Models (DDBMs), a natural\n",
            "alternative to this paradigm based on diffusion bridges, a family of processes\n",
            "that interpolate between two paired distributions given as endpoints. Our\n",
            "method learns the score of the diffusion bridge from data and maps from one\n",
            "endpoint distribution to the other by solving a (stochastic) differential\n",
            "equation based on the learned score. Our method naturally unifies several\n",
            "classes of generative models, such as score-based diffusion models and\n",
            "OT-Flow-Matching, allowing us to adapt existing design and architectural\n",
            "choices to our more general problem. Empirically, we apply DDBMs to challenging\n",
            "image datasets in both pixel and latent space. On standard image translation\n",
            "problems, DDBMs achieve significant improvement over baseline methods, and,\n",
            "when we reduce the problem to image generation by setting the source\n",
            "distribution to random noise, DDBMs achieve comparable FID scores to\n",
            "state-of-the-art methods despite being built for a more general task.\n",
            "\n",
            "393. Title: A Symmetry-Aware Exploration of Bayesian Neural Network Posteriors\n",
            "   Abstract: We present BayeSum (for ``Bayesian summarization''), a model for sentence\n",
            "extraction in query-focused summarization. BayeSum leverages the common case in\n",
            "which multiple documents are relevant to a single query. Using these documents\n",
            "as reinforcement for query terms, BayeSum is not afflicted by the paucity of\n",
            "information in short queries. We show that approximate inference in BayeSum is\n",
            "possible on large data sets and results in a state-of-the-art summarization\n",
            "system. Furthermore, we show how BayeSum can be understood as a justified query\n",
            "expansion technique in the language modeling for IR framework.\n",
            "\n",
            "394. Title: ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate\n",
            "   Abstract: We consider a binary classification problem under group fairness constraints,\n",
            "which can be one of Demographic Parity (DP), Equalized Opportunity (EOp), or\n",
            "Equalized Odds (EO). We propose an explicit characterization of Bayes optimal\n",
            "classifier under the fairness constraints, which turns out to be a simple\n",
            "modification rule of the unconstrained classifier. Namely, we introduce a novel\n",
            "instance-level measure of bias, which we call bias score, and the modification\n",
            "rule is a simple linear rule on top of the finite amount of bias scores.Based\n",
            "on this characterization, we develop a post-hoc approach that allows us to\n",
            "adapt to fairness constraints while maintaining high accuracy. In the case of\n",
            "DP and EOp constraints, the modification rule is thresholding a single bias\n",
            "score, while in the case of EO constraints we are required to fit a linear\n",
            "modification rule with 2 parameters. The method can also be applied for\n",
            "composite group-fairness criteria, such as ones involving several sensitive\n",
            "attributes.\n",
            "\n",
            "395. Title: ZeroFlow: Scalable Scene Flow via Distillation\n",
            "   Abstract: The distribution of the weights of modern deep neural networks (DNNs) -\n",
            "crucial for uncertainty quantification and robustness - is an eminently complex\n",
            "object due to its extremely high dimensionality. This paper proposes one of the\n",
            "first large-scale explorations of the posterior distribution of deep Bayesian\n",
            "Neural Networks (BNNs), expanding its study to real-world vision tasks and\n",
            "architectures. Specifically, we investigate the optimal approach for\n",
            "approximating the posterior, analyze the connection between posterior quality\n",
            "and uncertainty quantification, delve into the impact of modes on the\n",
            "posterior, and explore methods for visualizing the posterior. Moreover, we\n",
            "uncover weight-space symmetries as a critical aspect for understanding the\n",
            "posterior. To this extent, we develop an in-depth assessment of the impact of\n",
            "both permutation and scaling symmetries that tend to obfuscate the Bayesian\n",
            "posterior. While the first type of transformation is known for duplicating\n",
            "modes, we explore the relationship between the latter and L2 regularization,\n",
            "challenging previous misconceptions. Finally, to help the community improve our\n",
            "understanding of the Bayesian posterior, we will shortly release the first\n",
            "large-scale checkpoint dataset, including thousands of real-world models and\n",
            "our codes.\n",
            "\n",
            "396. Title: MVDream: Multi-view Diffusion for 3D Generation\n",
            "   Abstract: Molecular conformation optimization is crucial to computer-aided drug\n",
            "discovery and materials design. Traditional energy minimization techniques rely\n",
            "on iterative optimization methods that use molecular forces calculated by a\n",
            "physical simulator (oracle) as anti-gradients. However, this is a\n",
            "computationally expensive approach that requires many interactions with a\n",
            "physical simulator. One way to accelerate this procedure is to replace the\n",
            "physical simulator with a neural network. Despite recent progress in neural\n",
            "networks for molecular conformation energy prediction, such models are prone to\n",
            "distribution shift, leading to inaccurate energy minimization. We find that the\n",
            "quality of energy minimization with neural networks can be improved by\n",
            "providing optimization trajectories as additional training data. Still, it\n",
            "takes around $5 \\times 10^5$ additional conformations to match the physical\n",
            "simulator's optimization quality. In this work, we present the Gradual\n",
            "Optimization Learning Framework (GOLF) for energy minimization with neural\n",
            "networks that significantly reduces the required additional data. The framework\n",
            "consists of an efficient data-collecting scheme and an external optimizer. The\n",
            "external optimizer utilizes gradients from the energy prediction model to\n",
            "generate optimization trajectories, and the data-collecting scheme selects\n",
            "additional training data to be processed by the physical simulator. Our results\n",
            "demonstrate that the neural network trained with GOLF performs on par with the\n",
            "oracle on a benchmark of diverse drug-like molecules using $50$x less\n",
            "additional data.\n",
            "\n",
            "397. Title: Improving Non-Transferable Representation Learning by Harnessing Content and Style\n",
            "   Abstract: While dataset condensation effectively enhances training efficiency, its\n",
            "application in on-device scenarios brings unique challenges. 1) Due to the\n",
            "fluctuating computational resources of these devices, there's a demand for a\n",
            "flexible dataset size that diverges from a predefined size. 2) The limited\n",
            "computational power on devices often prevents additional condensation\n",
            "operations. These two challenges connect to the \"subset degradation problem\" in\n",
            "traditional dataset condensation: a subset from a larger condensed dataset is\n",
            "often unrepresentative compared to directly condensing the whole dataset to\n",
            "that smaller size. In this paper, we propose Multisize Dataset Condensation\n",
            "(MDC) by compressing N condensation processes into a single condensation\n",
            "process to obtain datasets with multiple sizes. Specifically, we introduce an\n",
            "\"adaptive subset loss\" on top of the basic condensation loss to mitigate the\n",
            "\"subset degradation problem\". Our MDC method offers several benefits: 1) No\n",
            "additional condensation process is required; 2) reduced storage requirement by\n",
            "reusing condensed images. Experiments validate our findings on networks\n",
            "including ConvNet, ResNet and DenseNet, and datasets including SVHN, CIFAR-10,\n",
            "CIFAR-100 and ImageNet. For example, we achieved 5.22%-6.40% average accuracy\n",
            "gains on condensing CIFAR-10 to ten images per class. Code is available at:\n",
            "https://github.com/he-y/Multisize-Dataset-Condensation.\n",
            "\n",
            "398. Title: OVOR: OnePrompt with Virtual Outlier Regularization for Rehearsal-Free Class-Incremental Learning\n",
            "   Abstract: Large language models (LLMs) have led to a surge in collaborative writing\n",
            "with model assistance. As different users incorporate suggestions from the same\n",
            "model, there is a risk of decreased diversity in the produced content,\n",
            "potentially limiting diverse perspectives in public discourse. In this work, we\n",
            "measure the impact of co-writing on diversity via a controlled experiment,\n",
            "where users write argumentative essays in three setups -- using a base LLM\n",
            "(GPT3), a feedback-tuned LLM (InstructGPT), and writing without model help. We\n",
            "develop a set of diversity metrics and find that writing with InstructGPT (but\n",
            "not the GPT3) results in a statistically significant reduction in diversity.\n",
            "Specifically, it increases the similarity between the writings of different\n",
            "authors and reduces the overall lexical and content diversity. We additionally\n",
            "find that this effect is mainly attributable to InstructGPT contributing less\n",
            "diverse text to co-written essays. In contrast, the user-contributed text\n",
            "remains unaffected by model collaboration. This suggests that the recent\n",
            "improvement in generation quality from adapting models to human feedback might\n",
            "come at the cost of more homogeneous and less diverse content.\n",
            "\n",
            "399. Title: AlpaGasus: Training a Better Alpaca with Fewer Data\n",
            "   Abstract: Current learning models often struggle with human-like systematic\n",
            "generalization, particularly in learning compositional rules from limited data\n",
            "and extrapolating them to novel combinations. We introduce the Neural-Symbolic\n",
            "Recursive Machine (NSR), whose core is a Grounded Symbol System (GSS), allowing\n",
            "for the emergence of combinatorial syntax and semantics directly from training\n",
            "data. The NSR employs a modular design that integrates neural perception,\n",
            "syntactic parsing, and semantic reasoning. These components are synergistically\n",
            "trained through a novel deduction-abduction algorithm. Our findings demonstrate\n",
            "that NSR's design, imbued with the inductive biases of equivariance and\n",
            "compositionality, grants it the expressiveness to adeptly handle diverse\n",
            "sequence-to-sequence tasks and achieve unparalleled systematic generalization.\n",
            "We evaluate NSR's efficacy across four challenging benchmarks designed to probe\n",
            "systematic generalization capabilities: SCAN for semantic parsing, PCFG for\n",
            "string manipulation, HINT for arithmetic reasoning, and a compositional machine\n",
            "translation task. The results affirm NSR's superiority over contemporary neural\n",
            "and hybrid models in terms of generalization and transferability.\n",
            "\n",
            "400. Title: Does Writing with Language Models Reduce Content Diversity?\n",
            "   Abstract: Multi-modal domain translation typically refers to synthesizing a novel image\n",
            "that inherits certain localized attributes from a 'content' image (e.g. layout,\n",
            "semantics, or geometry), and inherits everything else (e.g. texture, lighting,\n",
            "sometimes even semantics) from a 'style' image. The dominant approach to this\n",
            "task is attempting to learn disentangled 'content' and 'style' representations\n",
            "from scratch. However, this is not only challenging, but ill-posed, as what\n",
            "users wish to preserve during translation varies depending on their goals.\n",
            "Motivated by this inherent ambiguity, we define 'content' based on conditioning\n",
            "information extracted by off-the-shelf pre-trained models. We then train our\n",
            "style extractor and image decoder with an easy to optimize set of\n",
            "reconstruction objectives. The wide variety of high-quality pre-trained models\n",
            "available and simple training procedure makes our approach straightforward to\n",
            "apply across numerous domains and definitions of 'content'. Additionally it\n",
            "offers intuitive control over which aspects of 'content' are preserved across\n",
            "domains. We evaluate our method on traditional, well-aligned, datasets such as\n",
            "CelebA-HQ, and propose two novel datasets for evaluation on more complex\n",
            "scenes: ClassicTV and FFHQ-Wild. Our approach, Sensorium, enables higher\n",
            "quality domain translation for more complex scenes.\n",
            "\n",
            "401. Title: A Versatile Causal Discovery Framework to Allow Causally-Related Hidden Variables\n",
            "   Abstract: When the federated learning is adopted among competitive agents with siloed\n",
            "datasets, agents are self-interested and participate only if they are fairly\n",
            "rewarded. To encourage the application of federated learning, this paper\n",
            "employs a management strategy, i.e., more contributions should lead to more\n",
            "rewards. We propose a novel hierarchically fair federated learning (HFFL)\n",
            "framework. Under this framework, agents are rewarded in proportion to their\n",
            "pre-negotiated contribution levels. HFFL+ extends this to incorporate\n",
            "heterogeneous models. Theoretical analysis and empirical evaluation on several\n",
            "datasets confirm the efficacy of our frameworks in upholding fairness and thus\n",
            "facilitating federated learning in the competitive settings.\n",
            "\n",
            "402. Title: Let's do the time-warp-attend: Learning topological invariants of dynamical systems\n",
            "   Abstract: Most existing causal discovery methods rely on the assumption of no latent\n",
            "confounders, limiting their applicability in solving real-life problems. In\n",
            "this paper, we introduce a novel, versatile framework for causal discovery that\n",
            "accommodates the presence of causally-related hidden variables almost\n",
            "everywhere in the causal network (for instance, they can be effects of observed\n",
            "variables), based on rank information of covariance matrix over observed\n",
            "variables. We start by investigating the efficacy of rank in comparison to\n",
            "conditional independence and, theoretically, establish necessary and sufficient\n",
            "conditions for the identifiability of certain latent structural patterns.\n",
            "Furthermore, we develop a Rank-based Latent Causal Discovery algorithm, RLCD,\n",
            "that can efficiently locate hidden variables, determine their cardinalities,\n",
            "and discover the entire causal structure over both measured and hidden ones. We\n",
            "also show that, under certain graphical conditions, RLCD correctly identifies\n",
            "the Markov Equivalence Class of the whole latent causal graph asymptotically.\n",
            "Experimental results on both synthetic and real-world personality data sets\n",
            "demonstrate the efficacy of the proposed approach in finite-sample cases.\n",
            "\n",
            "403. Title: Candidate Label Set Pruning: A Data-centric Perspective for Deep Partial-label Learning\n",
            "   Abstract: Dynamical systems across the sciences, from electrical circuits to ecological\n",
            "networks, undergo qualitative and often catastrophic changes in behavior,\n",
            "called bifurcations, when their underlying parameters cross a threshold.\n",
            "Existing methods predict oncoming catastrophes in individual systems but are\n",
            "primarily time-series-based and struggle both to categorize qualitative\n",
            "dynamical regimes across diverse systems and to generalize to real data. To\n",
            "address this challenge, we propose a data-driven, physically-informed\n",
            "deep-learning framework for classifying dynamical regimes and characterizing\n",
            "bifurcation boundaries based on the extraction of topologically invariant\n",
            "features. We focus on the paradigmatic case of the supercritical Hopf\n",
            "bifurcation, which is used to model periodic dynamics across a wide range of\n",
            "applications. Our convolutional attention method is trained with data\n",
            "augmentations that encourage the learning of topological invariants which can\n",
            "be used to detect bifurcation boundaries in unseen systems and to design models\n",
            "of biological systems like oscillatory gene regulatory networks. We further\n",
            "demonstrate our method's use in analyzing real data by recovering distinct\n",
            "proliferation and differentiation dynamics along pancreatic endocrinogenesis\n",
            "trajectory in gene expression space based on single-cell data. Our method\n",
            "provides valuable insights into the qualitative, long-term behavior of a wide\n",
            "range of dynamical systems, and can detect bifurcations or catastrophic\n",
            "transitions in large-scale physical and biological systems.\n",
            "\n",
            "404. Title: Incentive-Aware Federated Learning with Training-Time Model Rewards\n",
            "   Abstract: Debiased collaborative filtering aims to learn an unbiased prediction model\n",
            "by removing different biases in observational datasets. To solve this problem,\n",
            "one of the simple and effective methods is based on the propensity score, which\n",
            "adjusts the observational sample distribution to the target one by reweighting\n",
            "observed instances. Ideally, propensity scores should be learned with causal\n",
            "balancing constraints. However, existing methods usually ignore such\n",
            "constraints or implement them with unreasonable approximations, which may\n",
            "affect the accuracy of the learned propensity scores. To bridge this gap, in\n",
            "this paper, we first analyze the gaps between the causal balancing requirements\n",
            "and existing methods such as learning the propensity with cross-entropy loss or\n",
            "manually selecting functions to balance. Inspired by these gaps, we propose to\n",
            "approximate the balancing functions in reproducing kernel Hilbert space and\n",
            "demonstrate that, based on the universal property and representer theorem of\n",
            "kernel functions, the causal balancing constraints can be better satisfied.\n",
            "Meanwhile, we propose an algorithm that adaptively balances the kernel function\n",
            "and theoretically analyze the generalization error bound of our methods. We\n",
            "conduct extensive experiments to demonstrate the effectiveness of our methods,\n",
            "and to promote this research direction, we have released our project at\n",
            "https://github.com/haoxuanli-pku/ICLR24-Kernel-Balancing.\n",
            "\n",
            "405. Title: Unified Language-Vision Pretraining in LLM with Dynamic Discrete Visual Tokenization\n",
            "   Abstract: Sequence-to-sequence deep neural models fine-tuned for abstractive\n",
            "summarization can achieve great performance on datasets with enough human\n",
            "annotations. Yet, it has been shown that they have not reached their full\n",
            "potential, with a wide gap between the top beam search output and the oracle\n",
            "beam. Recently, re-ranking methods have been proposed, to learn to select a\n",
            "better summary candidate. However, such methods are limited by the summary\n",
            "quality aspects captured by the first-stage candidates. To bypass this\n",
            "limitation, we propose a new paradigm in second-stage abstractive summarization\n",
            "called SummaFusion that fuses several summary candidates to produce a novel\n",
            "abstractive second-stage summary. Our method works well on several\n",
            "summarization datasets, improving both the ROUGE scores and qualitative\n",
            "properties of fused summaries. It is especially good when the candidates to\n",
            "fuse are worse, such as in the few-shot setup where we set a new\n",
            "state-of-the-art. We will make our code and checkpoints available at\n",
            "https://github.com/ntunlp/SummaFusion/.\n",
            "\n",
            "406. Title: SOInter: A Novel Deep Energy-Based Interpretation Method for Explaining Structured Output Models\n",
            "   Abstract: Interpreting a node-link graph is enhanced if similar subgraphs (or motifs)\n",
            "are depicted in a similar manner; that is, they have the same visual form.\n",
            "Small motifs within graphs may be perceived to be identical when they are\n",
            "structurally dissimilar, or may be perceived to be dissimilar when they are\n",
            "identical. This issue primarily relates to the Gestalt principle of similarity,\n",
            "but may also include an element of quick, low-level pattern-matching. We\n",
            "believe that if motifs are identical, they should be depicted identically; if\n",
            "they are nearly-identical, they should be depicted nearly-identically. This\n",
            "principle is particularly important in domains where motifs hold meaning and\n",
            "where their identification is important. We identified five small motifs:\n",
            "bi-cliques, cliques, cycles, double-cycles, and stars. For each, we defined\n",
            "visual variations on two dimensions: same or different structure, same or\n",
            "different shape. We conducted a crowd-sourced empirical study to test the\n",
            "perception of similarity of these varied motifs, and found that determining\n",
            "whether motifs are identical or similar is affected by both shape and\n",
            "structure.\n",
            "\n",
            "407. Title: PnP Inversion: Boosting Diffusion-based Editing with 3 Lines of Code\n",
            "   Abstract: Recently, the remarkable advance of the Large Language Model (LLM) has\n",
            "inspired researchers to transfer its extraordinary reasoning capability to both\n",
            "vision and language data. However, the prevailing approaches primarily regard\n",
            "the visual input as a prompt and focus exclusively on optimizing the text\n",
            "generation process conditioned upon vision content by a frozen LLM. Such an\n",
            "inequitable treatment of vision and language heavily constrains the model's\n",
            "potential. In this paper, we break through this limitation by representing both\n",
            "vision and language in a unified form. Specifically, we introduce a\n",
            "well-designed visual tokenizer to translate the non-linguistic image into a\n",
            "sequence of discrete tokens like a foreign language that LLM can read. The\n",
            "resulting visual tokens encompass high-level semantics worthy of a word and\n",
            "also support dynamic sequence length varying from the image. Coped with this\n",
            "tokenizer, the presented foundation model called LaVIT can handle both image\n",
            "and text indiscriminately under the same generative learning paradigm. This\n",
            "unification empowers LaVIT to serve as an impressive generalist interface to\n",
            "understand and generate multi-modal content simultaneously. Extensive\n",
            "experiments further showcase that it outperforms the existing models by a large\n",
            "margin on massive vision-language tasks. Our code and models are available at\n",
            "https://github.com/jy0205/LaVIT.\n",
            "\n",
            "408. Title: AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning\n",
            "   Abstract: Video Snapshot compressive imaging (SCI) is a promising technique to capture\n",
            "high-speed videos, which transforms the imaging speed from the detector to mask\n",
            "modulating and only needs a single measurement to capture multiple frames. The\n",
            "algorithm to reconstruct high-speed frames from the measurement plays a vital\n",
            "role in SCI. In this paper, we consider the promising reconstruction algorithm\n",
            "framework, namely plug-and-play (PnP), which is flexible to the encoding\n",
            "process comparing with other deep learning networks. One drawback of existing\n",
            "PnP algorithms is that they use a pre-trained denoising network as a plugged\n",
            "prior while the training data of the network might be different from the task\n",
            "in real applications. Towards this end, in this work, we propose the online PnP\n",
            "algorithm which can adaptively update the network's parameters within the PnP\n",
            "iteration; this makes the denoising network more applicable to the desired data\n",
            "in the SCI reconstruction. Furthermore, for color video imaging, RGB frames\n",
            "need to be recovered from Bayer pattern or named demosaicing in the camera\n",
            "pipeline. To address this challenge, we design a two-stage reconstruction\n",
            "framework to optimize these two coupled ill-posed problems and introduce a deep\n",
            "demosaicing prior specifically for video demosaicing which does not have much\n",
            "past works instead of using single image demosaicing networks. Extensive\n",
            "results on both simulation and real datasets verify the superiority of our\n",
            "adaptive deep PnP algorithm.\n",
            "\n",
            "409. Title: Customizable Combination of Parameter-Efficient Modules for Multi-Task Learning\n",
            "   Abstract: In this work, we tackle the challenging problem of denoising hand-object\n",
            "interactions (HOI). Given an erroneous interaction sequence, the objective is\n",
            "to refine the incorrect hand trajectory to remove interaction artifacts for a\n",
            "perceptually realistic sequence. This challenge involves intricate interaction\n",
            "noise, including unnatural hand poses and incorrect hand-object relations,\n",
            "alongside the necessity for robust generalization to new interactions and\n",
            "diverse noise patterns. We tackle those challenges through a novel approach,\n",
            "GeneOH Diffusion, incorporating two key designs: an innovative contact-centric\n",
            "HOI representation named GeneOH and a new domain-generalizable denoising\n",
            "scheme. The contact-centric representation GeneOH informatively parameterizes\n",
            "the HOI process, facilitating enhanced generalization across various HOI\n",
            "scenarios. The new denoising scheme consists of a canonical denoising model\n",
            "trained to project noisy data samples from a whitened noise space to a clean\n",
            "data manifold and a \"denoising via diffusion\" strategy which can handle input\n",
            "trajectories with various noise patterns by first diffusing them to align with\n",
            "the whitened noise space and cleaning via the canonical denoiser. Extensive\n",
            "experiments on four benchmarks with significant domain variations demonstrate\n",
            "the superior effectiveness of our method. GeneOH Diffusion also shows promise\n",
            "for various downstream applications. Project website:\n",
            "https://meowuu7.github.io/GeneOH-Diffusion/.\n",
            "\n",
            "410. Title: Empirical Likelihood for Fair Classification\n",
            "   Abstract: Classical clustering methods do not provide users with direct control of the\n",
            "clustering results, and the clustering results may not be consistent with the\n",
            "relevant criterion that a user has in mind. In this work, we present a new\n",
            "methodology for performing image clustering based on user-specified text\n",
            "criteria by leveraging modern vision-language models and large language models.\n",
            "We call our method Image Clustering Conditioned on Text Criteria (IC|TC), and\n",
            "it represents a different paradigm of image clustering. IC|TC requires a\n",
            "minimal and practical degree of human intervention and grants the user\n",
            "significant control over the clustering results in return. Our experiments show\n",
            "that IC|TC can effectively cluster images with various criteria, such as human\n",
            "action, physical location, or the person's mood, while significantly\n",
            "outperforming baselines.\n",
            "\n",
            "411. Title: Identifying the Risks of LM Agents with an LM-Emulated Sandbox\n",
            "   Abstract: As large language models (LLMs) are overwhelmingly more and more integrated\n",
            "into various applications, ensuring they generate safe and aligned responses is\n",
            "a pressing need. Previous research on alignment has largely focused on general\n",
            "instruction-following but has often overlooked the unique properties and\n",
            "challenges of safety alignment, such as the brittleness of safety mechanisms.\n",
            "To bridge the gap, we propose the Superficial Safety Alignment Hypothesis\n",
            "(SSAH), which posits that safety alignment should teach an otherwise unsafe\n",
            "model to choose the correct reasoning direction - interpreted as a specialized\n",
            "binary classification task - and incorporate a refusal mechanism with multiple\n",
            "reserved fallback options. Furthermore, through SSAH, we hypothesize that\n",
            "safety guardrails in LLMs can be established by just a small number of\n",
            "essential components. To verify this, we conduct an ablation study and\n",
            "successfully identify four types of attribute-critical components in\n",
            "safety-aligned LLMs: Exclusive Safety Unit (ESU), Exclusive Utility Unit (EUU),\n",
            "Complex Unit (CU), and Redundant Unit (RU). Our findings show that freezing\n",
            "certain safety-critical components 7.5\\% during fine-tuning allows the model to\n",
            "retain its safety attributes while adapting to new tasks. Additionally, we show\n",
            "that leveraging redundant units 20\\% in the pre-trained model as an ``alignment\n",
            "budget'' can effectively minimize the alignment tax while achieving the\n",
            "alignment goal. All considered, this paper concludes that the atomic functional\n",
            "unit for safety in LLMs is at the neuron level and underscores that safety\n",
            "alignment should not be complicated. We believe this work contributes to the\n",
            "foundation of efficient and scalable safety alignment for future LLMs.\n",
            "\n",
            "412. Title: Grokking in Linear Estimators -- A Solvable Model that Groks without Understanding\n",
            "   Abstract: Latest advances have achieved realistic virtual try-on (VTON) through\n",
            "localized garment inpainting using latent diffusion models, significantly\n",
            "enhancing consumers' online shopping experience. However, existing VTON\n",
            "technologies neglect the need for merchants to showcase garments\n",
            "comprehensively, including flexible control over garments, optional faces,\n",
            "poses, and scenes. To address this issue, we define a virtual dressing (VD)\n",
            "task focused on generating freely editable human images with fixed garments and\n",
            "optional conditions. Meanwhile, we design a comprehensive affinity metric index\n",
            "(CAMI) to evaluate the consistency between generated images and reference\n",
            "garments. Then, we propose IMAGDressing-v1, which incorporates a garment UNet\n",
            "that captures semantic features from CLIP and texture features from VAE. We\n",
            "present a hybrid attention module, including a frozen self-attention and a\n",
            "trainable cross-attention, to integrate garment features from the garment UNet\n",
            "into a frozen denoising UNet, ensuring users can control different scenes\n",
            "through text. IMAGDressing-v1 can be combined with other extension plugins,\n",
            "such as ControlNet and IP-Adapter, to enhance the diversity and controllability\n",
            "of generated images. Furthermore, to address the lack of data, we release the\n",
            "interactive garment pairing (IGPair) dataset, containing over 300,000 pairs of\n",
            "clothing and dressed images, and establish a standard pipeline for data\n",
            "assembly. Extensive experiments demonstrate that our IMAGDressing-v1 achieves\n",
            "state-of-the-art human image synthesis performance under various controlled\n",
            "conditions. The code and model will be available at\n",
            "https://github.com/muzishen/IMAGDressing.\n",
            "\n",
            "413. Title: Effective Structural Encodings via Local Curvature Profiles\n",
            "   Abstract: This paper studies a parametric family of algorithmic fairness metrics,\n",
            "called generalized entropy, which originally has been used in public welfare\n",
            "and recently introduced to machine learning community. As a meaningful metric\n",
            "to evaluate algorithmic fairness, it requires that generalized entropy specify\n",
            "fairness requirements of a classification problem and the fairness requirements\n",
            "should be realized with small deviation by an algorithm. We investigate the\n",
            "role of generalized entropy as a design parameter for fair classification\n",
            "algorithm through a fair empirical risk minimization with a constraint\n",
            "specified in terms of generalized entropy. We theoretically and experimentally\n",
            "study learnability of the problem.\n",
            "\n",
            "414. Title: RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval\n",
            "   Abstract: Structural and Positional Encodings can significantly improve the performance\n",
            "of Graph Neural Networks in downstream tasks. Recent literature has begun to\n",
            "systematically investigate differences in the structural properties that these\n",
            "approaches encode, as well as performance trade-offs between them. However, the\n",
            "question of which structural properties yield the most effective encoding\n",
            "remains open. In this paper, we investigate this question from a geometric\n",
            "perspective. We propose a novel structural encoding based on discrete Ricci\n",
            "curvature (Local Curvature Profiles, short LCP) and show that it significantly\n",
            "outperforms existing encoding approaches. We further show that combining local\n",
            "structural encodings, such as LCP, with global positional encodings improves\n",
            "downstream performance, suggesting that they capture complementary geometric\n",
            "information. Finally, we compare different encoding types with\n",
            "(curvature-based) rewiring techniques. Rewiring has recently received a surge\n",
            "of interest due to its ability to improve the performance of Graph Neural\n",
            "Networks by mitigating over-smoothing and over-squashing effects. Our results\n",
            "suggest that utilizing curvature information for structural encodings delivers\n",
            "significantly larger performance increases than rewiring.\n",
            "\n",
            "415. Title: The Reversal Curse: LLMs trained on “A is B” fail to learn “B is A”\n",
            "   Abstract: Recent advances in Language Model (LM) agents and tool use, exemplified by\n",
            "applications like ChatGPT Plugins, enable a rich set of capabilities but also\n",
            "amplify potential risks - such as leaking private data or causing financial\n",
            "losses. Identifying these risks is labor-intensive, necessitating implementing\n",
            "the tools, setting up the environment for each test scenario manually, and\n",
            "finding risky cases. As tools and agents become more complex, the high cost of\n",
            "testing these agents will make it increasingly difficult to find high-stakes,\n",
            "long-tailed risks. To address these challenges, we introduce ToolEmu: a\n",
            "framework that uses an LM to emulate tool execution and enables the testing of\n",
            "LM agents against a diverse range of tools and scenarios, without manual\n",
            "instantiation. Alongside the emulator, we develop an LM-based automatic safety\n",
            "evaluator that examines agent failures and quantifies associated risks. We test\n",
            "both the tool emulator and evaluator through human evaluation and find that\n",
            "68.8% of failures identified with ToolEmu would be valid real-world agent\n",
            "failures. Using our curated initial benchmark consisting of 36 high-stakes\n",
            "tools and 144 test cases, we provide a quantitative risk analysis of current LM\n",
            "agents and identify numerous failures with potentially severe outcomes.\n",
            "Notably, even the safest LM agent exhibits such failures 23.9% of the time\n",
            "according to our evaluator, underscoring the need to develop safer LM agents\n",
            "for real-world deployment.\n",
            "\n",
            "416. Title: AutoChunk: Automated Activation Chunk for Memory-Efficient Deep Learning Inference\n",
            "   Abstract: We expose a surprising failure of generalization in auto-regressive large\n",
            "language models (LLMs). If a model is trained on a sentence of the form \"A is\n",
            "B\", it will not automatically generalize to the reverse direction \"B is A\".\n",
            "This is the Reversal Curse. For instance, if a model is trained on \"Valentina\n",
            "Tereshkova was the first woman to travel to space\", it will not automatically\n",
            "be able to answer the question, \"Who was the first woman to travel to space?\".\n",
            "Moreover, the likelihood of the correct answer (\"Valentina Tershkova\") will not\n",
            "be higher than for a random name. Thus, models do not generalize a prevalent\n",
            "pattern in their training set: if \"A is B\" occurs, \"B is A\" is more likely to\n",
            "occur. It is worth noting, however, that if \"A is B\" appears in-context, models\n",
            "can deduce the reverse relationship. We provide evidence for the Reversal Curse\n",
            "by finetuning GPT-3 and Llama-1 on fictitious statements such as \"Uriah\n",
            "Hawthorne is the composer of Abyssal Melodies\" and showing that they fail to\n",
            "correctly answer \"Who composed Abyssal Melodies?\". The Reversal Curse is robust\n",
            "across model sizes and model families and is not alleviated by data\n",
            "augmentation. We also evaluate ChatGPT (GPT-3.5 and GPT-4) on questions about\n",
            "real-world celebrities, such as \"Who is Tom Cruise's mother? [A: Mary Lee\n",
            "Pfeiffer]\" and the reverse \"Who is Mary Lee Pfeiffer's son?\". GPT-4 correctly\n",
            "answers questions like the former 79% of the time, compared to 33% for the\n",
            "latter.\n",
            "  Code available at: https://github.com/lukasberglund/reversal_curse.\n",
            "\n",
            "417. Title: DiffAR: Denoising Diffusion Autoregressive Model for Raw Speech Waveform Generation\n",
            "   Abstract: Learning a precise dynamics model can be crucial for offline reinforcement\n",
            "learning, which, unfortunately, has been found to be quite challenging.\n",
            "Dynamics models that are learned by fitting historical transitions often\n",
            "struggle to generalize to unseen transitions. In this study, we identify a\n",
            "hidden but pivotal factor termed dynamics reward that remains consistent across\n",
            "transitions, offering a pathway to better generalization. Therefore, we propose\n",
            "the idea of reward-consistent dynamics models: any trajectory generated by the\n",
            "dynamics model should maximize the dynamics reward derived from the data. We\n",
            "implement this idea as the MOREC (Model-based Offline reinforcement learning\n",
            "with Reward Consistency) method, which can be seamlessly integrated into\n",
            "previous offline model-based reinforcement learning (MBRL) methods. MOREC\n",
            "learns a generalizable dynamics reward function from offline data, which is\n",
            "subsequently employed as a transition filter in any offline MBRL method: when\n",
            "generating transitions, the dynamics model generates a batch of transitions and\n",
            "selects the one with the highest dynamics reward value. On a synthetic task, we\n",
            "visualize that MOREC has a strong generalization ability and can surprisingly\n",
            "recover some distant unseen transitions. On 21 offline tasks in D4RL and NeoRL\n",
            "benchmarks, MOREC improves the previous state-of-the-art performance by a\n",
            "significant margin, i.e., 4.6% on D4RL tasks and 25.9% on NeoRL tasks. Notably,\n",
            "MOREC is the first method that can achieve above 95% online RL performance in 6\n",
            "out of 12 D4RL tasks and 3 out of 9 NeoRL tasks.\n",
            "\n",
            "418. Title: DreamFlow: High-quality text-to-3D generation by Approximating Probability Flow\n",
            "   Abstract: Understanding how overparameterized neural networks generalize despite\n",
            "perfect interpolation of noisy training data is a fundamental question.\n",
            "Mallinar et. al. 2022 noted that neural networks seem to often exhibit\n",
            "``tempered overfitting'', wherein the population risk does not converge to the\n",
            "Bayes optimal error, but neither does it approach infinity, yielding\n",
            "non-trivial generalization. However, this has not been studied rigorously. We\n",
            "provide the first rigorous analysis of the overfitting behavior of regression\n",
            "with minimum norm ($\\ell_2$ of weights), focusing on univariate two-layer ReLU\n",
            "networks. We show overfitting is tempered (with high probability) when measured\n",
            "with respect to the $L_1$ loss, but also show that the situation is more\n",
            "complex than suggested by Mallinar et. al., and overfitting is catastrophic\n",
            "with respect to the $L_2$ loss, or when taking an expectation over the training\n",
            "set.\n",
            "\n",
            "419. Title: REValueD: Regularised Ensemble Value-Decomposition for Factorisable Markov Decision Processes\n",
            "   Abstract: Zero-shot learning in prompted vision-language models, the practice of\n",
            "crafting prompts to build classifiers without an explicit training process, has\n",
            "achieved impressive performance in many settings. This success presents a\n",
            "seemingly surprising observation: these methods suffer relatively little from\n",
            "overfitting, i.e., when a prompt is manually engineered to achieve low error on\n",
            "a given training set (thus rendering the method no longer actually zero-shot),\n",
            "the approach still performs well on held-out test data. In this paper, we show\n",
            "that we can explain such performance well via recourse to classical PAC-Bayes\n",
            "bounds. Specifically, we show that the discrete nature of prompts, combined\n",
            "with a PAC-Bayes prior given by a language model, results in generalization\n",
            "bounds that are remarkably tight by the standards of the literature: for\n",
            "instance, the generalization bound of an ImageNet classifier is often within a\n",
            "few percentage points of the true test error. We demonstrate empirically that\n",
            "this holds for existing handcrafted prompts and prompts generated through\n",
            "simple greedy search. Furthermore, the resulting bound is well-suited for model\n",
            "selection: the models with the best bound typically also have the best test\n",
            "performance. This work thus provides a possible justification for the\n",
            "widespread practice of prompt engineering, even if it seems that such methods\n",
            "could potentially overfit the training data.\n",
            "\n",
            "420. Title: Dictionary Contrastive Learning for Efficient Local Supervision without Auxiliary Networks\n",
            "   Abstract: Varying dynamics parameters in simulation is a popular Domain Randomization\n",
            "(DR) approach for overcoming the reality gap in Reinforcement Learning (RL).\n",
            "Nevertheless, DR heavily hinges on the choice of the sampling distribution of\n",
            "the dynamics parameters, since high variability is crucial to regularize the\n",
            "agent's behavior but notoriously leads to overly conservative policies when\n",
            "randomizing excessively. In this paper, we propose a novel approach to address\n",
            "sim-to-real transfer, which automatically shapes dynamics distributions during\n",
            "training in simulation without requiring real-world data. We introduce DOmain\n",
            "RAndomization via Entropy MaximizatiON (DORAEMON), a constrained optimization\n",
            "problem that directly maximizes the entropy of the training distribution while\n",
            "retaining generalization capabilities. In achieving this, DORAEMON gradually\n",
            "increases the diversity of sampled dynamics parameters as long as the\n",
            "probability of success of the current policy is sufficiently high. We\n",
            "empirically validate the consistent benefits of DORAEMON in obtaining highly\n",
            "adaptive and generalizable policies, i.e. solving the task at hand across the\n",
            "widest range of dynamics parameters, as opposed to representative baselines\n",
            "from the DR literature. Notably, we also demonstrate the Sim2Real applicability\n",
            "of DORAEMON through its successful zero-shot transfer in a robotic manipulation\n",
            "setup under unknown real-world parameters.\n",
            "\n",
            "421. Title: Investigating the Benefits of Projection Head for Representation Learning\n",
            "   Abstract: We investigate the training and performance of generative adversarial\n",
            "networks using the Maximum Mean Discrepancy (MMD) as critic, termed MMD GANs.\n",
            "As our main theoretical contribution, we clarify the situation with bias in GAN\n",
            "loss functions raised by recent work: we show that gradient estimators used in\n",
            "the optimization process for both MMD GANs and Wasserstein GANs are unbiased,\n",
            "but learning a discriminator based on samples leads to biased gradients for the\n",
            "generator parameters. We also discuss the issue of kernel choice for the MMD\n",
            "critic, and characterize the kernel corresponding to the energy distance used\n",
            "for the Cramer GAN critic. Being an integral probability metric, the MMD\n",
            "benefits from training strategies recently developed for Wasserstein GANs. In\n",
            "experiments, the MMD GAN is able to employ a smaller critic network than the\n",
            "Wasserstein GAN, resulting in a simpler and faster-training algorithm with\n",
            "matching performance. We also propose an improved measure of GAN convergence,\n",
            "the Kernel Inception Distance, and show how to use it to dynamically adapt\n",
            "learning rates during GAN training.\n",
            "\n",
            "422. Title: Orbit-Equivariant Graph Neural Networks\n",
            "   Abstract: Traditional convolutional neural networks are limited to handling Euclidean\n",
            "space data, overlooking the vast realm of real-life scenarios represented as\n",
            "graph data, including transportation networks, social networks, and reference\n",
            "networks. The pivotal step in transferring convolutional neural networks to\n",
            "graph data analysis and processing lies in the construction of graph\n",
            "convolutional operators and graph pooling operators. This comprehensive review\n",
            "article delves into the world of graph convolutional neural networks. Firstly,\n",
            "it elaborates on the fundamentals of graph convolutional neural networks.\n",
            "Subsequently, it elucidates the graph neural network models based on attention\n",
            "mechanisms and autoencoders, summarizing their application in node\n",
            "classification, graph classification, and link prediction along with the\n",
            "associated datasets.\n",
            "\n",
            "423. Title: FeatUp: A Model-Agnostic Framework for Features at Any Resolution\n",
            "   Abstract: We present a probabilistic modeling and inference framework for\n",
            "discriminative analysis dictionary learning under a weak supervision setting.\n",
            "Dictionary learning approaches have been widely used for tasks such as\n",
            "low-level signal denoising and restoration as well as high-level classification\n",
            "tasks, which can be applied to audio and image analysis. Synthesis dictionary\n",
            "learning aims at jointly learning a dictionary and corresponding sparse\n",
            "coefficients to provide accurate data representation. This approach is useful\n",
            "for denoising and signal restoration, but may lead to sub-optimal\n",
            "classification performance. By contrast, analysis dictionary learning provides\n",
            "a transform that maps data to a sparse discriminative representation suitable\n",
            "for classification. We consider the problem of analysis dictionary learning for\n",
            "time-series data under a weak supervision setting in which signals are assigned\n",
            "with a global label instead of an instantaneous label signal. We propose a\n",
            "discriminative probabilistic model that incorporates both label information and\n",
            "sparsity constraints on the underlying latent instantaneous label signal using\n",
            "cardinality control. We present the expectation maximization (EM) procedure for\n",
            "maximum likelihood estimation (MLE) of the proposed model. To facilitate a\n",
            "computationally efficient E-step, we propose both a chain and a novel tree\n",
            "graph reformulation of the graphical model. The performance of the proposed\n",
            "model is demonstrated on both synthetic and real-world data.\n",
            "\n",
            "424. Title: Improved algorithm and bounds for successive projection\n",
            "   Abstract: An effective technique for obtaining high-quality representations is adding a\n",
            "projection head on top of the encoder during training, then discarding it and\n",
            "using the pre-projection representations. Despite its proven practical\n",
            "effectiveness, the reason behind the success of this technique is poorly\n",
            "understood. The pre-projection representations are not directly optimized by\n",
            "the loss function, raising the question: what makes them better? In this work,\n",
            "we provide a rigorous theoretical answer to this question. We start by\n",
            "examining linear models trained with self-supervised contrastive loss. We\n",
            "reveal that the implicit bias of training algorithms leads to layer-wise\n",
            "progressive feature weighting, where features become increasingly unequal as we\n",
            "go deeper into the layers. Consequently, lower layers tend to have more\n",
            "normalized and less specialized representations. We theoretically characterize\n",
            "scenarios where such representations are more beneficial, highlighting the\n",
            "intricate interplay between data augmentation and input features. Additionally,\n",
            "we demonstrate that introducing non-linearity into the network allows lower\n",
            "layers to learn features that are completely absent in higher layers. Finally,\n",
            "we show how this mechanism improves the robustness in supervised contrastive\n",
            "learning and supervised learning. We empirically validate our results through\n",
            "various experiments on CIFAR-10/100, UrbanCars and shifted versions of\n",
            "ImageNet. We also introduce a potential alternative to projection head, which\n",
            "offers a more interpretable and controllable design.\n",
            "\n",
            "425. Title: Offline RL with Observation Histories: Analyzing and Improving Sample Complexity\n",
            "   Abstract: Given a $K$-vertex simplex in a $d$-dimensional space, suppose we measure $n$\n",
            "points on the simplex with noise (hence, some of the observed points fall\n",
            "outside the simplex). Vertex hunting is the problem of estimating the $K$\n",
            "vertices of the simplex. A popular vertex hunting algorithm is successive\n",
            "projection algorithm (SPA). However, SPA is observed to perform\n",
            "unsatisfactorily under strong noise or outliers. We propose pseudo-point SPA\n",
            "(pp-SPA). It uses a projection step and a denoise step to generate\n",
            "pseudo-points and feed them into SPA for vertex hunting. We derive error bounds\n",
            "for pp-SPA, leveraging on extreme value theory of (possibly) high-dimensional\n",
            "random vectors. The results suggest that pp-SPA has faster rates and better\n",
            "numerical performances than SPA. Our analysis includes an improved\n",
            "non-asymptotic bound for the original SPA, which is of independent interest.\n",
            "\n",
            "426. Title: Universal Jailbreak Backdoors from Poisoned Human Feedback\n",
            "   Abstract: Offline reinforcement learning (RL) can in principle synthesize more optimal\n",
            "behavior from a dataset consisting only of suboptimal trials. One way that this\n",
            "can happen is by \"stitching\" together the best parts of otherwise suboptimal\n",
            "trajectories that overlap on similar states, to create new behaviors where each\n",
            "individual state is in-distribution, but the overall returns are higher.\n",
            "However, in many interesting and complex applications, such as autonomous\n",
            "navigation and dialogue systems, the state is partially observed. Even worse,\n",
            "the state representation is unknown or not easy to define. In such cases,\n",
            "policies and value functions are often conditioned on observation histories\n",
            "instead of states. In these cases, it is not clear if the same kind of\n",
            "\"stitching\" is feasible at the level of observation histories, since two\n",
            "different trajectories would always have different histories, and thus \"similar\n",
            "states\" that might lead to effective stitching cannot be leveraged.\n",
            "Theoretically, we show that standard offline RL algorithms conditioned on\n",
            "observation histories suffer from poor sample complexity, in accordance with\n",
            "the above intuition. We then identify sufficient conditions under which offline\n",
            "RL can still be efficient -- intuitively, it needs to learn a compact\n",
            "representation of history comprising only features relevant for action\n",
            "selection. We introduce a bisimulation loss that captures the extent to which\n",
            "this happens, and propose that offline RL can explicitly optimize this loss to\n",
            "aid worst-case sample complexity. Empirically, we show that across a variety of\n",
            "tasks either our proposed loss improves performance, or the value of this loss\n",
            "is already minimized as a consequence of standard offline RL, indicating that\n",
            "it correlates well with good performance.\n",
            "\n",
            "427. Title: PINNACLE: PINN Adaptive ColLocation and Experimental points selection\n",
            "   Abstract: Reinforcement Learning from Human Feedback (RLHF) is used to align large\n",
            "language models to produce helpful and harmless responses. Yet, prior work\n",
            "showed these models can be jailbroken by finding adversarial prompts that\n",
            "revert the model to its unaligned behavior. In this paper, we consider a new\n",
            "threat where an attacker poisons the RLHF training data to embed a \"jailbreak\n",
            "backdoor\" into the model. The backdoor embeds a trigger word into the model\n",
            "that acts like a universal \"sudo command\": adding the trigger word to any\n",
            "prompt enables harmful responses without the need to search for an adversarial\n",
            "prompt. Universal jailbreak backdoors are much more powerful than previously\n",
            "studied backdoors on language models, and we find they are significantly harder\n",
            "to plant using common backdoor attack techniques. We investigate the design\n",
            "decisions in RLHF that contribute to its purported robustness, and release a\n",
            "benchmark of poisoned models to stimulate future research on universal\n",
            "jailbreak backdoors.\n",
            "\n",
            "428. Title: A Benchmark Study on Calibration\n",
            "   Abstract: Deep neural networks are increasingly utilized in various machine learning\n",
            "tasks. However, as these models grow in complexity, they often face calibration\n",
            "issues, despite enhanced prediction accuracy. Many studies have endeavored to\n",
            "improve calibration performance through the use of specific loss functions,\n",
            "data preprocessing and training frameworks. Yet, investigations into\n",
            "calibration properties have been somewhat overlooked. Our study leverages the\n",
            "Neural Architecture Search (NAS) search space, offering an exhaustive model\n",
            "architecture space for thorough calibration properties exploration. We\n",
            "specifically create a model calibration dataset. This dataset evaluates 90\n",
            "bin-based and 12 additional calibration measurements across 117,702 unique\n",
            "neural networks within the widely employed NATS-Bench search space. Our\n",
            "analysis aims to answer several longstanding questions in the field, using our\n",
            "proposed dataset: (i) Can model calibration be generalized across different\n",
            "datasets? (ii) Can robustness be used as a calibration measurement? (iii) How\n",
            "reliable are calibration metrics? (iv) Does a post-hoc calibration method\n",
            "affect all models uniformly? (v) How does calibration interact with accuracy?\n",
            "(vi) What is the impact of bin size on calibration measurement? (vii) Which\n",
            "architectural designs are beneficial for calibration? Additionally, our study\n",
            "bridges an existing gap by exploring calibration within NAS. By providing this\n",
            "dataset, we enable further research into NAS calibration. As far as we are\n",
            "aware, our research represents the first large-scale investigation into\n",
            "calibration properties and the premier study of calibration issues within NAS.\n",
            "The project page can be found at https://www.taolinwei.com/calibration-study\n",
            "\n",
            "429. Title: Increasing Model Capacity for Free: A Simple Strategy for Parameter Efficient Fine-tuning\n",
            "   Abstract: With the growth of popularity of facial micro-expressions in recent years,\n",
            "the demand for long videos with micro- and macro-expressions remains high.\n",
            "Extended from SAMM, a micro-expressions dataset released in 2016, this paper\n",
            "presents SAMM Long Videos dataset for spontaneous micro- and macro-expressions\n",
            "recognition and spotting. SAMM Long Videos dataset consists of 147 long videos\n",
            "with 343 macro-expressions and 159 micro-expressions. The dataset is FACS-coded\n",
            "with detailed Action Units (AUs). We compare our dataset with Chinese Academy\n",
            "of Sciences Macro-Expressions and Micro-Expressions (CAS(ME)2) dataset, which\n",
            "is the only available fully annotated dataset with micro- and\n",
            "macro-expressions. Furthermore, we preprocess the long videos using OpenFace,\n",
            "which includes face alignment and detection of facial AUs. We conduct facial\n",
            "expression spotting using this dataset and compare it with the baseline of MEGC\n",
            "III. Our spotting method outperformed the baseline result with F1-score of\n",
            "0.3299.\n",
            "\n",
            "430. Title: On the Humanity of Conversational AI: Evaluating the Psychological Portrayal of LLMs\n",
            "   Abstract: Fine-tuning large pre-trained foundation models, such as the 175B GPT-3, has\n",
            "attracted more attention for downstream tasks recently. While\n",
            "parameter-efficient fine-tuning methods have been proposed and proven effective\n",
            "without retraining all model parameters, their performance is limited by the\n",
            "capacity of incremental modules, especially under constrained parameter\n",
            "budgets. \\\\ To overcome this challenge, we propose CapaBoost, a simple yet\n",
            "effective strategy that enhances model capacity by leveraging low-rank updates\n",
            "through parallel weight modules in target layers. By applying static random\n",
            "masks to the shared weight matrix, CapaBoost constructs a diverse set of weight\n",
            "matrices, effectively increasing the rank of incremental weights without adding\n",
            "parameters. Notably, our approach can be seamlessly integrated into various\n",
            "existing parameter-efficient fine-tuning methods. We extensively validate the\n",
            "efficacy of CapaBoost through experiments on diverse downstream tasks,\n",
            "including natural language understanding, question answering, and image\n",
            "classification. Our results demonstrate significant improvements over\n",
            "baselines, without incurring additional computation or storage costs. Our code\n",
            "is available at \\url{https://github.com/LINs-lab/CapaBoost}.\n",
            "\n",
            "431. Title: Unified Projection-Free Algorithms for Adversarial DR-Submodular Optimization\n",
            "   Abstract: We propose a unified compression framework that uses generative adversarial\n",
            "networks (GAN) to compress image and speech signals. The compressed signal is\n",
            "represented by a latent vector fed into a generator network which is trained to\n",
            "produce high quality signals that minimize a target objective function. To\n",
            "efficiently quantize the compressed signal, non-uniformly quantized optimal\n",
            "latent vectors are identified by iterative back-propagation with ADMM\n",
            "optimization performed for each iteration. Our experiments show that the\n",
            "proposed algorithm outperforms prior signal compression methods for both image\n",
            "and speech compression quantified in various metrics including bit rate, PSNR,\n",
            "and neural network based signal classification accuracy.\n",
            "\n",
            "432. Title: DMV3D: Denoising Multi-view Diffusion Using 3D Large Reconstruction Model\n",
            "   Abstract: This study investigates the efficacy of Large Language Models (LLMs) in\n",
            "interactive language therapy for high-functioning autistic adolescents. With\n",
            "the rapid advancement of artificial intelligence, particularly in natural\n",
            "language processing, LLMs present a novel opportunity to augment traditional\n",
            "psychological counseling methods. This research primarily focuses on evaluating\n",
            "the LLM's ability to engage in empathetic, adaptable, and contextually\n",
            "appropriate interactions within a therapeutic setting. A comprehensive\n",
            "evaluation was conducted by a panel of clinical psychologists and psychiatrists\n",
            "using a specially developed scorecard. The assessment covered various aspects\n",
            "of the LLM's performance, including empathy, communication skills,\n",
            "adaptability, engagement, and the ability to establish a therapeutic alliance.\n",
            "The study avoided direct testing with patients, prioritizing privacy and\n",
            "ethical considerations, and instead relied on simulated scenarios to gauge the\n",
            "LLM's effectiveness. The results indicate that LLMs hold significant promise as\n",
            "supportive tools in therapy, demonstrating strengths in empathetic engagement\n",
            "and adaptability in conversation. However, challenges in achieving the depth of\n",
            "personalization and emotional understanding characteristic of human therapists\n",
            "were noted. The study also highlights the importance of ethical considerations\n",
            "in the application of AI in therapeutic contexts. This research provides\n",
            "valuable insights into the potential and limitations of using LLMs in\n",
            "psychological counseling for autistic adolescents. It lays the groundwork for\n",
            "future explorations into AI's role in mental health care, emphasizing the need\n",
            "for ongoing development to enhance the capabilities of these models in\n",
            "therapeutic settings.\n",
            "\n",
            "433. Title: Lewis's Signaling Game as beta-VAE For Natural Word Lengths and Segments\n",
            "   Abstract: Maximizing the log-likelihood is a crucial aspect of learning latent variable\n",
            "models, and variational inference (VI) stands as the commonly adopted method.\n",
            "However, VI can encounter challenges in achieving a high log-likelihood when\n",
            "dealing with complicated posterior distributions. In response to this\n",
            "limitation, we introduce a novel variational importance sampling (VIS) approach\n",
            "that directly estimates and maximizes the log-likelihood. VIS leverages the\n",
            "optimal proposal distribution, achieved by minimizing the forward $\\chi^2$\n",
            "divergence, to enhance log-likelihood estimation. We apply VIS to various\n",
            "popular latent variable models, including mixture models, variational\n",
            "auto-encoders, and partially observable generalized linear models. Results\n",
            "demonstrate that our approach consistently outperforms state-of-the-art\n",
            "baselines, both in terms of log-likelihood and model parameter estimation.\n",
            "\n",
            "434. Title: Forward $\\chi^2$ Divergence Based Variational Importance Sampling\n",
            "   Abstract: As a sub-discipline of evolutionary and computational linguistics, emergent\n",
            "communication (EC) studies communication protocols, called emergent languages,\n",
            "arising in simulations where agents communicate. A key goal of EC is to give\n",
            "rise to languages that share statistical properties with natural languages. In\n",
            "this paper, we reinterpret Lewis's signaling game, a frequently used setting in\n",
            "EC, as beta-VAE and reformulate its objective function as ELBO. Consequently,\n",
            "we clarify the existence of prior distributions of emergent languages and show\n",
            "that the choice of the priors can influence their statistical properties.\n",
            "Specifically, we address the properties of word lengths and segmentation, known\n",
            "as Zipf's law of abbreviation (ZLA) and Harris's articulation scheme (HAS),\n",
            "respectively. It has been reported that the emergent languages do not follow\n",
            "them when using the conventional objective. We experimentally demonstrate that\n",
            "by selecting an appropriate prior distribution, more natural segments emerge,\n",
            "while suggesting that the conventional one prevents the languages from\n",
            "following ZLA and HAS.\n",
            "\n",
            "435. Title: \"What Data Benefits My Classifier?\" Enhancing Model Performance and Interpretability through Influence-Based Data Selection\n",
            "   Abstract: Many complex tasks can be decomposed into simpler, independent parts.\n",
            "Discovering such underlying compositional structure has the potential to enable\n",
            "compositional generalization. Despite progress, our most powerful systems\n",
            "struggle to compose flexibly. It therefore seems natural to make models more\n",
            "modular to help capture the compositional nature of many tasks. However, it is\n",
            "unclear under which circumstances modular systems can discover hidden\n",
            "compositional structure. To shed light on this question, we study a\n",
            "teacher-student setting with a modular teacher where we have full control over\n",
            "the composition of ground truth modules. This allows us to relate the problem\n",
            "of compositional generalization to that of identification of the underlying\n",
            "modules. In particular we study modularity in hypernetworks representing a\n",
            "general class of multiplicative interactions. We show theoretically that\n",
            "identification up to linear transformation purely from demonstrations is\n",
            "possible without having to learn an exponential number of module combinations.\n",
            "We further demonstrate empirically that under the theoretically identified\n",
            "conditions, meta-learning from finite data can discover modular policies that\n",
            "generalize compositionally in a number of complex environments.\n",
            "\n",
            "436. Title: Temporal Generalization Estimation in Evolving Graphs\n",
            "   Abstract: Prediction accuracy and model explainability are the two most important\n",
            "objectives when developing machine learning algorithms to solve real-world\n",
            "problems. The neural networks are known to possess good prediction performance,\n",
            "but lack of sufficient model interpretability. In this paper, we propose to\n",
            "enhance the explainability of neural networks through the following\n",
            "architecture constraints: a) sparse additive subnetworks; b) projection pursuit\n",
            "with orthogonality constraint; and c) smooth function approximation. It leads\n",
            "to an explainable neural network (xNN) with the superior balance between\n",
            "prediction performance and model interpretability. We derive the necessary and\n",
            "sufficient identifiability conditions for the proposed xNN model. The multiple\n",
            "parameters are simultaneously estimated by a modified mini-batch gradient\n",
            "descent method based on the backpropagation algorithm for calculating the\n",
            "derivatives and the Cayley transform for preserving the projection\n",
            "orthogonality. Through simulation study under six different scenarios, we\n",
            "compare the proposed method to several benchmarks including least absolute\n",
            "shrinkage and selection operator, support vector machine, random forest,\n",
            "extreme learning machine, and multi-layer perceptron. It is shown that the\n",
            "proposed xNN model keeps the flexibility of pursuing high prediction accuracy\n",
            "while attaining improved interpretability. Finally, a real data example is\n",
            "employed as a showcase application.\n",
            "\n",
            "437. Title: SparseDFF: Sparse-View Feature Distillation for One-Shot Dexterous Manipulation\n",
            "   Abstract: Graph Neural Networks (GNNs) are widely deployed in vast fields, but they\n",
            "often struggle to maintain accurate representations as graphs evolve. We\n",
            "theoretically establish a lower bound, proving that under mild conditions,\n",
            "representation distortion inevitably occurs over time. To estimate the temporal\n",
            "distortion without human annotation after deployment, one naive approach is to\n",
            "pre-train a recurrent model (e.g., RNN) before deployment and use this model\n",
            "afterwards, but the estimation is far from satisfactory. In this paper, we\n",
            "analyze the representation distortion from an information theory perspective,\n",
            "and attribute it primarily to inaccurate feature extraction during evolution.\n",
            "Consequently, we introduce Smart, a straightforward and effective baseline\n",
            "enhanced by an adaptive feature extractor through self-supervised graph\n",
            "reconstruction. In synthetic random graphs, we further refine the former lower\n",
            "bound to show the inevitable distortion over time and empirically observe that\n",
            "Smart achieves good estimation performance. Moreover, we observe that Smart\n",
            "consistently shows outstanding generalization estimation on four real-world\n",
            "evolving graphs. The ablation studies underscore the necessity of graph\n",
            "reconstruction. For example, on OGB-arXiv dataset, the estimation metric MAPE\n",
            "deteriorates from 2.19% to 8.00% without reconstruction.\n",
            "\n",
            "438. Title: Looped Transformers are Better at Learning Learning Algorithms\n",
            "   Abstract: Transformers have demonstrated effectiveness in in-context solving\n",
            "data-fitting problems from various (latent) models, as reported by Garg et al.\n",
            "However, the absence of an inherent iterative structure in the transformer\n",
            "architecture presents a challenge in emulating the iterative algorithms, which\n",
            "are commonly employed in traditional machine learning methods. To address this,\n",
            "we propose the utilization of looped transformer architecture and its\n",
            "associated training methodology, with the aim of incorporating iterative\n",
            "characteristics into the transformer architectures. Experimental results\n",
            "suggest that the looped transformer achieves performance comparable to the\n",
            "standard transformer in solving various data-fitting problems, while utilizing\n",
            "less than 10% of the parameter count.\n",
            "\n",
            "439. Title: Frequency-Aware Transformer for Learned Image Compression\n",
            "   Abstract: Learned image compression have attracted considerable interests in recent\n",
            "years. It typically comprises an analysis transform, a synthesis transform,\n",
            "quantization and an entropy coding model. The analysis transform and synthesis\n",
            "transform are used to encode an image to latent feature and decode the\n",
            "quantized feature to reconstruct the image, and can be regarded as coupled\n",
            "transforms. However, the analysis transform and synthesis transform are\n",
            "designed independently in the existing methods, making them unreliable in\n",
            "high-quality image compression. Inspired by the invertible neural networks in\n",
            "generative modeling, invertible modules are used to construct the coupled\n",
            "analysis and synthesis transforms. Considering the noise introduced in the\n",
            "feature quantization invalidates the invertible process, this paper proposes an\n",
            "Approximately Invertible Neural Network (A-INN) framework for learned image\n",
            "compression. It formulates the rate-distortion optimization in lossy image\n",
            "compression when using INN with quantization, which differentiates from using\n",
            "INN for generative modelling. Generally speaking, A-INN can be used as the\n",
            "theoretical foundation for any INN based lossy compression method. Based on\n",
            "this formulation, A-INN with a progressive denoising module (PDM) is developed\n",
            "to effectively reduce the quantization noise in the decoding. Moreover, a\n",
            "Cascaded Feature Recovery Module (CFRM) is designed to learn high-dimensional\n",
            "feature recovery from low-dimensional ones to further reduce the noise in\n",
            "feature channel compression. In addition, a Frequency-enhanced Decomposition\n",
            "and Synthesis Module (FDSM) is developed by explicitly enhancing the\n",
            "high-frequency components in an image to address the loss of high-frequency\n",
            "information inherent in neural network based image compression. Extensive\n",
            "experiments demonstrate that the proposed A-INN outperforms the existing\n",
            "learned image compression methods.\n",
            "\n",
            "440. Title: Symmetric Basis Convolutions for Learning Lagrangian Fluid Mechanics\n",
            "   Abstract: Learning physical simulations has been an essential and central aspect of\n",
            "many recent research efforts in machine learning, particularly for\n",
            "Navier-Stokes-based fluid mechanics. Classic numerical solvers have\n",
            "traditionally been computationally expensive and challenging to use in inverse\n",
            "problems, whereas Neural solvers aim to address both concerns through machine\n",
            "learning. We propose a general formulation for continuous convolutions using\n",
            "separable basis functions as a superset of existing methods and evaluate a\n",
            "large set of basis functions in the context of (a) a compressible 1D SPH\n",
            "simulation, (b) a weakly compressible 2D SPH simulation, and (c) an\n",
            "incompressible 2D SPH Simulation. We demonstrate that even and odd symmetries\n",
            "included in the basis functions are key aspects of stability and accuracy. Our\n",
            "broad evaluation shows that Fourier-based continuous convolutions outperform\n",
            "all other architectures regarding accuracy and generalization. Finally, using\n",
            "these Fourier-based networks, we show that prior inductive biases, such as\n",
            "window functions, are no longer necessary. An implementation of our approach,\n",
            "as well as complete datasets and solver implementations, is available at\n",
            "https://github.com/tum-pbs/SFBC.\n",
            "\n",
            "441. Title: Efficiently Computing Similarities to Private Datasets\n",
            "   Abstract: Offline reinforcement learning (RL) is a compelling framework for learning\n",
            "optimal policies from past experiences without additional interaction with the\n",
            "environment. Nevertheless, offline RL inevitably faces the problem of\n",
            "distributional shifts, where the states and actions encountered during policy\n",
            "execution may not be in the training dataset distribution. A common solution\n",
            "involves incorporating conservatism into the policy or the value function to\n",
            "safeguard against uncertainties and unknowns. In this work, we focus on\n",
            "achieving the same objectives of conservatism but from a different perspective.\n",
            "We propose COmpositional COnservatism with Anchor-seeking (COCOA) for offline\n",
            "RL, an approach that pursues conservatism in a compositional manner on top of\n",
            "the transductive reparameterization (Netanyahu et al., 2023), which decomposes\n",
            "the input variable (the state in our case) into an anchor and its difference\n",
            "from the original input. Our COCOA seeks both in-distribution anchors and\n",
            "differences by utilizing the learned reverse dynamics model, encouraging\n",
            "conservatism in the compositional input space for the policy or value function.\n",
            "Such compositional conservatism is independent of and agnostic to the prevalent\n",
            "behavioral conservatism in offline RL. We apply COCOA to four state-of-the-art\n",
            "offline RL algorithms and evaluate them on the D4RL benchmark, where COCOA\n",
            "generally improves the performance of each algorithm. The code is available at\n",
            "https://github.com/runamu/compositional-conservatism.\n",
            "\n",
            "442. Title: Compositional Conservatism: A Transductive Approach in Offline Reinforcement Learning\n",
            "   Abstract: Designing expressive Graph Neural Networks (GNNs) is a fundamental topic in\n",
            "the graph learning community. So far, GNN expressiveness has been primarily\n",
            "assessed via the Weisfeiler-Lehman (WL) hierarchy. However, such an\n",
            "expressivity measure has notable limitations: it is inherently coarse,\n",
            "qualitative, and may not well reflect practical requirements (e.g., the ability\n",
            "to encode substructures). In this paper, we introduce a unified framework for\n",
            "quantitatively studying the expressiveness of GNN architectures, addressing all\n",
            "the above limitations. Specifically, we identify a fundamental expressivity\n",
            "measure termed homomorphism expressivity, which quantifies the ability of GNN\n",
            "models to count graphs under homomorphism. Homomorphism expressivity offers a\n",
            "complete and practical assessment tool: the completeness enables direct\n",
            "expressivity comparisons between GNN models, while the practicality allows for\n",
            "understanding concrete GNN abilities such as subgraph counting. By examining\n",
            "four classes of prominent GNNs as case studies, we derive simple, unified, and\n",
            "elegant descriptions of their homomorphism expressivity for both invariant and\n",
            "equivariant settings. Our results provide novel insights into a series of\n",
            "previous work, unify the landscape of different subareas in the community, and\n",
            "settle several open questions. Empirically, extensive experiments on both\n",
            "synthetic and real-world tasks verify our theory, showing that the practical\n",
            "performance of GNN models aligns well with the proposed metric.\n",
            "\n",
            "443. Title: Beyond Weisfeiler-Lehman: A Quantitative Framework for GNN Expressiveness\n",
            "   Abstract: With the emergence of large-scale models trained on diverse datasets,\n",
            "in-context learning has emerged as a promising paradigm for multitasking,\n",
            "notably in natural language processing and image processing. However, its\n",
            "application in 3D point cloud tasks remains largely unexplored. In this work,\n",
            "we introduce Point-In-Context (PIC), a novel framework for 3D point cloud\n",
            "understanding via in-context learning. We address the technical challenge of\n",
            "effectively extending masked point modeling to 3D point clouds by introducing a\n",
            "Joint Sampling module and proposing a vanilla version of PIC called\n",
            "Point-In-Context-Generalist (PIC-G). PIC-G is designed as a generalist model\n",
            "for various 3D point cloud tasks, with inputs and outputs modeled as\n",
            "coordinates. In this paradigm, the challenging segmentation task is achieved by\n",
            "assigning label points with XYZ coordinates for each category; the final\n",
            "prediction is then chosen based on the label point closest to the predictions.\n",
            "To break the limitation by the fixed label-coordinate assignment, which has\n",
            "poor generalization upon novel classes, we propose two novel training\n",
            "strategies, In-Context Labeling and In-Context Enhancing, forming an extended\n",
            "version of PIC named Point-In-Context-Segmenter (PIC-S), targeting improving\n",
            "dynamic context labeling and model training. By utilizing dynamic in-context\n",
            "labels and extra in-context pairs, PIC-S achieves enhanced performance and\n",
            "generalization capability in and across part segmentation datasets. PIC is a\n",
            "general framework so that other tasks or datasets can be seamlessly introduced\n",
            "into our PIC through a unified data format. We conduct extensive experiments to\n",
            "validate the versatility and adaptability of our proposed methods in handling a\n",
            "wide range of tasks and segmenting multi-datasets. Our PIC-S is capable of\n",
            "generalizing unseen datasets and performing novel part segmentation by\n",
            "customizing prompts.\n",
            "\n",
            "444. Title: Learning to Compose: Improving Object Centric Learning by Injecting Compositionality\n",
            "   Abstract: Generative learning, recognized for its effective modeling of data\n",
            "distributions, offers inherent advantages in handling out-of-distribution\n",
            "instances, especially for enhancing robustness to adversarial attacks. Among\n",
            "these, diffusion classifiers, utilizing powerful diffusion models, have\n",
            "demonstrated superior empirical robustness. However, a comprehensive\n",
            "theoretical understanding of their robustness is still lacking, raising\n",
            "concerns about their vulnerability to stronger future attacks. In this study,\n",
            "we prove that diffusion classifiers possess $O(1)$ Lipschitzness, and establish\n",
            "their certified robustness, demonstrating their inherent resilience. To achieve\n",
            "non-constant Lipschitzness, thereby obtaining much tighter certified\n",
            "robustness, we generalize diffusion classifiers to classify Gaussian-corrupted\n",
            "data. This involves deriving the evidence lower bounds (ELBOs) for these\n",
            "distributions, approximating the likelihood using the ELBO, and calculating\n",
            "classification probabilities via Bayes' theorem. Experimental results show the\n",
            "superior certified robustness of these Noised Diffusion Classifiers (NDCs).\n",
            "Notably, we achieve over 80% and 70% certified robustness on CIFAR-10 under\n",
            "adversarial perturbations with \\(\\ell_2\\) norms less than 0.25 and 0.5,\n",
            "respectively, using a single off-the-shelf diffusion model without any\n",
            "additional data.\n",
            "\n",
            "445. Title: In-Context Learning through the Bayesian Prism\n",
            "   Abstract: Many methods in differentially private model training rely on computing the\n",
            "similarity between a query point (such as public or synthetic data) and private\n",
            "data. We abstract out this common subroutine and study the following\n",
            "fundamental algorithmic problem: Given a similarity function $f$ and a large\n",
            "high-dimensional private dataset $X \\subset \\mathbb{R}^d$, output a\n",
            "differentially private (DP) data structure which approximates $\\sum_{x \\in X}\n",
            "f(x,y)$ for any query $y$. We consider the cases where $f$ is a kernel\n",
            "function, such as $f(x,y) = e^{-\\|x-y\\|_2^2/\\sigma^2}$ (also known as DP kernel\n",
            "density estimation), or a distance function such as $f(x,y) = \\|x-y\\|_2$, among\n",
            "others.\n",
            "  Our theoretical results improve upon prior work and give better\n",
            "privacy-utility trade-offs as well as faster query times for a wide range of\n",
            "kernels and distance functions. The unifying approach behind our results is\n",
            "leveraging `low-dimensional structures' present in the specific functions $f$\n",
            "that we study, using tools such as provable dimensionality reduction,\n",
            "approximation theory, and one-dimensional decomposition of the functions. Our\n",
            "algorithms empirically exhibit improved query times and accuracy over prior\n",
            "state of the art. We also present an application to DP classification. Our\n",
            "experiments demonstrate that the simple methodology of classifying based on\n",
            "average similarity is orders of magnitude faster than prior DP-SGD based\n",
            "approaches for comparable accuracy.\n",
            "\n",
            "446. Title: Diffusion Sampling with Momentum for Mitigating Divergence Artifacts\n",
            "   Abstract: Learning compositional representation is a key aspect of object-centric\n",
            "learning as it enables flexible systematic generalization and supports complex\n",
            "visual reasoning. However, most of the existing approaches rely on\n",
            "auto-encoding objective, while the compositionality is implicitly imposed by\n",
            "the architectural or algorithmic bias in the encoder. This misalignment between\n",
            "auto-encoding objective and learning compositionality often results in failure\n",
            "of capturing meaningful object representations. In this study, we propose a\n",
            "novel objective that explicitly encourages compositionality of the\n",
            "representations. Built upon the existing object-centric learning framework\n",
            "(e.g., slot attention), our method incorporates additional constraints that an\n",
            "arbitrary mixture of object representations from two images should be valid by\n",
            "maximizing the likelihood of the composite data. We demonstrate that\n",
            "incorporating our objective to the existing framework consistently improves the\n",
            "objective-centric learning and enhances the robustness to the architectural\n",
            "choices.\n",
            "\n",
            "447. Title: FairerCLIP: Debiasing CLIP's Zero-Shot Predictions using Functions in RKHSs\n",
            "   Abstract: Despite the remarkable success of diffusion models in image generation, slow\n",
            "sampling remains a persistent issue. To accelerate the sampling process, prior\n",
            "studies have reformulated diffusion sampling as an ODE/SDE and introduced\n",
            "higher-order numerical methods. However, these methods often produce divergence\n",
            "artifacts, especially with a low number of sampling steps, which limits the\n",
            "achievable acceleration. In this paper, we investigate the potential causes of\n",
            "these artifacts and suggest that the small stability regions of these methods\n",
            "could be the principal cause. To address this issue, we propose two novel\n",
            "techniques. The first technique involves the incorporation of Heavy Ball (HB)\n",
            "momentum, a well-known technique for improving optimization, into existing\n",
            "diffusion numerical methods to expand their stability regions. We also prove\n",
            "that the resulting methods have first-order convergence. The second technique,\n",
            "called Generalized Heavy Ball (GHVB), constructs a new high-order method that\n",
            "offers a variable trade-off between accuracy and artifact suppression.\n",
            "Experimental results show that our techniques are highly effective in reducing\n",
            "artifacts and improving image quality, surpassing state-of-the-art diffusion\n",
            "solvers on both pixel-based and latent-based diffusion models for low-step\n",
            "sampling. Our research provides novel insights into the design of numerical\n",
            "methods for future diffusion work.\n",
            "\n",
            "448. Title: Neural Language of Thought Models\n",
            "   Abstract: The Language of Thought Hypothesis suggests that human cognition operates on\n",
            "a structured, language-like system of mental representations. While neural\n",
            "language models can naturally benefit from the compositional structure\n",
            "inherently and explicitly expressed in language data, learning such\n",
            "representations from non-linguistic general observations, like images, remains\n",
            "a challenge. In this work, we introduce the Neural Language of Thought Model\n",
            "(NLoTM), a novel approach for unsupervised learning of LoTH-inspired\n",
            "representation and generation. NLoTM comprises two key components: (1) the\n",
            "Semantic Vector-Quantized Variational Autoencoder, which learns hierarchical,\n",
            "composable discrete representations aligned with objects and their properties,\n",
            "and (2) the Autoregressive LoT Prior, an autoregressive transformer that learns\n",
            "to generate semantic concept tokens compositionally, capturing the underlying\n",
            "data distribution. We evaluate NLoTM on several 2D and 3D image datasets,\n",
            "demonstrating superior performance in downstream tasks, out-of-distribution\n",
            "generalization, and image generation quality compared to patch-based VQ-VAE and\n",
            "continuous object-centric representations. Our work presents a significant step\n",
            "towards creating neural networks exhibiting more human-like understanding by\n",
            "developing LoT-like representations and offers insights into the intersection\n",
            "of cognitive science and machine learning.\n",
            "\n",
            "449. Title: Set Learning for Accurate and Calibrated Models\n",
            "   Abstract: Predictors map individual instances in a population to the interval $[0,1]$.\n",
            "For a collection $\\mathcal C$ of subsets of a population, a predictor is\n",
            "multi-calibrated with respect to $\\mathcal C$ if it is simultaneously\n",
            "calibrated on each set in $\\mathcal C$. We initiate the study of the\n",
            "construction of scaffolding sets, a small collection $\\mathcal S$ of sets with\n",
            "the property that multi-calibration with respect to $\\mathcal S$ ensures\n",
            "correctness, and not just calibration, of the predictor. Our approach is\n",
            "inspired by the folk wisdom that the intermediate layers of a neural net learn\n",
            "a highly structured and useful data representation.\n",
            "\n",
            "450. Title: Principled Architecture-aware Scaling of Hyperparameters\n",
            "   Abstract: A simple nonlinear transmission-line model of the cochlea with longitudinal\n",
            "coupling is introduced that can reproduce Basilar membrane response and neural\n",
            "tuning in the chinchilla. It is found that the middle ear has little effect on\n",
            "cochlear resonances, and hence conclude that the theory of coherent reflections\n",
            "is not applicable to the model. The model also provides an explanation of the\n",
            "emergence of spontaneous otoacoustic emissions (SOAEs). It is argued that SOAEs\n",
            "arise from Hopf bifurcations of the transmission-line model and not from\n",
            "localized instabilities. The paper shows that emissions can become chaotic,\n",
            "intermittent and fragile to perturbations.\n",
            "\n",
            "451. Title: Towards Best Practices of Activation Patching in Language Models: Metrics and Methods\n",
            "   Abstract: Spatial relationships between objects represent key scene information for\n",
            "humans to understand and interact with the world. To study the capability of\n",
            "current computer vision systems to recognize physically grounded spatial\n",
            "relations, we start by proposing precise relation definitions that permit\n",
            "consistently annotating a benchmark dataset. Despite the apparent simplicity of\n",
            "this task relative to others in the recognition literature, we observe that\n",
            "existing approaches perform poorly on this benchmark. We propose new approaches\n",
            "exploiting the long-range attention capabilities of transformers for this task,\n",
            "and evaluating key design principles. We identify a simple \"RelatiViT\"\n",
            "architecture and demonstrate that it outperforms all current approaches. To our\n",
            "knowledge, this is the first method to convincingly outperform naive baselines\n",
            "on spatial relation prediction in in-the-wild settings. The code and datasets\n",
            "are available in \\url{https://sites.google.com/view/spatial-relation}.\n",
            "\n",
            "452. Title: Novel Quadratic Constraints for Extending LipSDP beyond Slope-Restricted Activations\n",
            "   Abstract: Training a high-quality deep neural network requires choosing suitable\n",
            "hyperparameters, which is a non-trivial and expensive process. Current works\n",
            "try to automatically optimize or design principles of hyperparameters, such\n",
            "that they can generalize to diverse unseen scenarios. However, most designs or\n",
            "optimization methods are agnostic to the choice of network structures, and thus\n",
            "largely ignore the impact of neural architectures on hyperparameters. In this\n",
            "work, we precisely characterize the dependence of initializations and maximal\n",
            "learning rates on the network architecture, which includes the network depth,\n",
            "width, convolutional kernel size, and connectivity patterns. By pursuing every\n",
            "parameter to be maximally updated with the same mean squared change in\n",
            "pre-activations, we can generalize our initialization and learning rates across\n",
            "MLPs (multi-layer perception) and CNNs (convolutional neural network) with\n",
            "sophisticated graph topologies. We verify our principles with comprehensive\n",
            "experiments. More importantly, our strategy further sheds light on advancing\n",
            "current benchmarks for architecture design. A fair comparison of AutoML\n",
            "algorithms requires accurate network rankings. However, we demonstrate that\n",
            "network rankings can be easily changed by better training networks in\n",
            "benchmarks with our architecture-aware learning rates and initialization.\n",
            "\n",
            "453. Title: SGD Finds then Tunes Features in Two-Layer Neural Networks with near-Optimal Sample Complexity: A Case Study in the XOR problem\n",
            "   Abstract: Mechanistic interpretability seeks to understand the internal mechanisms of\n",
            "machine learning models, where localization -- identifying the important model\n",
            "components -- is a key step. Activation patching, also known as causal tracing\n",
            "or interchange intervention, is a standard technique for this task (Vig et al.,\n",
            "2020), but the literature contains many variants with little consensus on the\n",
            "choice of hyperparameters or methodology. In this work, we systematically\n",
            "examine the impact of methodological details in activation patching, including\n",
            "evaluation metrics and corruption methods. In several settings of localization\n",
            "and circuit discovery in language models, we find that varying these\n",
            "hyperparameters could lead to disparate interpretability results. Backed by\n",
            "empirical observations, we give conceptual arguments for why certain metrics or\n",
            "methods may be preferred. Finally, we provide recommendations for the best\n",
            "practices of activation patching going forwards.\n",
            "\n",
            "454. Title: Can Transformers Capture Spatial Relations between Objects?\n",
            "   Abstract: Recent representation learning approaches enhance neural topic models by\n",
            "optimizing the weighted linear combination of the evidence lower bound (ELBO)\n",
            "of the log-likelihood and the contrastive learning objective that contrasts\n",
            "pairs of input documents. However, document-level contrastive learning might\n",
            "capture low-level mutual information, such as word ratio, which disturbs topic\n",
            "modeling. Moreover, there is a potential conflict between the ELBO loss that\n",
            "memorizes input details for better reconstruction quality, and the contrastive\n",
            "loss which attempts to learn topic representations that generalize among input\n",
            "documents. To address these issues, we first introduce a novel contrastive\n",
            "learning method oriented towards sets of topic vectors to capture useful\n",
            "semantics that are shared among a set of input documents. Secondly, we\n",
            "explicitly cast contrastive topic modeling as a gradient-based multi-objective\n",
            "optimization problem, with the goal of achieving a Pareto stationary solution\n",
            "that balances the trade-off between the ELBO and the contrastive objective.\n",
            "Extensive experiments demonstrate that our framework consistently produces\n",
            "higher-performing neural topic models in terms of topic coherence, topic\n",
            "diversity, and downstream performance.\n",
            "\n",
            "455. Title: Towards a statistical theory of data selection under weak supervision\n",
            "   Abstract: Recently, semidefinite programming (SDP) techniques have shown great promise\n",
            "in providing accurate Lipschitz bounds for neural networks. Specifically, the\n",
            "LipSDP approach (Fazlyab et al., 2019) has received much attention and provides\n",
            "the least conservative Lipschitz upper bounds that can be computed with\n",
            "polynomial time guarantees. However, one main restriction of LipSDP is that its\n",
            "formulation requires the activation functions to be slope-restricted on\n",
            "$[0,1]$, preventing its further use for more general activation functions such\n",
            "as GroupSort, MaxMin, and Householder. One can rewrite MaxMin activations for\n",
            "example as residual ReLU networks. However, a direct application of LipSDP to\n",
            "the resultant residual ReLU networks is conservative and even fails in\n",
            "recovering the well-known fact that the MaxMin activation is 1-Lipschitz. Our\n",
            "paper bridges this gap and extends LipSDP beyond slope-restricted activation\n",
            "functions. To this end, we provide novel quadratic constraints for GroupSort,\n",
            "MaxMin, and Householder activations via leveraging their underlying properties\n",
            "such as sum preservation. Our proposed analysis is general and provides a\n",
            "unified approach for estimating $\\ell_2$ and $\\ell_\\infty$ Lipschitz bounds for\n",
            "a rich class of neural network architectures, including non-residual and\n",
            "residual neural networks and implicit models, with GroupSort, MaxMin, and\n",
            "Householder activations. Finally, we illustrate the utility of our approach\n",
            "with a variety of experiments and show that our proposed SDPs generate less\n",
            "conservative Lipschitz bounds in comparison to existing approaches.\n",
            "\n",
            "456. Title: Dynamic Neural Response Tuning\n",
            "   Abstract: In this paper we focus on the problem of finding the optimal weights of the\n",
            "shallowest of neural networks consisting of a single Rectified Linear Unit\n",
            "(ReLU). These functions are of the form $\\mathbf{x}\\rightarrow\n",
            "\\max(0,\\langle\\mathbf{w},\\mathbf{x}\\rangle)$ with $\\mathbf{w}\\in\\mathbb{R}^d$\n",
            "denoting the weight vector. We focus on a planted model where the inputs are\n",
            "chosen i.i.d. from a Gaussian distribution and the labels are generated\n",
            "according to a planted weight vector. We first show that mini-batch stochastic\n",
            "gradient descent when suitably initialized, converges at a geometric rate to\n",
            "the planted model with a number of samples that is optimal up to numerical\n",
            "constants. Next we focus on a parallel implementation where in each iteration\n",
            "the mini-batch gradient is calculated in a distributed manner across multiple\n",
            "processors and then broadcast to a master or all other processors. To reduce\n",
            "the communication cost in this setting we utilize a Quanitzed Stochastic\n",
            "Gradient Scheme (QSGD) where the partial gradients are quantized. Perhaps\n",
            "unexpectedly, we show that QSGD maintains the fast convergence of SGD to a\n",
            "globally optimal model while significantly reducing the communication cost. We\n",
            "further corroborate our numerical findings via various experiments including\n",
            "distributed implementations over Amazon EC2.\n",
            "\n",
            "457. Title: Self-Guided Masked Autoencoders for Domain-Agnostic Self-Supervised Learning\n",
            "   Abstract: The field of imbalanced self-supervised learning, especially in the context\n",
            "of tabular data, has not been extensively studied. Existing research has\n",
            "predominantly focused on image datasets. This paper aims to fill this gap by\n",
            "examining the specific challenges posed by data imbalance in self-supervised\n",
            "learning in the domain of tabular data, with a primary focus on autoencoders.\n",
            "Autoencoders are widely employed for learning and constructing a new\n",
            "representation of a dataset, particularly for dimensionality reduction. They\n",
            "are also often used for generative model learning, as seen in variational\n",
            "autoencoders. When dealing with mixed tabular data, qualitative variables are\n",
            "often encoded using a one-hot encoder with a standard loss function (MSE or\n",
            "Cross Entropy). In this paper, we analyze the drawbacks of this approach,\n",
            "especially when categorical variables are imbalanced. We propose a novel metric\n",
            "to balance learning: a Multi-Supervised Balanced MSE. This approach reduces the\n",
            "reconstruction error by balancing the influence of variables. Finally, we\n",
            "empirically demonstrate that this new metric, compared to the standard MSE: i)\n",
            "outperforms when the dataset is imbalanced, especially when the learning\n",
            "process is insufficient, and ii) provides similar results in the opposite case.\n",
            "\n",
            "458. Title: Chain-of-Experts: When LLMs Meet Complex Operations Research Problems\n",
            "   Abstract: Given a sample of size $N$, it is often useful to select a subsample of\n",
            "smaller size $n<N$ to be used for statistical estimation or learning. Such a\n",
            "data selection step is useful to reduce the requirements of data labeling and\n",
            "the computational complexity of learning. We assume to be given $N$ unlabeled\n",
            "samples $\\{{\\boldsymbol x}_i\\}_{i\\le N}$, and to be given access to a\n",
            "`surrogate model' that can predict labels $y_i$ better than random guessing.\n",
            "Our goal is to select a subset of the samples, to be denoted by $\\{{\\boldsymbol\n",
            "x}_i\\}_{i\\in G}$, of size $|G|=n<N$. We then acquire labels for this set and we\n",
            "use them to train a model via regularized empirical risk minimization.\n",
            "  By using a mixture of numerical experiments on real and synthetic data, and\n",
            "mathematical derivations under low- and high- dimensional asymptotics, we show\n",
            "that: $(i)$~Data selection can be very effective, in particular beating\n",
            "training on the full sample in some cases; $(ii)$~Certain popular choices in\n",
            "data selection methods (e.g. unbiased reweighted subsampling, or influence\n",
            "function-based subsampling) can be substantially suboptimal.\n",
            "\n",
            "459. Title: Oracle Efficient Algorithms for Groupwise Regret\n",
            "   Abstract: We study the problem of online prediction, in which at each time step $t$, an\n",
            "individual $x_t$ arrives, whose label we must predict. Each individual is\n",
            "associated with various groups, defined based on their features such as age,\n",
            "sex, race etc., which may intersect. Our goal is to make predictions that have\n",
            "regret guarantees not just overall but also simultaneously on each sub-sequence\n",
            "comprised of the members of any single group. Previous work such as [Blum &\n",
            "Lykouris] and [Lee et al] provide attractive regret guarantees for these\n",
            "problems; however, these are computationally intractable on large model\n",
            "classes. We show that a simple modification of the sleeping experts technique\n",
            "of [Blum & Lykouris] yields an efficient reduction to the well-understood\n",
            "problem of obtaining diminishing external regret absent group considerations.\n",
            "Our approach gives similar regret guarantees compared to [Blum & Lykouris];\n",
            "however, we run in time linear in the number of groups, and are\n",
            "oracle-efficient in the hypothesis class. This in particular implies that our\n",
            "algorithm is efficient whenever the number of groups is polynomially bounded\n",
            "and the external-regret problem can be solved efficiently, an improvement on\n",
            "[Blum & Lykouris]'s stronger condition that the model class must be small. Our\n",
            "approach can handle online linear regression and online combinatorial\n",
            "optimization problems like online shortest paths. Beyond providing theoretical\n",
            "regret bounds, we evaluate this algorithm with an extensive set of experiments\n",
            "on synthetic data and on two real data sets -- Medical costs and the Adult\n",
            "income dataset, both instantiated with intersecting groups defined in terms of\n",
            "race, sex, and other demographic characteristics. We find that uniformly across\n",
            "groups, our algorithm gives substantial error improvements compared to running\n",
            "a standard online linear regression algorithm with no groupwise regret\n",
            "guarantees.\n",
            "\n",
            "460. Title: Energy-Based Concept Bottleneck Models: Unifying Prediction, Concept Intervention, and Probabilistic Interpretations\n",
            "   Abstract: Existing methods, such as concept bottleneck models (CBMs), have been\n",
            "successful in providing concept-based interpretations for black-box deep\n",
            "learning models. They typically work by predicting concepts given the input and\n",
            "then predicting the final class label given the predicted concepts. However,\n",
            "(1) they often fail to capture the high-order, nonlinear interaction between\n",
            "concepts, e.g., correcting a predicted concept (e.g., \"yellow breast\") does not\n",
            "help correct highly correlated concepts (e.g., \"yellow belly\"), leading to\n",
            "suboptimal final accuracy; (2) they cannot naturally quantify the complex\n",
            "conditional dependencies between different concepts and class labels (e.g., for\n",
            "an image with the class label \"Kentucky Warbler\" and a concept \"black bill\",\n",
            "what is the probability that the model correctly predicts another concept\n",
            "\"black crown\"), therefore failing to provide deeper insight into how a\n",
            "black-box model works. In response to these limitations, we propose\n",
            "Energy-based Concept Bottleneck Models (ECBMs). Our ECBMs use a set of neural\n",
            "networks to define the joint energy of candidate (input, concept, class)\n",
            "tuples. With such a unified interface, prediction, concept correction, and\n",
            "conditional dependency quantification are then represented as conditional\n",
            "probabilities, which are generated by composing different energy functions. Our\n",
            "ECBMs address both limitations of existing CBMs, providing higher accuracy and\n",
            "richer concept interpretations. Empirical results show that our approach\n",
            "outperforms the state-of-the-art on real-world datasets.\n",
            "\n",
            "461. Title: Understanding Domain Generalization: A Noise Robustness Perspective\n",
            "   Abstract: Despite the rapid development of machine learning algorithms for domain\n",
            "generalization (DG), there is no clear empirical evidence that the existing DG\n",
            "algorithms outperform the classic empirical risk minimization (ERM) across\n",
            "standard benchmarks. To better understand this phenomenon, we investigate\n",
            "whether there are benefits of DG algorithms over ERM through the lens of label\n",
            "noise. Specifically, our finite-sample analysis reveals that label noise\n",
            "exacerbates the effect of spurious correlations for ERM, undermining\n",
            "generalization. Conversely, we illustrate that DG algorithms exhibit implicit\n",
            "label-noise robustness during finite-sample training even when spurious\n",
            "correlation is present. Such desirable property helps mitigate spurious\n",
            "correlations and improve generalization in synthetic experiments. However,\n",
            "additional comprehensive experiments on real-world benchmark datasets indicate\n",
            "that label-noise robustness does not necessarily translate to better\n",
            "performance compared to ERM. We conjecture that the failure mode of ERM arising\n",
            "from spurious correlations may be less pronounced in practice.\n",
            "\n",
            "462. Title: DiffusionSat: A Generative Foundation Model for Satellite Imagery\n",
            "   Abstract: Despite the emergence of principled methods for domain adaptation under label\n",
            "shift, their sensitivity to shifts in class conditional distributions is\n",
            "precariously under explored. Meanwhile, popular deep domain adaptation\n",
            "heuristics tend to falter when faced with label proportions shifts. While\n",
            "several papers modify these heuristics in attempts to handle label proportions\n",
            "shifts, inconsistencies in evaluation standards, datasets, and baselines make\n",
            "it difficult to gauge the current best practices. In this paper, we introduce\n",
            "RLSbench, a large-scale benchmark for relaxed label shift, consisting of $>$500\n",
            "distribution shift pairs spanning vision, tabular, and language modalities,\n",
            "with varying label proportions. Unlike existing benchmarks, which primarily\n",
            "focus on shifts in class-conditional $p(x|y)$, our benchmark also focuses on\n",
            "label marginal shifts. First, we assess 13 popular domain adaptation methods,\n",
            "demonstrating more widespread failures under label proportion shifts than were\n",
            "previously known. Next, we develop an effective two-step meta-algorithm that is\n",
            "compatible with most domain adaptation heuristics: (i) pseudo-balance the data\n",
            "at each epoch; and (ii) adjust the final classifier with target label\n",
            "distribution estimate. The meta-algorithm improves existing domain adaptation\n",
            "heuristics under large label proportion shifts, often by 2--10\\% accuracy\n",
            "points, while conferring minimal effect ($<$0.5\\%) when label proportions do\n",
            "not shift. We hope that these findings and the availability of RLSbench will\n",
            "encourage researchers to rigorously evaluate proposed methods in relaxed label\n",
            "shift settings. Code is publicly available at\n",
            "https://github.com/acmi-lab/RLSbench.\n",
            "\n",
            "463. Title: Uncertainty-aware Constraint Inference in Inverse Constrained Reinforcement Learning\n",
            "   Abstract: Fleets of robots ingest massive amounts of heterogeneous streaming data silos\n",
            "generated by interacting with their environments, far more than what can be\n",
            "stored or transmitted with ease. At the same time, teams of robots should\n",
            "co-acquire diverse skills through their heterogeneous experiences in varied\n",
            "settings. How can we enable such fleet-level learning without having to\n",
            "transmit or centralize fleet-scale data? In this paper, we investigate policy\n",
            "merging (PoMe) from such distributed heterogeneous datasets as a potential\n",
            "solution. To efficiently merge policies in the fleet setting, we propose\n",
            "FLEET-MERGE, an instantiation of distributed learning that accounts for the\n",
            "permutation invariance that arises when parameterizing the control policies\n",
            "with recurrent neural networks. We show that FLEET-MERGE consolidates the\n",
            "behavior of policies trained on 50 tasks in the Meta-World environment, with\n",
            "good performance on nearly all training tasks at test time. Moreover, we\n",
            "introduce a novel robotic tool-use benchmark, FLEET-TOOLS, for fleet policy\n",
            "learning in compositional and contact-rich robot manipulation tasks, to\n",
            "validate the efficacy of FLEET-MERGE on the benchmark.\n",
            "\n",
            "464. Title: Performance Gaps in Multi-view Clustering under the Nested Matrix-Tensor Model\n",
            "   Abstract: Reinforcement Learning (RL) applied in healthcare can lead to unsafe medical\n",
            "decisions and treatment, such as excessive dosages or abrupt changes, often due\n",
            "to agents overlooking common-sense constraints. Consequently, Constrained\n",
            "Reinforcement Learning (CRL) is a natural choice for safe decisions. However,\n",
            "specifying the exact cost function is inherently difficult in healthcare.\n",
            "Recent Inverse Constrained Reinforcement Learning (ICRL) is a promising\n",
            "approach that infers constraints from expert demonstrations. ICRL algorithms\n",
            "model Markovian decisions in an interactive environment. These settings do not\n",
            "align with the practical requirement of a decision-making system in healthcare,\n",
            "where decisions rely on historical treatment recorded in an offline dataset. To\n",
            "tackle these issues, we propose the Constraint Transformer (CT). Specifically,\n",
            "1) we utilize a causal attention mechanism to incorporate historical decisions\n",
            "and observations into the constraint modeling, while employing a Non-Markovian\n",
            "layer for weighted constraints to capture critical states. 2) A generative\n",
            "world model is used to perform exploratory data augmentation, enabling offline\n",
            "RL methods to simulate unsafe decision sequences. In multiple medical\n",
            "scenarios, empirical results demonstrate that CT can capture unsafe states and\n",
            "achieve strategies that approximate lower mortality rates, reducing the\n",
            "occurrence probability of unsafe behaviors.\n",
            "\n",
            "465. Title: Adversarial Feature Map Pruning for Backdoor\n",
            "   Abstract: Deep neural networks have been widely used in many critical applications,\n",
            "such as autonomous vehicles and medical diagnosis. However, their security is\n",
            "threatened by backdoor attacks, which are achieved by adding artificial\n",
            "patterns to specific training data. Existing defense strategies primarily focus\n",
            "on using reverse engineering to reproduce the backdoor trigger generated by\n",
            "attackers and subsequently repair the DNN model by adding the trigger into\n",
            "inputs and fine-tuning the model with ground-truth labels. However, once the\n",
            "trigger generated by the attackers is complex and invisible, the defender\n",
            "cannot reproduce the trigger successfully then the DNN model will not be\n",
            "repaired, as the trigger is not effectively removed.\n",
            "  In this work, we propose Adversarial Feature Map Pruning for Backdoor (FMP)\n",
            "to mitigate backdoor from the DNN. Unlike existing defense strategies, which\n",
            "focus on reproducing backdoor triggers, FMP attempts to prune backdoor feature\n",
            "maps, which are trained to extract backdoor information from inputs. After\n",
            "pruning these backdoor feature maps, FMP will fine-tune the model with a secure\n",
            "subset of training data. Our experiments demonstrate that, compared to existing\n",
            "defense strategies, FMP can effectively reduce the Attack Success Rate (ASR)\n",
            "even against the most complex and invisible attack triggers (e.g., FMP\n",
            "decreases the ASR to 2.86\\% in CIFAR10, which is 19.2\\% to 65.41\\% lower than\n",
            "baselines). Second, unlike conventional defense methods that tend to exhibit\n",
            "low robust accuracy (that is, the accuracy of the model on poisoned data), FMP\n",
            "achieves a higher RA, indicating its superiority in maintaining model\n",
            "performance while mitigating the effects of backdoor attacks (e.g., FMP obtains\n",
            "87.40\\% RA in CIFAR10). Our code is publicly available at:\n",
            "https://github.com/retsuh-bqw/FMP.\n",
            "\n",
            "466. Title: Window Attention is Bugged: How not to Interpolate Position Embeddings\n",
            "   Abstract: We show how to obtain improved active learning methods in the agnostic\n",
            "(adversarial noise) setting by combining marginal leverage score sampling with\n",
            "non-independent sampling strategies that promote spatial coverage. In\n",
            "particular, we propose an easily implemented method based on the \\emph{pivotal\n",
            "sampling algorithm}, which we test on problems motivated by learning-based\n",
            "methods for parametric PDEs and uncertainty quantification. In comparison to\n",
            "independent sampling, our method reduces the number of samples needed to reach\n",
            "a given target accuracy by up to $50\\%$. We support our findings with two\n",
            "theoretical results. First, we show that any non-independent leverage score\n",
            "sampling method that obeys a weak \\emph{one-sided $\\ell_{\\infty}$ independence\n",
            "condition} (which includes pivotal sampling) can actively learn $d$ dimensional\n",
            "linear functions with $O(d\\log d)$ samples, matching independent sampling. This\n",
            "result extends recent work on matrix Chernoff bounds under $\\ell_{\\infty}$\n",
            "independence, and may be of interest for analyzing other sampling strategies\n",
            "beyond pivotal sampling. Second, we show that, for the important case of\n",
            "polynomial regression, our pivotal method obtains an improved bound on $O(d)$\n",
            "samples.\n",
            "\n",
            "467. Title: Learning Hierarchical Image Segmentation For Recognition and By Recognition\n",
            "   Abstract: Window attention, position embeddings, and high resolution finetuning are\n",
            "core concepts in the modern transformer era of computer vision. However, we\n",
            "find that naively combining these near ubiquitous components can have a\n",
            "detrimental effect on performance. The issue is simple: interpolating position\n",
            "embeddings while using window attention is wrong. We study two state-of-the-art\n",
            "methods that have these three components, namely Hiera and ViTDet, and find\n",
            "that both do indeed suffer from this bug. To fix it, we introduce a simple\n",
            "absolute window position embedding strategy, which solves the bug outright in\n",
            "Hiera and allows us to increase both speed and performance of the model in\n",
            "ViTDet. We finally combine the two to obtain HieraDet, which achieves 61.7 box\n",
            "mAP on COCO, making it state-of-the-art for models that only use ImageNet-1k\n",
            "pretraining. This all stems from what is essentially a 3 line bug fix, which we\n",
            "name \"absolute win\".\n",
            "\n",
            "468. Title: Davidsonian Scene Graph: Improving Reliability in Fine-grained Evaluation for Text-to-Image Generation\n",
            "   Abstract: Evaluating text-to-image models is notoriously difficult. A strong recent\n",
            "approach for assessing text-image faithfulness is based on QG/A (question\n",
            "generation and answering), which uses pre-trained foundational models to\n",
            "automatically generate a set of questions and answers from the prompt, and\n",
            "output images are scored based on whether these answers extracted with a visual\n",
            "question answering model are consistent with the prompt-based answers. This\n",
            "kind of evaluation is naturally dependent on the quality of the underlying QG\n",
            "and VQA models. We identify and address several reliability challenges in\n",
            "existing QG/A work: (a) QG questions should respect the prompt (avoiding\n",
            "hallucinations, duplications, and omissions) and (b) VQA answers should be\n",
            "consistent (not asserting that there is no motorcycle in an image while also\n",
            "claiming the motorcycle is blue). We address these issues with Davidsonian\n",
            "Scene Graph (DSG), an empirically grounded evaluation framework inspired by\n",
            "formal semantics, which is adaptable to any QG/A frameworks. DSG produces\n",
            "atomic and unique questions organized in dependency graphs, which (i) ensure\n",
            "appropriate semantic coverage and (ii) sidestep inconsistent answers. With\n",
            "extensive experimentation and human evaluation on a range of model\n",
            "configurations (LLM, VQA, and T2I), we empirically demonstrate that DSG\n",
            "addresses the challenges noted above. Finally, we present DSG-1k, an\n",
            "open-sourced evaluation benchmark that includes 1,060 prompts, covering a wide\n",
            "range of fine-grained semantic categories with a balanced distribution. We\n",
            "release the DSG-1k prompts and the corresponding DSG questions.\n",
            "\n",
            "469. Title: Vanishing Gradients in Reinforcement Finetuning of Language Models\n",
            "   Abstract: Pretrained language models are commonly aligned with human preferences and\n",
            "downstream tasks via reinforcement finetuning (RFT), which refers to maximizing\n",
            "a (possibly learned) reward function using policy gradient algorithms. This\n",
            "work identifies a fundamental optimization obstacle in RFT: we prove that the\n",
            "expected gradient for an input vanishes when its reward standard deviation\n",
            "under the model is small, even if the expected reward is far from optimal.\n",
            "Through experiments on an RFT benchmark and controlled environments, as well as\n",
            "a theoretical analysis, we then demonstrate that vanishing gradients due to\n",
            "small reward standard deviation are prevalent and detrimental, leading to\n",
            "extremely slow reward maximization. Lastly, we explore ways to overcome\n",
            "vanishing gradients in RFT. We find the common practice of an initial\n",
            "supervised finetuning (SFT) phase to be the most promising candidate, which\n",
            "sheds light on its importance in an RFT pipeline. Moreover, we show that a\n",
            "relatively small number of SFT optimization steps on as few as 1% of the input\n",
            "samples can suffice, indicating that the initial SFT phase need not be\n",
            "expensive in terms of compute and data labeling efforts. Overall, our results\n",
            "emphasize that being mindful for inputs whose expected gradient vanishes, as\n",
            "measured by the reward standard deviation, is crucial for successful execution\n",
            "of RFT.\n",
            "\n",
            "470. Title: DP-OPT: Make Large Language Model Your Privacy-Preserving Prompt Engineer\n",
            "   Abstract: Large vision and language models learned directly through image-text\n",
            "associations often lack detailed visual substantiation, whereas image\n",
            "segmentation tasks are treated separately from recognition, supervisedly\n",
            "learned without interconnections. Our key observation is that, while an image\n",
            "can be recognized in multiple ways, each has a consistent part-and-whole visual\n",
            "organization. Segmentation thus should be treated not as an end task to be\n",
            "mastered through supervised learning, but as an internal process that evolves\n",
            "with and supports the ultimate goal of recognition. We propose to integrate a\n",
            "hierarchical segmenter into the recognition process, train and adapt the entire\n",
            "model solely on image-level recognition objectives. We learn hierarchical\n",
            "segmentation for free alongside recognition, automatically uncovering\n",
            "part-to-whole relationships that not only underpin but also enhance\n",
            "recognition. Enhancing the Vision Transformer (ViT) with adaptive segment\n",
            "tokens and graph pooling, our model surpasses ViT in unsupervised part-whole\n",
            "discovery, semantic segmentation, image classification, and efficiency.\n",
            "Notably, our model (trained on unlabeled 1M ImageNet images) outperforms SAM\n",
            "(trained on 11M images and 1 billion masks) by absolute 8% in mIoU on\n",
            "PartImageNet object segmentation.\n",
            "\n",
            "471. Title: GraphChef: Decision-Tree Recipes to Explain Graph Neural Networks\n",
            "   Abstract: The startling success of ChatGPT and other large language models (LLMs) using\n",
            "transformer-based generative neural network architecture in applications such\n",
            "as natural language processing and image synthesis has many researchers excited\n",
            "about potential opportunities in process systems engineering (PSE). The almost\n",
            "human-like performance of LLMs in these areas is indeed very impressive,\n",
            "surprising, and a major breakthrough. Their capabilities are very useful in\n",
            "certain tasks, such as writing first drafts of documents, code writing\n",
            "assistance, text summarization, etc. However, their success is limited in\n",
            "highly scientific domains as they cannot yet reason, plan, or explain due to\n",
            "their lack of in-depth domain knowledge. This is a problem in domains such as\n",
            "chemical engineering as they are governed by fundamental laws of physics and\n",
            "chemistry (and biology), constitutive relations, and highly technical knowledge\n",
            "about materials, processes, and systems. Although purely data-driven machine\n",
            "learning has its immediate uses, the long-term success of AI in scientific and\n",
            "engineering domains would depend on developing hybrid AI systems that use first\n",
            "principles and technical knowledge effectively. We call these hybrid AI systems\n",
            "Large Knowledge Models (LKMs), as they will not be limited to only NLP-based\n",
            "techniques or NLP-like applications. In this paper, we discuss the challenges\n",
            "and opportunities in developing such systems in chemical engineering.\n",
            "\n",
            "472. Title: Large Language Models Cannot Self-Correct Reasoning Yet\n",
            "   Abstract: Stochastic video prediction models take in a sequence of image frames, and\n",
            "generate a sequence of consecutive future image frames. These models typically\n",
            "generate future frames in an autoregressive fashion, which is slow and requires\n",
            "the input and output frames to be consecutive. We introduce a model that\n",
            "overcomes these drawbacks by generating a latent representation from an\n",
            "arbitrary set of frames that can then be used to simultaneously and efficiently\n",
            "sample temporally consistent frames at arbitrary time-points. For example, our\n",
            "model can \"jump\" and directly sample frames at the end of the video, without\n",
            "sampling intermediate frames. Synthetic video evaluations confirm substantial\n",
            "gains in speed and functionality without loss in fidelity. We also apply our\n",
            "framework to a 3D scene reconstruction dataset. Here, our model is conditioned\n",
            "on camera location and can sample consistent sets of images for what an\n",
            "occluded region of a 3D scene might look like, even if there are multiple\n",
            "possibilities for what that region might contain. Reconstructions and videos\n",
            "are available at https://bit.ly/2O4Pc4R.\n",
            "\n",
            "473. Title: Consistent Video-to-Video Transfer Using Synthetic Dataset\n",
            "   Abstract: The use of tangible interfaces in teaching has been proved more effective,\n",
            "user -friendly and helpful in collaborative learning departments, when compared\n",
            "to traditional teaching approaches. In particular, the tangible interface\n",
            "\"Makey Makey\"is a modern tool that enhances collaboration between pupils, with\n",
            "positive results in education, despite the limited research done on this\n",
            "interface so far. \"Makey Makey\" succeeds in motivating and engaging young\n",
            "learners in the learning process, showing better performance and scoring\n",
            "results. In addition, its use in teaching has been shown to benefit the\n",
            "learning process in every age learning group.The development and use of such an\n",
            "innovative teaching/learning approach helps young learners perceive the\n",
            "educational process in a different way and assimilate new cognitive fields more\n",
            "effectively. Moreover, educators profit as well, as they can eliminate\n",
            "difficulties and teach more efficiently using examples based on their teaching\n",
            "approach, while enhancing young learners parallel skills as well. This study\n",
            "will confirm previous research results stating that assimilation of new\n",
            "concepts is easier with tangible interfaces than with graphical ones, as well\n",
            "as that young learners participating in the survey have shown significant\n",
            "progress in knowledge acquisition when compared to their prior knowledge.\n",
            "\n",
            "474. Title: Talk like a Graph: Encoding Graphs for Large Language Models\n",
            "   Abstract: Endowing machines with abstract reasoning ability has been a long-term\n",
            "research topic in artificial intelligence. Raven's Progressive Matrix (RPM) is\n",
            "widely used to probe abstract visual reasoning in machine intelligence, where\n",
            "models will analyze the underlying rules and select one image from candidates\n",
            "to complete the image matrix. Participators of RPM tests can show powerful\n",
            "reasoning ability by inferring and combining attribute-changing rules and\n",
            "imagining the missing images at arbitrary positions of a matrix. However,\n",
            "existing solvers can hardly manifest such an ability in realistic RPM tests. In\n",
            "this paper, we propose a deep latent variable model for answer generation\n",
            "problems through Rule AbstractIon and SElection (RAISE). RAISE can encode image\n",
            "attributes into latent concepts and abstract atomic rules that act on the\n",
            "latent concepts. When generating answers, RAISE selects one atomic rule out of\n",
            "the global knowledge set for each latent concept to constitute the underlying\n",
            "rule of an RPM. In the experiments of bottom-right and arbitrary-position\n",
            "answer generation, RAISE outperforms the compared solvers in most\n",
            "configurations of realistic RPM datasets. In the odd-one-out task and two\n",
            "held-out configurations, RAISE can leverage acquired latent concepts and atomic\n",
            "rules to find the rule-breaking image in a matrix and handle problems with\n",
            "unseen combinations of rules and attributes.\n",
            "\n",
            "475. Title: A Good Learner can Teach Better: Teacher-Student Collaborative Knowledge Distillation\n",
            "   Abstract: Graphs are a powerful tool for representing and analyzing complex\n",
            "relationships in real-world applications such as social networks, recommender\n",
            "systems, and computational finance. Reasoning on graphs is essential for\n",
            "drawing inferences about the relationships between entities in a complex\n",
            "system, and to identify hidden patterns and trends. Despite the remarkable\n",
            "progress in automated reasoning with natural text, reasoning on graphs with\n",
            "large language models (LLMs) remains an understudied problem. In this work, we\n",
            "perform the first comprehensive study of encoding graph-structured data as text\n",
            "for consumption by LLMs. We show that LLM performance on graph reasoning tasks\n",
            "varies on three fundamental levels: (1) the graph encoding method, (2) the\n",
            "nature of the graph task itself, and (3) interestingly, the very structure of\n",
            "the graph considered. These novel results provide valuable insight on\n",
            "strategies for encoding graphs as text. Using these insights we illustrate how\n",
            "the correct choice of encoders can boost performance on graph reasoning tasks\n",
            "inside LLMs by 4.8% to 61.8%, depending on the task.\n",
            "\n",
            "476. Title: VQ-TR: Vector Quantized Attention for Time Series Forecasting\n",
            "   Abstract: We study the estimation of a planted signal hidden in a recently introduced\n",
            "nested matrix-tensor model, which is an extension of the classical spiked\n",
            "rank-one tensor model, motivated by multi-view clustering. Prior work has\n",
            "theoretically examined the performance of a tensor-based approach, which relies\n",
            "on finding a best rank-one approximation, a problem known to be computationally\n",
            "hard. A tractable alternative approach consists in computing instead the best\n",
            "rank-one (matrix) approximation of an unfolding of the observed tensor data,\n",
            "but its performance was hitherto unknown. We quantify here the performance gap\n",
            "between these two approaches, in particular by deriving the precise algorithmic\n",
            "threshold of the unfolding approach and demonstrating that it exhibits a\n",
            "BBP-type transition behavior. This work is therefore in line with recent\n",
            "contributions which deepen our understanding of why tensor-based methods\n",
            "surpass matrix-based methods in handling structured tensor data.\n",
            "\n",
            "477. Title: Bellman Optimal Stepsize Straightening of Flow-Matching Models\n",
            "   Abstract: Flow matching is a powerful framework for generating high-quality samples in\n",
            "various applications, especially image synthesis. However, the intensive\n",
            "computational demands of these models, especially during the finetuning process\n",
            "and sampling processes, pose significant challenges for low-resource scenarios.\n",
            "This paper introduces Bellman Optimal Stepsize Straightening (BOSS) technique\n",
            "for distilling flow-matching generative models: it aims specifically for a\n",
            "few-step efficient image sampling while adhering to a computational budget\n",
            "constraint. First, this technique involves a dynamic programming algorithm that\n",
            "optimizes the stepsizes of the pretrained network. Then, it refines the\n",
            "velocity network to match the optimal step sizes, aiming to straighten the\n",
            "generation paths. Extensive experimental evaluations across image generation\n",
            "tasks demonstrate the efficacy of BOSS in terms of both resource utilization\n",
            "and image quality. Our results reveal that BOSS achieves substantial gains in\n",
            "efficiency while maintaining competitive sample quality, effectively bridging\n",
            "the gap between low-resource constraints and the demanding requirements of\n",
            "flow-matching generative models. Our paper also fortifies the responsible\n",
            "development of artificial intelligence, offering a more sustainable generative\n",
            "model that reduces computational costs and environmental footprints. Our code\n",
            "can be found at https://github.com/nguyenngocbaocmt02/BOSS.\n",
            "\n",
            "478. Title: Private Zeroth-Order Nonsmooth Nonconvex Optimization\n",
            "   Abstract: We introduce a new zeroth-order algorithm for private stochastic optimization\n",
            "on nonconvex and nonsmooth objectives. Given a dataset of size $M$, our\n",
            "algorithm ensures $(\\alpha,\\alpha\\rho^2/2)$-R\\'enyi differential privacy and\n",
            "finds a $(\\delta,\\epsilon)$-stationary point so long as\n",
            "$M=\\tilde\\Omega\\left(\\frac{d}{\\delta\\epsilon^3} +\n",
            "\\frac{d^{3/2}}{\\rho\\delta\\epsilon^2}\\right)$. This matches the optimal\n",
            "complexity of its non-private zeroth-order analog. Notably, although the\n",
            "objective is not smooth, we have privacy ``for free'' whenever $\\rho \\ge\n",
            "\\sqrt{d}\\epsilon$.\n",
            "\n",
            "479. Title: Efficient Score Matching with Deep Equilibrium Layers\n",
            "   Abstract: Deep pretrained transformer networks are effective at various ranking tasks,\n",
            "such as question answering and ad-hoc document ranking. However, their\n",
            "computational expenses deem them cost-prohibitive in practice. Our proposed\n",
            "approach, called PreTTR (Precomputing Transformer Term Representations),\n",
            "considerably reduces the query-time latency of deep transformer networks (up to\n",
            "a 42x speedup on web document ranking) making these networks more practical to\n",
            "use in a real-time ranking scenario. Specifically, we precompute part of the\n",
            "document term representations at indexing time (without a query), and merge\n",
            "them with the query representation at query time to compute the final ranking\n",
            "score. Due to the large size of the token representations, we also propose an\n",
            "effective approach to reduce the storage requirement by training a compression\n",
            "layer to match attention scores. Our compression technique reduces the storage\n",
            "required up to 95% and it can be applied without a substantial degradation in\n",
            "ranking performance.\n",
            "\n",
            "480. Title: Toward Optimal Policy Population Growth in Two-Player Zero-Sum Games\n",
            "   Abstract: In competitive two-agent environments, deep reinforcement learning (RL)\n",
            "methods based on the \\emph{Double Oracle (DO)} algorithm, such as \\emph{Policy\n",
            "Space Response Oracles (PSRO)} and \\emph{Anytime PSRO (APSRO)}, iteratively add\n",
            "RL best response policies to a population. Eventually, an optimal mixture of\n",
            "these population policies will approximate a Nash equilibrium. However, these\n",
            "methods might need to add all deterministic policies before converging. In this\n",
            "work, we introduce \\emph{Self-Play PSRO (SP-PSRO)}, a method that adds an\n",
            "approximately optimal stochastic policy to the population in each iteration.\n",
            "Instead of adding only deterministic best responses to the opponent's least\n",
            "exploitable population mixture, SP-PSRO also learns an approximately optimal\n",
            "stochastic policy and adds it to the population as well. As a result, SP-PSRO\n",
            "empirically tends to converge much faster than APSRO and in many games\n",
            "converges in just a few iterations.\n",
            "\n",
            "481. Title: Rethinking Label Poisoning for GNNs: Pitfalls and Attacks\n",
            "   Abstract: Semi-supervised learning methods can train high-accuracy machine learning\n",
            "models with a fraction of the labeled training samples required for traditional\n",
            "supervised learning. Such methods do not typically involve close review of the\n",
            "unlabeled training samples, making them tempting targets for data poisoning\n",
            "attacks. In this paper we investigate the vulnerabilities of semi-supervised\n",
            "learning methods to backdoor data poisoning attacks on the unlabeled samples.\n",
            "We show that simple poisoning attacks that influence the distribution of the\n",
            "poisoned samples' predicted labels are highly effective - achieving an average\n",
            "attack success rate as high as 96.9%. We introduce a generalized attack\n",
            "framework targeting semi-supervised learning methods to better understand and\n",
            "exploit their limitations and to motivate future defense strategies.\n",
            "\n",
            "482. Title: Sample-Efficient Quality-Diversity by Cooperative Coevolution\n",
            "   Abstract: Coevolution between strategy and network structure is established as a means\n",
            "to arrive at optimal conditions for resolving social dilemmas. Yet recent\n",
            "research highlights that the interdependence between networks may be just as\n",
            "important as the structure of an individual network. We therefore introduce\n",
            "coevolution of strategy and network interdependence to study whether it can\n",
            "give rise to elevated levels of cooperation in the prisoner's dilemma game. We\n",
            "show that the interdependence between networks self-organizes so as to yield\n",
            "optimal conditions for the evolution of cooperation. Even under extremely\n",
            "adverse conditions cooperators can prevail where on isolated networks they\n",
            "would perish. This is due to the spontaneous emergence of a two-class society,\n",
            "with only the upper class being allowed to control and take advantage of the\n",
            "interdependence. Spatial patterns reveal that cooperators, once arriving to the\n",
            "upper class, are much more competent than defectors in sustaining compact\n",
            "clusters of followers. Indeed, the asymmetric exploitation of interdependence\n",
            "confers to them a strong evolutionary advantage that may resolve even the\n",
            "toughest of social dilemmas.\n",
            "\n",
            "483. Title: Koopman-based generalization bound: New aspect for full-rank weights\n",
            "   Abstract: Despite the promising progress in multi-modal tasks, current large\n",
            "multi-modal models (LMMs) are prone to hallucinating inconsistent descriptions\n",
            "with respect to the associated image and human instructions. This paper\n",
            "addresses this issue by introducing the first large and diverse visual\n",
            "instruction tuning dataset, named Large-scale Robust Visual (LRV)-Instruction.\n",
            "Our dataset comprises 400k visual instructions generated by GPT4, covering 16\n",
            "vision-and-language tasks with open-ended instructions and answers. Unlike\n",
            "existing studies that primarily focus on positive instruction samples, we\n",
            "design LRV-Instruction to include both positive and negative instructions for\n",
            "more robust visual instruction tuning. Our negative instructions are designed\n",
            "at three semantic levels: (i) Nonexistent Object Manipulation, (ii) Existent\n",
            "Object Manipulation and (iii) Knowledge Manipulation. To efficiently measure\n",
            "the hallucination generated by LMMs, we propose GPT4-Assisted Visual\n",
            "Instruction Evaluation (GAVIE), a stable approach to evaluate visual\n",
            "instruction tuning like human experts. GAVIE does not require human-annotated\n",
            "groundtruth answers and can adapt to diverse instruction formats. We conduct\n",
            "comprehensive experiments to investigate the hallucination of LMMs. Our results\n",
            "demonstrate existing LMMs exhibit significant hallucinations when presented\n",
            "with our negative instructions, particularly Existent Object and Knowledge\n",
            "Manipulation instructions. Moreover, we successfully mitigate hallucination by\n",
            "finetuning MiniGPT4 and mPLUG-Owl on LRV-Instruction while improving\n",
            "performance on several public datasets compared to state-of-the-art methods.\n",
            "Additionally, we observed that a balanced ratio of positive and negative\n",
            "instances in the training data leads to a more robust model. Code and data are\n",
            "available at https://github.com/FuxiaoLiu/LRV-Instruction.\n",
            "\n",
            "484. Title: On the Stability of Iterative Retraining of Generative Models on their own Data\n",
            "   Abstract: Topic modeling often requires examining topics from multiple perspectives to\n",
            "uncover hidden patterns, especially in less explored areas. This paper presents\n",
            "an approach to address this need, utilizing weighted keywords from various\n",
            "aspects derived from a domain knowledge. The research method starts with\n",
            "standard topic modeling. Then, it adds a process consisting of four key steps.\n",
            "First, it defines keywords for each aspect. Second, it gives weights to these\n",
            "keywords based on their relevance. Third, it calculates relevance scores for\n",
            "aspect-weighted keywords and topic keywords to create aspect-topic models.\n",
            "Fourth, it uses these scores to tune relevant new documents. Finally, the\n",
            "generated topic models are interpreted and validated. The findings show that\n",
            "top-scoring documents are more likely to be about the same aspect of a topic.\n",
            "This highlights the model's effectiveness in finding the related documents to\n",
            "the aspects.\n",
            "\n",
            "485. Title: On the Role of General Function Approximation in Offline Reinforcement Learning\n",
            "   Abstract: The largest experiments in machine learning now require resources far beyond\n",
            "the budget of all but a few institutions. Fortunately, it has recently been\n",
            "shown that the results of these huge experiments can often be extrapolated from\n",
            "the results of a sequence of far smaller, cheaper experiments. In this work, we\n",
            "show that not only can the extrapolation be done based on the size of the\n",
            "model, but on the size of the problem as well. By conducting a sequence of\n",
            "experiments using AlphaZero and Hex, we show that the performance achievable\n",
            "with a fixed amount of compute degrades predictably as the game gets larger and\n",
            "harder. Along with our main result, we further show that the test-time and\n",
            "train-time compute available to an agent can be traded off while maintaining\n",
            "performance.\n",
            "\n",
            "486. Title: AirPhyNet: Harnessing Physics-Guided Neural Networks for Air Quality Prediction\n",
            "   Abstract: Deep generative models have made tremendous progress in modeling complex\n",
            "data, often exhibiting generation quality that surpasses a typical human's\n",
            "ability to discern the authenticity of samples. Undeniably, a key driver of\n",
            "this success is enabled by the massive amounts of web-scale data consumed by\n",
            "these models. Due to these models' striking performance and ease of\n",
            "availability, the web will inevitably be increasingly populated with synthetic\n",
            "content. Such a fact directly implies that future iterations of generative\n",
            "models will be trained on both clean and artificially generated data from past\n",
            "models. In this paper, we develop a framework to rigorously study the impact of\n",
            "training generative models on mixed datasets -- from classical training on real\n",
            "data to self-consuming generative models trained on purely synthetic data. We\n",
            "first prove the stability of iterative training under the condition that the\n",
            "initial generative models approximate the data distribution well enough and the\n",
            "proportion of clean training data (w.r.t. synthetic data) is large enough. We\n",
            "empirically validate our theory on both synthetic and natural images by\n",
            "iteratively training normalizing flows and state-of-the-art diffusion models on\n",
            "CIFAR10 and FFHQ.\n",
            "\n",
            "487. Title: ARM: Refining Multivariate Forecasting with Adaptive Temporal-Contextual Learning\n",
            "   Abstract: Epistemic planning can be used for decision making in multi-agent situations\n",
            "with distributed knowledge and capabilities. Dynamic Epistemic Logic (DEL) has\n",
            "been shown to provide a very natural and expressive framework for epistemic\n",
            "planning. In this paper, we aim to give an accessible introduction to DEL-based\n",
            "epistemic planning. The paper starts with the most classical framework for\n",
            "planning, STRIPS, and then moves towards epistemic planning in a number of\n",
            "smaller steps, where each step is motivated by the need to be able to model\n",
            "more complex planning scenarios.\n",
            "\n",
            "488. Title: Optimal criterion for feature learning of two-layer linear neural network in high dimensional interpolation regime\n",
            "   Abstract: As large language models (LLMs) generate texts with increasing fluency and\n",
            "realism, there is a growing need to identify the source of texts to prevent the\n",
            "abuse of LLMs. Text watermarking techniques have proven reliable in\n",
            "distinguishing whether a text is generated by LLMs by injecting hidden\n",
            "patterns. However, we argue that existing LLM watermarking methods are\n",
            "encoding-inefficient and cannot flexibly meet the diverse information encoding\n",
            "needs (such as encoding model version, generation time, user id, etc.). In this\n",
            "work, we conduct the first systematic study on the topic of Codable Text\n",
            "Watermarking for LLMs (CTWL) that allows text watermarks to carry multi-bit\n",
            "customizable information. First of all, we study the taxonomy of LLM\n",
            "watermarking technologies and give a mathematical formulation for CTWL.\n",
            "Additionally, we provide a comprehensive evaluation system for CTWL: (1)\n",
            "watermarking success rate, (2) robustness against various corruptions, (3)\n",
            "coding rate of payload information, (4) encoding and decoding efficiency, (5)\n",
            "impacts on the quality of the generated text. To meet the requirements of these\n",
            "non-Pareto-improving metrics, we follow the most prominent vocabulary\n",
            "partition-based watermarking direction, and devise an advanced CTWL method\n",
            "named Balance-Marking. The core idea of our method is to use a proxy language\n",
            "model to split the vocabulary into probability-balanced parts, thereby\n",
            "effectively maintaining the quality of the watermarked text. Our code is\n",
            "available at https://github.com/lancopku/codable-watermarking-for-llm.\n",
            "\n",
            "489. Title: Multimodal Patient Representation Learning with Missing Modalities and Labels\n",
            "   Abstract: Off-policy reinforcement learning (RL) using a fixed offline dataset of\n",
            "logged interactions is an important consideration in real world applications.\n",
            "This paper studies offline RL using the DQN replay dataset comprising the\n",
            "entire replay experience of a DQN agent on 60 Atari 2600 games. We demonstrate\n",
            "that recent off-policy deep RL algorithms, even when trained solely on this\n",
            "fixed dataset, outperform the fully trained DQN agent. To enhance\n",
            "generalization in the offline setting, we present Random Ensemble Mixture\n",
            "(REM), a robust Q-learning algorithm that enforces optimal Bellman consistency\n",
            "on random convex combinations of multiple Q-value estimates. Offline REM\n",
            "trained on the DQN replay dataset surpasses strong RL baselines. Ablation\n",
            "studies highlight the role of offline dataset size and diversity as well as the\n",
            "algorithm choice in our positive results. Overall, the results here present an\n",
            "optimistic view that robust RL algorithms trained on sufficiently large and\n",
            "diverse offline datasets can lead to high quality policies. The DQN replay\n",
            "dataset can serve as an offline RL benchmark and is open-sourced.\n",
            "\n",
            "490. Title: iTransformer: Inverted Transformers Are Effective for Time Series Forecasting\n",
            "   Abstract: Learning multimodal representations is a fundamentally complex research\n",
            "problem due to the presence of multiple heterogeneous sources of information.\n",
            "Although the presence of multiple modalities provides additional valuable\n",
            "information, there are two key challenges to address when learning from\n",
            "multimodal data: 1) models must learn the complex intra-modal and cross-modal\n",
            "interactions for prediction and 2) models must be robust to unexpected missing\n",
            "or noisy modalities during testing. In this paper, we propose to optimize for a\n",
            "joint generative-discriminative objective across multimodal data and labels. We\n",
            "introduce a model that factorizes representations into two sets of independent\n",
            "factors: multimodal discriminative and modality-specific generative factors.\n",
            "Multimodal discriminative factors are shared across all modalities and contain\n",
            "joint multimodal features required for discriminative tasks such as sentiment\n",
            "prediction. Modality-specific generative factors are unique for each modality\n",
            "and contain the information required for generating data. Experimental results\n",
            "show that our model is able to learn meaningful multimodal representations that\n",
            "achieve state-of-the-art or competitive performance on six multimodal datasets.\n",
            "Our model demonstrates flexible generative capabilities by conditioning on\n",
            "independent factors and can reconstruct missing modalities without\n",
            "significantly impacting performance. Lastly, we interpret our factorized\n",
            "representations to understand the interactions that influence multimodal\n",
            "learning.\n",
            "\n",
            "491. Title: SKILL-MIX: a Flexible and Expandable Family of Evaluations for AI Models\n",
            "   Abstract: We study the node classification problem on feature-decorated graphs in the\n",
            "sparse setting, i.e., when the expected degree of a node is $O(1)$ in the\n",
            "number of nodes, in the fixed-dimensional asymptotic regime, i.e., the\n",
            "dimension of the feature data is fixed while the number of nodes is large. Such\n",
            "graphs are typically known to be locally tree-like. We introduce a notion of\n",
            "Bayes optimality for node classification tasks, called asymptotic local Bayes\n",
            "optimality, and compute the optimal classifier according to this criterion for\n",
            "a fairly general statistical data model with arbitrary distributions of the\n",
            "node features and edge connectivity. The optimal classifier is implementable\n",
            "using a message-passing graph neural network architecture. We then compute the\n",
            "generalization error of this classifier and compare its performance against\n",
            "existing learning methods theoretically on a well-studied statistical model\n",
            "with naturally identifiable signal-to-noise ratios (SNRs) in the data. We find\n",
            "that the optimal message-passing architecture interpolates between a standard\n",
            "MLP in the regime of low graph signal and a typical convolution in the regime\n",
            "of high graph signal. Furthermore, we prove a corresponding non-asymptotic\n",
            "result.\n",
            "\n",
            "492. Title: FLATTEN: optical FLow-guided ATTENtion for consistent text-to-video editing\n",
            "   Abstract: Artificial General Intelligence (AGI) has been a long-standing goal of\n",
            "humanity, with the aim of creating machines capable of performing any\n",
            "intellectual task that humans can do. To achieve this, AGI researchers draw\n",
            "inspiration from the human brain and seek to replicate its principles in\n",
            "intelligent machines. Brain-inspired artificial intelligence is a field that\n",
            "has emerged from this endeavor, combining insights from neuroscience,\n",
            "psychology, and computer science to develop more efficient and powerful AI\n",
            "systems. In this article, we provide a comprehensive overview of brain-inspired\n",
            "AI from the perspective of AGI. We begin with the current progress in\n",
            "brain-inspired AI and its extensive connection with AGI. We then cover the\n",
            "important characteristics for both human intelligence and AGI (e.g., scaling,\n",
            "multimodality, and reasoning). We discuss important technologies toward\n",
            "achieving AGI in current AI systems, such as in-context learning and prompt\n",
            "tuning. We also investigate the evolution of AGI systems from both algorithmic\n",
            "and infrastructural perspectives. Finally, we explore the limitations and\n",
            "future of AGI.\n",
            "\n",
            "493. Title: Rethinking Channel Dependence for Multivariate Time Series Forecasting: Learning from Leading Indicators\n",
            "   Abstract: Recently, channel-independent methods have achieved state-of-the-art\n",
            "performance in multivariate time series (MTS) forecasting. Despite reducing\n",
            "overfitting risks, these methods miss potential opportunities in utilizing\n",
            "channel dependence for accurate predictions. We argue that there exist locally\n",
            "stationary lead-lag relationships between variates, i.e., some lagged variates\n",
            "may follow the leading indicators within a short time period. Exploiting such\n",
            "channel dependence is beneficial since leading indicators offer advance\n",
            "information that can be used to reduce the forecasting difficulty of the lagged\n",
            "variates. In this paper, we propose a new method named LIFT that first\n",
            "efficiently estimates leading indicators and their leading steps at each time\n",
            "step and then judiciously allows the lagged variates to utilize the advance\n",
            "information from leading indicators. LIFT plays as a plugin that can be\n",
            "seamlessly collaborated with arbitrary time series forecasting methods.\n",
            "Extensive experiments on six real-world datasets demonstrate that LIFT improves\n",
            "the state-of-the-art methods by 5.5% in average forecasting performance. Our\n",
            "code is available at https://github.com/SJTU-Quant/LIFT.\n",
            "\n",
            "494. Title: Repeated Random Sampling for Minimizing the Time-to-Accuracy of Learning\n",
            "   Abstract: Kernel methods are widely used in machine learning, especially for\n",
            "classification problems. However, the theoretical analysis of kernel\n",
            "classification is still limited. This paper investigates the statistical\n",
            "performances of kernel classifiers. With some mild assumptions on the\n",
            "conditional probability $\\eta(x)=\\mathbb{P}(Y=1\\mid X=x)$, we derive an upper\n",
            "bound on the classification excess risk of a kernel classifier using recent\n",
            "advances in the theory of kernel regression. We also obtain a minimax lower\n",
            "bound for Sobolev spaces, which shows the optimality of the proposed\n",
            "classifier. Our theoretical results can be extended to the generalization error\n",
            "of overparameterized neural network classifiers. To make our theoretical\n",
            "results more applicable in realistic settings, we also propose a simple method\n",
            "to estimate the interpolation smoothness of $2\\eta(x)-1$ and apply the method\n",
            "to real datasets.\n",
            "\n",
            "495. Title: A Mutual Information Perspective on Federated Contrastive Learning\n",
            "   Abstract: We study the performance of transformers as a function of the number of\n",
            "repetitions of training examples with algorithmically generated datasets. On\n",
            "three problems of mathematics: the greatest common divisor, modular\n",
            "multiplication, and matrix eigenvalues, we show that for a fixed number of\n",
            "training steps, models trained on smaller sets of repeated examples outperform\n",
            "models trained on larger sets of single-use examples. We also demonstrate that\n",
            "two-set training - repeated use of a small random subset of examples, along\n",
            "normal sampling on the rest of the training set - provides for faster learning\n",
            "and better performance. This highlights that the benefits of repetition can\n",
            "outweigh those of data diversity. These datasets and problems provide a\n",
            "controlled setting to shed light on the still poorly understood interplay\n",
            "between generalization and memorization in deep learning.\n",
            "\n",
            "496. Title: A Hard-to-Beat Baseline for Training-free CLIP-based Adaptation\n",
            "   Abstract: We investigate contrastive learning in the federated setting through the lens\n",
            "of SimCLR and multi-view mutual information maximization. In doing so, we\n",
            "uncover a connection between contrastive representation learning and user\n",
            "verification; by adding a user verification loss to each client's local SimCLR\n",
            "loss we recover a lower bound to the global multi-view mutual information. To\n",
            "accommodate for the case of when some labelled data are available at the\n",
            "clients, we extend our SimCLR variant to the federated semi-supervised setting.\n",
            "We see that a supervised SimCLR objective can be obtained with two changes: a)\n",
            "the contrastive loss is computed between datapoints that share the same label\n",
            "and b) we require an additional auxiliary head that predicts the correct labels\n",
            "from either of the two views. Along with the proposed SimCLR extensions, we\n",
            "also study how different sources of non-i.i.d.-ness can impact the performance\n",
            "of federated unsupervised learning through global mutual information\n",
            "maximization; we find that a global objective is beneficial for some sources of\n",
            "non-i.i.d.-ness but can be detrimental for others. We empirically evaluate our\n",
            "proposed extensions in various tasks to validate our claims and furthermore\n",
            "demonstrate that our proposed modifications generalize to other pretraining\n",
            "methods.\n",
            "\n",
            "497. Title: Exploring Weight Balancing on Long-Tailed Recognition Problem\n",
            "   Abstract: The performance of automatic speech recognition systems can be improved by\n",
            "adapting an acoustic model to compensate for the mismatch between training and\n",
            "testing conditions, for example by adapting to unseen speakers. The success of\n",
            "speaker adaptation methods relies on selecting weights that are suitable for\n",
            "adaptation and using good adaptation schedules to update these weights in order\n",
            "not to overfit to the adaptation data. In this paper we investigate a\n",
            "principled way of adapting all the weights of the acoustic model using a\n",
            "meta-learning. We show that the meta-learner can learn to perform supervised\n",
            "and unsupervised speaker adaptation and that it outperforms a strong baseline\n",
            "adapting LHUC parameters when adapting a DNN AM with 1.5M parameters. We also\n",
            "report initial experiments on adapting TDNN AMs, where the meta-learner\n",
            "achieves comparable performance with LHUC.\n",
            "\n",
            "498. Title: ReFusion: Improving Natural Language Understanding with Computation-Efficient Retrieval Representation Fusion\n",
            "   Abstract: Recognition problems in long-tailed data, in which the sample size per class\n",
            "is heavily skewed, have gained importance because the distribution of the\n",
            "sample size per class in a dataset is generally exponential unless the sample\n",
            "size is intentionally adjusted. Various methods have been devised to address\n",
            "these problems.Recently, weight balancing, which combines well-known classical\n",
            "regularization techniques with two-stage training, has been proposed. Despite\n",
            "its simplicity, it is known for its high performance compared with existing\n",
            "methods devised in various ways. However, there is a lack of understanding as\n",
            "to why this method is effective for long-tailed data. In this study, we analyze\n",
            "weight balancing by focusing on neural collapse and the cone effect at each\n",
            "training stage and found that it can be decomposed into an increase in Fisher's\n",
            "discriminant ratio of the feature extractor caused by weight decay and cross\n",
            "entropy loss and implicit logit adjustment caused by weight decay and\n",
            "class-balanced loss. Our analysis enables the training method to be further\n",
            "simplified by reducing the number of training stages to one while increasing\n",
            "accuracy. Code is available at\n",
            "https://github.com/HN410/Exploring-Weight-Balancing-on-Long-Tailed-Recognition-Problem.\n",
            "\n",
            "499. Title: Rethinking Channel Dimensions to Isolate Outliers for Low-bit Weight Quantization of Large Language Models\n",
            "   Abstract: Large Language Models (LLMs) have recently demonstrated remarkable success\n",
            "across various tasks. However, efficiently serving LLMs has been a challenge\n",
            "due to the large memory bottleneck, specifically in small batch inference\n",
            "settings (e.g. mobile devices). Weight-only quantization can be a promising\n",
            "approach, but sub-4 bit quantization remains a challenge due to large-magnitude\n",
            "activation outliers. To mitigate the undesirable outlier effect, we first\n",
            "propose per-IC quantization, a simple yet effective method that creates\n",
            "quantization groups within each input channel (IC) rather than the conventional\n",
            "per-output-channel (per-OC). Our method is motivated by the observation that\n",
            "activation outliers affect the input dimension of the weight matrix, so\n",
            "similarly grouping the weights in the IC direction can isolate outliers within\n",
            "a group. We also find that activation outliers do not dictate quantization\n",
            "difficulty, and inherent weight sensitivities also exist. With per-IC\n",
            "quantization as a new outlier-friendly scheme, we propose Adaptive Dimensions\n",
            "(AdaDim), a versatile quantization framework that can adapt to various weight\n",
            "sensitivity patterns. We demonstrate the effectiveness of AdaDim by augmenting\n",
            "prior methods such as Round-To-Nearest and GPTQ, showing significant\n",
            "improvements across various language modeling benchmarks for both base (up to\n",
            "+4.7% on MMLU) and instruction-tuned (up to +10% on HumanEval) LLMs. Code is\n",
            "available at https://github.com/johnheo/adadim-llm\n",
            "\n",
            "500. Title: Conditional Information Bottleneck Approach for Time Series Imputation\n",
            "   Abstract: In this paper, we study inverse game theory (resp. inverse multiagent\n",
            "learning) in which the goal is to find parameters of a game's payoff functions\n",
            "for which the expected (resp. sampled) behavior is an equilibrium. We formulate\n",
            "these problems as generative-adversarial (i.e., min-max) optimization problems,\n",
            "for which we develop polynomial-time algorithms to solve, the former of which\n",
            "relies on an exact first-order oracle, and the latter, a stochastic one. We\n",
            "extend our approach to solve inverse multiagent simulacral learning in\n",
            "polynomial time and number of samples. In these problems, we seek a simulacrum,\n",
            "meaning parameters and an associated equilibrium that replicate the given\n",
            "observations in expectation. We find that our approach outperforms the\n",
            "widely-used ARIMA method in predicting prices in Spanish electricity markets\n",
            "based on time-series data.\n",
            "\n",
            "501. Title: CAMBranch: Contrastive Learning with Augmented MILPs for Branching\n",
            "   Abstract: The imputation of missing values in time series has many applications in\n",
            "healthcare and finance. While autoregressive models are natural candidates for\n",
            "time series imputation, score-based diffusion models have recently outperformed\n",
            "existing counterparts including autoregressive models in many tasks such as\n",
            "image generation and audio synthesis, and would be promising for time series\n",
            "imputation. In this paper, we propose Conditional Score-based Diffusion models\n",
            "for Imputation (CSDI), a novel time series imputation method that utilizes\n",
            "score-based diffusion models conditioned on observed data. Unlike existing\n",
            "score-based approaches, the conditional diffusion model is explicitly trained\n",
            "for imputation and can exploit correlations between observed values. On\n",
            "healthcare and environmental data, CSDI improves by 40-65% over existing\n",
            "probabilistic imputation methods on popular performance metrics. In addition,\n",
            "deterministic imputation by CSDI reduces the error by 5-20% compared to the\n",
            "state-of-the-art deterministic imputation methods. Furthermore, CSDI can also\n",
            "be applied to time series interpolation and probabilistic forecasting, and is\n",
            "competitive with existing baselines. The code is available at\n",
            "https://github.com/ermongroup/CSDI.\n",
            "\n",
            "502. Title: Robust Similarity Learning with Difference Alignment Regularization\n",
            "   Abstract: Not all positive pairs are beneficial to time series contrastive learning. In\n",
            "this paper, we study two types of bad positive pairs that can impair the\n",
            "quality of time series representation learned through contrastive learning: the\n",
            "noisy positive pair and the faulty positive pair. We observe that, with the\n",
            "presence of noisy positive pairs, the model tends to simply learn the pattern\n",
            "of noise (Noisy Alignment). Meanwhile, when faulty positive pairs arise, the\n",
            "model wastes considerable amount of effort aligning non-representative patterns\n",
            "(Faulty Alignment). To address this problem, we propose a Dynamic Bad Pair\n",
            "Mining (DBPM) algorithm, which reliably identifies and suppresses bad positive\n",
            "pairs in time series contrastive learning. Specifically, DBPM utilizes a memory\n",
            "module to dynamically track the training behavior of each positive pair along\n",
            "training process. This allows us to identify potential bad positive pairs at\n",
            "each epoch based on their historical training behaviors. The identified bad\n",
            "pairs are subsequently down-weighted through a transformation module, thereby\n",
            "mitigating their negative impact on the representation learning process. DBPM\n",
            "is a simple algorithm designed as a lightweight plug-in without learnable\n",
            "parameters to enhance the performance of existing state-of-the-art methods.\n",
            "Through extensive experiments conducted on four large-scale, real-world time\n",
            "series datasets, we demonstrate DBPM's efficacy in mitigating the adverse\n",
            "effects of bad positive pairs.\n",
            "\n",
            "503. Title: VersVideo: Leveraging Enhanced Temporal Diffusion Models for Versatile Video Generation\n",
            "   Abstract: Large training sets have become a cornerstone of machine learning and are the\n",
            "foundation for recent advances in language modeling and multimodal learning.\n",
            "While data curation for pre-training is often still ad-hoc, one common paradigm\n",
            "is to first collect a massive pool of data from the Web and then filter this\n",
            "candidate pool down to an actual training set via various heuristics. In this\n",
            "work, we study the problem of learning a data filtering network (DFN) for this\n",
            "second step of filtering a large uncurated dataset. Our key finding is that the\n",
            "quality of a network for filtering is distinct from its performance on\n",
            "downstream tasks: for instance, a model that performs well on ImageNet can\n",
            "yield worse training sets than a model with low ImageNet accuracy that is\n",
            "trained on a small amount of high-quality data. Based on our insights, we\n",
            "construct new data filtering networks that induce state-of-the-art image-text\n",
            "datasets. Specifically, our best performing dataset DFN-5B enables us to train\n",
            "state-of-the-art CLIP models for their compute budgets: among other\n",
            "improvements on a variety of tasks, a ViT-H trained on our dataset achieves\n",
            "84.4% zero-shot transfer accuracy on ImageNet, out-performing models trained on\n",
            "other datasets such as LAION-2B, DataComp-1B, or OpenAI's WIT. In order to\n",
            "facilitate further research in dataset design, we also release a new 2 billion\n",
            "example dataset DFN-2B and show that high performance data filtering networks\n",
            "can be trained from scratch using only publicly available data.\n",
            "\n",
            "504. Title: ReMasker: Imputing Tabular Data with Masked Autoencoding\n",
            "   Abstract: A key challenge in training Large Language Models (LLMs) is properly aligning\n",
            "them with human preferences. Reinforcement Learning with Human Feedback (RLHF)\n",
            "uses pairwise comparisons from human annotators to train reward functions and\n",
            "has emerged as a popular alignment method. However, input datasets in RLHF are\n",
            "not necessarily balanced in the types of questions and answers that are\n",
            "included. Therefore, we want RLHF algorithms to perform well even when the set\n",
            "of alternatives is not uniformly distributed. Drawing on insights from social\n",
            "choice theory, we introduce robustness to approximate clones, a desirable\n",
            "property of RLHF algorithms which requires that adding near-duplicate\n",
            "alternatives does not significantly change the learned reward function. We\n",
            "first demonstrate that the standard RLHF algorithm based on regularized maximum\n",
            "likelihood estimation (MLE) fails to satisfy this property. We then propose the\n",
            "weighted MLE, a new RLHF algorithm that modifies the standard regularized MLE\n",
            "by weighting alternatives based on their similarity to other alternatives. This\n",
            "new algorithm guarantees robustness to approximate clones while preserving\n",
            "desirable theoretical properties.\n",
            "\n",
            "505. Title: At Which Training Stage Does Code Data Help LLMs Reasoning?\n",
            "   Abstract: With the breakthrough of large models, Segment Anything Model (SAM) and its\n",
            "extensions have been attempted to apply in diverse tasks of computer vision.\n",
            "Underwater salient instance segmentation is a foundational and vital step for\n",
            "various underwater vision tasks, which often suffer from low segmentation\n",
            "accuracy due to the complex underwater circumstances and the adaptive ability\n",
            "of models. Moreover, the lack of large-scale datasets with pixel-level salient\n",
            "instance annotations has impeded the development of machine learning techniques\n",
            "in this field. To address these issues, we construct the first large-scale\n",
            "underwater salient instance segmentation dataset (USIS10K), which contains\n",
            "10,632 underwater images with pixel-level annotations in 7 categories from\n",
            "various underwater scenes. Then, we propose an Underwater Salient Instance\n",
            "Segmentation architecture based on Segment Anything Model (USIS-SAM)\n",
            "specifically for the underwater domain. We devise an Underwater Adaptive Visual\n",
            "Transformer (UA-ViT) encoder to incorporate underwater domain visual prompts\n",
            "into the segmentation network. We further design an out-of-the-box underwater\n",
            "Salient Feature Prompter Generator (SFPG) to automatically generate salient\n",
            "prompters instead of explicitly providing foreground points or boxes as prompts\n",
            "in SAM. Comprehensive experimental results show that our USIS-SAM method can\n",
            "achieve superior performance on USIS10K datasets compared to the\n",
            "state-of-the-art methods. Datasets and codes are released on\n",
            "https://github.com/LiamLian0727/USIS10K.\n",
            "\n",
            "506. Title: HiGen: Hierarchical Graph Generative Networks\n",
            "   Abstract: Understanding, predicting, and learning from other people's actions are\n",
            "fundamental human social-cognitive skills. Little is known about how and when\n",
            "we consider other's actions and outcomes when making our own decisions. We\n",
            "developed a novel task to study social influence in decision-making: the social\n",
            "multi-armed bandit task. This task assesses how people learn policies for\n",
            "optimal choices based on their own outcomes and another player's (observed)\n",
            "outcomes. The majority of participants integrated information gained through\n",
            "observation of their partner similarly as information gained through their own\n",
            "actions. This lead to a suboptimal decision-making strategy. Interestingly,\n",
            "event-related potentials time-locked to stimulus onset qualitatively similar\n",
            "but the amplitudes are attenuated in the solo compared to the dyadic version.\n",
            "This might indicate that arousal and attention after receiving a reward are\n",
            "sustained when a second agent is present but not when playing alone.\n",
            "\n",
            "507. Title: LEAP: Liberate Sparse-View 3D Modeling from Camera Poses\n",
            "   Abstract: Large Language Models (LLMs) have exhibited remarkable reasoning capabilities\n",
            "and become the foundation of language technologies. Inspired by the great\n",
            "success of code data in training LLMs, we naturally wonder at which training\n",
            "stage introducing code data can really help LLMs reasoning. To this end, this\n",
            "paper systematically explores the impact of code data on LLMs at different\n",
            "stages. Concretely, we introduce the code data at the pre-training stage,\n",
            "instruction-tuning stage, and both of them, respectively. Then, the reasoning\n",
            "capability of LLMs is comprehensively and fairly evaluated via six reasoning\n",
            "tasks in five domains. We critically analyze the experimental results and\n",
            "provide conclusions with insights. First, pre-training LLMs with the mixture of\n",
            "code and text can significantly enhance LLMs' general reasoning capability\n",
            "almost without negative transfer on other tasks. Besides, at the\n",
            "instruction-tuning stage, code data endows LLMs the task-specific reasoning\n",
            "capability. Moreover, the dynamic mixing strategy of code and text data assists\n",
            "LLMs to learn reasoning capability step-by-step during training. These insights\n",
            "deepen the understanding of LLMs regarding reasoning ability for their\n",
            "application, such as scientific question answering, legal support, etc. The\n",
            "source code and model parameters are released at the\n",
            "link:~\\url{https://github.com/yingweima2022/CodeLLM}.\n",
            "\n",
            "508. Title: Proving Test Set Contamination in Black-Box Language Models\n",
            "   Abstract: 3D Reconstruction of moving articulated objects without additional\n",
            "information about object structure is a challenging problem. Current methods\n",
            "overcome such challenges by employing category-specific skeletal models.\n",
            "Consequently, they do not generalize well to articulated objects in the wild.\n",
            "We treat an articulated object as an unknown, semi-rigid skeletal structure\n",
            "surrounded by nonrigid material (e.g., skin). Our method simultaneously\n",
            "estimates the visible (explicit) representation (3D shapes, colors, camera\n",
            "parameters) and the implicit skeletal representation, from motion cues in the\n",
            "object video without 3D supervision. Our implicit representation consists of\n",
            "four parts. (1) Skeleton, which specifies how semi-rigid parts are connected.\n",
            "(2) \\textcolor{black}{Skinning Weights}, which associates each surface vertex\n",
            "with semi-rigid parts with probability. (3) Rigidity Coefficients, specifying\n",
            "the articulation of the local surface. (4) Time-Varying Transformations, which\n",
            "specify the skeletal motion and surface deformation parameters. We introduce an\n",
            "algorithm that uses physical constraints as regularization terms and\n",
            "iteratively estimates both implicit and explicit representations. Our method is\n",
            "category-agnostic, thus eliminating the need for category-specific skeletons,\n",
            "we show that our method outperforms state-of-the-art across standard video\n",
            "datasets.\n",
            "\n",
            "509. Title: Memorization in Self-Supervised Learning Improves Downstream Generalization\n",
            "   Abstract: Learning from Label Proportions (LLP) is a learning problem where only\n",
            "aggregate level labels are available for groups of instances, called bags,\n",
            "during training, and the aim is to get the best performance at the\n",
            "instance-level on the test data. This setting arises in domains like\n",
            "advertising and medicine due to privacy considerations. We propose a novel\n",
            "algorithmic framework for this problem that iteratively performs two main\n",
            "steps. For the first step (Pseudo Labeling) in every iteration, we define a\n",
            "Gibbs distribution over binary instance labels that incorporates a) covariate\n",
            "information through the constraint that instances with similar covariates\n",
            "should have similar labels and b) the bag level aggregated label. We then use\n",
            "Belief Propagation (BP) to marginalize the Gibbs distribution to obtain pseudo\n",
            "labels. In the second step (Embedding Refinement), we use the pseudo labels to\n",
            "provide supervision for a learner that yields a better embedding. Further, we\n",
            "iterate on the two steps again by using the second step's embeddings as new\n",
            "covariates for the next iteration. In the final iteration, a classifier is\n",
            "trained using the pseudo labels. Our algorithm displays strong gains against\n",
            "several SOTA baselines (up to 15%) for the LLP Binary Classification problem on\n",
            "various dataset types - tabular and Image. We achieve these improvements with\n",
            "minimal computational overhead above standard supervised learning due to Belief\n",
            "Propagation, for large bag sizes, even for a million samples.\n",
            "\n",
            "510. Title: Towards Robust Multi-Modal Reasoning via Model Selection\n",
            "   Abstract: Language models typically need to be trained or finetuned in order to acquire\n",
            "new knowledge, which involves updating their weights. We instead envision\n",
            "language models that can simply read and memorize new data at inference time,\n",
            "thus acquiring new knowledge immediately. In this work, we extend language\n",
            "models with the ability to memorize the internal representations of past\n",
            "inputs. We demonstrate that an approximate kNN lookup into a non-differentiable\n",
            "memory of recent (key, value) pairs improves language modeling across various\n",
            "benchmarks and tasks, including generic webtext (C4), math papers (arXiv),\n",
            "books (PG-19), code (Github), as well as formal theorems (Isabelle). We show\n",
            "that the performance steadily improves when we increase the size of memory up\n",
            "to 262K tokens. On benchmarks including code and mathematics, we find that the\n",
            "model is capable of making use of newly defined functions and theorems during\n",
            "test time.\n",
            "\n",
            "511. Title: CORN: Contact-based Object Representation for Nonprehensile Manipulation of General Unseen Objects\n",
            "   Abstract: The reasoning capabilities of LLM (Large Language Model) are widely\n",
            "acknowledged in recent research, inspiring studies on tool learning and\n",
            "autonomous agents. LLM serves as the \"brain\" of the agent, orchestrating\n",
            "multiple tools for collaborative multi-step task solving. Unlike methods\n",
            "invoking tools like calculators or weather APIs for straightforward tasks,\n",
            "multi-modal agents excel by integrating diverse AI models for complex\n",
            "challenges. However, current multi-modal agents neglect the significance of\n",
            "model selection: they primarily focus on the planning and execution phases, and\n",
            "will only invoke predefined task-specific models for each subtask, making the\n",
            "execution fragile. Meanwhile, other traditional model selection methods are\n",
            "either incompatible with or suboptimal for the multi-modal agent scenarios, due\n",
            "to ignorance of dependencies among subtasks arising by multi-step reasoning. To\n",
            "this end, we identify the key challenges therein and propose the $\\textit{M}^3$\n",
            "framework as a plug-in with negligible runtime overhead at test-time. This\n",
            "framework improves model selection and bolsters the robustness of multi-modal\n",
            "agents in multi-step reasoning. In the absence of suitable benchmarks, we\n",
            "create MS-GQA, a new dataset specifically designed to investigate the model\n",
            "selection challenge in multi-modal agents. Our experiments reveal that our\n",
            "framework enables dynamic model selection, considering both user inputs and\n",
            "subtask dependencies, thereby robustifying the overall reasoning process. Our\n",
            "code and benchmark: https://github.com/LINs-lab/M3.\n",
            "\n",
            "512. Title: MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts\n",
            "   Abstract: Given a node-attributed graph, and a graph task (link prediction or node\n",
            "classification), can we tell if a graph neural network (GNN) will perform well?\n",
            "More specifically, do the graph structure and the node features carry enough\n",
            "usable information for the task? Our goals are (1) to develop a fast tool to\n",
            "measure how much information is in the graph structure and in the node\n",
            "features, and (2) to exploit the information to solve the task, if there is\n",
            "enough. We propose NetInfoF, a framework including NetInfoF_Probe and\n",
            "NetInfoF_Act, for the measurement and the exploitation of network usable\n",
            "information (NUI), respectively. Given a graph data, NetInfoF_Probe measures\n",
            "NUI without any model training, and NetInfoF_Act solves link prediction and\n",
            "node classification, while two modules share the same backbone. In summary,\n",
            "NetInfoF has following notable advantages: (a) General, handling both link\n",
            "prediction and node classification; (b) Principled, with theoretical guarantee\n",
            "and closed-form solution; (c) Effective, thanks to the proposed adjustment to\n",
            "node similarity; (d) Scalable, scaling linearly with the input size. In our\n",
            "carefully designed synthetic datasets, NetInfoF correctly identifies the ground\n",
            "truth of NUI and is the only method being robust to all graph scenarios.\n",
            "Applied on real-world datasets, NetInfoF wins in 11 out of 12 times on link\n",
            "prediction compared to general GNN baselines.\n",
            "\n",
            "513. Title: NetInfoF Framework: Measuring and Exploiting Network Usable Information\n",
            "   Abstract: Large language models are trained on vast amounts of internet data, prompting\n",
            "concerns and speculation that they have memorized public benchmarks. Going from\n",
            "speculation to proof of contamination is challenging, as the pretraining data\n",
            "used by proprietary models are often not publicly accessible. We show that it\n",
            "is possible to provide provable guarantees of test set contamination in\n",
            "language models without access to pretraining data or model weights. Our\n",
            "approach leverages the fact that when there is no data contamination, all\n",
            "orderings of an exchangeable benchmark should be equally likely. In contrast,\n",
            "the tendency for language models to memorize example order means that a\n",
            "contaminated language model will find certain canonical orderings to be much\n",
            "more likely than others. Our test flags potential contamination whenever the\n",
            "likelihood of a canonically ordered benchmark dataset is significantly higher\n",
            "than the likelihood after shuffling the examples. We demonstrate that our\n",
            "procedure is sensitive enough to reliably prove test set contamination in\n",
            "challenging situations, including models as small as 1.4 billion parameters, on\n",
            "small test sets of only 1000 examples, and datasets that appear only a few\n",
            "times in the pretraining corpus. Using our test, we audit five popular publicly\n",
            "accessible language models for test set contamination and find little evidence\n",
            "for pervasive contamination.\n",
            "\n",
            "514. Title: Depthwise Hyperparameter Transfer in Residual Networks: Dynamics and Scaling Limit\n",
            "   Abstract: The structure learning problem consists of fitting data generated by a\n",
            "Directed Acyclic Graph (DAG) to correctly reconstruct its arcs. In this\n",
            "context, differentiable approaches constrain or regularize the optimization\n",
            "problem using a continuous relaxation of the acyclicity property. The\n",
            "computational cost of evaluating graph acyclicity is cubic on the number of\n",
            "nodes and significantly affects scalability. In this paper we introduce COSMO,\n",
            "a constraint-free continuous optimization scheme for acyclic structure\n",
            "learning. At the core of our method, we define a differentiable approximation\n",
            "of an orientation matrix parameterized by a single priority vector. Differently\n",
            "from previous work, our parameterization fits a smooth orientation matrix and\n",
            "the resulting acyclic adjacency matrix without evaluating acyclicity at any\n",
            "step. Despite the absence of explicit constraints, we prove that COSMO always\n",
            "converges to an acyclic solution. In addition to being asymptotically faster,\n",
            "our empirical analysis highlights how COSMO performance on graph reconstruction\n",
            "compares favorably with competing structure learning methods.\n",
            "\n",
            "515. Title: SNIP: Bridging Mathematical Symbolic and Numeric Realms with Unified Pre-training\n",
            "   Abstract: The cost of hyperparameter tuning in deep learning has been rising with model\n",
            "sizes, prompting practitioners to find new tuning methods using a proxy of\n",
            "smaller networks. One such proposal uses $\\mu$P parameterized networks, where\n",
            "the optimal hyperparameters for small width networks transfer to networks with\n",
            "arbitrarily large width. However, in this scheme, hyperparameters do not\n",
            "transfer across depths. As a remedy, we study residual networks with a residual\n",
            "branch scale of $1/\\sqrt{\\text{depth}}$ in combination with the $\\mu$P\n",
            "parameterization. We provide experiments demonstrating that residual\n",
            "architectures including convolutional ResNets and Vision Transformers trained\n",
            "with this parameterization exhibit transfer of optimal hyperparameters across\n",
            "width and depth on CIFAR-10 and ImageNet. Furthermore, our empirical findings\n",
            "are supported and motivated by theory. Using recent developments in the\n",
            "dynamical mean field theory (DMFT) description of neural network learning\n",
            "dynamics, we show that this parameterization of ResNets admits a well-defined\n",
            "feature learning joint infinite-width and infinite-depth limit and show\n",
            "convergence of finite-size network dynamics towards this limit.\n",
            "\n",
            "516. Title: ReTaSA: A Nonparametric Functional Estimation Approach for Addressing Continuous Target Shift\n",
            "   Abstract: Evaluating the performance of a well-trained GNN model on real-world graphs\n",
            "is a pivotal step for reliable GNN online deployment and serving. Due to a lack\n",
            "of test node labels and unknown potential training-test graph data distribution\n",
            "shifts, conventional model evaluation encounters limitations in calculating\n",
            "performance metrics (e.g., test error) and measuring graph data-level\n",
            "discrepancies, particularly when the training graph used for developing GNNs\n",
            "remains unobserved during test time. In this paper, we study a new research\n",
            "problem, online GNN evaluation, which aims to provide valuable insights into\n",
            "the well-trained GNNs's ability to effectively generalize to real-world\n",
            "unlabeled graphs under the test-time graph distribution shifts. Concretely, we\n",
            "develop an effective learning behavior discrepancy score, dubbed LeBeD, to\n",
            "estimate the test-time generalization errors of well-trained GNN models.\n",
            "Through a novel GNN re-training strategy with a parameter-free optimality\n",
            "criterion, the proposed LeBeD comprehensively integrates learning behavior\n",
            "discrepancies from both node prediction and structure reconstruction\n",
            "perspectives. This enables the effective evaluation of the well-trained GNNs'\n",
            "ability to capture test node semantics and structural representations, making\n",
            "it an expressive metric for estimating the generalization error in online GNN\n",
            "evaluation. Extensive experiments on real-world test graphs under diverse graph\n",
            "distribution shifts could verify the effectiveness of the proposed method,\n",
            "revealing its strong correlation with ground-truth test errors on various\n",
            "well-trained GNN models.\n",
            "\n",
            "517. Title: Language Model Cascades: Token-Level Uncertainty And Beyond\n",
            "   Abstract: We propose the cascade attribute learning network (CALNet), which can learn\n",
            "attributes in a control task separately and assemble them together. Our\n",
            "contribution is twofold: first we propose attribute learning in reinforcement\n",
            "learning (RL). Attributes used to be modeled using constraint functions or\n",
            "terms in the objective function, making it hard to transfer. Attribute\n",
            "learning, on the other hand, models these task properties as modules in the\n",
            "policy network. We also propose using novel cascading compensative networks in\n",
            "the CALNet to learn and assemble attributes. Using the CALNet, one can zero\n",
            "shoot an unseen task by separately learning all its attributes, and assembling\n",
            "the attribute modules. We have validated the capacity of our model on a wide\n",
            "variety of control problems with attributes in time, position, velocity and\n",
            "acceleration phases.\n",
            "\n",
            "518. Title: DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning\n",
            "   Abstract: Hyperbolic embeddings have demonstrated their effectiveness in capturing\n",
            "measures of uncertainty and hierarchical relationships across various\n",
            "deep-learning tasks, including image segmentation and active learning. However,\n",
            "their application in modern vision-language models (VLMs) has been limited. A\n",
            "notable exception is MERU, which leverages the hierarchical properties of\n",
            "hyperbolic space in the CLIP ViT-large model, consisting of hundreds of\n",
            "millions parameters. In our work, we address the challenges of scaling\n",
            "multi-modal hyperbolic models by orders of magnitude in terms of parameters\n",
            "(billions) and training complexity using the BLIP-2 architecture. Although\n",
            "hyperbolic embeddings offer potential insights into uncertainty not present in\n",
            "Euclidean embeddings, our analysis reveals that scaling these models is\n",
            "particularly difficult. We propose a novel training strategy for a hyperbolic\n",
            "version of BLIP-2, which allows to achieve comparable performance to its\n",
            "Euclidean counterpart, while maintaining stability throughout the training\n",
            "process and showing a meaningful indication of uncertainty with each embedding.\n",
            "\n",
            "519. Title: An Extensible Framework for Open Heterogeneous Collaborative Perception\n",
            "   Abstract: Collaborative perception aims to mitigate the limitations of single-agent\n",
            "perception, such as occlusions, by facilitating data exchange among multiple\n",
            "agents. However, most current works consider a homogeneous scenario where all\n",
            "agents use identity sensors and perception models. In reality, heterogeneous\n",
            "agent types may continually emerge and inevitably face a domain gap when\n",
            "collaborating with existing agents. In this paper, we introduce a new open\n",
            "heterogeneous problem: how to accommodate continually emerging new\n",
            "heterogeneous agent types into collaborative perception, while ensuring high\n",
            "perception performance and low integration cost? To address this problem, we\n",
            "propose HEterogeneous ALliance (HEAL), a novel extensible collaborative\n",
            "perception framework. HEAL first establishes a unified feature space with\n",
            "initial agents via a novel multi-scale foreground-aware Pyramid Fusion network.\n",
            "When heterogeneous new agents emerge with previously unseen modalities or\n",
            "models, we align them to the established unified space with an innovative\n",
            "backward alignment. This step only involves individual training on the new\n",
            "agent type, thus presenting extremely low training costs and high\n",
            "extensibility. To enrich agents' data heterogeneity, we bring OPV2V-H, a new\n",
            "large-scale dataset with more diverse sensor types. Extensive experiments on\n",
            "OPV2V-H and DAIR-V2X datasets show that HEAL surpasses SOTA methods in\n",
            "performance while reducing the training parameters by 91.5% when integrating 3\n",
            "new agent types. We further implement a comprehensive codebase at:\n",
            "https://github.com/yifanlu0227/HEAL\n",
            "\n",
            "520. Title: FedHyper: A Universal and Robust Learning Rate Scheduler for Federated Learning with Hypergradient Descent\n",
            "   Abstract: Recent neural rendering and reconstruction techniques, such as NeRFs or\n",
            "Gaussian Splatting, have shown remarkable novel view synthesis capabilities but\n",
            "require hundreds of images of the scene from diverse viewpoints to render\n",
            "high-quality novel views. With fewer images available, these methods start to\n",
            "fail since they can no longer correctly triangulate the underlying 3D geometry\n",
            "and converge to a non-optimal solution. These failures can manifest as floaters\n",
            "or blurry renderings in sparsely observed areas of the scene. In this paper, we\n",
            "propose Re-Nerfing, a simple and general add-on approach that leverages novel\n",
            "view synthesis itself to tackle this problem. Using an already trained NVS\n",
            "method, we render novel views between existing ones and augment the training\n",
            "data to optimize a second model. This introduces additional multi-view\n",
            "constraints and allows the second model to converge to a better solution. With\n",
            "Re-Nerfing we achieve significant improvements upon multiple pipelines based on\n",
            "NeRF and Gaussian-Splatting in sparse view settings of the mip-NeRF 360 and\n",
            "LLFF datasets. Notably, Re-Nerfing does not require prior knowledge or extra\n",
            "supervision signals, making it a flexible and practical add-on.\n",
            "\n",
            "521. Title: Mitigating Emergent Robustness Degradation while Scaling Graph Learning\n",
            "   Abstract: Powerful as they are, graph neural networks (GNNs) are known to be vulnerable\n",
            "to distribution shifts. Recently, test-time adaptation (TTA) has attracted\n",
            "attention due to its ability to adapt a pre-trained model to a target domain,\n",
            "without re-accessing the source domain. However, existing TTA algorithms are\n",
            "primarily designed for attribute shifts in vision tasks, where samples are\n",
            "independent. These methods perform poorly on graph data that experience\n",
            "structure shifts, where node connectivity differs between source and target\n",
            "graphs. We attribute this performance gap to the distinct impact of node\n",
            "attribute shifts versus graph structure shifts: the latter significantly\n",
            "degrades the quality of node representations and blurs the boundaries between\n",
            "different node categories. To address structure shifts in graphs, we propose\n",
            "Matcha, an innovative framework designed for effective and efficient adaptation\n",
            "to structure shifts by adjusting the htop-aggregation parameters in GNNs. To\n",
            "enhance the representation quality, we design a prediction-informed clustering\n",
            "loss to encourage the formation of distinct clusters for different node\n",
            "categories. Additionally, Matcha seamlessly integrates with existing TTA\n",
            "algorithms, allowing it to handle attribute shifts effectively while improving\n",
            "overall performance under combined structure and attribute shifts. We validate\n",
            "the effectiveness of Matcha on both synthetic and real-world datasets,\n",
            "demonstrating its robustness across various combinations of structure and\n",
            "attribute shifts. Our code is available at https://github.com/baowenxuan/Matcha .\n",
            "\n",
            "522. Title: Generative Pre-training for Speech with Flow Matching\n",
            "   Abstract: Generative models have gained more and more attention in recent years for\n",
            "their remarkable success in tasks that required estimating and sampling data\n",
            "distribution to generate high-fidelity synthetic data. In speech,\n",
            "text-to-speech synthesis and neural vocoder are good examples where generative\n",
            "models have shined. While generative models have been applied to different\n",
            "applications in speech, there exists no general-purpose generative model that\n",
            "models speech directly. In this work, we take a step toward this direction by\n",
            "showing a single pre-trained generative model can be adapted to different\n",
            "downstream tasks with strong performance. Specifically, we pre-trained a\n",
            "generative model, named SpeechFlow, on 60k hours of untranscribed speech with\n",
            "Flow Matching and masked conditions. Experiment results show the pre-trained\n",
            "generative model can be fine-tuned with task-specific data to match or surpass\n",
            "existing expert models on speech enhancement, separation, and synthesis. Our\n",
            "work suggested a foundational model for generation tasks in speech can be built\n",
            "with generative pre-training.\n",
            "\n",
            "523. Title: Particle Guidance: non-I.I.D. Diverse Sampling with Diffusion Models\n",
            "   Abstract: In light of the widespread success of generative models, a significant amount\n",
            "of research has gone into speeding up their sampling time. However, generative\n",
            "models are often sampled multiple times to obtain a diverse set incurring a\n",
            "cost that is orthogonal to sampling time. We tackle the question of how to\n",
            "improve diversity and sample efficiency by moving beyond the common assumption\n",
            "of independent samples. We propose particle guidance, an extension of\n",
            "diffusion-based generative sampling where a joint-particle time-evolving\n",
            "potential enforces diversity. We analyze theoretically the joint distribution\n",
            "that particle guidance generates, how to learn a potential that achieves\n",
            "optimal diversity, and the connections with methods in other disciplines.\n",
            "Empirically, we test the framework both in the setting of conditional image\n",
            "generation, where we are able to increase diversity without affecting quality,\n",
            "and molecular conformer generation, where we reduce the state-of-the-art median\n",
            "error by 13% on average.\n",
            "\n",
            "524. Title: MetaPhysiCa: Improving OOD Robustness in Physics-informed Machine Learning\n",
            "   Abstract: Neural algorithmic reasoning is an emerging research direction that endows\n",
            "neural networks with the ability to mimic algorithmic executions step-by-step.\n",
            "A common paradigm in existing designs involves the use of historical embeddings\n",
            "in predicting the results of future execution steps. Our observation in this\n",
            "work is that such historical dependence intrinsically contradicts the Markov\n",
            "nature of algorithmic reasoning tasks. Based on this motivation, we present our\n",
            "ForgetNet, which does not use historical embeddings and thus is consistent with\n",
            "the Markov nature of the tasks. To address challenges in training ForgetNet at\n",
            "early stages, we further introduce G-ForgetNet, which uses a gating mechanism\n",
            "to allow for the selective integration of historical embeddings. Such an\n",
            "enhanced capability provides valuable computational pathways during the model's\n",
            "early training phase. Our extensive experiments, based on the CLRS-30\n",
            "algorithmic reasoning benchmark, demonstrate that both ForgetNet and\n",
            "G-ForgetNet achieve better generalization capability than existing methods.\n",
            "Furthermore, we investigate the behavior of the gating mechanism, highlighting\n",
            "its degree of alignment with our intuitions and its effectiveness for robust\n",
            "performance.\n",
            "\n",
            "525. Title: The Wasserstein Believer: Learning Belief Updates for Partially Observable Environments through Reliable Latent Space Models\n",
            "   Abstract: Partially Observable Markov Decision Processes (POMDPs) are used to model\n",
            "environments where the full state cannot be perceived by an agent. As such the\n",
            "agent needs to reason taking into account the past observations and actions.\n",
            "However, simply remembering the full history is generally intractable due to\n",
            "the exponential growth in the history space. Maintaining a probability\n",
            "distribution that models the belief over what the true state is can be used as\n",
            "a sufficient statistic of the history, but its computation requires access to\n",
            "the model of the environment and is often intractable. While SOTA algorithms\n",
            "use Recurrent Neural Networks to compress the observation-action history aiming\n",
            "to learn a sufficient statistic, they lack guarantees of success and can lead\n",
            "to sub-optimal policies. To overcome this, we propose the Wasserstein Belief\n",
            "Updater, an RL algorithm that learns a latent model of the POMDP and an\n",
            "approximation of the belief update. Our approach comes with theoretical\n",
            "guarantees on the quality of our approximation ensuring that our outputted\n",
            "beliefs allow for learning the optimal value function.\n",
            "\n",
            "526. Title: Thin-Shell Object Manipulations With Differentiable Physics Simulations\n",
            "   Abstract: We introduce a method for manipulating objects in three-dimensional space\n",
            "using controlled fluid streams. To achieve this, we train a neural network\n",
            "controller in a differentiable simulation and evaluate it in a simulated\n",
            "environment consisting of an 8x8 grid of vertical emitters. By carrying out\n",
            "various horizontal displacement tasks such as moving objects to specific\n",
            "positions while reacting to external perturbations, we demonstrate that a\n",
            "controller, trained with a limited number of iterations, can generalise to\n",
            "longer episodes and learn the complex dynamics of fluid-solid interactions.\n",
            "Importantly, our approach requires only the observation of the manipulated\n",
            "object's state, paving the way for the development of physical systems that\n",
            "enable contactless manipulation of objects using air streams.\n",
            "\n",
            "527. Title: Teaching Large Language Models to Self-Debug\n",
            "   Abstract: Recently there has been a significant surge in multimodal learning in terms\n",
            "of both image-to-text and text-to-image generation. However, the success is\n",
            "typically limited to English, leaving other languages largely behind. Building\n",
            "a competitive counterpart in other languages is highly challenging due to the\n",
            "low-resource nature of non-English multimodal data (i.e., lack of large-scale,\n",
            "high-quality image-text data). In this work, we propose MPM, an effective\n",
            "training paradigm for training large multimodal models in non-English\n",
            "languages. MPM demonstrates that Multilingual language models can Pivot\n",
            "zero-shot Multimodal learning across languages. Specifically, based on a strong\n",
            "multilingual large language model, multimodal models pretrained on English-only\n",
            "image-text data can well generalize to other languages in a (quasi)-zero-shot\n",
            "manner, even surpassing models trained on image-text data in native languages.\n",
            "Taking Chinese as a practice of MPM, we build large multimodal models VisCPM in\n",
            "image-to-text and text-to-image generation, which achieve state-of-the-art\n",
            "(open-source) performance in Chinese. To facilitate future research, we\n",
            "open-source codes and model weights at https://github.com/OpenBMB/VisCPM.git.\n",
            "\n",
            "528. Title: Large Multilingual Models Pivot Zero-Shot Multimodal Learning across Languages\n",
            "   Abstract: Computing the optimal transport distance between statistical distributions is\n",
            "a fundamental task in machine learning. One remarkable recent advancement is\n",
            "entropic regularization and the Sinkhorn algorithm, which utilizes only matrix\n",
            "scaling and guarantees an approximated solution with near-linear runtime.\n",
            "Despite the success of the Sinkhorn algorithm, its runtime may still be slow\n",
            "due to the potentially large number of iterations needed for convergence. To\n",
            "achieve possibly super-exponential convergence, we present\n",
            "Sinkhorn-Newton-Sparse (SNS), an extension to the Sinkhorn algorithm, by\n",
            "introducing early stopping for the matrix scaling steps and a second stage\n",
            "featuring a Newton-type subroutine. Adopting the variational viewpoint that the\n",
            "Sinkhorn algorithm maximizes a concave Lyapunov potential, we offer the insight\n",
            "that the Hessian matrix of the potential function is approximately sparse.\n",
            "Sparsification of the Hessian results in a fast $O(n^2)$ per-iteration\n",
            "complexity, the same as the Sinkhorn algorithm. In terms of total iteration\n",
            "count, we observe that the SNS algorithm converges orders of magnitude faster\n",
            "across a wide range of practical cases, including optimal transportation\n",
            "between empirical distributions and calculating the Wasserstein $W_1, W_2$\n",
            "distance of discretized densities. The empirical performance is corroborated by\n",
            "a rigorous bound on the approximate sparsity of the Hessian matrix.\n",
            "\n",
            "529. Title: Accelerating Sinkhorn algorithm with sparse Newton iterations\n",
            "   Abstract: We introduce AgreeMate, a framework for training Large Language Models (LLMs)\n",
            "to perform strategic price negotiations through natural language. We apply\n",
            "recent advances to a negotiation setting where two agents (i.e. buyer or\n",
            "seller) use natural language to bargain on goods using coarse actions.\n",
            "Specifically, we present the performance of Large Language Models when used as\n",
            "agents within a decoupled (modular) bargaining architecture. We demonstrate\n",
            "that using prompt engineering, fine-tuning, and chain-of-thought prompting\n",
            "enhances model performance, as defined by novel metrics. We use attention\n",
            "probing to show model attention to semantic relationships between tokens during\n",
            "negotiations.\n",
            "\n",
            "530. Title: The False Promise of Imitating Proprietary Language Models\n",
            "   Abstract: An emerging method to cheaply improve a weaker language model is to finetune\n",
            "it on outputs from a stronger model, such as a proprietary system like ChatGPT\n",
            "(e.g., Alpaca, Self-Instruct, and others). This approach looks to cheaply\n",
            "imitate the proprietary model's capabilities using a weaker open-source model.\n",
            "In this work, we critically analyze this approach. We first finetune a series\n",
            "of LMs that imitate ChatGPT using varying base model sizes (1.5B--13B), data\n",
            "sources, and imitation data amounts (0.3M--150M tokens). We then evaluate the\n",
            "models using crowd raters and canonical NLP benchmarks. Initially, we were\n",
            "surprised by the output quality of our imitation models -- they appear far\n",
            "better at following instructions, and crowd workers rate their outputs as\n",
            "competitive with ChatGPT. However, when conducting more targeted automatic\n",
            "evaluations, we find that imitation models close little to none of the gap from\n",
            "the base LM to ChatGPT on tasks that are not heavily supported in the imitation\n",
            "data. We show that these performance discrepancies may slip past human raters\n",
            "because imitation models are adept at mimicking ChatGPT's style but not its\n",
            "factuality. Overall, we conclude that model imitation is a false promise: there\n",
            "exists a substantial capabilities gap between open and closed LMs that, with\n",
            "current methods, can only be bridged using an unwieldy amount of imitation data\n",
            "or by using more capable base LMs. In turn, we argue that the highest leverage\n",
            "action for improving open-source models is to tackle the difficult challenge of\n",
            "developing better base LMs, rather than taking the shortcut of imitating\n",
            "proprietary systems.\n",
            "\n",
            "531. Title: Select to Perfect: Imitating desired behavior from large multi-agent data\n",
            "   Abstract: In many neural networks, different values of the parameters may result in the\n",
            "same loss value. Parameter space symmetries are loss-invariant transformations\n",
            "that change the model parameters. Teleportation applies such transformations to\n",
            "accelerate optimization. However, the exact mechanism behind this algorithm's\n",
            "success is not well understood. In this paper, we show that teleportation not\n",
            "only speeds up optimization in the short-term, but gives overall faster time to\n",
            "convergence. Additionally, teleporting to minima with different curvatures\n",
            "improves generalization, which suggests a connection between the curvature of\n",
            "the minimum and generalization ability. Finally, we show that integrating\n",
            "teleportation into a wide range of optimization algorithms and\n",
            "optimization-based meta-learning improves convergence. Our results showcase the\n",
            "versatility of teleportation and demonstrate the potential of incorporating\n",
            "symmetry in optimization.\n",
            "\n",
            "532. Title: ODICE: Revealing the Mystery of Distribution Correction Estimation via Orthogonal-gradient Update\n",
            "   Abstract: While large language models (LLMs) have enabled learning knowledge from the\n",
            "pre-training corpora, the acquired knowledge may be fundamentally incorrect or\n",
            "outdated over time, which necessitates rectifying the knowledge of the language\n",
            "model (LM) after the training. A promising approach involves employing a\n",
            "hyper-network to generate parameter shift, whereas existing hyper-networks\n",
            "suffer from inferior scalability in synchronous editing operation amount. To\n",
            "mitigate the problem, we propose the MAssive Language Model Editing Network\n",
            "(MALMEN), which formulates the parameter shift aggregation as the least square\n",
            "problem, subsequently updating the LM parameters using the normal equation. To\n",
            "accommodate editing multiple facts simultaneously with limited memory budgets,\n",
            "we separate the computation on the hyper-network and LM, enabling arbitrary\n",
            "batch size on both neural networks. Our method is evaluated by editing up to\n",
            "thousands of facts on LMs with different architectures, i.e., BERT-base, GPT-2,\n",
            "T5-XL (2.8B), and GPT-J (6B), across various knowledge-intensive NLP tasks,\n",
            "i.e., closed book fact-checking and question answering. Remarkably, MALMEN is\n",
            "capable of editing hundreds of times more facts than strong baselines with the\n",
            "identical hyper-network architecture and outperforms editor specifically\n",
            "designed for GPT. Our code is available at\n",
            "https://github.com/ChenmienTan/malmen.\n",
            "\n",
            "533. Title: Concept Bottleneck Generative Models\n",
            "   Abstract: The design of interpretable deep learning models working in relational\n",
            "domains poses an open challenge: interpretable deep learning methods, such as\n",
            "Concept Bottleneck Models (CBMs), are not designed to solve relational\n",
            "problems, while relational deep learning models, such as Graph Neural Networks\n",
            "(GNNs), are not as interpretable as CBMs. To overcome these limitations, we\n",
            "propose Relational Concept Bottleneck Models (R-CBMs), a family of relational\n",
            "deep learning methods providing interpretable task predictions. As special\n",
            "cases, we show that R-CBMs are capable of both representing standard CBMs and\n",
            "message-passing GNNs. To evaluate the effectiveness and versatility of these\n",
            "models, we designed a class of experimental problems, ranging from image\n",
            "classification to link prediction in knowledge graphs. In particular we show\n",
            "that R-CBMs (i) match generalization performance of existing relational\n",
            "black-boxes, (ii) support the generation of quantified concept-based\n",
            "explanations, (iii) effectively respond to test-time interventions, and (iv)\n",
            "withstand demanding settings including out-of-distribution scenarios, limited\n",
            "training data regimes, and scarce concept supervisions.\n",
            "\n",
            "534. Title: ZipIt! Merging Models from Different Tasks without Training\n",
            "   Abstract: Batch reinforcement learning (RL) defines the task of learning from a fixed\n",
            "batch of data lacking exhaustive exploration. Worst-case optimality algorithms,\n",
            "which calibrate a value-function model class from logged experience and perform\n",
            "some type of pessimistic evaluation under the learned model, have emerged as a\n",
            "promising paradigm for batch RL. However, contemporary works on this stream\n",
            "have commonly overlooked the hierarchical decision-making structure hidden in\n",
            "the optimization landscape. In this paper, we adopt a game-theoretical\n",
            "viewpoint and model the policy learning diagram as a two-player general-sum\n",
            "game with a leader-follower structure. We propose a novel stochastic\n",
            "gradient-based learning algorithm: StackelbergLearner, in which the leader\n",
            "player updates according to the total derivative of its objective instead of\n",
            "the usual individual gradient, and the follower player makes individual updates\n",
            "and ensures transition-consistent pessimistic reasoning. The derived learning\n",
            "dynamic naturally lends StackelbergLearner to a game-theoretic interpretation\n",
            "and provides a convergence guarantee to differentiable Stackelberg equilibria.\n",
            "From a theoretical standpoint, we provide instance-dependent regret bounds with\n",
            "general function approximation, which shows that our algorithm can learn a\n",
            "best-effort policy that is able to compete against any comparator policy that\n",
            "is covered by batch data. Notably, our theoretical regret guarantees only\n",
            "require realizability without any data coverage and strong function\n",
            "approximation conditions, e.g., Bellman closedness, which is in contrast to\n",
            "prior works lacking such guarantees. Through comprehensive experiments, we find\n",
            "that our algorithm consistently performs as well or better as compared to\n",
            "state-of-the-art methods in batch RL benchmark and real-world datasets.\n",
            "\n",
            "535. Title: SWAP: Sparse Entropic Wasserstein Regression for Robust Network Pruning\n",
            "   Abstract: AI agents are commonly trained with large datasets of demonstrations of human\n",
            "behavior. However, not all behaviors are equally safe or desirable. Desired\n",
            "characteristics for an AI agent can be expressed by assigning desirability\n",
            "scores, which we assume are not assigned to individual behaviors but to\n",
            "collective trajectories. For example, in a dataset of vehicle interactions,\n",
            "these scores might relate to the number of incidents that occurred. We first\n",
            "assess the effect of each individual agent's behavior on the collective\n",
            "desirability score, e.g., assessing how likely an agent is to cause incidents.\n",
            "This allows us to selectively imitate agents with a positive effect, e.g., only\n",
            "imitating agents that are unlikely to cause incidents. To enable this, we\n",
            "propose the concept of an agent's Exchange Value, which quantifies an\n",
            "individual agent's contribution to the collective desirability score. The\n",
            "Exchange Value is the expected change in desirability score when substituting\n",
            "the agent for a randomly selected agent. We propose additional methods for\n",
            "estimating Exchange Values from real-world datasets, enabling us to learn\n",
            "desired imitation policies that outperform relevant baselines. The project\n",
            "website can be found at https://tinyurl.com/select-to-perfect.\n",
            "\n",
            "536. Title: Beyond Imitation: Leveraging Fine-grained Quality Signals for Alignment\n",
            "   Abstract: An increasing number of vision-language tasks can be handled with little to\n",
            "no training, i.e., in a zero and few-shot manner, by marrying large language\n",
            "models (LLMs) to vision encoders, resulting in large vision-language models\n",
            "(LVLMs). While this has huge upsides, such as not requiring training data or\n",
            "custom architectures, how an input is presented to an LVLM can have a major\n",
            "impact on zero-shot model performance. In particular, inputs phrased in an\n",
            "underspecified way can result in incorrect answers due to factors like missing\n",
            "visual information, complex implicit reasoning, or linguistic ambiguity.\n",
            "Therefore, adding visually-grounded information to the input as a preemptive\n",
            "clarification should improve model performance by reducing underspecification,\n",
            "e.g., by localizing objects and disambiguating references. Similarly, in the\n",
            "VQA setting, changing the way questions are framed can make them easier for\n",
            "models to answer. To this end, we present Rephrase, Augment and Reason\n",
            "(RepARe), a gradient-free framework that extracts salient details about the\n",
            "image using the underlying LVLM as a captioner and reasoner, in order to\n",
            "propose modifications to the original question. We then use the LVLM's\n",
            "confidence over a generated answer as an unsupervised scoring function to\n",
            "select the rephrased question most likely to improve zero-shot performance.\n",
            "Focusing on three visual question answering tasks, we show that RepARe can\n",
            "result in a 3.85% (absolute) increase in zero-shot accuracy on VQAv2, 6.41%,\n",
            "and 7.94% points increase on A-OKVQA, and VizWiz respectively. Additionally, we\n",
            "find that using gold answers for oracle question candidate selection achieves a\n",
            "substantial gain in VQA accuracy by up to 14.41%. Through extensive analysis,\n",
            "we demonstrate that outputs from RepARe increase syntactic complexity, and\n",
            "effectively utilize vision-language interaction and the frozen LLM.\n",
            "\n",
            "537. Title: CCIL: Continuity-Based Data Augmentation for Corrective Imitation Learning\n",
            "   Abstract: Transfer learning is widely used in deep neural network models when there are\n",
            "few labeled examples available. The common approach is to take a pre-trained\n",
            "network in a similar task and finetune the model parameters. This is usually\n",
            "done blindly without a pre-selection from a set of pre-trained models, or by\n",
            "finetuning a set of models trained on different tasks and selecting the best\n",
            "performing one by cross-validation. We address this problem by proposing an\n",
            "approach to assess the relationship between visual tasks and their\n",
            "task-specific models. Our method uses Representation Similarity Analysis (RSA),\n",
            "which is commonly used to find a correlation between neuronal responses from\n",
            "brain data and models. With RSA we obtain a similarity score among tasks by\n",
            "computing correlations between models trained on different tasks. Our method is\n",
            "efficient as it requires only pre-trained models, and a few images with no\n",
            "further training. We demonstrate the effectiveness and efficiency of our method\n",
            "for generating task taxonomy on Taskonomy dataset. We next evaluate the\n",
            "relationship of RSA with the transfer learning performance on Taskonomy tasks\n",
            "and a new task: Pascal VOC semantic segmentation. Our results reveal that\n",
            "models trained on tasks with higher similarity score show higher transfer\n",
            "learning performance. Surprisingly, the best transfer learning result for\n",
            "Pascal VOC semantic segmentation is not obtained from the pre-trained model on\n",
            "semantic segmentation, probably due to the domain differences, and our method\n",
            "successfully selects the high performing models.\n",
            "\n",
            "538. Title: SineNet: Learning Temporal Dynamics in Time-Dependent Partial Differential Equations\n",
            "   Abstract: Alignment with human preference is a desired property of large language\n",
            "models (LLMs). Currently, the main alignment approach is based on reinforcement\n",
            "learning from human feedback (RLHF). Despite the effectiveness of RLHF, it is\n",
            "intricate to implement and train, thus recent studies explore how to develop\n",
            "alternative alignment approaches based on supervised fine-tuning (SFT). A major\n",
            "limitation of SFT is that it essentially does imitation learning, which cannot\n",
            "fully understand what are the expected behaviors. To address this issue, we\n",
            "propose an improved alignment approach named FIGA. Different from prior\n",
            "methods, we incorporate fine-grained (i.e., token or phrase level) quality\n",
            "signals that are derived by contrasting good and bad responses. Our approach\n",
            "has made two major contributions. Firstly, we curate a refined alignment\n",
            "dataset that pairs initial responses and the corresponding revised ones.\n",
            "Secondly, we devise a new loss function can leverage fine-grained quality\n",
            "signals to instruct the learning of LLMs for alignment. Extensive experiments\n",
            "have demonstrated the effectiveness of our approaches by comparing a number of\n",
            "competitive baselines.\n",
            "\n",
            "539. Title: In-Context Pretraining: Language Modeling Beyond Document Boundaries\n",
            "   Abstract: The key challenge in multiagent learning is learning a best response to the\n",
            "behaviour of other agents, which may be non-stationary: if the other agents\n",
            "adapt their strategy as well, the learning target moves. Disparate streams of\n",
            "research have approached non-stationarity from several angles, which make a\n",
            "variety of implicit assumptions that make it hard to keep an overview of the\n",
            "state of the art and to validate the innovation and significance of new works.\n",
            "This survey presents a coherent overview of work that addresses\n",
            "opponent-induced non-stationarity with tools from game theory, reinforcement\n",
            "learning and multi-armed bandits. Further, we reflect on the principle\n",
            "approaches how algorithms model and cope with this non-stationarity, arriving\n",
            "at a new framework and five categories (in increasing order of sophistication):\n",
            "ignore, forget, respond to target models, learn models, and theory of mind. A\n",
            "wide range of state-of-the-art algorithms is classified into a taxonomy, using\n",
            "these categories and key characteristics of the environment (e.g.,\n",
            "observability) and adaptation behaviour of the opponents (e.g., smooth,\n",
            "abrupt). To clarify even further we present illustrative variations of one\n",
            "domain, contrasting the strengths and limitations of each category. Finally, we\n",
            "discuss in which environments the different approaches yield most merit, and\n",
            "point to promising avenues of future research.\n",
            "\n",
            "540. Title: Zoology: Measuring and Improving Recall in Efficient Language Models\n",
            "   Abstract: Transformer framework has been showing superior performances in visual object\n",
            "tracking for its great strength in information aggregation across the template\n",
            "and search image with the well-known attention mechanism. Most recent advances\n",
            "focus on exploring attention mechanism variants for better information\n",
            "aggregation. We find these schemes are equivalent to or even just a subset of\n",
            "the basic self-attention mechanism. In this paper, we prove that the vanilla\n",
            "self-attention structure is sufficient for information aggregation, and\n",
            "structural adaption is unnecessary. The key is not the attention structure, but\n",
            "how to extract the discriminative feature for tracking and enhance the\n",
            "communication between the target and search image. Based on this finding, we\n",
            "adopt the basic vision transformer (ViT) architecture as our main tracker and\n",
            "concatenate the template and search image for feature embedding. To guide the\n",
            "encoder to capture the invariant feature for tracking, we attach a lightweight\n",
            "correlative masked decoder which reconstructs the original template and search\n",
            "image from the corresponding masked tokens. The correlative masked decoder\n",
            "serves as a plugin for the compact transform tracker and is skipped in\n",
            "inference. Our compact tracker uses the most simple structure which only\n",
            "consists of a ViT backbone and a box head, and can run at 40 fps. Extensive\n",
            "experiments show the proposed compact transform tracker outperforms existing\n",
            "approaches, including advanced attention variants, and demonstrates the\n",
            "sufficiency of self-attention in tracking tasks. Our method achieves\n",
            "state-of-the-art performance on five challenging datasets, along with the\n",
            "VOT2020, UAV123, LaSOT, TrackingNet, and GOT-10k benchmarks. Our project is\n",
            "available at https://github.com/HUSTDML/CTTrack.\n",
            "\n",
            "541. Title: H-GAP: Humanoid Control with a Generalist Planner\n",
            "   Abstract: Reinforcement learning (RL) tackles sequential decision-making problems by\n",
            "creating agents that interacts with their environment. However, existing\n",
            "algorithms often view these problem as static, focusing on point estimates for\n",
            "model parameters to maximize expected rewards, neglecting the stochastic\n",
            "dynamics of agent-environment interactions and the critical role of uncertainty\n",
            "quantification. Our research leverages the Kalman filtering paradigm to\n",
            "introduce a novel and scalable sampling algorithm called Langevinized Kalman\n",
            "Temporal-Difference (LKTD) for deep reinforcement learning. This algorithm,\n",
            "grounded in Stochastic Gradient Markov Chain Monte Carlo (SGMCMC), efficiently\n",
            "draws samples from the posterior distribution of deep neural network\n",
            "parameters. Under mild conditions, we prove that the posterior samples\n",
            "generated by the LKTD algorithm converge to a stationary distribution. This\n",
            "convergence not only enables us to quantify uncertainties associated with the\n",
            "value function and model parameters but also allows us to monitor these\n",
            "uncertainties during policy updates throughout the training phase. The LKTD\n",
            "algorithm paves the way for more robust and adaptable reinforcement learning\n",
            "approaches.\n",
            "\n",
            "542. Title: JoMA: Demystifying Multilayer Transformers via Joint Dynamics of MLP and Attention\n",
            "   Abstract: Real-world multi-agent tasks usually involve dynamic team composition with\n",
            "the emergence of roles, which should also be a key to efficient cooperation in\n",
            "multi-agent reinforcement learning (MARL). Drawing inspiration from the\n",
            "correlation between roles and agent's behavior patterns, we propose a novel\n",
            "framework of **A**ttention-guided **CO**ntrastive **R**ole representation\n",
            "learning for **M**ARL (**ACORM**) to promote behavior heterogeneity, knowledge\n",
            "transfer, and skillful coordination across agents. First, we introduce mutual\n",
            "information maximization to formalize role representation learning, derive a\n",
            "contrastive learning objective, and concisely approximate the distribution of\n",
            "negative pairs. Second, we leverage an attention mechanism to prompt the global\n",
            "state to attend to learned role representations in value decomposition,\n",
            "implicitly guiding agent coordination in a skillful role space to yield more\n",
            "expressive credit assignment. Experiments on challenging StarCraft II\n",
            "micromanagement and Google research football tasks demonstrate the\n",
            "state-of-the-art performance of our method and its advantages over existing\n",
            "approaches. Our code is available at\n",
            "[https://github.com/NJU-RL/ACORM](https://github.com/NJU-RL/ACORM).\n",
            "\n",
            "543. Title: Instructive Decoding: Instruction-Tuned Large Language Models are Self-Refiner from Noisy Instructions\n",
            "   Abstract: Fully-parametric language models generally require a huge number of model\n",
            "parameters to store the necessary knowledge for solving multiple natural\n",
            "language tasks in zero/few-shot settings. In addition, it is hard to adapt to\n",
            "the evolving world knowledge without the costly model re-training. In this\n",
            "paper, we develop a novel semi-parametric language model architecture,\n",
            "Knowledge-in-Context (KiC), which empowers a parametric text-to-text language\n",
            "model with a knowledge-rich external memory. Specifically, the external memory\n",
            "contains six different types of knowledge: entity, dictionary, commonsense,\n",
            "event, script, and causality knowledge. For each input instance, the KiC model\n",
            "adaptively selects a knowledge type and retrieves the most helpful pieces of\n",
            "knowledge. The input instance along with its knowledge augmentation is fed into\n",
            "a text-to-text model (e.g., T5) to generate the output answer, where both the\n",
            "input and the output are in natural language forms after prompting.\n",
            "Interestingly, we find that KiC can be identified as a special\n",
            "mixture-of-experts (MoE) model, where the knowledge selector plays the role of\n",
            "a router that is used to determine the sequence-to-expert assignment in MoE.\n",
            "This key observation inspires us to develop a novel algorithm for training KiC\n",
            "with an instance-adaptive knowledge selector. As a knowledge-rich\n",
            "semi-parametric language model, KiC only needs a much smaller parametric part\n",
            "to achieve superior zero-shot performance on unseen tasks. By evaluating on 40+\n",
            "different tasks, we show that KiC_Large with 770M parameters easily outperforms\n",
            "large language models (LMs) that are 4-39x larger by a large margin. We also\n",
            "demonstrate that KiC exhibits emergent abilities at a much smaller model scale\n",
            "compared to the fully-parametric models.\n",
            "\n",
            "544. Title: Delta-AI: Local objectives for amortized inference in sparse graphical models\n",
            "   Abstract: While instruction-tuned language models have demonstrated impressive\n",
            "zero-shot generalization, these models often struggle to generate accurate\n",
            "responses when faced with instructions that fall outside their training set.\n",
            "This paper presents Instructive Decoding (ID), a simple yet effective approach\n",
            "that augments the efficacy of instruction-tuned models. Specifically, ID\n",
            "adjusts the logits for next-token prediction in a contrastive manner, utilizing\n",
            "predictions generated from a manipulated version of the original instruction,\n",
            "referred to as a noisy instruction. This noisy instruction aims to elicit\n",
            "responses that could diverge from the intended instruction yet remain\n",
            "plausible. We conduct experiments across a spectrum of such noisy instructions,\n",
            "ranging from those that insert semantic noise via random words to others like\n",
            "'opposite' that elicit the deviated responses. Our approach achieves\n",
            "considerable performance gains across various instruction-tuned models and\n",
            "tasks without necessitating any additional parameter updates. Notably,\n",
            "utilizing 'opposite' as the noisy instruction in ID, which exhibits the maximum\n",
            "divergence from the original instruction, consistently produces the most\n",
            "significant performance gains across multiple models and tasks.\n",
            "\n",
            "545. Title: Confidence-aware Reward Optimization for Fine-tuning Text-to-Image Models\n",
            "   Abstract: In cooperative multi-agent reinforcement learning (MARL), agents aim to\n",
            "achieve a common goal, such as defeating enemies or scoring a goal. Existing\n",
            "MARL algorithms are effective but still require significant learning time and\n",
            "often get trapped in local optima by complex tasks, subsequently failing to\n",
            "discover a goal-reaching policy. To address this, we introduce Efficient\n",
            "episodic Memory Utilization (EMU) for MARL, with two primary objectives: (a)\n",
            "accelerating reinforcement learning by leveraging semantically coherent memory\n",
            "from an episodic buffer and (b) selectively promoting desirable transitions to\n",
            "prevent local convergence. To achieve (a), EMU incorporates a trainable\n",
            "encoder/decoder structure alongside MARL, creating coherent memory embeddings\n",
            "that facilitate exploratory memory recall. To achieve (b), EMU introduces a\n",
            "novel reward structure called episodic incentive based on the desirability of\n",
            "states. This reward improves the TD target in Q-learning and acts as an\n",
            "additional incentive for desirable transitions. We provide theoretical support\n",
            "for the proposed incentive and demonstrate the effectiveness of EMU compared to\n",
            "conventional episodic control. The proposed method is evaluated in StarCraft II\n",
            "and Google Research Football, and empirical results indicate further\n",
            "performance improvement over state-of-the-art methods.\n",
            "\n",
            "546. Title: Layer-wise linear mode connectivity\n",
            "   Abstract: We develop new algorithms for Riemannian bilevel optimization. We focus in\n",
            "particular on batch and stochastic gradient-based methods, with the explicit\n",
            "goal of avoiding second-order information such as Riemannian hyper-gradients.\n",
            "We propose and analyze $\\mathrm{RF^2SA}$, a method that leverages first-order\n",
            "gradient information to navigate the complex geometry of Riemannian manifolds\n",
            "efficiently. Notably, $\\mathrm{RF^2SA}$ is a single-loop algorithm, and thus\n",
            "easier to implement and use. Under various setups, including stochastic\n",
            "optimization, we provide explicit convergence rates for reaching\n",
            "$\\epsilon$-stationary points. We also address the challenge of optimizing over\n",
            "Riemannian manifolds with constraints by adjusting the multiplier in the\n",
            "Lagrangian, ensuring convergence to the desired solution without requiring\n",
            "access to second-order derivatives.\n",
            "\n",
            "547. Title: A Graph is Worth 1-bit Spikes: When Graph Contrastive Learning Meets Spiking Neural Networks\n",
            "   Abstract: Federated learning (FL) enables multiple clients to train a model while\n",
            "keeping their data private collaboratively. Previous studies have shown that\n",
            "data heterogeneity between clients leads to drifts across client updates.\n",
            "However, there are few studies on the relationship between client and global\n",
            "modes, making it unclear where these updates end up drifting. We perform\n",
            "empirical and theoretical studies on this relationship by utilizing mode\n",
            "connectivity, which measures performance change (i.e., connectivity) along\n",
            "parametric paths between different modes. Empirically, reducing data\n",
            "heterogeneity makes the connectivity on different paths more similar, forming\n",
            "more low-error overlaps between client and global modes. We also find that a\n",
            "barrier to connectivity occurs when linearly connecting two global modes, while\n",
            "it disappears with considering non-linear mode connectivity. Theoretically, we\n",
            "establish a quantitative bound on the global-mode connectivity using mean-field\n",
            "theory or dropout stability. The bound demonstrates that the connectivity\n",
            "improves when reducing data heterogeneity and widening trained models.\n",
            "Numerical results further corroborate our analytical findings.\n",
            "\n",
            "548. Title: 3D Feature Prediction for Masked-AutoEncoder-Based Point Cloud Pretraining\n",
            "   Abstract: Direct Preference Optimization (DPO) has emerged as a more computationally\n",
            "efficient alternative to Reinforcement Learning from Human Feedback (RLHF) with\n",
            "Proximal Policy Optimization (PPO), eliminating the need for reward models and\n",
            "online sampling. Despite these benefits, DPO and its variants remain sensitive\n",
            "to hyper-parameters and prone to instability, particularly on mathematical\n",
            "datasets. We argue that these issues arise from the unidirectional\n",
            "likelihood-derivative negative feedback inherent in the log-likelihood loss\n",
            "function. To address this, we propose a novel LLM alignment loss that\n",
            "establishes a stable Bidirectional Negative Feedback (BNF) during optimization.\n",
            "Our proposed BNF loss eliminates the need for pairwise contrastive losses and\n",
            "does not require any extra tunable hyper-parameters or pairwise preference\n",
            "data, streamlining the alignment pipeline to be as simple as supervised\n",
            "fine-tuning. We conduct extensive experiments across two challenging QA\n",
            "benchmarks and four reasoning benchmarks. The experimental results show that\n",
            "BNF achieves comparable performance to the best methods on QA benchmarks, while\n",
            "its performance decrease on the four reasoning benchmarks is significantly\n",
            "lower compared to the best methods, thus striking a better balance between\n",
            "value alignment and reasoning ability. In addition, we further validate the\n",
            "performance of BNF on non-pairwise datasets, and conduct in-depth analysis of\n",
            "log-likelihood and logit shifts across different preference optimization\n",
            "methods.\n",
            "\n",
            "549. Title: Bilevel Optimization under Unbounded Smoothness: A New Algorithm and Convergence Analysis\n",
            "   Abstract: Fusion is a technique for merging multiple independently-trained neural\n",
            "networks in order to combine their capabilities. Past attempts have been\n",
            "restricted to the case of fully-connected, convolutional, and residual\n",
            "networks. This paper presents a systematic approach for fusing two or more\n",
            "transformer-based networks exploiting Optimal Transport to (soft-)align the\n",
            "various architectural components. We flesh out an abstraction for layer\n",
            "alignment, that can generalize to arbitrary architectures - in principle - and\n",
            "we apply this to the key ingredients of Transformers such as multi-head\n",
            "self-attention, layer-normalization, and residual connections, and we discuss\n",
            "how to handle them via various ablation studies. Furthermore, our method allows\n",
            "the fusion of models of different sizes (heterogeneous fusion), providing a new\n",
            "and efficient way to compress Transformers. The proposed approach is evaluated\n",
            "on both image classification tasks via Vision Transformer and natural language\n",
            "modeling tasks using BERT. Our approach consistently outperforms vanilla\n",
            "fusion, and, after a surprisingly short finetuning, also outperforms the\n",
            "individual converged parent models. In our analysis, we uncover intriguing\n",
            "insights about the significant role of soft alignment in the case of\n",
            "Transformers. Our results showcase the potential of fusing multiple\n",
            "Transformers, thus compounding their expertise, in the budding paradigm of\n",
            "model fusion and recombination. Code is available at\n",
            "https://github.com/graldij/transformer-fusion.\n",
            "\n",
            "550. Title: LaneSegNet: Map Learning with Lane Segment Perception for Autonomous Driving\n",
            "   Abstract: While contrastive self-supervised learning has become the de-facto learning\n",
            "paradigm for graph neural networks, the pursuit of higher task accuracy\n",
            "requires a larger hidden dimensionality to learn informative and discriminative\n",
            "full-precision representations, raising concerns about computation, memory\n",
            "footprint, and energy consumption burden (largely overlooked) for real-world\n",
            "applications. This work explores a promising direction for graph contrastive\n",
            "learning (GCL) with spiking neural networks (SNNs), which leverage sparse and\n",
            "binary characteristics to learn more biologically plausible and compact\n",
            "representations. We propose SpikeGCL, a novel GCL framework to learn binarized\n",
            "1-bit representations for graphs, making balanced trade-offs between efficiency\n",
            "and performance. We provide theoretical guarantees to demonstrate that SpikeGCL\n",
            "has comparable expressiveness with its full-precision counterparts.\n",
            "Experimental results demonstrate that, with nearly 32x representation storage\n",
            "compression, SpikeGCL is either comparable to or outperforms many fancy\n",
            "state-of-the-art supervised and self-supervised methods across several graph\n",
            "benchmarks.\n",
            "\n",
            "551. Title: InstructScene: Instruction-Driven 3D Indoor Scene Synthesis with Semantic Graph Prior\n",
            "   Abstract: Masked autoencoders (MAE) have recently been introduced to 3D self-supervised\n",
            "pretraining for point clouds due to their great success in NLP and computer\n",
            "vision. Unlike MAEs used in the image domain, where the pretext task is to\n",
            "restore features at the masked pixels, such as colors, the existing 3D MAE\n",
            "works reconstruct the missing geometry only, i.e, the location of the masked\n",
            "points. In contrast to previous studies, we advocate that point location\n",
            "recovery is inessential and restoring intrinsic point features is much\n",
            "superior. To this end, we propose to ignore point position reconstruction and\n",
            "recover high-order features at masked points including surface normals and\n",
            "surface variations, through a novel attention-based decoder which is\n",
            "independent of the encoder design. We validate the effectiveness of our pretext\n",
            "task and decoder design using different encoder structures for 3D training and\n",
            "demonstrate the advantages of our pretrained networks on various point cloud\n",
            "analysis tasks.\n",
            "\n",
            "552. Title: Exploiting Causal Graph Priors with Posterior Sampling for Reinforcement Learning\n",
            "   Abstract: Supervised learning datasets may contain multiple cues that explain the\n",
            "training set equally well, i.e., learning any of them would lead to the correct\n",
            "predictions on the training data. However, many of them can be spurious, i.e.,\n",
            "lose their predictive power under a distribution shift and consequently fail to\n",
            "generalize to out-of-distribution (OOD) data. Recently developed\n",
            "\"diversification\" methods (Lee et al., 2023; Pagliardini et al., 2023) approach\n",
            "this problem by finding multiple diverse hypotheses that rely on different\n",
            "features. This paper aims to study this class of methods and identify the key\n",
            "components contributing to their OOD generalization abilities.\n",
            "  We show that (1) diversification methods are highly sensitive to the\n",
            "distribution of the unlabeled data used for diversification and can\n",
            "underperform significantly when away from a method-specific sweet spot. (2)\n",
            "Diversification alone is insufficient for OOD generalization. The choice of the\n",
            "used learning algorithm, e.g., the model's architecture and pretraining, is\n",
            "crucial. In standard experiments (classification on Waterbirds and Office-Home\n",
            "datasets), using the second-best choice leads to an up to 20\\% absolute drop in\n",
            "accuracy. (3) The optimal choice of learning algorithm depends on the unlabeled\n",
            "data and vice versa i.e. they are co-dependent. (4) Finally, we show that, in\n",
            "practice, the above pitfalls cannot be alleviated by increasing the number of\n",
            "diverse hypotheses, the major feature of diversification methods.\n",
            "  These findings provide a clearer understanding of the critical design factors\n",
            "influencing the OOD generalization abilities of diversification methods. They\n",
            "can guide practitioners in how to use the existing methods best and guide\n",
            "researchers in developing new, better ones.\n",
            "\n",
            "553. Title: AMAGO: Scalable In-Context Reinforcement Learning for Adaptive Agents\n",
            "   Abstract: In this paper, we present a new locomotion control method for soft robot\n",
            "snakes. Inspired by biological snakes, our control architecture is composed of\n",
            "two key modules: A deep reinforcement learning (RL) module for achieving\n",
            "adaptive goal-tracking behaviors with changing goals, and a central pattern\n",
            "generator (CPG) system with Matsuoka oscillators for generating stable and\n",
            "diverse locomotion patterns. The two modules are interconnected into a\n",
            "closed-loop system: The RL module, analogizing the locomotion region located in\n",
            "the midbrain of vertebrate animals, regulates the input to the CPG system given\n",
            "state feedback from the robot. The output of the CPG system is then translated\n",
            "into pressure inputs to pneumatic actuators of the soft snake robot. Based on\n",
            "the fact that the oscillation frequency and wave amplitude of the Matsuoka\n",
            "oscillator can be independently controlled under different time scales, we\n",
            "further adapt the option-critic framework to improve the learning performance\n",
            "measured by optimality and data efficiency. The performance of the proposed\n",
            "controller is experimentally validated with both simulated and real soft snake\n",
            "robots.\n",
            "\n",
            "554. Title: Plug-and-Play Policy Planner for Large Language Model Powered Dialogue Agents\n",
            "   Abstract: Posterior sampling allows exploitation of prior knowledge on the\n",
            "environment's transition dynamics to improve the sample efficiency of\n",
            "reinforcement learning. The prior is typically specified as a class of\n",
            "parametric distributions, the design of which can be cumbersome in practice,\n",
            "often resulting in the choice of uninformative priors. In this work, we propose\n",
            "a novel posterior sampling approach in which the prior is given as a (partial)\n",
            "causal graph over the environment's variables. The latter is often more natural\n",
            "to design, such as listing known causal dependencies between biometric features\n",
            "in a medical treatment study. Specifically, we propose a hierarchical Bayesian\n",
            "procedure, called C-PSRL, simultaneously learning the full causal graph at the\n",
            "higher level and the parameters of the resulting factored dynamics at the lower\n",
            "level. We provide an analysis of the Bayesian regret of C-PSRL that explicitly\n",
            "connects the regret rate with the degree of prior knowledge. Our numerical\n",
            "evaluation conducted in illustrative domains confirms that C-PSRL strongly\n",
            "improves the efficiency of posterior sampling with an uninformative prior while\n",
            "performing close to posterior sampling with the full causal graph.\n",
            "\n",
            "555. Title: Improving Offline RL by Blending Heuristics\n",
            "   Abstract: We propose Heuristic Blending (HUBL), a simple performance-improving\n",
            "technique for a broad class of offline RL algorithms based on value\n",
            "bootstrapping. HUBL modifies the Bellman operators used in these algorithms,\n",
            "partially replacing the bootstrapped values with heuristic ones that are\n",
            "estimated with Monte-Carlo returns. For trajectories with higher returns, HUBL\n",
            "relies more on the heuristic values and less on bootstrapping; otherwise, it\n",
            "leans more heavily on bootstrapping. HUBL is very easy to combine with many\n",
            "existing offline RL implementations by relabeling the offline datasets with\n",
            "adjusted rewards and discount factors. We derive a theory that explains HUBL's\n",
            "effect on offline RL as reducing offline RL's complexity and thus increasing\n",
            "its finite-sample performance. Furthermore, we empirically demonstrate that\n",
            "HUBL consistently improves the policy quality of four state-of-the-art\n",
            "bootstrapping-based offline RL algorithms (ATAC, CQL, TD3+BC, and IQL), by 9%\n",
            "on average over 27 datasets of the D4RL and Meta-World benchmarks.\n",
            "\n",
            "556. Title: The Effectiveness of Random Forgetting for Robust Generalization\n",
            "   Abstract: We rigorously study the joint evolution of training dynamics via stochastic\n",
            "gradient descent (SGD) and the spectra of empirical Hessian and gradient\n",
            "matrices. We prove that in two canonical classification tasks for multi-class\n",
            "high-dimensional mixtures and either 1 or 2-layer neural networks, the SGD\n",
            "trajectory rapidly aligns with emerging low-rank outlier eigenspaces of the\n",
            "Hessian and gradient matrices. Moreover, in multi-layer settings this alignment\n",
            "occurs per layer, with the final layer's outlier eigenspace evolving over the\n",
            "course of training, and exhibiting rank deficiency when the SGD converges to\n",
            "sub-optimal classifiers. This establishes some of the rich predictions that\n",
            "have arisen from extensive numerical studies in the last decade about the\n",
            "spectra of Hessian and information matrices over the course of training in\n",
            "overparametrized networks.\n",
            "\n",
            "557. Title: Learning interpretable control inputs and dynamics underlying animal locomotion\n",
            "   Abstract: We uncover a surprising phenomenon in deep reinforcement learning: training a\n",
            "diverse ensemble of data-sharing agents -- a well-established exploration\n",
            "strategy -- can significantly impair the performance of the individual ensemble\n",
            "members when compared to standard single-agent training. Through careful\n",
            "analysis, we attribute the degradation in performance to the low proportion of\n",
            "self-generated data in the shared training data for each ensemble member, as\n",
            "well as the inefficiency of the individual ensemble members to learn from such\n",
            "highly off-policy data. We thus name this phenomenon the curse of diversity. We\n",
            "find that several intuitive solutions -- such as a larger replay buffer or a\n",
            "smaller ensemble size -- either fail to consistently mitigate the performance\n",
            "loss or undermine the advantages of ensembling. Finally, we demonstrate the\n",
            "potential of representation learning to counteract the curse of diversity with\n",
            "a novel method named Cross-Ensemble Representation Learning (CERL) in both\n",
            "discrete and continuous control domains. Our work offers valuable insights into\n",
            "an unexpected pitfall in ensemble-based exploration and raises important\n",
            "caveats for future applications of similar approaches.\n",
            "\n",
            "558. Title: High-dimensional SGD aligns with emerging outlier eigenspaces\n",
            "   Abstract: Deep neural networks are susceptible to adversarial attacks, which can\n",
            "compromise their performance and accuracy. Adversarial Training (AT) has\n",
            "emerged as a popular approach for protecting neural networks against such\n",
            "attacks. However, a key challenge of AT is robust overfitting, where the\n",
            "network's robust performance on test data deteriorates with further training,\n",
            "thus hindering generalization. Motivated by the concept of active forgetting in\n",
            "the brain, we introduce a novel learning paradigm called \"Forget to Mitigate\n",
            "Overfitting (FOMO)\". FOMO alternates between the forgetting phase, which\n",
            "randomly forgets a subset of weights and regulates the model's information\n",
            "through weight reinitialization, and the relearning phase, which emphasizes\n",
            "learning generalizable features. Our experiments on benchmark datasets and\n",
            "adversarial attacks show that FOMO alleviates robust overfitting by\n",
            "significantly reducing the gap between the best and last robust test accuracy\n",
            "while improving the state-of-the-art robustness. Furthermore, FOMO provides a\n",
            "better trade-off between standard and robust accuracy, outperforming baseline\n",
            "adversarial methods. Finally, our framework is robust to AutoAttacks and\n",
            "increases generalization in many real-world scenarios.\n",
            "\n",
            "559. Title: Symphony: Symmetry-Equivariant Point-Centered Spherical Harmonics for 3D Molecule Generation\n",
            "   Abstract: Proactive dialogues serve as a practical yet challenging dialogue problem in\n",
            "the era of large language models (LLMs), where the dialogue policy planning is\n",
            "the key to improving the proactivity of LLMs. Most existing studies enable the\n",
            "dialogue policy planning of LLMs using various prompting schemes or iteratively\n",
            "enhance this capability in handling the given case with verbal AI feedback.\n",
            "However, these approaches are either bounded by the policy planning capability\n",
            "of the frozen LLMs or hard to be transferred to new cases. In this work, we\n",
            "introduce a new dialogue policy planning paradigm to strategize LLMs for\n",
            "proactive dialogue problems with a tunable language model plug-in as a\n",
            "plug-and-play dialogue policy planner, named PPDPP. Specifically, we develop a\n",
            "novel training framework to facilitate supervised fine-tuning over available\n",
            "human-annotated data as well as reinforcement learning from goal-oriented AI\n",
            "feedback with dynamic interaction data collected by the LLM-based self-play\n",
            "simulation. In this manner, the LLM-powered dialogue agent can not only be\n",
            "generalized to different cases after the training, but also be applicable to\n",
            "different applications by just substituting the learned plug-in. In addition,\n",
            "we propose to evaluate the policy planning capability of dialogue systems under\n",
            "the interactive setting. Experimental results demonstrate that PPDPP\n",
            "consistently and substantially outperforms existing approaches on three\n",
            "different proactive dialogue applications, including negotiation, emotional\n",
            "support, and tutoring dialogues.\n",
            "\n",
            "560. Title: Knowledge Distillation Based on Transformed Teacher Matching\n",
            "   Abstract: As a technique to bridge logit matching and probability distribution\n",
            "matching, temperature scaling plays a pivotal role in knowledge distillation\n",
            "(KD). Conventionally, temperature scaling is applied to both teacher's logits\n",
            "and student's logits in KD. Motivated by some recent works, in this paper, we\n",
            "drop instead temperature scaling on the student side, and systematically study\n",
            "the resulting variant of KD, dubbed transformed teacher matching (TTM). By\n",
            "reinterpreting temperature scaling as a power transform of probability\n",
            "distribution, we show that in comparison with the original KD, TTM has an\n",
            "inherent R\\'enyi entropy term in its objective function, which serves as an\n",
            "extra regularization term. Extensive experiment results demonstrate that thanks\n",
            "to this inherent regularization, TTM leads to trained students with better\n",
            "generalization than the original KD. To further enhance student's capability to\n",
            "match teacher's power transformed probability distribution, we introduce a\n",
            "sample-adaptive weighting coefficient into TTM, yielding a novel distillation\n",
            "approach dubbed weighted TTM (WTTM). It is shown, by comprehensive experiments,\n",
            "that although WTTM is simple, it is effective, improves upon TTM, and achieves\n",
            "state-of-the-art accuracy performance. Our source code is available at\n",
            "https://github.com/zkxufo/TTM.\n",
            "\n",
            "561. Title: GENOME: Generative Neuro-Symbolic Visual Reasoning by Growing and Reusing Modules\n",
            "   Abstract: Reinforcement learning (RL) algorithms update an agent's parameters according\n",
            "to one of several possible rules, discovered manually through years of\n",
            "research. Automating the discovery of update rules from data could lead to more\n",
            "efficient algorithms, or algorithms that are better adapted to specific\n",
            "environments. Although there have been prior attempts at addressing this\n",
            "significant scientific challenge, it remains an open question whether it is\n",
            "feasible to discover alternatives to fundamental concepts of RL such as value\n",
            "functions and temporal-difference learning. This paper introduces a new\n",
            "meta-learning approach that discovers an entire update rule which includes both\n",
            "'what to predict' (e.g. value functions) and 'how to learn from it' (e.g.\n",
            "bootstrapping) by interacting with a set of environments. The output of this\n",
            "method is an RL algorithm that we call Learned Policy Gradient (LPG). Empirical\n",
            "results show that our method discovers its own alternative to the concept of\n",
            "value functions. Furthermore it discovers a bootstrapping mechanism to maintain\n",
            "and use its predictions. Surprisingly, when trained solely on toy environments,\n",
            "LPG generalises effectively to complex Atari games and achieves non-trivial\n",
            "performance. This shows the potential to discover general RL algorithms from\n",
            "data.\n",
            "\n",
            "562. Title: Achieving Sample and Computational Efficient Reinforcement Learning by Action Space Reduction via Grouping\n",
            "   Abstract: The Euler Characteristic Transform (ECT) has proven to be a powerful\n",
            "representation, combining geometrical and topological characteristics of shapes\n",
            "and graphs. However, the ECT was hitherto unable to learn task-specific\n",
            "representations. We overcome this issue and develop a novel computational layer\n",
            "that enables learning the ECT in an end-to-end fashion. Our method, the\n",
            "Differentiable Euler Characteristic Transform (DECT), is fast and\n",
            "computationally efficient, while exhibiting performance on a par with more\n",
            "complex models in both graph and point cloud classification tasks. Moreover, we\n",
            "show that this seemingly simple statistic provides the same topological\n",
            "expressivity as more complex topological deep learning layers.\n",
            "\n",
            "563. Title: One-hot Generalized Linear Model for Switching Brain State Discovery\n",
            "   Abstract: Most interpretability research in NLP focuses on understanding the behavior\n",
            "and features of a fully trained model. However, certain insights into model\n",
            "behavior may only be accessible by observing the trajectory of the training\n",
            "process. We present a case study of syntax acquisition in masked language\n",
            "models (MLMs) that demonstrates how analyzing the evolution of interpretable\n",
            "artifacts throughout training deepens our understanding of emergent behavior.\n",
            "In particular, we study Syntactic Attention Structure (SAS), a naturally\n",
            "emerging property of MLMs wherein specific Transformer heads tend to focus on\n",
            "specific syntactic relations. We identify a brief window in pretraining when\n",
            "models abruptly acquire SAS, concurrent with a steep drop in loss. This\n",
            "breakthrough precipitates the subsequent acquisition of linguistic\n",
            "capabilities. We then examine the causal role of SAS by manipulating SAS during\n",
            "training, and demonstrate that SAS is necessary for the development of\n",
            "grammatical capabilities. We further find that SAS competes with other\n",
            "beneficial traits during training, and that briefly suppressing SAS improves\n",
            "model quality. These findings offer an interpretation of a real-world example\n",
            "of both simplicity bias and breakthrough training dynamics.\n",
            "\n",
            "564. Title: DrM: Mastering Visual Reinforcement Learning through Dormant Ratio Minimization\n",
            "   Abstract: Modular approaches that use a different composition of modules for each\n",
            "problem are a promising direction in continual learning (CL). However,\n",
            "searching through the large, discrete space of module compositions is\n",
            "challenging, especially because evaluating a composition's performance requires\n",
            "a round of neural network training. We address this challenge through a modular\n",
            "CL framework, PICLE, that uses a probabilistic model to cheaply compute the\n",
            "fitness of each composition, allowing PICLE to achieve both perceptual,\n",
            "few-shot and latent transfer. The model combines prior knowledge about good\n",
            "module compositions with dataset-specific information. We evaluate PICLE using\n",
            "two benchmark suites designed to assess different desiderata of CL techniques.\n",
            "Comparing to a wide range of approaches, we show that PICLE is the first\n",
            "modular CL algorithm to achieve perceptual, few-shot and latent transfer while\n",
            "scaling well to large search spaces, outperforming previous state-of-the-art\n",
            "modular CL approaches on long problem sequences.\n",
            "\n",
            "565. Title: A Probabilistic Framework for Modular Continual Learning\n",
            "   Abstract: Neuromorphic computing with spiking neural networks is promising for\n",
            "energy-efficient artificial intelligence (AI) applications. However, different\n",
            "from humans who continually learn different tasks in a lifetime, neural network\n",
            "models suffer from catastrophic forgetting. How could neuronal operations solve\n",
            "this problem is an important question for AI and neuroscience. Many previous\n",
            "studies draw inspiration from observed neuroscience phenomena and propose\n",
            "episodic replay or synaptic metaplasticity, but they are not guaranteed to\n",
            "explicitly preserve knowledge for neuron populations. Other works focus on\n",
            "machine learning methods with more mathematical grounding, e.g., orthogonal\n",
            "projection on high dimensional spaces, but there is no neural correspondence\n",
            "for neuromorphic computing. In this work, we develop a new method with neuronal\n",
            "operations based on lateral connections and Hebbian learning, which can protect\n",
            "knowledge by projecting activity traces of neurons into an orthogonal subspace\n",
            "so that synaptic weight update will not interfere with old tasks. We show that\n",
            "Hebbian and anti-Hebbian learning on recurrent lateral connections can\n",
            "effectively extract the principal subspace of neural activities and enable\n",
            "orthogonal projection. This provides new insights into how neural circuits and\n",
            "Hebbian learning can help continual learning, and also how the concept of\n",
            "orthogonal projection can be realized in neuronal systems. Our method is also\n",
            "flexible to utilize arbitrary training methods based on presynaptic\n",
            "activities/traces. Experiments show that our method consistently solves\n",
            "forgetting for spiking neural networks with nearly zero forgetting under\n",
            "various supervised training methods with different error propagation\n",
            "approaches, and outperforms previous approaches under various settings. Our\n",
            "method can pave a solid path for building continual neuromorphic computing\n",
            "systems.\n",
            "\n",
            "566. Title: The Need for Speed: Pruning Transformers with One Recipe\n",
            "   Abstract: Reinforcement learning often needs to deal with the exponential growth of\n",
            "states and actions when exploring optimal control in high-dimensional spaces\n",
            "(often known as the curse of dimensionality). In this work, we address this\n",
            "issue by learning the inherent structure of action-wise similar MDP to\n",
            "appropriately balance the performance degradation versus sample/computational\n",
            "complexity. In particular, we partition the action spaces into multiple groups\n",
            "based on the similarity in transition distribution and reward function, and\n",
            "build a linear decomposition model to capture the difference between the\n",
            "intra-group transition kernel and the intra-group rewards. Both our theoretical\n",
            "analysis and experiments reveal a \\emph{surprising and counter-intuitive\n",
            "result}: while a more refined grouping strategy can reduce the approximation\n",
            "error caused by treating actions in the same group as identical, it also leads\n",
            "to increased estimation error when the size of samples or the computation\n",
            "resources is limited. This finding highlights the grouping strategy as a new\n",
            "degree of freedom that can be optimized to minimize the overall performance\n",
            "loss. To address this issue, we formulate a general optimization problem for\n",
            "determining the optimal grouping strategy, which strikes a balance between\n",
            "performance loss and sample/computational complexity. We further propose a\n",
            "computationally efficient method for selecting a nearly-optimal grouping\n",
            "strategy, which maintains its computational complexity independent of the size\n",
            "of the action space.\n",
            "\n",
            "567. Title: Denoising Task Routing for Diffusion Models\n",
            "   Abstract: Standard practice within Reinforcement Learning from Human Feedback (RLHF)\n",
            "involves optimizing against a Reward Model (RM), which itself is trained to\n",
            "reflect human preferences for desirable generations. A notable subject that is\n",
            "understudied is the (in-)consistency of RMs -- whether they can recognize the\n",
            "semantic changes to different prompts and appropriately adapt their reward\n",
            "assignments -- and their impact on the downstream RLHF model.\n",
            "  In this paper, we visit a series of research questions relevant to RM\n",
            "inconsistency: (1) How can we measure the consistency of reward models? (2) How\n",
            "consistent are the existing RMs and how can we improve them? (3) In what ways\n",
            "does reward inconsistency influence the chatbots resulting from the RLHF model\n",
            "training?\n",
            "  We propose Contrast Instructions -- a benchmarking strategy for the\n",
            "consistency of RM. Each example in Contrast Instructions features a pair of\n",
            "lexically similar instructions with different ground truth responses. A\n",
            "consistent RM is expected to rank the corresponding instruction and response\n",
            "higher than other combinations. We observe that current RMs trained with the\n",
            "standard ranking objective fail miserably on Contrast Instructions compared to\n",
            "average humans. To show that RM consistency can be improved efficiently without\n",
            "using extra training budget, we propose two techniques ConvexDA and\n",
            "RewardFusion, which enhance reward consistency through extrapolation during the\n",
            "RM training and inference stage, respectively. We show that RLHF models trained\n",
            "with a more consistent RM yield more useful responses, suggesting that reward\n",
            "inconsistency exhibits a trickle-down effect on the downstream RLHF process.\n",
            "\n",
            "568. Title: GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher\n",
            "   Abstract: Diffusion models generate highly realistic images by learning a multi-step\n",
            "denoising process, naturally embodying the principles of multi-task learning\n",
            "(MTL). Despite the inherent connection between diffusion models and MTL, there\n",
            "remains an unexplored area in designing neural architectures that explicitly\n",
            "incorporate MTL into the framework of diffusion models. In this paper, we\n",
            "present Denoising Task Routing (DTR), a simple add-on strategy for existing\n",
            "diffusion model architectures to establish distinct information pathways for\n",
            "individual tasks within a single architecture by selectively activating subsets\n",
            "of channels in the model. What makes DTR particularly compelling is its\n",
            "seamless integration of prior knowledge of denoising tasks into the framework:\n",
            "(1) Task Affinity: DTR activates similar channels for tasks at adjacent\n",
            "timesteps and shifts activated channels as sliding windows through timesteps,\n",
            "capitalizing on the inherent strong affinity between tasks at adjacent\n",
            "timesteps. (2) Task Weights: During the early stages (higher timesteps) of the\n",
            "denoising process, DTR assigns a greater number of task-specific channels,\n",
            "leveraging the insight that diffusion models prioritize reconstructing global\n",
            "structure and perceptually rich contents in earlier stages, and focus on simple\n",
            "noise removal in later stages. Our experiments reveal that DTR not only\n",
            "consistently boosts diffusion models' performance across different evaluation\n",
            "protocols without adding extra parameters but also accelerates training\n",
            "convergence. Finally, we show the complementarity between our architectural\n",
            "approach and existing MTL optimization techniques, providing a more complete\n",
            "view of MTL in the context of diffusion training. Significantly, by leveraging\n",
            "this complementarity, we attain matched performance of DiT-XL using the smaller\n",
            "DiT-L with a reduction in training iterations from 7M to 2M.\n",
            "\n",
            "569. Title: L2P-MIP: Learning to Presolve for Mixed Integer Programming\n",
            "   Abstract: Cross-modal recipe retrieval has recently gained substantial attention due to\n",
            "the importance of food in people's lives, as well as the availability of vast\n",
            "amounts of digital cooking recipes and food images to train machine learning\n",
            "models. In this work, we revisit existing approaches for cross-modal recipe\n",
            "retrieval and propose a simplified end-to-end model based on well established\n",
            "and high performing encoders for text and images. We introduce a hierarchical\n",
            "recipe Transformer which attentively encodes individual recipe components\n",
            "(titles, ingredients and instructions). Further, we propose a self-supervised\n",
            "loss function computed on top of pairs of individual recipe components, which\n",
            "is able to leverage semantic relationships within recipes, and enables training\n",
            "using both image-recipe and recipe-only samples. We conduct a thorough analysis\n",
            "and ablation studies to validate our design choices. As a result, our proposed\n",
            "method achieves state-of-the-art performance in the cross-modal recipe\n",
            "retrieval task on the Recipe1M dataset. We make code and models publicly\n",
            "available.\n",
            "\n",
            "570. Title: Hebbian Learning based Orthogonal Projection for Continual Learning of Spiking Neural Networks\n",
            "   Abstract: Exposing meaningful and interpretable neural interactions is critical to\n",
            "understanding neural circuits. Inferred neural interactions from neural signals\n",
            "primarily reflect functional interactions. In a long experiment, subject\n",
            "animals may experience different stages defined by the experiment, stimuli, or\n",
            "behavioral states, and hence functional interactions can change over time. To\n",
            "model dynamically changing functional interactions, prior work employs\n",
            "state-switching generalized linear models with hidden Markov models (i.e.,\n",
            "HMM-GLMs). However, we argue they lack biological plausibility, as functional\n",
            "interactions are shaped and confined by the underlying anatomical connectome.\n",
            "Here, we propose a novel prior-informed state-switching GLM. We introduce both\n",
            "a Gaussian prior and a one-hot prior over the GLM in each state. The priors are\n",
            "learnable. We will show that the learned prior should capture the\n",
            "state-constant interaction, shedding light on the underlying anatomical\n",
            "connectome and revealing more likely physical neuron interactions. The\n",
            "state-dependent interaction modeled by each GLM offers traceability to capture\n",
            "functional variations across multiple brain states. Our methods effectively\n",
            "recover true interaction structures in simulated data, achieve the highest\n",
            "predictive likelihood with real neural datasets, and render interaction\n",
            "structures and hidden states more interpretable when applied to real neural\n",
            "data.\n",
            "\n",
            "571. Title: The Trickle-down Impact of Reward Inconsistency on RLHF\n",
            "   Abstract: Safety lies at the core of the development of Large Language Models (LLMs).\n",
            "There is ample work on aligning LLMs with human ethics and preferences,\n",
            "including data filtering in pretraining, supervised fine-tuning, reinforcement\n",
            "learning from human feedback, and red teaming, etc. In this study, we discover\n",
            "that chat in cipher can bypass the safety alignment techniques of LLMs, which\n",
            "are mainly conducted in natural languages. We propose a novel framework\n",
            "CipherChat to systematically examine the generalizability of safety alignment\n",
            "to non-natural languages -- ciphers. CipherChat enables humans to chat with\n",
            "LLMs through cipher prompts topped with system role descriptions and few-shot\n",
            "enciphered demonstrations. We use CipherChat to assess state-of-the-art LLMs,\n",
            "including ChatGPT and GPT-4 for different representative human ciphers across\n",
            "11 safety domains in both English and Chinese. Experimental results show that\n",
            "certain ciphers succeed almost 100% of the time to bypass the safety alignment\n",
            "of GPT-4 in several safety domains, demonstrating the necessity of developing\n",
            "safety alignment for non-natural languages. Notably, we identify that LLMs seem\n",
            "to have a ''secret cipher'', and propose a novel SelfCipher that uses only role\n",
            "play and several demonstrations in natural language to evoke this capability.\n",
            "SelfCipher surprisingly outperforms existing human ciphers in almost all cases.\n",
            "Our code and data will be released at https://github.com/RobustNLP/CipherChat.\n",
            "\n",
            "572. Title: Learning to Act from Actionless Videos through Dense Correspondences\n",
            "   Abstract: Deep Ensemble (DE) approach is a straightforward technique used to enhance\n",
            "the performance of deep neural networks by training them from different initial\n",
            "points, converging towards various local optima. However, a limitation of this\n",
            "methodology lies in its high computational overhead for inference, arising from\n",
            "the necessity to store numerous learned parameters and execute individual\n",
            "forward passes for each parameter during the inference stage. We propose a\n",
            "novel approach called Diffusion Bridge Network (DBN) to address this challenge.\n",
            "Based on the theory of the Schr\\\"odinger bridge, this method directly learns to\n",
            "simulate an Stochastic Differential Equation (SDE) that connects the output\n",
            "distribution of a single ensemble member to the output distribution of the\n",
            "ensembled model, allowing us to obtain ensemble prediction without having to\n",
            "invoke forward pass through all the ensemble models. By substituting the heavy\n",
            "ensembles with this lightweight neural network constructing DBN, we achieved\n",
            "inference with reduced computational cost while maintaining accuracy and\n",
            "uncertainty scores on benchmark datasets such as CIFAR-10, CIFAR-100, and\n",
            "TinyImageNet. Our implementation is available at\n",
            "https://github.com/kim-hyunsu/dbn.\n",
            "\n",
            "573. Title: COCO-Periph: Bridging the Gap Between Human and Machine Perception in the Periphery\n",
            "   Abstract: This paper presents a data-driven, task-specific paradigm for experimental\n",
            "design, to shorten acquisition time, reduce costs, and accelerate the\n",
            "deployment of imaging devices. Current approaches in experimental design focus\n",
            "on model-parameter estimation and require specification of a particular model,\n",
            "whereas in imaging, other tasks may drive the design. Furthermore, such\n",
            "approaches often lead to intractable optimization problems in real-world\n",
            "imaging applications. Here we present a new paradigm for experimental design\n",
            "that simultaneously optimizes the design (set of image channels) and trains a\n",
            "machine-learning model to execute a user-specified image-analysis task. The\n",
            "approach obtains data densely-sampled over the measurement space (many image\n",
            "channels) for a small number of acquisitions, then identifies a subset of\n",
            "channels of prespecified size that best supports the task. We propose a method:\n",
            "TADRED for TAsk-DRiven Experimental Design in imaging, to identify the most\n",
            "informative channel-subset whilst simultaneously training a network to execute\n",
            "the task given the subset. Experiments demonstrate the potential of TADRED in\n",
            "diverse imaging applications: several clinically-relevant tasks in magnetic\n",
            "resonance imaging; and remote sensing and physiological applications of\n",
            "hyperspectral imaging. Results show substantial improvement over classical\n",
            "experimental design, two recent application-specific methods within the new\n",
            "paradigm, and state-of-the-art approaches in supervised feature selection. We\n",
            "anticipate further applications of our approach. Code is available:\n",
            "https://github.com/sbb-gh/experimental-design-multichannel\n",
            "\n",
            "574. Title: Experimental Design for Multi-Channel Imaging via Task-Driven Feature Selection\n",
            "   Abstract: In this work, we present an approach to construct a video-based robot policy\n",
            "capable of reliably executing diverse tasks across different robots and\n",
            "environments from few video demonstrations without using any action\n",
            "annotations. Our method leverages images as a task-agnostic representation,\n",
            "encoding both the state and action information, and text as a general\n",
            "representation for specifying robot goals. By synthesizing videos that\n",
            "``hallucinate'' robot executing actions and in combination with dense\n",
            "correspondences between frames, our approach can infer the closed-formed action\n",
            "to execute to an environment without the need of any explicit action labels.\n",
            "This unique capability allows us to train the policy solely based on RGB videos\n",
            "and deploy learned policies to various robotic tasks. We demonstrate the\n",
            "efficacy of our approach in learning policies on table-top manipulation and\n",
            "navigation tasks. Additionally, we contribute an open-source framework for\n",
            "efficient video modeling, enabling the training of high-fidelity policy models\n",
            "with four GPUs within a single day.\n",
            "\n",
            "575. Title: Forward Learning with Top-Down Feedback: Empirical and Analytical Characterization\n",
            "   Abstract: Deep Candidate Generation plays an important role in large-scale recommender\n",
            "systems. It takes user history behaviors as inputs and learns user and item\n",
            "latent embeddings for candidate generation. In the literature, conventional\n",
            "methods suffer from two problems. First, a user has multiple embeddings to\n",
            "reflect various interests, and such number is fixed. However, taking into\n",
            "account different levels of user activeness, a fixed number of interest\n",
            "embeddings is sub-optimal. For example, for less active users, they may need\n",
            "fewer embeddings to represent their interests compared to active users. Second,\n",
            "the negative samples are often generated by strategies with unobserved\n",
            "supervision, and similar items could have different labels. Such a problem is\n",
            "termed as class collision. In this paper, we aim to advance the typical\n",
            "two-tower DNN candidate generation model. Specifically, an Adaptive Interest\n",
            "Selection Layer is designed to learn the number of user embeddings adaptively\n",
            "in an end-to-end way, according to the level of their activeness. Furthermore,\n",
            "we propose a Prototypical Contrastive Learning Module to tackle the class\n",
            "collision problem introduced by negative sampling. Extensive experimental\n",
            "evaluations show that the proposed scheme remarkably outperforms competitive\n",
            "baselines on multiple benchmarks.\n",
            "\n",
            "576. Title: Learning Multi-Faceted Prototypical User Interests\n",
            "   Abstract: In this study, we aim to enhance the arithmetic reasoning ability of Large\n",
            "Language Models (LLMs) through zero-shot prompt optimization. We identify a\n",
            "previously overlooked objective of query dependency in such optimization and\n",
            "elucidate two ensuing challenges that impede the successful and economical\n",
            "design of prompt optimization techniques. One primary issue is the absence of\n",
            "an effective method to evaluate prompts during inference when the golden answer\n",
            "is unavailable. Concurrently, learning via interactions with the LLMs to\n",
            "navigate the expansive natural language prompting space proves to be\n",
            "resource-intensive. To address this, we introduce Prompt-OIRL, which harnesses\n",
            "offline inverse reinforcement learning to draw insights from offline prompting\n",
            "demonstration data. Such data exists as by-products when diverse prompts are\n",
            "benchmarked on open-accessible datasets. With Prompt-OIRL, the query-dependent\n",
            "prompt optimization objective is achieved by first learning an offline reward\n",
            "model. This model can evaluate any query-prompt pairs without accessing LLMs.\n",
            "Subsequently, a best-of-N strategy is deployed to recommend the optimal prompt.\n",
            "Our experimental evaluations across various LLM scales and arithmetic reasoning\n",
            "datasets underscore both the efficacy and economic viability of the proposed\n",
            "approach.\n",
            "\n",
            "577. Title: Vision-Language Models are Zero-Shot Reward Models for Reinforcement Learning\n",
            "   Abstract: Medical image classification is one of the most important tasks for\n",
            "computer-aided diagnosis. Deep learning models, particularly convolutional\n",
            "neural networks, have been successfully used for disease classification from\n",
            "medical images, facilitated by automated feature learning. However, the diverse\n",
            "imaging modalities and clinical pathology make it challenging to construct\n",
            "generalized and robust classifications. Towards improving the model\n",
            "performance, we propose a novel pretraining approach, namely Forward Forward\n",
            "Contrastive Learning (FFCL), which leverages the Forward-Forward Algorithm in a\n",
            "contrastive learning framework--both locally and globally. Our experimental\n",
            "results on the chest X-ray dataset indicate that the proposed FFCL achieves\n",
            "superior performance (3.69% accuracy over ImageNet pretrained ResNet-18) over\n",
            "existing pretraining models in the pneumonia classification task. Moreover,\n",
            "extensive ablation experiments support the particular local and global\n",
            "contrastive pretraining design in FFCL.\n",
            "\n",
            "578. Title: Low Rank Matrix Completion via Robust Alternating Minimization in Nearly Linear Time\n",
            "   Abstract: Recent research has explored the memorization capacity of multi-head\n",
            "attention, but these findings are constrained by unrealistic limitations on the\n",
            "context size. We present a novel proof for language-based Transformers that\n",
            "extends the current hypothesis to any context size. Our approach improves upon\n",
            "the state-of-the-art by achieving more effective exact memorization with an\n",
            "attention layer, while also introducing the concept of approximate memorization\n",
            "of distributions. Through experimental validation, we demonstrate that our\n",
            "proposed bounds more accurately reflect the true memorization capacity of\n",
            "language models, and provide a precise comparison with prior work.\n",
            "\n",
            "579. Title: TESTAM: A Time-Enhanced Spatio-Temporal Attention Model with Mixture of Experts\n",
            "   Abstract: For most reinforcement learning approaches, the learning is performed by\n",
            "maximizing an accumulative reward that is expectedly and manually defined for\n",
            "specific tasks. However, in real world, rewards are emergent phenomena from the\n",
            "complex interactions between agents and environments. In this paper, we propose\n",
            "an implicit generic reward model for reinforcement learning. Unlike those\n",
            "rewards that are manually defined for specific tasks, such implicit reward is\n",
            "task independent. It only comes from the deviation from the agents' previous\n",
            "experiences.\n",
            "\n",
            "580. Title: Win-Win: Training High-Resolution Vision Transformers from Two Windows\n",
            "   Abstract: We consider causal models with two observed variables and one latent\n",
            "variables, each variable being discrete, with the goal of characterizing the\n",
            "possible distributions on outcomes that can result from controlling one of the\n",
            "observed variables. We optimize linear functions over the space of all possible\n",
            "interventional distributions, which allows us find properties of the\n",
            "interventional distribution even when we cannot uniquely identify what it is.\n",
            "We show that, under certain mild assumptions about the correlation between\n",
            "controlled variable and the latent variable, the resulting interventional\n",
            "distribution must be close to the observed conditional distribution in a\n",
            "quantitative sense. Specifically, we show that if the observed variables are\n",
            "sufficiently highly correlated, and the latent variable can only take on a\n",
            "small number of distinct values, then the variables will remain causally\n",
            "related after passing to the interventional distribution. Another result,\n",
            "possibly of more general interest, is a bound on the distance between the\n",
            "interventional distribution and the observed conditional distribution in terms\n",
            "of the mutual information between the controlled variable and the latent\n",
            "variable, which shows that the controlled variable and the latent variable must\n",
            "be tightly correlated for the interventional distribution to differ\n",
            "significantly from the observed distribution. We believe that this type of\n",
            "result may make it possible to rigorously consider 'weak' experiments, where\n",
            "the causal variable is not entirely independent from the environment, but only\n",
            "approximately so. More generally, we suggest a connection between the theory of\n",
            "causality to polynomial optimization, which give useful bounds on the space of\n",
            "interventional distributions.\n",
            "\n",
            "581. Title: Query-Dependent Prompt Evaluation and Optimization with Offline Inverse RL\n",
            "   Abstract: Adversarial training is a widely used strategy for making neural networks\n",
            "resistant to adversarial perturbations. For a neural network of width $m$, $n$\n",
            "input training data in $d$ dimension, it takes $\\Omega(mnd)$ time cost per\n",
            "training iteration for the forward and backward computation. In this paper we\n",
            "analyze the convergence guarantee of adversarial training procedure on a\n",
            "two-layer neural network with shifted ReLU activation, and shows that only\n",
            "$o(m)$ neurons will be activated for each input data per iteration.\n",
            "Furthermore, we develop an algorithm for adversarial training with time cost\n",
            "$o(m n d)$ per iteration by applying half-space reporting data structure.\n",
            "\n",
            "582. Title: MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models\n",
            "   Abstract: Given a matrix $M\\in \\mathbb{R}^{m\\times n}$, the low rank matrix completion\n",
            "problem asks us to find a rank-$k$ approximation of $M$ as $UV^\\top$ for $U\\in\n",
            "\\mathbb{R}^{m\\times k}$ and $V\\in \\mathbb{R}^{n\\times k}$ by only observing a\n",
            "few entries specified by a set of entries $\\Omega\\subseteq [m]\\times [n]$. In\n",
            "particular, we examine an approach that is widely used in practice -- the\n",
            "alternating minimization framework. Jain, Netrapalli, and Sanghavi [JNS13]\n",
            "showed that if $M$ has incoherent rows and columns, then alternating\n",
            "minimization provably recovers the matrix $M$ by observing a nearly linear in\n",
            "$n$ number of entries. While the sample complexity has been subsequently\n",
            "improved [GLZ17], alternating minimization steps are required to be computed\n",
            "exactly. This hinders the development of more efficient algorithms and fails to\n",
            "depict the practical implementation of alternating minimization, where the\n",
            "updates are usually performed approximately in favor of efficiency.\n",
            "  In this paper, we take a major step towards a more efficient and error-robust\n",
            "alternating minimization framework. To this end, we develop an analytical\n",
            "framework for alternating minimization that can tolerate a moderate amount of\n",
            "errors caused by approximate updates. Moreover, our algorithm runs in time\n",
            "$\\widetilde O(|\\Omega| k)$, which is nearly linear in the time to verify the\n",
            "solution while preserving the sample complexity. This improves upon all prior\n",
            "known alternating minimization approaches which require $\\widetilde O(|\\Omega|\n",
            "k^2)$ time.\n",
            "\n",
            "583. Title: Efficient Streaming Language Models with Attention Sinks\n",
            "   Abstract: We present a general framework that enables one to model high-order\n",
            "interaction among entangled dynamical systems, via hypergraphs. Several\n",
            "relevant processes can be ideally traced back to the proposed scheme. We shall\n",
            "here solely elaborate on the conditions that seed the spontaneous emergence of\n",
            "patterns, spatially heterogeneous solutions resulting from the many-body\n",
            "interaction between fundamental units. In particular we will focus, on two\n",
            "relevant settings. First, we will assume long-ranged mean field interactions\n",
            "between populations, and then turn to considering diffusive-like couplings. Two\n",
            "applications are presented, respectively to a generalised Volterra system and\n",
            "the Brusselator model.\n",
            "\n",
            "584. Title: Reclaiming the Source of Programmatic Policies: Programmatic versus Latent Spaces\n",
            "   Abstract: Low-rank adaptation (LoRA) is one of the most popular task-specific\n",
            "parameter-efficient fine-tuning (PEFT) methods on pre-trained language models\n",
            "for its good performance and computational efficiency. LoRA injects a product\n",
            "of two trainable rank decomposition matrices over the top of each frozen\n",
            "pre-trained model module. However, when applied in the setting of\n",
            "privacy-preserving federated learning (FL), LoRA may become unstable due to the\n",
            "following facts: 1) the effects of data heterogeneity and multi-step local\n",
            "updates are non-negligible, 2) additive noise enforced on updating gradients to\n",
            "guarantee differential privacy (DP) can be amplified and 3) the final\n",
            "performance is susceptible to hyper-parameters. A key factor leading to these\n",
            "phenomena is the discordance between jointly optimizing the two low-rank\n",
            "matrices by local clients and separately aggregating them by the central\n",
            "server. Thus, this paper proposes an efficient and effective version of LoRA,\n",
            "Federated Freeze A LoRA (FFA-LoRA), to alleviate these challenges and further\n",
            "halve the communication cost of federated fine-tuning LLMs. The core idea of\n",
            "FFA-LoRA is to fix the randomly initialized non-zero matrices and only\n",
            "fine-tune the zero-initialized matrices. Compared to LoRA, FFA-LoRA is\n",
            "motivated by practical and theoretical benefits in privacy-preserved FL. Our\n",
            "experiments demonstrate that FFA-LoRA provides more consistent performance with\n",
            "better computational efficiency over vanilla LoRA in various FL tasks.\n",
            "\n",
            "585. Title: Improving LoRA in Privacy-preserving Federated Learning\n",
            "   Abstract: Recent works have introduced LEAPS and HPRL, systems that learn latent spaces\n",
            "of domain-specific languages, which are used to define programmatic policies\n",
            "for partially observable Markov decision processes (POMDPs). These systems\n",
            "induce a latent space while optimizing losses such as the behavior loss, which\n",
            "aim to achieve locality in program behavior, meaning that vectors close in the\n",
            "latent space should correspond to similarly behaving programs. In this paper,\n",
            "we show that the programmatic space, induced by the domain-specific language\n",
            "and requiring no training, presents values for the behavior loss similar to\n",
            "those observed in latent spaces presented in previous work. Moreover,\n",
            "algorithms searching in the programmatic space significantly outperform those\n",
            "in LEAPS and HPRL. To explain our results, we measured the \"friendliness\" of\n",
            "the two spaces to local search algorithms. We discovered that algorithms are\n",
            "more likely to stop at local maxima when searching in the latent space than\n",
            "when searching in the programmatic space. This implies that the optimization\n",
            "topology of the programmatic space, induced by the reward function in\n",
            "conjunction with the neighborhood function, is more conducive to search than\n",
            "that of the latent space. This result provides an explanation for the superior\n",
            "performance in the programmatic space.\n",
            "\n",
            "586. Title: Hypergraph Dynamic System\n",
            "   Abstract: Deploying Large Language Models (LLMs) in streaming applications such as\n",
            "multi-round dialogue, where long interactions are expected, is urgently needed\n",
            "but poses two major challenges. Firstly, during the decoding stage, caching\n",
            "previous tokens' Key and Value states (KV) consumes extensive memory. Secondly,\n",
            "popular LLMs cannot generalize to longer texts than the training sequence\n",
            "length. Window attention, where only the most recent KVs are cached, is a\n",
            "natural approach -- but we show that it fails when the text length surpasses\n",
            "the cache size. We observe an interesting phenomenon, namely attention sink,\n",
            "that keeping the KV of initial tokens will largely recover the performance of\n",
            "window attention. In this paper, we first demonstrate that the emergence of\n",
            "attention sink is due to the strong attention scores towards initial tokens as\n",
            "a \"sink\" even if they are not semantically important. Based on the above\n",
            "analysis, we introduce StreamingLLM, an efficient framework that enables LLMs\n",
            "trained with a finite length attention window to generalize to infinite\n",
            "sequence lengths without any fine-tuning. We show that StreamingLLM can enable\n",
            "Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language\n",
            "modeling with up to 4 million tokens and more. In addition, we discover that\n",
            "adding a placeholder token as a dedicated attention sink during pre-training\n",
            "can further improve streaming deployment. In streaming settings, StreamingLLM\n",
            "outperforms the sliding window recomputation baseline by up to 22.2x speedup.\n",
            "Code and datasets are provided at https://github.com/mit-han-lab/streaming-llm.\n",
            "\n",
            "587. Title: Hindsight PRIORs for Reward Learning from Human Preferences\n",
            "   Abstract: Contrastive learning is a highly successful technique for learning\n",
            "representations of data from labeled tuples, specifying the distance relations\n",
            "within the tuple. We study the sample complexity of contrastive learning, i.e.\n",
            "the minimum number of labeled tuples sufficient for getting high generalization\n",
            "accuracy. We give tight bounds on the sample complexity in a variety of\n",
            "settings, focusing on arbitrary distance functions, both general\n",
            "$\\ell_p$-distances, and tree metrics. Our main result is an (almost) optimal\n",
            "bound on the sample complexity of learning $\\ell_p$-distances for integer $p$.\n",
            "For any $p \\ge 1$ we show that $\\tilde \\Theta(\\min(nd,n^2))$ labeled tuples are\n",
            "necessary and sufficient for learning $d$-dimensional representations of\n",
            "$n$-point datasets. Our results hold for an arbitrary distribution of the input\n",
            "samples and are based on giving the corresponding bounds on the\n",
            "Vapnik-Chervonenkis/Natarajan dimension of the associated problems. We further\n",
            "show that the theoretical bounds on sample complexity obtained via VC/Natarajan\n",
            "dimension can have strong predictive power for experimental results, in\n",
            "contrast with the folklore belief about a substantial gap between the\n",
            "statistical learning theory and the practice of deep learning.\n",
            "\n",
            "588. Title: Learning Over Molecular Conformer Ensembles: Datasets and Benchmarks\n",
            "   Abstract: Diffusion models are the current state of the art for generating\n",
            "photorealistic images. Controlling the sampling process for constrained image\n",
            "generation tasks such as inpainting, however, remains challenging since exact\n",
            "conditioning on such constraints is intractable. While existing methods use\n",
            "various techniques to approximate the constrained posterior, this paper\n",
            "proposes to exploit the ability of Tractable Probabilistic Models (TPMs) to\n",
            "exactly and efficiently compute the constrained posterior, and to leverage this\n",
            "signal to steer the denoising process of diffusion models. Specifically, this\n",
            "paper adopts a class of expressive TPMs termed Probabilistic Circuits (PCs).\n",
            "Building upon prior advances, we further scale up PCs and make them capable of\n",
            "guiding the image generation process of diffusion models. Empirical results\n",
            "suggest that our approach can consistently improve the overall quality and\n",
            "semantic coherence of inpainted images across three natural image datasets\n",
            "(i.e., CelebA-HQ, ImageNet, and LSUN) with only $\\sim\\! 10 \\%$ additional\n",
            "computational overhead brought by the TPM. Further, with the help of an image\n",
            "encoder and decoder, our method can readily accept semantic constraints on\n",
            "specific regions of the image, which opens up the potential for more controlled\n",
            "image generation tasks. In addition to proposing a new framework for\n",
            "constrained image generation, this paper highlights the benefit of more\n",
            "tractable models and motivates the development of expressive TPMs.\n",
            "\n",
            "589. Title: Unified Generative Modeling of 3D Molecules with Bayesian Flow Networks\n",
            "   Abstract: Preference based Reinforcement Learning (PbRL) removes the need to hand\n",
            "specify a reward function by learning a reward from preference feedback over\n",
            "policy behaviors. Current approaches to PbRL do not address the credit\n",
            "assignment problem inherent in determining which parts of a behavior most\n",
            "contributed to a preference, which result in data intensive approaches and\n",
            "subpar reward functions. We address such limitations by introducing a credit\n",
            "assignment strategy (Hindsight PRIOR) that uses a world model to approximate\n",
            "state importance within a trajectory and then guides rewards to be proportional\n",
            "to state importance through an auxiliary predicted return redistribution\n",
            "objective. Incorporating state importance into reward learning improves the\n",
            "speed of policy learning, overall policy performance, and reward recovery on\n",
            "both locomotion and manipulation tasks. For example, Hindsight PRIOR recovers\n",
            "on average significantly (p<0.05) more reward on MetaWorld (20%) and DMC (15%).\n",
            "The performance gains and our ablations demonstrate the benefits even a simple\n",
            "credit assignment strategy can have on reward learning and that state\n",
            "importance in forward dynamics prediction is a strong proxy for a state's\n",
            "contribution to a preference decision. Code repository can be found at\n",
            "https://github.com/apple/ml-rlhf-hindsight-prior.\n",
            "\n",
            "590. Title: Optimal Sample Complexity of Contrastive Learning\n",
            "   Abstract: We provide another look at the statistical calibration problem in computer\n",
            "models. This viewpoint is inspired by two overarching practical considerations\n",
            "of computer models: (i) many computer models are inadequate for perfectly\n",
            "modeling physical systems, even with the best-tuned calibration parameters;\n",
            "(ii) only a finite number of data points are available from the physical\n",
            "experiment associated with a computer model. Following this new line of\n",
            "thinking, we provide a non-asymptotic theory and derive a prediction-oriented\n",
            "calibration method. Our calibration method minimizes the predictive mean\n",
            "squared error for a finite sample size with statistical guarantees. We\n",
            "introduce an algorithm to perform the proposed calibration method and connect\n",
            "it to existing Bayesian calibration methods. Synthetic and real examples are\n",
            "provided to corroborate the derived theory and illustrate some advantages of\n",
            "the proposed calibration method.\n",
            "\n",
            "591. Title: Long-tailed Diffusion Models with Oriented Calibration\n",
            "   Abstract: We study how to generate molecule conformations (i.e., 3D structures) from a\n",
            "molecular graph. Traditional methods, such as molecular dynamics, sample\n",
            "conformations via computationally expensive simulations. Recently, machine\n",
            "learning methods have shown great potential by training on a large collection\n",
            "of conformation data. Challenges arise from the limited model capacity for\n",
            "capturing complex distributions of conformations and the difficulty in modeling\n",
            "long-range dependencies between atoms. Inspired by the recent progress in deep\n",
            "generative models, in this paper, we propose a novel probabilistic framework to\n",
            "generate valid and diverse conformations given a molecular graph. We propose a\n",
            "method combining the advantages of both flow-based and energy-based models,\n",
            "enjoying: (1) a high model capacity to estimate the multimodal conformation\n",
            "distribution; (2) explicitly capturing the complex long-range dependencies\n",
            "between atoms in the observation space. Extensive experiments demonstrate the\n",
            "superior performance of the proposed method on several benchmarks, including\n",
            "conformation generation and distance modeling tasks, with a significant\n",
            "improvement over existing generative models for molecular conformation\n",
            "sampling.\n",
            "\n",
            "592. Title: Federated Text-driven Prompt Generation for Vision-Language Models\n",
            "   Abstract: Autonomous and learning systems based on Deep Reinforcement Learning have\n",
            "firmly established themselves as a foundation for approaches to creating\n",
            "resilient and efficient Cyber-Physical Energy Systems. However, most current\n",
            "approaches suffer from two distinct problems: Modern model-free algorithms such\n",
            "as Soft Actor Critic need a high number of samples to learn a meaningful\n",
            "policy, as well as a fallback to ward against concept drifts (e. g.,\n",
            "catastrophic forgetting). In this paper, we present the work in progress\n",
            "towards a hybrid agent architecture that combines model-based Deep\n",
            "Reinforcement Learning with imitation learning to overcome both problems.\n",
            "\n",
            "593. Title: Fake It Till Make It: Federated Learning with Consensus-Oriented Generation\n",
            "   Abstract: Most real-world data are scattered across different companies or government\n",
            "organizations, and cannot be easily integrated under data privacy and related\n",
            "regulations such as the European Union's General Data Protection Regulation\n",
            "(GDPR) and China' Cyber Security Law. Such data islands situation and data\n",
            "privacy & security are two major challenges for applications of artificial\n",
            "intelligence. In this paper, we tackle these challenges and propose a\n",
            "privacy-preserving machine learning model, called Federated Forest, which is a\n",
            "lossless learning model of the traditional random forest method, i.e.,\n",
            "achieving the same level of accuracy as the non-privacy-preserving approach.\n",
            "Based on it, we developed a secure cross-regional machine learning system that\n",
            "allows a learning process to be jointly trained over different regions' clients\n",
            "with the same user samples but different attribute sets, processing the data\n",
            "stored in each of them without exchanging their raw data. A novel prediction\n",
            "algorithm was also proposed which could largely reduce the communication\n",
            "overhead. Experiments on both real-world and UCI data sets demonstrate the\n",
            "performance of the Federated Forest is as accurate as the non-federated\n",
            "version. The efficiency and robustness of our proposed system had been\n",
            "verified. Overall, our model is practical, scalable and extensible for\n",
            "real-life tasks.\n",
            "\n",
            "594. Title: GIM: Learning Generalizable Image Matcher From Internet Videos\n",
            "   Abstract: DeepFake technology has gained significant attention due to its ability to\n",
            "manipulate facial attributes with high realism, raising serious societal\n",
            "concerns. Face-Swap DeepFake is the most harmful among these techniques, which\n",
            "fabricates behaviors by swapping original faces with synthesized ones. Existing\n",
            "forensic methods, primarily based on Deep Neural Networks (DNNs), effectively\n",
            "expose these manipulations and have become important authenticity indicators.\n",
            "However, these methods mainly concentrate on capturing the blending\n",
            "inconsistency in DeepFake faces, raising a new security issue, termed Active\n",
            "Fake, emerges when individuals intentionally create blending inconsistency in\n",
            "their authentic videos to evade responsibility. This tactic is called DeepFake\n",
            "Camouflage. To achieve this, we introduce a new framework for creating DeepFake\n",
            "camouflage that generates blending inconsistencies while ensuring\n",
            "imperceptibility, effectiveness, and transferability. This framework, optimized\n",
            "via an adversarial learning strategy, crafts imperceptible yet effective\n",
            "inconsistencies to mislead forensic detectors. Extensive experiments\n",
            "demonstrate the effectiveness and robustness of our method, highlighting the\n",
            "need for further research in active fake detection.\n",
            "\n",
            "595. Title: Replay across Experiments: A Natural Extension of Off-Policy RL\n",
            "   Abstract: The optimization of expensive-to-evaluate black-box functions is prevalent in\n",
            "various scientific disciplines. Bayesian optimization is an automatic, general\n",
            "and sample-efficient method to solve these problems with minimal knowledge of\n",
            "the underlying function dynamics. However, the ability of Bayesian optimization\n",
            "to incorporate prior knowledge or beliefs about the function at hand in order\n",
            "to accelerate the optimization is limited, which reduces its appeal for\n",
            "knowledgeable practitioners with tight budgets. To allow domain experts to\n",
            "customize the optimization routine, we propose ColaBO, the first\n",
            "Bayesian-principled framework for incorporating prior beliefs beyond the\n",
            "typical kernel structure, such as the likely location of the optimizer or the\n",
            "optimal value. The generality of ColaBO makes it applicable across different\n",
            "Monte Carlo acquisition functions and types of user beliefs. We empirically\n",
            "demonstrate ColaBO's ability to substantially accelerate optimization when the\n",
            "prior information is accurate, and to retain approximately default performance\n",
            "when it is misleading.\n",
            "\n",
            "596. Title: Predictive, scalable and interpretable knowledge tracing on structured domains\n",
            "   Abstract: Experience replay is central to off-policy algorithms in deep reinforcement\n",
            "learning (RL), but there remain significant gaps in our understanding. We\n",
            "therefore present a systematic and extensive analysis of experience replay in\n",
            "Q-learning methods, focusing on two fundamental properties: the replay capacity\n",
            "and the ratio of learning updates to experience collected (replay ratio). Our\n",
            "additive and ablative studies upend conventional wisdom around experience\n",
            "replay -- greater capacity is found to substantially increase the performance\n",
            "of certain algorithms, while leaving others unaffected. Counterintuitively we\n",
            "show that theoretically ungrounded, uncorrected n-step returns are uniquely\n",
            "beneficial while other techniques confer limited benefit for sifting through\n",
            "larger memory. Separately, by directly controlling the replay ratio we\n",
            "contextualize previous observations in the literature and empirically measure\n",
            "its importance across a variety of deep RL algorithms. Finally, we conclude by\n",
            "testing a set of hypotheses on the nature of these performance benefits.\n",
            "\n",
            "597. Title: The Expressive Power of Transformers with Chain of Thought\n",
            "   Abstract: Advanced generative model (e.g., diffusion model) derived from simplified\n",
            "continuity assumptions of data distribution, though showing promising progress,\n",
            "has been difficult to apply directly to geometry generation applications due to\n",
            "the multi-modality and noise-sensitive nature of molecule geometry. This work\n",
            "introduces Geometric Bayesian Flow Networks (GeoBFN), which naturally fits\n",
            "molecule geometry by modeling diverse modalities in the differentiable\n",
            "parameter space of distributions. GeoBFN maintains the SE-(3) invariant density\n",
            "modeling property by incorporating equivariant inter-dependency modeling on\n",
            "parameters of distributions and unifying the probabilistic modeling of\n",
            "different modalities. Through optimized training and sampling techniques, we\n",
            "demonstrate that GeoBFN achieves state-of-the-art performance on multiple 3D\n",
            "molecule generation benchmarks in terms of generation quality (90.87% molecule\n",
            "stability in QM9 and 85.6% atom stability in GEOM-DRUG. GeoBFN can also conduct\n",
            "sampling with any number of steps to reach an optimal trade-off between\n",
            "efficiency and quality (e.g., 20-times speedup without sacrificing\n",
            "performance).\n",
            "\n",
            "598. Title: VeRA: Vector-based Random Matrix Adaptation\n",
            "   Abstract: Intelligent tutoring systems optimize the selection and timing of learning\n",
            "materials to enhance understanding and long-term retention. This requires\n",
            "estimates of both the learner's progress (''knowledge tracing''; KT), and the\n",
            "prerequisite structure of the learning domain (''knowledge mapping''). While\n",
            "recent deep learning models achieve high KT accuracy, they do so at the expense\n",
            "of the interpretability of psychologically-inspired models. In this work, we\n",
            "present a solution to this trade-off. PSI-KT is a hierarchical generative\n",
            "approach that explicitly models how both individual cognitive traits and the\n",
            "prerequisite structure of knowledge influence learning dynamics, thus achieving\n",
            "interpretability by design. Moreover, by using scalable Bayesian inference,\n",
            "PSI-KT targets the real-world need for efficient personalization even with a\n",
            "growing body of learners and learning histories. Evaluated on three datasets\n",
            "from online learning platforms, PSI-KT achieves superior multi-step predictive\n",
            "accuracy and scalable inference in continual-learning settings, all while\n",
            "providing interpretable representations of learner-specific traits and the\n",
            "prerequisite structure of knowledge that causally supports learning. In sum,\n",
            "predictive, scalable and interpretable knowledge tracing with solid knowledge\n",
            "mapping lays a key foundation for effective personalized learning to make\n",
            "education accessible to a broad, global audience.\n",
            "\n",
            "599. Title: A General Framework for User-Guided Bayesian Optimization\n",
            "   Abstract: Social alignment in AI systems aims to ensure that these models behave\n",
            "according to established societal values. However, unlike humans, who derive\n",
            "consensus on value judgments through social interaction, current language\n",
            "models (LMs) are trained to rigidly replicate their training corpus in\n",
            "isolation, leading to subpar generalization in unfamiliar scenarios and\n",
            "vulnerability to adversarial attacks. This work presents a novel training\n",
            "paradigm that permits LMs to learn from simulated social interactions. In\n",
            "comparison to existing methodologies, our approach is considerably more\n",
            "scalable and efficient, demonstrating superior performance in alignment\n",
            "benchmarks and human evaluations. This paradigm shift in the training of LMs\n",
            "brings us a step closer to developing AI systems that can robustly and\n",
            "accurately reflect societal norms and values.\n",
            "\n",
            "600. Title: Fantastic Generalization Measures are Nowhere to be Found\n",
            "   Abstract: Recent theoretical work has identified surprisingly simple reasoning\n",
            "problems, such as checking if two nodes in a graph are connected or simulating\n",
            "finite-state machines, that are provably unsolvable by standard transformers\n",
            "that answer immediately after reading their input. However, in practice,\n",
            "transformers' reasoning can be improved by allowing them to use a \"chain of\n",
            "thought\" or \"scratchpad\", i.e., generate and condition on a sequence of\n",
            "intermediate tokens before answering. Motivated by this, we ask: Does such\n",
            "intermediate generation fundamentally extend the computational power of a\n",
            "decoder-only transformer? We show that the answer is yes, but the amount of\n",
            "increase depends crucially on the amount of intermediate generation. For\n",
            "instance, we find that transformer decoders with a logarithmic number of\n",
            "decoding steps (w.r.t. the input length) push the limits of standard\n",
            "transformers only slightly, while a linear number of decoding steps, assuming\n",
            "projected pre-norm (a slight generalization of standard pre-norm), adds a clear\n",
            "new ability (under standard complexity conjectures): recognizing all regular\n",
            "languages. Our results also imply that linear steps keep transformer decoders\n",
            "within context-sensitive languages, and polynomial steps with generalized\n",
            "pre-norm make them recognize exactly the class of polynomial-time solvable\n",
            "problems -- the first exact characterization of a type of transformers in terms\n",
            "of standard complexity classes. Together, this provides a nuanced framework for\n",
            "understanding how the length of a transformer's chain of thought or scratchpad\n",
            "impacts its reasoning power.\n",
            "\n",
            "601. Title: Perceptual Group Tokenizer: Building Perception with Iterative Grouping\n",
            "   Abstract: We study the notion of a generalization bound being uniformly tight, meaning\n",
            "that the difference between the bound and the population loss is small for all\n",
            "learning algorithms and all population distributions. Numerous generalization\n",
            "bounds have been proposed in the literature as potential explanations for the\n",
            "ability of neural networks to generalize in the overparameterized setting.\n",
            "However, in their paper ``Fantastic Generalization Measures and Where to Find\n",
            "Them,'' Jiang et al. (2020) examine more than a dozen generalization bounds,\n",
            "and show empirically that none of them are uniformly tight. This raises the\n",
            "question of whether uniformly-tight generalization bounds are at all possible\n",
            "in the overparameterized setting. We consider two types of generalization\n",
            "bounds: (1) bounds that may depend on the training set and the learned\n",
            "hypothesis (e.g., margin bounds). We prove mathematically that no such bound\n",
            "can be uniformly tight in the overparameterized setting; (2) bounds that may in\n",
            "addition also depend on the learning algorithm (e.g., stability bounds). For\n",
            "these bounds, we show a trade-off between the algorithm's performance and the\n",
            "bound's tightness. Namely, if the algorithm achieves good accuracy on certain\n",
            "distributions, then no generalization bound can be uniformly tight for it in\n",
            "the overparameterized setting. We explain how these formal results can, in our\n",
            "view, inform research on generalization bounds for neural networks, while\n",
            "stressing that other interpretations of these results are also possible.\n",
            "\n",
            "602. Title: PromptTTS 2: Describing and Generating Voices with Text Prompt\n",
            "   Abstract: We introduce Lagrangian Flow Networks (LFlows) for modeling fluid densities\n",
            "and velocities continuously in space and time. By construction, the proposed\n",
            "LFlows satisfy the continuity equation, a PDE describing mass conservation in\n",
            "its differentiable form. Our model is based on the insight that solutions to\n",
            "the continuity equation can be expressed as time-dependent density\n",
            "transformations via differentiable and invertible maps. This follows from\n",
            "classical theory of the existence and uniqueness of Lagrangian flows for smooth\n",
            "vector fields. Hence, we model fluid densities by transforming a base density\n",
            "with parameterized diffeomorphisms conditioned on time. The key benefit\n",
            "compared to methods relying on numerical ODE solvers or PINNs is that the\n",
            "analytic expression of the velocity is always consistent with changes in\n",
            "density. Furthermore, we require neither expensive numerical solvers, nor\n",
            "additional penalties to enforce the PDE. LFlows show higher predictive accuracy\n",
            "in density modeling tasks compared to competing models in 2D and 3D, while\n",
            "being computationally efficient. As a real-world application, we model bird\n",
            "migration based on sparse weather radar measurements.\n",
            "\n",
            "603. Title: Lagrangian Flow Networks for Conservation Laws\n",
            "   Abstract: Human visual recognition system shows astonishing capability of compressing\n",
            "visual information into a set of tokens containing rich representations without\n",
            "label supervision. One critical driving principle behind it is perceptual\n",
            "grouping. Despite being widely used in computer vision in the early 2010s, it\n",
            "remains a mystery whether perceptual grouping can be leveraged to derive a\n",
            "neural visual recognition backbone that generates as powerful representations.\n",
            "In this paper, we propose the Perceptual Group Tokenizer, a model that entirely\n",
            "relies on grouping operations to extract visual features and perform\n",
            "self-supervised representation learning, where a series of grouping operations\n",
            "are used to iteratively hypothesize the context for pixels or superpixels to\n",
            "refine feature representations. We show that the proposed model can achieve\n",
            "competitive performance compared to state-of-the-art vision architectures, and\n",
            "inherits desirable properties including adaptive computation without\n",
            "re-training, and interpretability. Specifically, Perceptual Group Tokenizer\n",
            "achieves 80.3% on ImageNet-1K self-supervised learning benchmark with linear\n",
            "probe evaluation, marking a new progress under this paradigm.\n",
            "\n",
            "604. Title: Learning to Jointly Understand Visual and Tactile Signals\n",
            "   Abstract: In the real world, the strong episode resetting mechanisms that are needed to\n",
            "train agents in simulation are unavailable. The \\textit{resetting} assumption\n",
            "limits the potential of reinforcement learning in the real world, as providing\n",
            "resets to an agent usually requires the creation of additional handcrafted\n",
            "mechanisms or human interventions. Recent work aims to train agents\n",
            "(\\textit{forward}) with learned resets by constructing a second\n",
            "(\\textit{backward}) agent that returns the forward agent to the initial state.\n",
            "We find that the termination and timing of the transitions between these two\n",
            "agents are crucial for algorithm success. With this in mind, we create a new\n",
            "algorithm, Reset Free RL with Intelligently Switching Controller (RISC) which\n",
            "intelligently switches between the two agents based on the agent's confidence\n",
            "in achieving its current goal. Our new method achieves state-of-the-art\n",
            "performance on several challenging environments for reset-free RL.\n",
            "\n",
            "605. Title: 3D Reconstruction with Generalizable Neural Fields using Scene Priors\n",
            "   Abstract: Speech conveys more information than text, as the same word can be uttered in\n",
            "various voices to convey diverse information. Compared to traditional\n",
            "text-to-speech (TTS) methods relying on speech prompts (reference speech) for\n",
            "voice variability, using text prompts (descriptions) is more user-friendly\n",
            "since speech prompts can be hard to find or may not exist at all. TTS\n",
            "approaches based on the text prompt face two main challenges: 1) the\n",
            "one-to-many problem, where not all details about voice variability can be\n",
            "described in the text prompt, and 2) the limited availability of text prompt\n",
            "datasets, where vendors and large cost of data labeling are required to write\n",
            "text prompts for speech. In this work, we introduce PromptTTS 2 to address\n",
            "these challenges with a variation network to provide variability information of\n",
            "voice not captured by text prompts, and a prompt generation pipeline to utilize\n",
            "the large language models (LLM) to compose high quality text prompts.\n",
            "Specifically, the variation network predicts the representation extracted from\n",
            "the reference speech (which contains full information about voice variability)\n",
            "based on the text prompt representation. For the prompt generation pipeline, it\n",
            "generates text prompts for speech with a speech language understanding model to\n",
            "recognize voice attributes (e.g., gender, speed) from speech and a large\n",
            "language model to formulate text prompts based on the recognition results.\n",
            "Experiments on a large-scale (44K hours) speech dataset demonstrate that\n",
            "compared to the previous works, PromptTTS 2 generates voices more consistent\n",
            "with text prompts and supports the sampling of diverse voice variability,\n",
            "thereby offering users more choices on voice generation. Additionally, the\n",
            "prompt generation pipeline produces high-quality text prompts, eliminating the\n",
            "large labeling cost. The demo page of PromptTTS 2 is available online.\n",
            "\n",
            "606. Title: InstructCV: Instruction-Tuned Text-to-Image Diffusion Models as Vision Generalists\n",
            "   Abstract: Generative pre-trained models have demonstrated remarkable effectiveness in\n",
            "language and vision domains by learning useful representations. In this paper,\n",
            "we extend the scope of this effectiveness by showing that visual robot\n",
            "manipulation can significantly benefit from large-scale video generative\n",
            "pre-training. We introduce GR-1, a straightforward GPT-style model designed for\n",
            "multi-task language-conditioned visual robot manipulation. GR-1 takes as inputs\n",
            "a language instruction, a sequence of observation images, and a sequence of\n",
            "robot states. It predicts robot actions as well as future images in an\n",
            "end-to-end manner. Thanks to a flexible design, GR-1 can be seamlessly\n",
            "finetuned on robot data after pre-trained on a large-scale video dataset. We\n",
            "perform extensive experiments on the challenging CALVIN benchmark and a real\n",
            "robot. On CALVIN benchmark, our method outperforms state-of-the-art baseline\n",
            "methods and improves the success rate from 88.9% to 94.9%. In the setting of\n",
            "zero-shot unseen scene generalization, GR-1 improves the success rate from\n",
            "53.3% to 85.4%. In real robot experiments, GR-1 also outperforms baseline\n",
            "methods and shows strong potentials in generalization to unseen scenes and\n",
            "objects. We provide inaugural evidence that a unified GPT-style transformer,\n",
            "augmented with large-scale video generative pre-training, exhibits remarkable\n",
            "generalization to multi-task visual robot manipulation. Project page:\n",
            "https://GR1-Manipulation.github.io\n",
            "\n",
            "607. Title: FOSI: Hybrid First and Second Order Optimization\n",
            "   Abstract: High-fidelity 3D scene reconstruction has been substantially advanced by\n",
            "recent progress in neural fields. However, most existing methods train a\n",
            "separate network from scratch for each individual scene. This is not scalable,\n",
            "inefficient, and unable to yield good results given limited views. While\n",
            "learning-based multi-view stereo methods alleviate this issue to some extent,\n",
            "their multi-view setting makes it less flexible to scale up and to broad\n",
            "applications. Instead, we introduce training generalizable Neural Fields\n",
            "incorporating scene Priors (NFPs). The NFP network maps any single-view RGB-D\n",
            "image into signed distance and radiance values. A complete scene can be\n",
            "reconstructed by merging individual frames in the volumetric space WITHOUT a\n",
            "fusion module, which provides better flexibility. The scene priors can be\n",
            "trained on large-scale datasets, allowing for fast adaptation to the\n",
            "reconstruction of a new scene with fewer views. NFP not only demonstrates SOTA\n",
            "scene reconstruction performance and efficiency, but it also supports\n",
            "single-image novel-view synthesis, which is underexplored in neural fields.\n",
            "More qualitative results are available at:\n",
            "https://oasisyang.github.io/neural-prior\n",
            "\n",
            "608. Title: Unleashing Large-Scale Video Generative Pre-training for Visual Robot Manipulation\n",
            "   Abstract: The missing signal caused by the objects being occluded or an unstable sensor\n",
            "is a common challenge during data collection. Such missing signals will\n",
            "adversely affect the results obtained from the data, and this issue is observed\n",
            "more frequently in robotic tactile perception. In tactile perception, due to\n",
            "the limited working space and the dynamic environment, the contact between the\n",
            "tactile sensor and the object is frequently insufficient and unstable, which\n",
            "causes the partial loss of signals, thus leading to incomplete tactile data.\n",
            "The tactile data will therefore contain fewer tactile cues with low information\n",
            "density. In this paper, we propose a tactile representation learning method,\n",
            "named TacMAE, based on Masked Autoencoder to address the problem of incomplete\n",
            "tactile data in tactile perception. In our framework, a portion of the tactile\n",
            "image is masked out to simulate the missing contact region. By reconstructing\n",
            "the missing signals in the tactile image, the trained model can achieve a\n",
            "high-level understanding of surface geometry and tactile properties from\n",
            "limited tactile cues. The experimental results of tactile texture recognition\n",
            "show that our proposed TacMAE can achieve a high recognition accuracy of 71.4%\n",
            "in the zero-shot transfer and 85.8% after fine-tuning, which are 15.2% and 8.2%\n",
            "higher than the results without using masked modeling. The extensive\n",
            "experiments on YCB objects demonstrate the knowledge transferability of our\n",
            "proposed method and the potential to improve efficiency in tactile exploration.\n",
            "\n",
            "609. Title: Masked Audio Generation using a Single Non-Autoregressive Transformer\n",
            "   Abstract: Attention-based models are appealing for multimodal processing because inputs\n",
            "from multiple modalities can be concatenated and fed to a single backbone\n",
            "network - thus requiring very little fusion engineering. The resulting\n",
            "representations are however fully entangled throughout the network, which may\n",
            "not always be desirable: in learning, contrastive audio-visual self-supervised\n",
            "learning requires independent audio and visual features to operate, otherwise\n",
            "learning collapses; in inference, evaluation of audio-visual models should be\n",
            "possible on benchmarks having just audio or just video. In this paper, we\n",
            "introduce Zorro, a technique that uses masks to control how inputs from each\n",
            "modality are routed inside Transformers, keeping some parts of the\n",
            "representation modality-pure. We apply this technique to three popular\n",
            "transformer-based architectures (ViT, Swin and HiP) and show that with\n",
            "contrastive pre-training Zorro achieves state-of-the-art results on most\n",
            "relevant benchmarks for multimodal tasks (AudioSet and VGGSound). Furthermore,\n",
            "the resulting models are able to perform unimodal inference on both video and\n",
            "audio benchmarks such as Kinetics-400 or ESC-50.\n",
            "\n",
            "610. Title: Toward effective protection against diffusion-based mimicry through score distillation\n",
            "   Abstract: In the battle against widespread online misinformation, a growing problem is\n",
            "text-image inconsistency, where images are misleadingly paired with texts with\n",
            "different intent or meaning. Existing classification-based methods for\n",
            "text-image inconsistency can identify contextual inconsistencies but fail to\n",
            "provide explainable justifications for their decisions that humans can\n",
            "understand. Although more nuanced, human evaluation is impractical at scale and\n",
            "susceptible to errors. To address these limitations, this study introduces\n",
            "D-TIIL (Diffusion-based Text-Image Inconsistency Localization), which employs\n",
            "text-to-image diffusion models to localize semantic inconsistencies in text and\n",
            "image pairs. These models, trained on large-scale datasets act as ``omniscient\"\n",
            "agents that filter out irrelevant information and incorporate background\n",
            "knowledge to identify inconsistencies. In addition, D-TIIL uses text embeddings\n",
            "and modified image regions to visualize these inconsistencies. To evaluate\n",
            "D-TIIL's efficacy, we introduce a new TIIL dataset containing 14K consistent\n",
            "and inconsistent text-image pairs. Unlike existing datasets, TIIL enables\n",
            "assessment at the level of individual words and image regions and is carefully\n",
            "designed to represent various inconsistencies. D-TIIL offers a scalable and\n",
            "evidence-based approach to identifying and localizing text-image inconsistency,\n",
            "providing a robust framework for future research combating misinformation.\n",
            "\n",
            "611. Title: Progressive3D: Progressively Local Editing for Text-to-3D Content Creation with Complex Semantic Prompts\n",
            "   Abstract: While generative diffusion models excel in producing high-quality images,\n",
            "they can also be misused to mimic authorized images, posing a significant\n",
            "threat to AI systems. Efforts have been made to add calibrated perturbations to\n",
            "protect images from diffusion-based mimicry pipelines. However, most of the\n",
            "existing methods are too ineffective and even impractical to be used by\n",
            "individual users due to their high computation and memory requirements. In this\n",
            "work, we present novel findings on attacking latent diffusion models (LDM) and\n",
            "propose new plug-and-play strategies for more effective protection. In\n",
            "particular, we explore the bottleneck in attacking an LDM, discovering that the\n",
            "encoder module rather than the denoiser module is the vulnerable point. Based\n",
            "on this insight, we present our strategy using Score Distillation Sampling\n",
            "(SDS) to double the speed of protection and reduce memory occupation by half\n",
            "without compromising its strength. Additionally, we provide a robust protection\n",
            "strategy by counterintuitively minimizing the semantic loss, which can assist\n",
            "in generating more natural perturbations. Finally, we conduct extensive\n",
            "experiments to substantiate our findings and comprehensively evaluate our newly\n",
            "proposed strategies. We hope our insights and protective measures can\n",
            "contribute to better defense against malicious diffusion-based mimicry,\n",
            "advancing the development of secure AI systems. The code is available in\n",
            "https://github.com/xavihart/Diff-Protect\n",
            "\n",
            "612. Title: Modelling complex vector drawings with stroke-clouds\n",
            "   Abstract: With the rising popularity of Large Language Models (LLMs), there has been an\n",
            "increasing interest in compression techniques that enable their efficient\n",
            "deployment. This study focuses on the Post-Training Quantization (PTQ) of LLMs.\n",
            "Drawing from recent advances, our work introduces QuantEase, a layer-wise\n",
            "quantization framework where individual layers undergo separate quantization.\n",
            "The problem is framed as a discrete-structured non-convex optimization,\n",
            "prompting the development of algorithms rooted in Coordinate Descent (CD)\n",
            "techniques. These CD-based methods provide high-quality solutions to the\n",
            "complex non-convex layer-wise quantization problems. Notably, our CD-based\n",
            "approach features straightforward updates, relying solely on matrix and vector\n",
            "operations, circumventing the need for matrix inversion or decomposition. We\n",
            "also explore an outlier-aware variant of our approach, allowing for retaining\n",
            "significant weights (outliers) with complete precision. Our proposal attains\n",
            "state-of-the-art performance in terms of perplexity and zero-shot accuracy in\n",
            "empirical evaluations across various LLMs and datasets, with relative\n",
            "improvements up to 15% over methods such as GPTQ. Leveraging careful linear\n",
            "algebra optimizations, QuantEase can quantize models like Falcon-180B on a\n",
            "single NVIDIA A100 GPU in $\\sim$3 hours. Particularly noteworthy is our\n",
            "outlier-aware algorithm's capability to achieve near or sub-3-bit quantization\n",
            "of LLMs with an acceptable drop in accuracy, obviating the need for non-uniform\n",
            "quantization or grouping techniques, improving upon methods such as SpQR by up\n",
            "to two times in terms of perplexity.\n",
            "\n",
            "613. Title: Leveraging Optimization for Adaptive Attacks on Image Watermarks\n",
            "   Abstract: Deep convolutional neural networks (DCNNs) have dominated the recent\n",
            "developments in computer vision through making various record-breaking models.\n",
            "However, it is still a great challenge to achieve powerful DCNNs in\n",
            "resource-limited environments, such as on embedded devices and smart phones.\n",
            "Researchers have realized that 1-bit CNNs can be one feasible solution to\n",
            "resolve the issue; however, they are baffled by the inferior performance\n",
            "compared to the full-precision DCNNs. In this paper, we propose a novel\n",
            "approach, called Bayesian optimized 1-bit CNNs (denoted as BONNs), taking the\n",
            "advantage of Bayesian learning, a well-established strategy for hard problems,\n",
            "to significantly improve the performance of extreme 1-bit CNNs. We incorporate\n",
            "the prior distributions of full-precision kernels and features into the\n",
            "Bayesian framework to construct 1-bit CNNs in an end-to-end manner, which have\n",
            "not been considered in any previous related methods. The Bayesian losses are\n",
            "achieved with a theoretical support to optimize the network simultaneously in\n",
            "both continuous and discrete spaces, aggregating different losses jointly to\n",
            "improve the model capacity. Extensive experiments on the ImageNet and CIFAR\n",
            "datasets show that BONNs achieve the best classification performance compared\n",
            "to state-of-the-art 1-bit CNNs.\n",
            "\n",
            "614. Title: Biased Temporal Convolution Graph Network for Time Series Forecasting with Missing Values\n",
            "   Abstract: Untrustworthy users can misuse image generators to synthesize high-quality\n",
            "deepfakes and engage in unethical activities. Watermarking deters misuse by\n",
            "marking generated content with a hidden message, enabling its detection using a\n",
            "secret watermarking key. A core security property of watermarking is\n",
            "robustness, which states that an attacker can only evade detection by\n",
            "substantially degrading image quality. Assessing robustness requires designing\n",
            "an adaptive attack for the specific watermarking algorithm. When evaluating\n",
            "watermarking algorithms and their (adaptive) attacks, it is challenging to\n",
            "determine whether an adaptive attack is optimal, i.e., the best possible\n",
            "attack. We solve this problem by defining an objective function and then\n",
            "approach adaptive attacks as an optimization problem. The core idea of our\n",
            "adaptive attacks is to replicate secret watermarking keys locally by creating\n",
            "surrogate keys that are differentiable and can be used to optimize the attack's\n",
            "parameters. We demonstrate for Stable Diffusion models that such an attacker\n",
            "can break all five surveyed watermarking methods at no visible degradation in\n",
            "image quality. Optimizing our attacks is efficient and requires less than 1 GPU\n",
            "hour to reduce the detection accuracy to 6.3% or less. Our findings emphasize\n",
            "the need for more rigorous robustness testing against adaptive, learnable\n",
            "attackers.\n",
            "\n",
            "615. Title: Defining and extracting generalizable interaction primitives from DNNs\n",
            "   Abstract: Understanding the 3D structures of protein multimers is crucial, as they play\n",
            "a vital role in regulating various cellular processes. It has been empirically\n",
            "confirmed that the multimer structure prediction~(MSP) can be well handled in a\n",
            "step-wise assembly fashion using provided dimer structures and predicted\n",
            "protein-protein interactions~(PPIs). However, due to the biological gap in the\n",
            "formation of dimers and larger multimers, directly applying PPI prediction\n",
            "techniques can often cause a \\textit{poor generalization} to the MSP task. To\n",
            "address this challenge, we aim to extend the PPI knowledge to multimers of\n",
            "different scales~(i.e., chain numbers). Specifically, we propose\n",
            "\\textbf{\\textsc{PromptMSP}}, a pre-training and \\textbf{Prompt} tuning\n",
            "framework for \\textbf{M}ultimer \\textbf{S}tructure \\textbf{P}rediction. First,\n",
            "we tailor the source and target tasks for effective PPI knowledge learning and\n",
            "efficient inference, respectively. We design PPI-inspired prompt learning to\n",
            "narrow the gaps of two task formats and generalize the PPI knowledge to\n",
            "multimers of different scales. We provide a meta-learning strategy to learn a\n",
            "reliable initialization of the prompt model, enabling our prompting framework\n",
            "to effectively adapt to limited data for large-scale multimers. Empirically, we\n",
            "achieve both significant accuracy (RMSD and TM-Score) and efficiency\n",
            "improvements compared to advanced MSP models. The code, data and checkpoints\n",
            "are released at \\url{https://github.com/zqgao22/PromptMSP}.\n",
            "\n",
            "616. Title: Aligning Relational Learning with Lipschitz Fairness\n",
            "   Abstract: Assigning importance weights to adversarial data has achieved great success\n",
            "in training adversarially robust networks under limited model capacity.\n",
            "However, existing instance-reweighted adversarial training (AT) methods heavily\n",
            "depend on heuristics and/or geometric interpretations to determine those\n",
            "importance weights, making these algorithms lack rigorous theoretical\n",
            "justification/guarantee. Moreover, recent research has shown that adversarial\n",
            "training suffers from a severe non-uniform robust performance across the\n",
            "training distribution, e.g., data points belonging to some classes can be much\n",
            "more vulnerable to adversarial attacks than others. To address both issues, in\n",
            "this paper, we propose a novel doubly-robust instance reweighted AT framework,\n",
            "which allows to obtain the importance weights via exploring distributionally\n",
            "robust optimization (DRO) techniques, and at the same time boosts the\n",
            "robustness on the most vulnerable examples. In particular, our importance\n",
            "weights are obtained by optimizing the KL-divergence regularized loss function,\n",
            "which allows us to devise new algorithms with a theoretical convergence\n",
            "guarantee. Experiments on standard classification datasets demonstrate that our\n",
            "proposed approach outperforms related state-of-the-art baseline methods in\n",
            "terms of average robust performance, and at the same time improves the\n",
            "robustness against attacks on the weakest data points. Codes will be available\n",
            "soon.\n",
            "\n",
            "617. Title: DragonDiffusion: Enabling Drag-style Manipulation on Diffusion Models\n",
            "   Abstract: In this paper, we show that although the minimizers of cross-entropy and\n",
            "related classification losses are off at infinity, network weights learned by\n",
            "gradient flow converge in direction, with an immediate corollary that network\n",
            "predictions, training errors, and the margin distribution also converge. This\n",
            "proof holds for deep homogeneous networks -- a broad class of networks allowing\n",
            "for ReLU, max-pooling, linear, and convolutional layers -- and we additionally\n",
            "provide empirical support not just close to the theory (e.g., the AlexNet), but\n",
            "also on non-homogeneous networks (e.g., the DenseNet). If the network further\n",
            "has locally Lipschitz gradients, we show that these gradients also converge in\n",
            "direction, and asymptotically align with the gradient flow path, with\n",
            "consequences on margin maximization, convergence of saliency maps, and a few\n",
            "other settings. Our analysis complements and is distinct from the well-known\n",
            "neural tangent and mean-field theories, and in particular makes no requirements\n",
            "on network width and initialization, instead merely requiring perfect\n",
            "classification accuracy. The proof proceeds by developing a theory of unbounded\n",
            "nonsmooth Kurdyka-{\\L}ojasiewicz inequalities for functions definable in an\n",
            "o-minimal structure, and is also applicable outside deep learning.\n",
            "\n",
            "618. Title: Doubly Robust Instance-Reweighted Adversarial Training\n",
            "   Abstract: Faithfully summarizing the knowledge encoded by a deep neural network (DNN)\n",
            "into a few symbolic primitive patterns without losing much information\n",
            "represents a core challenge in explainable AI. To this end, Ren et al. (2024)\n",
            "have derived a series of theorems to prove that the inference score of a DNN\n",
            "can be explained as a small set of interactions between input variables.\n",
            "However, the lack of generalization power makes it still hard to consider such\n",
            "interactions as faithful primitive patterns encoded by the DNN. Therefore,\n",
            "given different DNNs trained for the same task, we develop a new method to\n",
            "extract interactions that are shared by these DNNs. Experiments show that the\n",
            "extracted interactions can better reflect common knowledge shared by different\n",
            "DNNs.\n",
            "\n",
            "619. Title: Multi-View Causal Representation Learning with Partial Observability\n",
            "   Abstract: In [Amir et al.], the authors consider the generalization $\\Gor$ of the\n",
            "Erd\\H{o}s-R\\'enyi random graph process $G$, where instead of adding new edges\n",
            "uniformly, $\\Gor$ gives a weight of size 1 to missing edges between pairs of\n",
            "isolated vertices, and a weight of size $K\\in[0,\\infty)$ otherwise. This can\n",
            "correspond to the linking of settlements or the spreading of an epidemic. The\n",
            "authors investigate $\\tgor(K)$, the critical time for the appearance of a giant\n",
            "component as a function of $K$, and prove that\n",
            "$\\tgor=(1+o(1))\\frac{4}{\\sqrt{3K}}$, using a proper timescale.\n",
            "  In this work, we show that a natural variation of the model $\\Gor$ has\n",
            "interesting properties. Define the process $\\Gand$, where a weight of size $K$\n",
            "is assigned to edges between pairs of non-isolated vertices, and a weight of\n",
            "size 1 otherwise. We prove that the asymptotical behavior of the giant\n",
            "component threshold is essentially the same for $\\Gand$, and namely $\\tgand /\n",
            "\\tgor$ tends to $\\frac{64\\sqrt{6}}{\\pi(24+\\pi^2)}\\approx 1.47$ as $K\\to\\infty$.\n",
            "However, the corresponding thresholds for connectivity satisfy $\\tcand /\n",
            "\\tcor=\\max\\{{1/2},K\\}$ for every $K>0$. Following the methods of [Amir et al.],\n",
            "$\\tgand$ is characterized as the singularity point to a system of differential\n",
            "equations, and computer simulations of both models agree with the analytical\n",
            "results as well as with the asymptotic analysis. In the process, we answer the\n",
            "following question: when does a giant component emerge in a graph process where\n",
            "edges are chosen uniformly out of all edges incident to isolated vertices,\n",
            "while such exist, and otherwise uniformly? This corresponds to the value of\n",
            "$\\tgand(0)$, which we show to be ${3/2}+\\frac{4}{3\\mathrm{e}^2-1}$.\n",
            "\n",
            "620. Title: Protein Multimer Structure Prediction via Prompt Learning\n",
            "   Abstract: Intelligent agents can cope with sensory-rich environments by learning\n",
            "task-agnostic state abstractions. In this paper, we propose an algorithm to\n",
            "approximate causal states, which are the coarsest partition of the joint\n",
            "history of actions and observations in partially-observable Markov decision\n",
            "processes (POMDP). Our method learns approximate causal state representations\n",
            "from RNNs trained to predict subsequent observations given the history. We\n",
            "demonstrate that these learned state representations are useful for learning\n",
            "policies efficiently in reinforcement learning problems with rich observation\n",
            "spaces. We connect causal states with causal feature sets from the causal\n",
            "inference literature, and also provide theoretical guarantees on the optimality\n",
            "of the continuous version of this causal state representation under Lipschitz\n",
            "assumptions by proving equivalence to bisimulation, a relation between\n",
            "behaviorally equivalent systems. This allows for lower bounds on the optimal\n",
            "value function of the learned representation, which is tight given certain\n",
            "assumptions. Finally, we empirically evaluate causal state representations\n",
            "using multiple partially observable tasks and compare with prior methods.\n",
            "\n",
            "621. Title: Diffusion Generative Flow Samplers: Improving learning signals through partial trajectory optimization\n",
            "   Abstract: We tackle the problem of sampling from intractable high-dimensional density\n",
            "functions, a fundamental task that often appears in machine learning and\n",
            "statistics. We extend recent sampling-based approaches that leverage controlled\n",
            "stochastic processes to model approximate samples from these target densities.\n",
            "The main drawback of these approaches is that the training objective requires\n",
            "full trajectories to compute, resulting in sluggish credit assignment issues\n",
            "due to use of entire trajectories and a learning signal present only at the\n",
            "terminal time. In this work, we present Diffusion Generative Flow Samplers\n",
            "(DGFS), a sampling-based framework where the learning process can be tractably\n",
            "broken down into short partial trajectory segments, via parameterizing an\n",
            "additional \"flow function\". Our method takes inspiration from the theory\n",
            "developed for generative flow networks (GFlowNets), allowing us to make use of\n",
            "intermediate learning signals. Through various challenging experiments, we\n",
            "demonstrate that DGFS achieves more accurate estimates of the normalization\n",
            "constant than closely-related prior methods.\n",
            "\n",
            "622. Title: CoT3DRef: Chain-of-Thoughts Data-Efficient 3D Visual Grounding\n",
            "   Abstract: Equipping agents with the capacity to justify made decisions using supporting\n",
            "evidence represents a cornerstone of accountable decision-making. Furthermore,\n",
            "ensuring that justifications are in line with human expectations and societal\n",
            "norms is vital, especially in high-stakes situations such as healthcare. In\n",
            "this work, we propose the use of a debate-based reward model for reinforcement\n",
            "learning agents, where the outcome of a zero-sum debate game quantifies the\n",
            "justifiability of a decision in a particular state. This reward model is then\n",
            "used to train a justifiable policy, whose decisions can be more easily\n",
            "corroborated with supporting evidence. In the debate game, two argumentative\n",
            "agents take turns providing supporting evidence for two competing decisions.\n",
            "Given the proposed evidence, a proxy of a human judge evaluates which decision\n",
            "is better justified. We demonstrate the potential of our approach in learning\n",
            "policies for prescribing and justifying treatment decisions of septic patients.\n",
            "We show that augmenting the reward with the feedback signal generated by the\n",
            "debate-based reward model yields policies highly favored by the judge when\n",
            "compared to the policy obtained solely from the environment rewards, while\n",
            "hardly sacrificing any performance. Moreover, in terms of the overall\n",
            "performance and justifiability of trained policies, the debate-based feedback\n",
            "is comparable to the feedback obtained from an ideal judge proxy that evaluates\n",
            "decisions using the full information encoded in the state. This suggests that\n",
            "the debate game outputs key information contained in states that is most\n",
            "relevant for evaluating decisions, which in turn substantiates the practicality\n",
            "of combining our approach with human-in-the-loop evaluations. Lastly, we\n",
            "showcase that agents trained via multi-agent debate learn to propose evidence\n",
            "that is resilient to refutations and closely aligns with human preferences.\n",
            "\n",
            "623. Title: $\\infty$-Diff: Infinite Resolution Diffusion with Subsampled Mollified States\n",
            "   Abstract: We present a new method to localize a camera within a previously unseen\n",
            "environment perceived from an egocentric point of view. Although this is, in\n",
            "general, an ill-posed problem, humans can effortlessly and efficiently\n",
            "determine their relative location and orientation and navigate into a\n",
            "previously unseen environments, e.g., finding a specific item in a new grocery\n",
            "store. To enable such a capability, we design a new egocentric representation,\n",
            "which we call ECO (Egocentric COgnitive map). ECO is biologically inspired, by\n",
            "the cognitive map that allows human navigation, and it encodes the surrounding\n",
            "visual semantics with respect to both distance and orientation. ECO possesses\n",
            "three main properties: (1) reconfigurability: complex semantics and geometry is\n",
            "captured via the synthesis of atomic visual representations (e.g., image\n",
            "patch); (2) robustness: the visual semantics are registered in a geometrically\n",
            "consistent way (e.g., aligning with respect to the gravity vector,\n",
            "frontalizing, and rescaling to canonical depth), thus enabling us to learn\n",
            "meaningful atomic representations; (3) adaptability: a domain adaptation\n",
            "framework is designed to generalize the learned representation without manual\n",
            "calibration. As a proof-of-concept, we use ECO to localize a camera within\n",
            "real-world scenes---various grocery stores---and demonstrate performance\n",
            "improvements when compared to existing semantic localization approaches.\n",
            "\n",
            "624. Title: Reward Design for Justifiable Sequential Decision-Making\n",
            "   Abstract: Knowledge distillation aims to train a compact student network using soft\n",
            "supervision from a larger teacher network and hard supervision from ground\n",
            "truths. However, determining an optimal knowledge fusion ratio that balances\n",
            "these supervisory signals remains challenging. Prior methods generally resort\n",
            "to a constant or heuristic-based fusion ratio, which often falls short of a\n",
            "proper balance. In this study, we introduce a novel adaptive method for\n",
            "learning a sample-wise knowledge fusion ratio, exploiting both the correctness\n",
            "of teacher and student, as well as how well the student mimics the teacher on\n",
            "each sample. Our method naturally leads to the intra-sample trilateral\n",
            "geometric relations among the student prediction ($S$), teacher prediction\n",
            "($T$), and ground truth ($G$). To counterbalance the impact of outliers, we\n",
            "further extend to the inter-sample relations, incorporating the teacher's\n",
            "global average prediction $\\bar{T}$ for samples within the same class. A simple\n",
            "neural network then learns the implicit mapping from the intra- and\n",
            "inter-sample relations to an adaptive, sample-wise knowledge fusion ratio in a\n",
            "bilevel-optimization manner. Our approach provides a simple, practical, and\n",
            "adaptable solution for knowledge distillation that can be employed across\n",
            "various architectures and model sizes. Extensive experiments demonstrate\n",
            "consistent improvements over other loss re-weighting methods on image\n",
            "classification, attack detection, and click-through rate prediction.\n",
            "\n",
            "625. Title: Evoke: Evoking Critical Thinking Abilities in LLMs via Reviewer-Author Prompt Editing\n",
            "   Abstract: Sharpness-aware minimization (SAM) has received increasing attention in\n",
            "computer vision since it can effectively eliminate the sharp local minima from\n",
            "the training trajectory and mitigate generalization degradation. However, SAM\n",
            "requires two sequential gradient computations during the optimization of each\n",
            "step: one to obtain the perturbation gradient and the other to obtain the\n",
            "updating gradient. Compared with the base optimizer (e.g., Adam), SAM doubles\n",
            "the time overhead due to the additional perturbation gradient. By dissecting\n",
            "the theory of SAM and observing the training gradient of the molecular graph\n",
            "transformer, we propose a new algorithm named GraphSAM, which reduces the\n",
            "training cost of SAM and improves the generalization performance of graph\n",
            "transformer models. There are two key factors that contribute to this result:\n",
            "(i) \\textit{gradient approximation}: we use the updating gradient of the\n",
            "previous step to approximate the perturbation gradient at the intermediate\n",
            "steps smoothly (\\textbf{increases efficiency}); (ii) \\textit{loss landscape\n",
            "approximation}: we theoretically prove that the loss landscape of GraphSAM is\n",
            "limited to a small range centered on the expected loss of SAM\n",
            "(\\textbf{guarantees generalization performance}). The extensive experiments on\n",
            "six datasets with different tasks demonstrate the superiority of GraphSAM,\n",
            "especially in optimizing the model update process. The code is\n",
            "in:https://github.com/YL-wang/GraphSAM/tree/graphsam\n",
            "\n",
            "626. Title: Less or More From Teacher: Exploiting Trilateral Geometry For Knowledge Distillation\n",
            "   Abstract: Self-supervised pre-training of language models usually consists in\n",
            "predicting probability distributions over extensive token vocabularies. In this\n",
            "study, we propose an innovative method that shifts away from probability\n",
            "prediction and instead focuses on reconstructing input embeddings in a\n",
            "contrastive fashion via Constrastive Weight Tying (CWT). We apply this approach\n",
            "to pretrain Headless Language Models in both monolingual and multilingual\n",
            "contexts. Our method offers practical advantages, substantially reducing\n",
            "training computational requirements by up to 20 times, while simultaneously\n",
            "enhancing downstream performance and data efficiency. We observe a significant\n",
            "+1.6 GLUE score increase and a notable +2.7 LAMBADA accuracy improvement\n",
            "compared to classical LMs within similar compute budgets.\n",
            "\n",
            "627. Title: Efficient Sharpness-Aware Minimization for Molecular Graph Transformer Models\n",
            "   Abstract: Bayesian optimization (BO) is a powerful approach for optimizing complex and\n",
            "expensive-to-evaluate black-box functions. Its importance is underscored in\n",
            "many applications, notably including hyperparameter tuning, but its efficacy\n",
            "depends on efficiently balancing exploration and exploitation. While there has\n",
            "been substantial progress in BO methods, striking this balance remains a\n",
            "delicate process. In this light, we present LLAMBO, a novel approach that\n",
            "integrates the capabilities of Large Language Models (LLM) within BO. At a high\n",
            "level, we frame the BO problem in natural language, enabling LLMs to\n",
            "iteratively propose and evaluate promising solutions conditioned on historical\n",
            "evaluations. More specifically, we explore how combining contextual\n",
            "understanding, few-shot learning proficiency, and domain knowledge of LLMs can\n",
            "improve model-based BO. Our findings illustrate that LLAMBO is effective at\n",
            "zero-shot warmstarting, and enhances surrogate modeling and candidate sampling,\n",
            "especially in the early stages of search when observations are sparse. Our\n",
            "approach is performed in context and does not require LLM finetuning.\n",
            "Additionally, it is modular by design, allowing individual components to be\n",
            "integrated into existing BO frameworks, or function cohesively as an end-to-end\n",
            "method. We empirically validate LLAMBO's efficacy on the problem of\n",
            "hyperparameter tuning, highlighting strong empirical performance across a range\n",
            "of diverse benchmarks, proprietary, and synthetic tasks.\n",
            "\n",
            "628. Title: Spectrally Transformed Kernel Regression\n",
            "   Abstract: Denoising Score Matching with Annealed Langevin Sampling (DSM-ALS) has\n",
            "recently found success in generative modeling. The approach works by first\n",
            "training a neural network to estimate the score of a distribution, and then\n",
            "using Langevin dynamics to sample from the data distribution assumed by the\n",
            "score network. Despite the convincing visual quality of samples, this method\n",
            "appears to perform worse than Generative Adversarial Networks (GANs) under the\n",
            "Fr\\'echet Inception Distance, a standard metric for generative models.\n",
            "  We show that this apparent gap vanishes when denoising the final Langevin\n",
            "samples using the score network. In addition, we propose two improvements to\n",
            "DSM-ALS: 1) Consistent Annealed Sampling as a more stable alternative to\n",
            "Annealed Langevin Sampling, and 2) a hybrid training formulation, composed of\n",
            "both Denoising Score Matching and adversarial objectives. By combining these\n",
            "two techniques and exploring different network architectures, we elevate score\n",
            "matching methods and obtain results competitive with state-of-the-art image\n",
            "generation on CIFAR-10.\n",
            "\n",
            "629. Title: Estimating Conditional Mutual Information for Dynamic Feature Selection\n",
            "   Abstract: In our era of enormous neural networks, empirical progress has been driven by\n",
            "the philosophy that more is better. Recent deep learning practice has found\n",
            "repeatedly that larger model size, more data, and more computation (resulting\n",
            "in lower training loss) improves performance. In this paper, we give\n",
            "theoretical backing to these empirical observations by showing that these three\n",
            "properties hold in random feature (RF) regression, a class of models equivalent\n",
            "to shallow networks with only the last layer trained.\n",
            "  Concretely, we first show that the test risk of RF regression decreases\n",
            "monotonically with both the number of features and the number of samples,\n",
            "provided the ridge penalty is tuned optimally. In particular, this implies that\n",
            "infinite width RF architectures are preferable to those of any finite width. We\n",
            "then proceed to demonstrate that, for a large class of tasks characterized by\n",
            "powerlaw eigenstructure, training to near-zero training loss is obligatory:\n",
            "near-optimal performance can only be achieved when the training error is much\n",
            "smaller than the test error. Grounding our theory in real-world data, we find\n",
            "empirically that standard computer vision tasks with convolutional neural\n",
            "tangent kernels clearly fall into this class. Taken together, our results tell\n",
            "a simple, testable story of the benefits of overparameterization, overfitting,\n",
            "and more data in random feature models.\n",
            "\n",
            "630. Title: DiLu: A Knowledge-Driven Approach to Autonomous Driving with Large Language Models\n",
            "   Abstract: Unlabeled data is a key component of modern machine learning. In general, the\n",
            "role of unlabeled data is to impose a form of smoothness, usually from the\n",
            "similarity information encoded in a base kernel, such as the\n",
            "$\\epsilon$-neighbor kernel or the adjacency matrix of a graph. This work\n",
            "revisits the classical idea of spectrally transformed kernel regression (STKR),\n",
            "and provides a new class of general and scalable STKR estimators able to\n",
            "leverage unlabeled data. Intuitively, via spectral transformation, STKR\n",
            "exploits the data distribution for which unlabeled data can provide additional\n",
            "information. First, we show that STKR is a principled and general approach, by\n",
            "characterizing a universal type of \"target smoothness\", and proving that any\n",
            "sufficiently smooth function can be learned by STKR. Second, we provide\n",
            "scalable STKR implementations for the inductive setting and a general\n",
            "transformation function, while prior work is mostly limited to the transductive\n",
            "setting. Third, we derive statistical guarantees for two scenarios: STKR with a\n",
            "known polynomial transformation, and STKR with kernel PCA when the\n",
            "transformation is unknown. Overall, we believe that this work helps deepen our\n",
            "understanding of how to work with unlabeled data, and its generality makes it\n",
            "easier to inspire new methods.\n",
            "\n",
            "631. Title: Universal Humanoid Motion Representations for Physics-Based Control\n",
            "   Abstract: We present a universal motion representation that encompasses a comprehensive\n",
            "range of motor skills for physics-based humanoid control. Due to the high\n",
            "dimensionality of humanoids and the inherent difficulties in reinforcement\n",
            "learning, prior methods have focused on learning skill embeddings for a narrow\n",
            "range of movement styles (e.g. locomotion, game characters) from specialized\n",
            "motion datasets. This limited scope hampers their applicability in complex\n",
            "tasks. We close this gap by significantly increasing the coverage of our motion\n",
            "representation space. To achieve this, we first learn a motion imitator that\n",
            "can imitate all of human motion from a large, unstructured motion dataset. We\n",
            "then create our motion representation by distilling skills directly from the\n",
            "imitator. This is achieved by using an encoder-decoder structure with a\n",
            "variational information bottleneck. Additionally, we jointly learn a prior\n",
            "conditioned on proprioception (humanoid's own pose and velocities) to improve\n",
            "model expressiveness and sampling efficiency for downstream tasks. By sampling\n",
            "from the prior, we can generate long, stable, and diverse human motions. Using\n",
            "this latent space for hierarchical RL, we show that our policies solve tasks\n",
            "using human-like behavior. We demonstrate the effectiveness of our motion\n",
            "representation by solving generative tasks (e.g. strike, terrain traversal) and\n",
            "motion tracking using VR controllers.\n",
            "\n",
            "632. Title: ImagenHub: Standardizing the evaluation of conditional image generation models\n",
            "   Abstract: Dynamic feature selection, where we sequentially query features to make\n",
            "accurate predictions with a minimal budget, is a promising paradigm to reduce\n",
            "feature acquisition costs and provide transparency into a model's predictions.\n",
            "The problem is challenging, however, as it requires both predicting with\n",
            "arbitrary feature sets and learning a policy to identify valuable selections.\n",
            "Here, we take an information-theoretic perspective and prioritize features\n",
            "based on their mutual information with the response variable. The main\n",
            "challenge is implementing this policy, and we design a new approach that\n",
            "estimates the mutual information in a discriminative rather than generative\n",
            "fashion. Building on our approach, we then introduce several further\n",
            "improvements: allowing variable feature budgets across samples, enabling\n",
            "non-uniform feature costs, incorporating prior information, and exploring\n",
            "modern architectures to handle partial inputs. Our experiments show that our\n",
            "method provides consistent gains over recent methods across a variety of\n",
            "datasets.\n",
            "\n",
            "633. Title: Amortizing intractable inference in large language models\n",
            "   Abstract: Triggered by limitations of graph-based deep learning methods in terms of\n",
            "computational expressivity and model flexibility, recent years have seen a\n",
            "surge of interest in computational models that operate on higher-order\n",
            "topological domains such as hypergraphs and simplicial complexes. While the\n",
            "increased expressivity of these models can indeed lead to a better\n",
            "classification performance and a more faithful representation of the underlying\n",
            "system, the computational cost of these higher-order models can increase\n",
            "dramatically. To this end, we here explore a simplicial complex neural network\n",
            "learning architecture based on random walks and fast 1D convolutions (SCRaWl),\n",
            "in which we can adjust the increase in computational cost by varying the length\n",
            "and number of random walks considered while accounting for higher-order\n",
            "relationships. Importantly, due to the random walk-based design, the\n",
            "expressivity of the proposed architecture is provably incomparable to that of\n",
            "existing message-passing simplicial neural networks. We empirically evaluate\n",
            "SCRaWl on real-world datasets and show that it outperforms other simplicial\n",
            "neural networks.\n",
            "\n",
            "634. Title: Unlocking the Power of Representations in Long-term Novelty-based Exploration\n",
            "   Abstract: Multilevel models (MLMs) are a central building block of the Bayesian\n",
            "workflow. They enable joint, interpretable modeling of data across hierarchical\n",
            "levels and provide a fully probabilistic quantification of uncertainty. Despite\n",
            "their well-recognized advantages, MLMs pose significant computational\n",
            "challenges, often rendering their estimation and evaluation intractable within\n",
            "reasonable time constraints. Recent advances in simulation-based inference\n",
            "offer promising solutions for addressing complex probabilistic models using\n",
            "deep generative networks. However, the utility and reliability of deep learning\n",
            "methods for estimating Bayesian MLMs remains largely unexplored, especially\n",
            "when compared with gold-standard samplers. To this end, we explore a family of\n",
            "neural network architectures that leverage the probabilistic factorization of\n",
            "multilevel models to facilitate efficient neural network training and\n",
            "subsequent near-instant posterior inference on unseen data sets. We test our\n",
            "method on several real-world case studies and provide comprehensive comparisons\n",
            "to Stan as a gold-standard method where possible. Finally, we provide an\n",
            "open-source implementation of our methods to stimulate further research in the\n",
            "nascent field of amortized Bayesian inference.\n",
            "\n",
            "635. Title: TD-MPC2: Scalable, Robust World Models for Continuous Control\n",
            "   Abstract: Deterministic policies are often preferred over stochastic ones when\n",
            "implemented on physical systems. They can prevent erratic and harmful behaviors\n",
            "while being easier to implement and interpret. However, in practice,\n",
            "exploration is largely performed by stochastic policies. First-order Bayesian\n",
            "Optimization (BO) methods offer a principled way of performing exploration\n",
            "using deterministic policies. This is done through a learned probabilistic\n",
            "model of the objective function and its gradient. Nonetheless, such approaches\n",
            "treat policy search as a black-box problem, and thus, neglect the reinforcement\n",
            "learning nature of the problem. In this work, we leverage the performance\n",
            "difference lemma to introduce a novel mean function for the probabilistic\n",
            "model. This results in augmenting BO methods with the action-value function.\n",
            "Hence, we call our method Augmented Bayesian Search~(ABS). Interestingly, this\n",
            "new mean function enhances the posterior gradient with the deterministic policy\n",
            "gradient, effectively bridging the gap between BO and policy gradient methods.\n",
            "The resulting algorithm combines the convenience of the direct policy search\n",
            "with the scalability of reinforcement learning. We validate ABS on\n",
            "high-dimensional locomotion problems and demonstrate competitive performance\n",
            "compared to existing direct policy search schemes.\n",
            "\n",
            "636. Title: H2O-SDF: Two-phase Learning for 3D Indoor Reconstruction using Object Surface Fields\n",
            "   Abstract: The $L_{2}$-regularized loss of Deep Linear Networks (DLNs) with more than\n",
            "one hidden layers has multiple local minima, corresponding to matrices with\n",
            "different ranks. In tasks such as matrix completion, the goal is to converge to\n",
            "the local minimum with the smallest rank that still fits the training data.\n",
            "While rank-underestimating minima can be avoided since they do not fit the\n",
            "data, GD might get stuck at rank-overestimating minima. We show that with SGD,\n",
            "there is always a probability to jump from a higher rank minimum to a lower\n",
            "rank one, but the probability of jumping back is zero. More precisely, we\n",
            "define a sequence of sets $B_{1}\\subset B_{2}\\subset\\cdots\\subset B_{R}$ so\n",
            "that $B_{r}$ contains all minima of rank $r$ or less (and not more) that are\n",
            "absorbing for small enough ridge parameters $\\lambda$ and learning rates\n",
            "$\\eta$: SGD has prob. 0 of leaving $B_{r}$, and from any starting point there\n",
            "is a non-zero prob. for SGD to go in $B_{r}$.\n",
            "\n",
            "637. Title: Safe and Robust Watermark Injection with a Single OoD Image\n",
            "   Abstract: This paper studies generative flow networks (GFlowNets) to sample objects\n",
            "from the Boltzmann energy distribution via a sequence of actions. In\n",
            "particular, we focus on improving GFlowNet with partial inference: training\n",
            "flow functions with the evaluation of the intermediate states or transitions.\n",
            "To this end, the recently developed forward-looking GFlowNet reparameterizes\n",
            "the flow functions based on evaluating the energy of intermediate states.\n",
            "However, such an evaluation of intermediate energies may (i) be too expensive\n",
            "or impossible to evaluate and (ii) even provide misleading training signals\n",
            "under large energy fluctuations along the sequence of actions. To resolve this\n",
            "issue, we propose learning energy decompositions for GFlowNets (LED-GFN). Our\n",
            "main idea is to (i) decompose the energy of an object into learnable potential\n",
            "functions defined on state transitions and (ii) reparameterize the flow\n",
            "functions using the potential functions. In particular, to produce informative\n",
            "local credits, we propose to regularize the potential to change smoothly over\n",
            "the sequence of actions. It is also noteworthy that training GFlowNet with our\n",
            "learned potential can preserve the optimal policy. We empirically verify the\n",
            "superiority of LED-GFN in five problems including the generation of\n",
            "unstructured and maximum independent sets, molecular graphs, and RNA sequences.\n",
            "\n",
            "638. Title: CoVLM: Composing Visual Entities and Relationships in Large Language Models Via Communicative Decoding\n",
            "   Abstract: Training a high-performance deep neural network requires large amounts of\n",
            "data and computational resources. Protecting the intellectual property (IP) and\n",
            "commercial ownership of a deep model is challenging yet increasingly crucial. A\n",
            "major stream of watermarking strategies implants verifiable backdoor triggers\n",
            "by poisoning training samples, but these are often unrealistic due to data\n",
            "privacy and safety concerns and are vulnerable to minor model changes such as\n",
            "fine-tuning. To overcome these challenges, we propose a safe and robust\n",
            "backdoor-based watermark injection technique that leverages the diverse\n",
            "knowledge from a single out-of-distribution (OoD) image, which serves as a\n",
            "secret key for IP verification. The independence of training data makes it\n",
            "agnostic to third-party promises of IP security. We induce robustness via\n",
            "random perturbation of model parameters during watermark injection to defend\n",
            "against common watermark removal attacks, including fine-tuning, pruning, and\n",
            "model extraction. Our experimental results demonstrate that the proposed\n",
            "watermarking approach is not only time- and sample-efficient without training\n",
            "data, but also robust against the watermark removal attacks above.\n",
            "\n",
            "639. Title: Unconstrained Stochastic CCA: Unifying Multiview and Self-Supervised Learning\n",
            "   Abstract: CLIP is a foundational multimodal model that aligns image and text features\n",
            "into a shared space using contrastive learning on large-scale image-text pairs.\n",
            "Its strength lies in leveraging natural language as a rich supervisory signal.\n",
            "With the rapid progress of large language models (LLMs), we explore their\n",
            "potential to further enhance CLIP's multimodal representation learning. This\n",
            "work introduces a fine-tuning approach that integrates LLMs with the pretrained\n",
            "CLIP visual encoder, leveraging LLMs' advanced text understanding and\n",
            "open-world knowledge to improve CLIP's ability to process long and complex\n",
            "captions. To address the challenge of LLMs' autoregressive nature, we propose a\n",
            "caption-to-caption contrastive learning framework to enhance the discriminative\n",
            "power of their outputs. Our method achieves substantial performance gains on\n",
            "various downstream tasks, demonstrating the effectiveness of combining LLMs\n",
            "with CLIP for enhanced multimodal learning.\n",
            "\n",
            "640. Title: Reinforcement Symbolic Regression Machine\n",
            "   Abstract: The Canonical Correlation Analysis (CCA) family of methods is foundational in\n",
            "multiview learning. Regularised linear CCA methods can be seen to generalise\n",
            "Partial Least Squares (PLS) and be unified with a Generalized Eigenvalue\n",
            "Problem (GEP) framework. However, classical algorithms for these linear methods\n",
            "are computationally infeasible for large-scale data. Extensions to Deep CCA\n",
            "show great promise, but current training procedures are slow and complicated.\n",
            "First we propose a novel unconstrained objective that characterizes the top\n",
            "subspace of GEPs. Our core contribution is a family of fast algorithms for\n",
            "stochastic PLS, stochastic CCA, and Deep CCA, simply obtained by applying\n",
            "stochastic gradient descent (SGD) to the corresponding CCA objectives. Our\n",
            "algorithms show far faster convergence and recover higher correlations than the\n",
            "previous state-of-the-art on all standard CCA and Deep CCA benchmarks. These\n",
            "improvements allow us to perform a first-of-its-kind PLS analysis of an\n",
            "extremely large biomedical dataset from the UK Biobank, with over 33,000\n",
            "individuals and 500,000 features. Finally, we apply our algorithms to match the\n",
            "performance of `CCA-family' Self-Supervised Learning (SSL) methods on CIFAR-10\n",
            "and CIFAR-100 with minimal hyper-parameter tuning, and also present theory to\n",
            "clarify the links between these methods and classical CCA, laying the\n",
            "groundwork for future insights.\n",
            "\n",
            "641. Title: EasyTPP: Towards Open Benchmarking Temporal Point Processes\n",
            "   Abstract: In nature, the behaviors of many complex systems can be described by\n",
            "parsimonious math equations. Automatically distilling these equations from\n",
            "limited data is cast as a symbolic regression process which hitherto remains a\n",
            "grand challenge. Keen efforts in recent years have been placed on tackling this\n",
            "issue and demonstrated success in symbolic regression. However, there still\n",
            "exist bottlenecks that current methods struggle to break when the discrete\n",
            "search space tends toward infinity and especially when the underlying math\n",
            "formula is intricate. To this end, we propose a novel Reinforcement Symbolic\n",
            "Regression Machine (RSRM) that masters the capability of uncovering complex\n",
            "math equations from only scarce data. The RSRM model is composed of three key\n",
            "modules: (1) a Monte Carlo tree search (MCTS) agent that explores optimal math\n",
            "expression trees consisting of pre-defined math operators and variables, (2) a\n",
            "Double Q-learning block that helps reduce the feasible search space of MCTS via\n",
            "properly understanding the distribution of reward, and (3) a modulated sub-tree\n",
            "discovery block that heuristically learns and defines new math operators to\n",
            "improve representation ability of math expression trees. Biding of these\n",
            "modules yields the state-of-the-art performance of RSRM in symbolic regression\n",
            "as demonstrated by multiple sets of benchmark examples. The RSRM model shows\n",
            "clear superiority over several representative baseline models.\n",
            "\n",
            "642. Title: ACRF: Compressing Explicit Neural Radiance Fields via Attribute Compression\n",
            "   Abstract: Knowledge Distillation (KD) aims at improving the performance of a\n",
            "low-capacity student model by inheriting knowledge from a high-capacity teacher\n",
            "model. Previous KD methods typically train a student by minimizing a\n",
            "task-related loss and the KD loss simultaneously, using a pre-defined loss\n",
            "weight to balance these two terms. In this work, we propose to first transfer\n",
            "the backbone knowledge from a teacher to the student, and then only learn the\n",
            "task-head of the student network. Such a decomposition of the training process\n",
            "circumvents the need of choosing an appropriate loss weight, which is often\n",
            "difficult in practice, and thus makes it easier to apply to different datasets\n",
            "and tasks. Importantly, the decomposition permits the core of our method,\n",
            "Stage-by-Stage Knowledge Distillation (SSKD), which facilitates progressive\n",
            "feature mimicking from teacher to student. Extensive experiments on CIFAR-100\n",
            "and ImageNet suggest that SSKD significantly narrows down the performance gap\n",
            "between student and teacher, outperforming state-of-the-art approaches. We also\n",
            "demonstrate the generalization ability of SSKD on other challenging benchmarks,\n",
            "including face recognition on IJB-A dataset as well as object detection on COCO\n",
            "dataset.\n",
            "\n",
            "643. Title: Transport meets Variational Inference: Controlled Monte Carlo Diffusions\n",
            "   Abstract: Existing vision-language models exhibit strong generalization on a variety of\n",
            "visual domains and tasks. However, such models mainly perform zero-shot\n",
            "recognition in a closed-set manner, and thus struggle to handle open-domain\n",
            "visual concepts by design. There are recent finetuning methods, such as prompt\n",
            "learning, that not only study the discrimination between in-distribution (ID)\n",
            "and out-of-distribution (OOD) samples, but also show some improvements in both\n",
            "ID and OOD accuracies. In this paper, we first demonstrate that vision-language\n",
            "models, after long enough finetuning but without proper regularization, tend to\n",
            "overfit the known classes in the given dataset, with degraded performance on\n",
            "unknown classes. Then we propose a novel approach OGEN to address this pitfall,\n",
            "with the main focus on improving the OOD GENeralization of finetuned models.\n",
            "Specifically, a class-conditional feature generator is introduced to synthesize\n",
            "OOD features using just the class name of any unknown class. Such synthesized\n",
            "features will provide useful knowledge about unknowns and help regularize the\n",
            "decision boundary between ID and OOD data when optimized jointly. Equally\n",
            "important is our adaptive self-distillation mechanism to regularize our feature\n",
            "generation model during joint optimization, i.e., adaptively transferring\n",
            "knowledge between model states to further prevent overfitting. Experiments\n",
            "validate that our method yields convincing gains in OOD generalization\n",
            "performance in different settings. Code: https://github.com/apple/ml-ogen.\n",
            "\n",
            "644. Title: Decentralized Riemannian Conjugate Gradient Method on the Stiefel Manifold\n",
            "   Abstract: Connecting optimal transport and variational inference, we present a\n",
            "principled and systematic framework for sampling and generative modelling\n",
            "centred around divergences on path space. Our work culminates in the\n",
            "development of the \\emph{Controlled Monte Carlo Diffusion} sampler (CMCD) for\n",
            "Bayesian computation, a score-based annealing technique that crucially adapts\n",
            "both forward and backward dynamics in a diffusion model. On the way, we clarify\n",
            "the relationship between the EM-algorithm and iterative proportional fitting\n",
            "(IPF) for Schr{\\\"o}dinger bridges, deriving as well a regularised objective\n",
            "that bypasses the iterative bottleneck of standard IPF-updates. Finally, we\n",
            "show that CMCD has a strong foundation in the Jarzinsky and Crooks identities\n",
            "from statistical physics, and that it convincingly outperforms competing\n",
            "approaches across a wide array of experiments.\n",
            "\n",
            "645. Title: MixSATGEN: Learning Graph Mixing for SAT Instance Generation\n",
            "   Abstract: The conjugate gradient method is a crucial first-order optimization method\n",
            "that generally converges faster than the steepest descent method, and its\n",
            "computational cost is much lower than that of second-order methods. However,\n",
            "while various types of conjugate gradient methods have been studied in\n",
            "Euclidean spaces and on Riemannian manifolds, there is little study for those\n",
            "in distributed scenarios. This paper proposes a decentralized Riemannian\n",
            "conjugate gradient descent (DRCGD) method that aims at minimizing a global\n",
            "function over the Stiefel manifold. The optimization problem is distributed\n",
            "among a network of agents, where each agent is associated with a local\n",
            "function, and the communication between agents occurs over an undirected\n",
            "connected graph. Since the Stiefel manifold is a non-convex set, a global\n",
            "function is represented as a finite sum of possibly non-convex (but smooth)\n",
            "local functions. The proposed method is free from expensive Riemannian\n",
            "geometric operations such as retractions, exponential maps, and vector\n",
            "transports, thereby reducing the computational complexity required by each\n",
            "agent. To the best of our knowledge, DRCGD is the first decentralized\n",
            "Riemannian conjugate gradient algorithm to achieve global convergence over the\n",
            "Stiefel manifold.\n",
            "\n",
            "646. Title: Synapse: Trajectory-as-Exemplar Prompting with Memory for Computer Control\n",
            "   Abstract: Medical Vision-Language Pre-training (VLP) learns representations jointly\n",
            "from medical images and paired radiology reports. It typically requires\n",
            "large-scale paired image-text datasets to achieve effective pre-training for\n",
            "both the image encoder and text encoder. The advent of text-guided generative\n",
            "models raises a compelling question: Can VLP be implemented solely with\n",
            "synthetic images generated from genuine radiology reports, thereby mitigating\n",
            "the need for extensively pairing and curating image-text datasets? In this\n",
            "work, we scrutinize this very question by examining the feasibility and\n",
            "effectiveness of employing synthetic images for medical VLP. We replace real\n",
            "medical images with their synthetic equivalents, generated from authentic\n",
            "medical reports. Utilizing three state-of-the-art VLP algorithms, we\n",
            "exclusively train on these synthetic samples. Our empirical evaluation across\n",
            "three subsequent tasks, namely image classification, semantic segmentation and\n",
            "object detection, reveals that the performance achieved through synthetic data\n",
            "is on par with or even exceeds that obtained with real images. As a pioneering\n",
            "contribution to this domain, we introduce a large-scale synthetic medical image\n",
            "dataset, paired with anonymized real radiology reports. This alleviates the\n",
            "need of sharing medical images, which are not easy to curate and share in\n",
            "practice. The code and the dataset can be found in\n",
            "\\href{https://github.com/cheliu-computation/MedSyn-RepLearn/tree/main}{https://github.com/cheliu-computation/MedSyn-RepLearn/tree/main}.\n",
            "\n",
            "647. Title: Pre-training with Synthetic Data Helps Offline Reinforcement Learning\n",
            "   Abstract: Reinforcement Learning algorithms that learn from human feedback (RLHF) need\n",
            "to be efficient in terms of statistical complexity, computational complexity,\n",
            "and query complexity. In this work, we consider the RLHF setting where the\n",
            "feedback is given in the format of preferences over pairs of trajectories. In\n",
            "the linear MDP model, using randomization in algorithm design, we present an\n",
            "algorithm that is sample efficient (i.e., has near-optimal worst-case regret\n",
            "bounds) and has polynomial running time (i.e., computational complexity is\n",
            "polynomial with respect to relevant parameters). Our algorithm further\n",
            "minimizes the query complexity through a novel randomized active learning\n",
            "procedure. In particular, our algorithm demonstrates a near-optimal tradeoff\n",
            "between the regret bound and the query complexity. To extend the results to\n",
            "more general nonlinear function approximation, we design a model-based\n",
            "randomized algorithm inspired by the idea of Thompson sampling. Our algorithm\n",
            "minimizes Bayesian regret bound and query complexity, again achieving a\n",
            "near-optimal tradeoff between these two quantities. Computation-wise, similar\n",
            "to the prior Thompson sampling algorithms under the regular RL setting, the\n",
            "main computation primitives of our algorithm are Bayesian supervised learning\n",
            "oracles which have been heavily investigated on the empirical side when\n",
            "applying Thompson sampling algorithms to RL benchmark problems.\n",
            "\n",
            "648. Title: CrossQ: Batch Normalization in Deep Reinforcement Learning for Greater Sample Efficiency and Simplicity\n",
            "   Abstract: Training AI models that generalize across tasks and domains has long been\n",
            "among the open problems driving AI research. The emergence of Foundation Models\n",
            "made it easier to obtain expert models for a given task, but the heterogeneity\n",
            "of data that may be encountered at test time often means that any single expert\n",
            "is insufficient. We consider the Fusion of Experts (FoE) problem of fusing\n",
            "outputs of expert models with complementary knowledge of the data distribution\n",
            "and formulate it as an instance of supervised learning. Our method is\n",
            "applicable to both discriminative and generative tasks and leads to significant\n",
            "performance improvements in image and text classification, text summarization,\n",
            "multiple-choice QA, and automatic evaluation of generated text. We also extend\n",
            "our method to the \"frugal\" setting where it is desired to reduce the number of\n",
            "expert model evaluations at test time. Our implementation is publicly available\n",
            "at https://github.com/hwang595/FoE-ICLR2024.\n",
            "\n",
            "649. Title: Never Train from Scratch: Fair Comparison of Long-Sequence Models Requires Data-Driven Priors\n",
            "   Abstract: Training a task-completion dialogue agent via reinforcement learning (RL) is\n",
            "costly because it requires many interactions with real users. One common\n",
            "alternative is to use a user simulator. However, a user simulator usually lacks\n",
            "the language complexity of human interlocutors and the biases in its design may\n",
            "tend to degrade the agent. To address these issues, we present Deep Dyna-Q,\n",
            "which to our knowledge is the first deep RL framework that integrates planning\n",
            "for task-completion dialogue policy learning. We incorporate into the dialogue\n",
            "agent a model of the environment, referred to as the world model, to mimic real\n",
            "user response and generate simulated experience. During dialogue policy\n",
            "learning, the world model is constantly updated with real user experience to\n",
            "approach real user behavior, and in turn, the dialogue agent is optimized using\n",
            "both real experience and simulated experience. The effectiveness of our\n",
            "approach is demonstrated on a movie-ticket booking task in both simulated and\n",
            "human-in-the-loop settings.\n",
            "\n",
            "650. Title: From Molecules to Materials: Pre-training Large Generalizable Models for Atomic Property Prediction\n",
            "   Abstract: Counterfactual data augmentation (CDA) -- i.e., adding minimally perturbed\n",
            "inputs during training -- helps reduce model reliance on spurious correlations\n",
            "and improves generalization to out-of-distribution (OOD) data. Prior work on\n",
            "generating counterfactuals only considered restricted classes of perturbations,\n",
            "limiting their effectiveness. We present COunterfactual Generation via\n",
            "Retrieval and Editing (CORE), a retrieval-augmented generation framework for\n",
            "creating diverse counterfactual perturbations for CDA. For each training\n",
            "example, CORE first performs a dense retrieval over a task-related unlabeled\n",
            "text corpus using a learned bi-encoder and extracts relevant counterfactual\n",
            "excerpts. CORE then incorporates these into prompts to a large language model\n",
            "with few-shot learning capabilities, for counterfactual editing. Conditioning\n",
            "language model edits on naturally occurring data results in diverse\n",
            "perturbations. Experiments on natural language inference and sentiment analysis\n",
            "benchmarks show that CORE counterfactuals are more effective at improving\n",
            "generalization to OOD data compared to other DA approaches. We also show that\n",
            "the CORE retrieval framework can be used to encourage diversity in manually\n",
            "authored perturbations\n",
            "\n",
            "651. Title: Fusing Models with Complementary Expertise\n",
            "   Abstract: Foundation models have been transformational in machine learning fields such\n",
            "as natural language processing and computer vision. Similar success in atomic\n",
            "property prediction has been limited due to the challenges of training\n",
            "effective models across multiple chemical domains. To address this, we\n",
            "introduce Joint Multi-domain Pre-training (JMP), a supervised pre-training\n",
            "strategy that simultaneously trains on multiple datasets from different\n",
            "chemical domains, treating each dataset as a unique pre-training task within a\n",
            "multi-task framework. Our combined training dataset consists of $\\sim$120M\n",
            "systems from OC20, OC22, ANI-1x, and Transition-1x. We evaluate performance and\n",
            "generalization by fine-tuning over a diverse set of downstream tasks and\n",
            "datasets including: QM9, rMD17, MatBench, QMOF, SPICE, and MD22. JMP\n",
            "demonstrates an average improvement of 59% over training from scratch, and\n",
            "matches or sets state-of-the-art on 34 out of 40 tasks. Our work highlights the\n",
            "potential of pre-training strategies that utilize diverse data to advance\n",
            "property prediction across chemical domains, especially for low-data tasks.\n",
            "Please visit https://nima.sh/jmp for further information.\n",
            "\n",
            "652. Title: Deep Geodesic Canonical Correlation Analysis for Covariance-Based Neuroimaging Data\n",
            "   Abstract: We examine Deep Canonically Correlated LSTMs as a way to learn nonlinear\n",
            "transformations of variable length sequences and embed them into a correlated,\n",
            "fixed dimensional space. We use LSTMs to transform multi-view time-series data\n",
            "non-linearly while learning temporal relationships within the data. We then\n",
            "perform correlation analysis on the outputs of these neural networks to find a\n",
            "correlated subspace through which we get our final representation via\n",
            "projection. This work follows from previous work done on Deep Canonical\n",
            "Correlation (DCCA), in which deep feed-forward neural networks were used to\n",
            "learn nonlinear transformations of data while maximizing correlation.\n",
            "\n",
            "653. Title: A Linear Algebraic Framework for Counterfactual Generation\n",
            "   Abstract: Modeling long-range dependencies across sequences is a longstanding goal in\n",
            "machine learning and has led to architectures, such as state space models, that\n",
            "dramatically outperform Transformers on long sequences. However, these\n",
            "impressive empirical gains have been by and large demonstrated on benchmarks\n",
            "(e.g. Long Range Arena), where models are randomly initialized and trained to\n",
            "predict a target label from an input sequence. In this work, we show that\n",
            "random initialization leads to gross overestimation of the differences between\n",
            "architectures and that pretraining with standard denoising objectives, using\n",
            "$\\textit{only the downstream task data}$, leads to dramatic gains across\n",
            "multiple architectures and to very small gaps between Transformers and state\n",
            "space models (SSMs). In stark contrast to prior works, we find vanilla\n",
            "Transformers to match the performance of S4 on Long Range Arena when properly\n",
            "pretrained, and we improve the best reported results of SSMs on the PathX-256\n",
            "task by 20 absolute points. Subsequently, we analyze the utility of\n",
            "previously-proposed structured parameterizations for SSMs and show they become\n",
            "mostly redundant in the presence of data-driven initialization obtained through\n",
            "pretraining. Our work shows that, when evaluating different architectures on\n",
            "supervised tasks, incorporation of data-driven priors via pretraining is\n",
            "essential for reliable performance estimation, and can be done efficiently.\n",
            "\n",
            "654. Title: Learning to Solve Bilevel Programs with Binary Tender\n",
            "   Abstract: Bilevel programs (BPs) find a wide range of applications in fields such as\n",
            "energy, transportation, and machine learning. As compared to BPs with\n",
            "continuous (linear/convex) optimization problems in both levels, the BPs with\n",
            "discrete decision variables have received much less attention, largely due to\n",
            "the ensuing computational intractability and the incapability of gradient-based\n",
            "algorithms for handling discrete optimization formulations. In this paper, we\n",
            "develop deep learning techniques to address this challenge. Specifically, we\n",
            "consider a BP with binary tender, wherein the upper and lower levels are linked\n",
            "via binary variables. We train a neural network to approximate the optimal\n",
            "value of the lower-level problem, as a function of the binary tender. Then, we\n",
            "obtain a single-level reformulation of the BP through a mixed-integer\n",
            "representation of the value function. Furthermore, we conduct a comparative\n",
            "analysis between two types of neural networks: general neural networks and the\n",
            "novel input supermodular neural networks, studying their representational\n",
            "capacities. To solve high-dimensional BPs, we introduce an enhanced sampling\n",
            "method to generate higher-quality samples and implement an iterative process to\n",
            "refine solutions. We demonstrate the performance of these approaches through\n",
            "extensive numerical experiments, whose lower-level problems are linear and\n",
            "mixed-integer programs, respectively.\n",
            "\n",
            "655. Title: Geographic Location Encoding with Spherical Harmonics and Sinusoidal Representation Networks\n",
            "   Abstract: Large language models (LLMs) fine-tuned with reinforcement learning from\n",
            "human feedback (RLHF) have been used in some of the most widely deployed AI\n",
            "models to date, such as OpenAI's ChatGPT or Anthropic's Claude. While there has\n",
            "been significant work developing these methods, our understanding of the\n",
            "benefits and downsides of each stage in RLHF is still limited. To fill this\n",
            "gap, we present an extensive analysis of how each stage of the process (i.e.\n",
            "supervised fine-tuning (SFT), reward modelling, and RLHF) affects two key\n",
            "properties: out-of-distribution (OOD) generalisation and output diversity. OOD\n",
            "generalisation is crucial given the wide range of real-world scenarios in which\n",
            "these models are being used, while output diversity refers to the model's\n",
            "ability to generate varied outputs and is important for a variety of use cases.\n",
            "We perform our analysis across two base models on both summarisation and\n",
            "instruction following tasks, the latter being highly relevant for current LLM\n",
            "use cases. We find that RLHF generalises better than SFT to new inputs,\n",
            "particularly as the distribution shift between train and test becomes larger.\n",
            "However, RLHF significantly reduces output diversity compared to SFT across a\n",
            "variety of measures, implying a tradeoff in current LLM fine-tuning methods\n",
            "between generalisation and diversity. Our results provide guidance on which\n",
            "fine-tuning method should be used depending on the application, and show that\n",
            "more research is needed to improve the tradeoff between generalisation and\n",
            "diversity.\n",
            "\n",
            "656. Title: A Simple and Effective Pruning Approach for Large Language Models\n",
            "   Abstract: Modern learning frameworks often train deep neural networks with massive\n",
            "amounts of unlabeled data to learn representations by solving simple pretext\n",
            "tasks, then use the representations as foundations for downstream tasks. These\n",
            "networks are empirically designed; as such, they are usually not interpretable,\n",
            "their representations are not structured, and their designs are potentially\n",
            "redundant. White-box deep networks, in which each layer explicitly identifies\n",
            "and transforms structures in the data, present a promising alternative.\n",
            "However, existing white-box architectures have only been shown to work at scale\n",
            "in supervised settings with labeled data, such as classification. In this work,\n",
            "we provide the first instantiation of the white-box design paradigm that can be\n",
            "applied to large-scale unsupervised representation learning. We do this by\n",
            "exploiting a fundamental connection between diffusion, compression, and\n",
            "(masked) completion, deriving a deep transformer-like masked autoencoder\n",
            "architecture, called CRATE-MAE, in which the role of each layer is\n",
            "mathematically fully interpretable: they transform the data distribution to and\n",
            "from a structured representation. Extensive empirical evaluations confirm our\n",
            "analytical insights. CRATE-MAE demonstrates highly promising performance on\n",
            "large-scale imagery datasets while using only ~30% of the parameters compared\n",
            "to the standard masked autoencoder with the same model configuration. The\n",
            "representations learned by CRATE-MAE have explicit structure and also contain\n",
            "semantic meaning. Code is available at https://github.com/Ma-Lab-Berkeley/CRATE .\n",
            "\n",
            "657. Title: SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression\n",
            "   Abstract: Learning representations of geographical space is vital for any machine\n",
            "learning model that integrates geolocated data, spanning application domains\n",
            "such as remote sensing, ecology, or epidemiology. Recent work embeds\n",
            "coordinates using sine and cosine projections based on Double Fourier Sphere\n",
            "(DFS) features. These embeddings assume a rectangular data domain even on\n",
            "global data, which can lead to artifacts, especially at the poles. At the same\n",
            "time, little attention has been paid to the exact design of the neural network\n",
            "architectures with which these functional embeddings are combined. This work\n",
            "proposes a novel location encoder for globally distributed geographic data that\n",
            "combines spherical harmonic basis functions, natively defined on spherical\n",
            "surfaces, with sinusoidal representation networks (SirenNets) that can be\n",
            "interpreted as learned Double Fourier Sphere embedding. We systematically\n",
            "evaluate positional embeddings and neural network architectures across various\n",
            "benchmarks and synthetic evaluation datasets. In contrast to previous\n",
            "approaches that require the combination of both positional encoding and neural\n",
            "networks to learn meaningful representations, we show that both spherical\n",
            "harmonics and sinusoidal representation networks are competitive on their own\n",
            "but set state-of-the-art performances across tasks when combined. The model\n",
            "code and experiments are available at\n",
            "https://github.com/marccoru/locationencoder.\n",
            "\n",
            "658. Title: MixSup: Mixed-grained Supervision for Label-efficient LiDAR-based 3D Object Detection\n",
            "   Abstract: While neural networks can be approximated by linear models as their width\n",
            "increases, certain properties of wide neural networks cannot be captured by\n",
            "linear models. In this work we show that recently proposed Neural Quadratic\n",
            "Models can exhibit the \"catapult phase\" [Lewkowycz et al. 2020] that arises\n",
            "when training such models with large learning rates. We then empirically show\n",
            "that the behaviour of neural quadratic models parallels that of neural networks\n",
            "in generalization, especially in the catapult phase regime. Our analysis\n",
            "further demonstrates that quadratic models can be an effective tool for\n",
            "analysis of neural networks.\n",
            "\n",
            "659. Title: Adaptive Sharpness-Aware Pruning for Robust Sparse Networks\n",
            "   Abstract: In this work, we propose a Multi-Window Masked Autoencoder (MW-MAE) fitted\n",
            "with a novel Multi-Window Multi-Head Attention (MW-MHA) module that facilitates\n",
            "the modelling of local-global interactions in every decoder transformer block\n",
            "through attention heads of several distinct local and global windows. Empirical\n",
            "results on ten downstream audio tasks show that MW-MAEs consistently outperform\n",
            "standard MAEs in overall performance and learn better general-purpose audio\n",
            "representations, along with demonstrating considerably better scaling\n",
            "characteristics. Investigating attention distances and entropies reveals that\n",
            "MW-MAE encoders learn heads with broader local and global attention. Analyzing\n",
            "attention head feature representations through Projection Weighted Canonical\n",
            "Correlation Analysis (PWCCA) shows that attention heads with the same window\n",
            "sizes across the decoder layers of the MW-MAE learn correlated feature\n",
            "representations which enables each block to independently capture local and\n",
            "global information, leading to a decoupled decoder feature hierarchy. Code for\n",
            "feature extraction and downstream experiments along with pre-trained models\n",
            "will be released publically.\n",
            "\n",
            "660. Title: Guaranteed Approximation Bounds for Mixed-Precision Neural Operators\n",
            "   Abstract: As their size increases, Large Languages Models (LLMs) are natural candidates\n",
            "for network pruning methods: approaches that drop a subset of network weights\n",
            "while striving to preserve performance. Existing methods, however, require\n",
            "either retraining, which is rarely affordable for billion-scale LLMs, or\n",
            "solving a weight reconstruction problem reliant on second-order information,\n",
            "which may also be computationally expensive. In this paper, we introduce a\n",
            "novel, straightforward yet effective pruning method, termed Wanda (Pruning by\n",
            "Weights and activations), designed to induce sparsity in pretrained LLMs.\n",
            "Motivated by the recent observation of emergent large magnitude features in\n",
            "LLMs, our approach prunes weights with the smallest magnitudes multiplied by\n",
            "the corresponding input activations, on a per-output basis. Notably, Wanda\n",
            "requires no retraining or weight update, and the pruned LLM can be used as is.\n",
            "We conduct a thorough evaluation of our method Wanda on LLaMA and LLaMA-2\n",
            "across various language benchmarks. Wanda significantly outperforms the\n",
            "established baseline of magnitude pruning and performs competitively against\n",
            "recent method involving intensive weight update. Code is available at\n",
            "https://github.com/locuslab/wanda.\n",
            "\n",
            "661. Title: Decodable and Sample Invariant Continuous Object Encoder\n",
            "   Abstract: Neural operators, such as Fourier Neural Operators (FNO), form a principled\n",
            "approach for learning solution operators for PDEs and other mappings between\n",
            "function spaces. However, many real-world problems require high-resolution\n",
            "training data, and the training time and limited GPU memory pose big barriers.\n",
            "One solution is to train neural operators in mixed precision to reduce the\n",
            "memory requirement and increase training speed. However, existing\n",
            "mixed-precision training techniques are designed for standard neural networks,\n",
            "and we find that their direct application to FNO leads to numerical overflow\n",
            "and poor memory efficiency. Further, at first glance, it may appear that mixed\n",
            "precision in FNO will lead to drastic accuracy degradation since reducing the\n",
            "precision of the Fourier transform yields poor results in classical numerical\n",
            "solvers. We show that this is not the case; in fact, we prove that reducing the\n",
            "precision in FNO still guarantees a good approximation bound, when done in a\n",
            "targeted manner. Specifically, we build on the intuition that neural operator\n",
            "learning inherently induces an approximation error, arising from discretizing\n",
            "the infinite-dimensional ground-truth input function, implying that training in\n",
            "full precision is not needed. We formalize this intuition by rigorously\n",
            "characterizing the approximation and precision errors of FNO and bounding these\n",
            "errors for general input functions. We prove that the precision error is\n",
            "asymptotically comparable to the approximation error. Based on this, we design\n",
            "a simple method to optimize the memory-intensive half-precision tensor\n",
            "contractions by greedily finding the optimal contraction order. Through\n",
            "extensive experiments on different state-of-the-art neural operators, datasets,\n",
            "and GPUs, we demonstrate that our approach reduces GPU memory usage by up to\n",
            "50% and improves throughput by 58% with little or no reduction in accuracy.\n",
            "\n",
            "662. Title: MovingParts: Motion-based 3D Part Discovery in Dynamic Radiance Field\n",
            "   Abstract: Robustness and compactness are two essential attributes of deep learning\n",
            "models that are deployed in the real world. The goals of robustness and\n",
            "compactness may seem to be at odds, since robustness requires generalization\n",
            "across domains, while the process of compression exploits specificity in one\n",
            "domain. We introduce Adaptive Sharpness-Aware Pruning (AdaSAP), which unifies\n",
            "these goals through the lens of network sharpness. The AdaSAP method produces\n",
            "sparse networks that are robust to input variations which are unseen at\n",
            "training time. We achieve this by strategically incorporating weight\n",
            "perturbations in order to optimize the loss landscape. This allows the model to\n",
            "be both primed for pruning and regularized for improved robustness. AdaSAP\n",
            "improves the robust accuracy of pruned models on image classification by up to\n",
            "+6% on ImageNet C and +4% on ImageNet V2, and on object detection by +4% on a\n",
            "corrupted Pascal VOC dataset, over a wide range of compression ratios, pruning\n",
            "criteria, and network architectures, outperforming recent pruning art by large\n",
            "margins.\n",
            "\n",
            "663. Title: MOFI: Learning Image Representations from Noisy Entity Annotated Images\n",
            "   Abstract: We propose Hyper-Dimensional Function Encoding (HDFE). Given samples of a\n",
            "continuous object (e.g. a function), HDFE produces an explicit vector\n",
            "representation of the given object, invariant to the sample distribution and\n",
            "density. Sample distribution and density invariance enables HDFE to\n",
            "consistently encode continuous objects regardless of their sampling, and\n",
            "therefore allows neural networks to receive continuous objects as inputs for\n",
            "machine learning tasks, such as classification and regression. Besides, HDFE\n",
            "does not require any training and is proved to map the object into an organized\n",
            "embedding space, which facilitates the training of the downstream tasks. In\n",
            "addition, the encoding is decodable, which enables neural networks to regress\n",
            "continuous objects by regressing their encodings. Therefore, HDFE serves as an\n",
            "interface for processing continuous objects.\n",
            "  We apply HDFE to function-to-function mapping, where vanilla HDFE achieves\n",
            "competitive performance as the state-of-the-art algorithm. We apply HDFE to\n",
            "point cloud surface normal estimation, where a simple replacement from PointNet\n",
            "to HDFE leads to immediate 12% and 15% error reductions in two benchmarks. In\n",
            "addition, by integrating HDFE into the PointNet-based SOTA network, we improve\n",
            "the SOTA baseline by 2.5% and 1.7% in the same benchmarks.\n",
            "\n",
            "664. Title: OpenTab: Advancing Large Language Models as Open-domain Table Reasoners\n",
            "   Abstract: Deep neural networks are typically trained using global error signals that\n",
            "backpropagate (BP) end-to-end, which is not only biologically implausible but\n",
            "also suffers from the update locking problem and requires huge memory\n",
            "consumption. Local learning, which updates each layer independently with a\n",
            "gradient-isolated auxiliary network, offers a promising alternative to address\n",
            "the above problems. However, existing local learning methods are confronted\n",
            "with a large accuracy gap with the BP counterpart, particularly for large-scale\n",
            "networks. This is due to the weak coupling between local layers and their\n",
            "subsequent network layers, as there is no gradient communication across layers.\n",
            "To tackle this issue, we put forward an augmented local learning method, dubbed\n",
            "AugLocal. AugLocal constructs each hidden layer's auxiliary network by\n",
            "uniformly selecting a small subset of layers from its subsequent network layers\n",
            "to enhance their synergy. We also propose to linearly reduce the depth of\n",
            "auxiliary networks as the hidden layer goes deeper, ensuring sufficient network\n",
            "capacity while reducing the computational cost of auxiliary networks. Our\n",
            "extensive experiments on four image classification datasets (i.e., CIFAR-10,\n",
            "SVHN, STL-10, and ImageNet) demonstrate that AugLocal can effectively scale up\n",
            "to tens of local layers with a comparable accuracy to BP-trained networks while\n",
            "reducing GPU memory usage by around 40%. The proposed AugLocal method,\n",
            "therefore, opens up a myriad of opportunities for training high-performance\n",
            "deep neural networks on resource-constrained platforms.Code is available at\n",
            "https://github.com/ChenxiangMA/AugLocal.\n",
            "\n",
            "665. Title: Scaling Supervised Local Learning with Augmented Auxiliary Networks\n",
            "   Abstract: To deduce new facts on a knowledge graph (KG), a link predictor learns from\n",
            "the graph structure and collects local evidence to find the answer to a given\n",
            "query. However, existing methods suffer from a severe scalability problem due\n",
            "to the utilization of the whole KG for prediction, which hinders their promise\n",
            "on large scale KGs and cannot be directly addressed by vanilla sampling\n",
            "methods. In this work, we propose the one-shot-subgraph link prediction to\n",
            "achieve efficient and adaptive prediction. The design principle is that,\n",
            "instead of directly acting on the whole KG, the prediction procedure is\n",
            "decoupled into two steps, i.e., (i) extracting only one subgraph according to\n",
            "the query and (ii) predicting on this single, query dependent subgraph. We\n",
            "reveal that the non-parametric and computation-efficient heuristics\n",
            "Personalized PageRank (PPR) can effectively identify the potential answers and\n",
            "supporting evidence. With efficient subgraph-based prediction, we further\n",
            "introduce the automated searching of the optimal configurations in both data\n",
            "and model spaces. Empirically, we achieve promoted efficiency and leading\n",
            "performances on five large-scale benchmarks. The code is publicly available at:\n",
            "https://github.com/tmlr-group/one-shot-subgraph.\n",
            "\n",
            "666. Title: Scalable and Effective Implicit Graph Neural Networks on Large Graphs\n",
            "   Abstract: The redundancy of Convolutional neural networks not only depends on weights\n",
            "but also depends on inputs. Shuffling is an efficient operation for mixing\n",
            "channel information but the shuffle order is usually pre-defined. To reduce the\n",
            "data-dependent redundancy, we devise a dynamic shuffle module to generate\n",
            "data-dependent permutation matrices for shuffling. Since the dimension of\n",
            "permutation matrix is proportional to the square of the number of input\n",
            "channels, to make the generation process efficiently, we divide the channels\n",
            "into groups and generate two shared small permutation matrices for each group,\n",
            "and utilize Kronecker product and cross group shuffle to obtain the final\n",
            "permutation matrices. To make the generation process learnable, based on\n",
            "theoretical analysis, softmax, orthogonal regularization, and binarization are\n",
            "employed to asymptotically approximate the permutation matrix. Dynamic shuffle\n",
            "adaptively mixes channel information with negligible extra computation and\n",
            "memory occupancy. Experiment results on image classification benchmark datasets\n",
            "CIFAR-10, CIFAR-100, Tiny ImageNet and ImageNet have shown that our method\n",
            "significantly increases ShuffleNets' performance. Adding dynamic generated\n",
            "matrix with learnable static matrix, we further propose static-dynamic-shuffle\n",
            "and show that it can serve as a lightweight replacement of ordinary pointwise\n",
            "convolution.\n",
            "\n",
            "667. Title: Learning Hierarchical Polynomials with Three-Layer Neural Networks\n",
            "   Abstract: We study the problem of learning hierarchical polynomials over the standard\n",
            "Gaussian distribution with three-layer neural networks. We specifically\n",
            "consider target functions of the form $h = g \\circ p$ where $p : \\mathbb{R}^d\n",
            "\\rightarrow \\mathbb{R}$ is a degree $k$ polynomial and $g: \\mathbb{R}\n",
            "\\rightarrow \\mathbb{R}$ is a degree $q$ polynomial. This function class\n",
            "generalizes the single-index model, which corresponds to $k=1$, and is a\n",
            "natural class of functions possessing an underlying hierarchical structure. Our\n",
            "main result shows that for a large subclass of degree $k$ polynomials $p$, a\n",
            "three-layer neural network trained via layerwise gradient descent on the square\n",
            "loss learns the target $h$ up to vanishing test error in\n",
            "$\\widetilde{\\mathcal{O}}(d^k)$ samples and polynomial time. This is a strict\n",
            "improvement over kernel methods, which require $\\widetilde \\Theta(d^{kq})$\n",
            "samples, as well as existing guarantees for two-layer networks, which require\n",
            "the target function to be low-rank. Our result also generalizes prior works on\n",
            "three-layer neural networks, which were restricted to the case of $p$ being a\n",
            "quadratic. When $p$ is indeed a quadratic, we achieve the\n",
            "information-theoretically optimal sample complexity\n",
            "$\\widetilde{\\mathcal{O}}(d^2)$, which is an improvement over prior\n",
            "work~\\citep{nichani2023provable} requiring a sample size of\n",
            "$\\widetilde\\Theta(d^4)$. Our proof proceeds by showing that during the initial\n",
            "stage of training the network performs feature learning to recover the feature\n",
            "$p$ with $\\widetilde{\\mathcal{O}}(d^k)$ samples. This work demonstrates the\n",
            "ability of three-layer neural networks to learn complex features and as a\n",
            "result, learn a broad class of hierarchical functions.\n",
            "\n",
            "668. Title: Efficient Heterogeneous Meta-Learning via Channel Shuffling Modulation\n",
            "   Abstract: Graphons are continuous models that represent the structure of graphs and\n",
            "allow the generation of graphs of varying sizes. We propose Scalable Implicit\n",
            "Graphon Learning (SIGL), a scalable method that combines implicit neural\n",
            "representations (INRs) and graph neural networks (GNNs) to estimate a graphon\n",
            "from observed graphs. Unlike existing methods, which face important limitations\n",
            "like fixed resolution and scalability issues, SIGL learns a continuous graphon\n",
            "at arbitrary resolutions. GNNs are used to determine the correct node ordering,\n",
            "improving graph alignment. Furthermore, we characterize the asymptotic\n",
            "consistency of our estimator, showing that more expressive INRs and GNNs lead\n",
            "to consistent estimators. We evaluate SIGL in synthetic and real-world graphs,\n",
            "showing that it outperforms existing methods and scales effectively to larger\n",
            "graphs, making it ideal for tasks like graph data augmentation.\n",
            "\n",
            "669. Title: Towards Category Unification of 3D Single Object Tracking on Point Clouds\n",
            "   Abstract: This paper studies the problem of training a two-layer ReLU network for\n",
            "binary classification using gradient flow with small initialization. We\n",
            "consider a training dataset with well-separated input vectors: Any pair of\n",
            "input data with the same label are positively correlated, and any pair with\n",
            "different labels are negatively correlated. Our analysis shows that, during the\n",
            "early phase of training, neurons in the first layer try to align with either\n",
            "the positive data or the negative data, depending on its corresponding weight\n",
            "on the second layer. A careful analysis of the neurons' directional dynamics\n",
            "allows us to provide an $\\mathcal{O}(\\frac{\\log n}{\\sqrt{\\mu}})$ upper bound on\n",
            "the time it takes for all neurons to achieve good alignment with the input\n",
            "data, where $n$ is the number of data points and $\\mu$ measures how well the\n",
            "data are separated. After the early alignment phase, the loss converges to zero\n",
            "at a $\\mathcal{O}(\\frac{1}{t})$ rate, and the weight matrix on the first layer\n",
            "is approximately low-rank. Numerical experiments on the MNIST dataset\n",
            "illustrate our theoretical findings.\n",
            "\n",
            "670. Title: Localizing and Editing Knowledge In Text-to-Image Generative Models\n",
            "   Abstract: Category-specific models are provenly valuable methods in 3D single object\n",
            "tracking (SOT) regardless of Siamese or motion-centric paradigms. However, such\n",
            "over-specialized model designs incur redundant parameters, thus limiting the\n",
            "broader applicability of 3D SOT task. This paper first introduces unified\n",
            "models that can simultaneously track objects across all categories using a\n",
            "single network with shared model parameters. Specifically, we propose to\n",
            "explicitly encode distinct attributes associated to different object\n",
            "categories, enabling the model to adapt to cross-category data. We find that\n",
            "the attribute variances of point cloud objects primarily occur from the varying\n",
            "size and shape (e.g., large and square vehicles v.s. small and slender humans).\n",
            "Based on this observation, we design a novel point set representation learning\n",
            "network inheriting transformer architecture, termed AdaFormer, which adaptively\n",
            "encodes the dynamically varying shape and size information from cross-category\n",
            "data in a unified manner. We further incorporate the size and shape prior\n",
            "derived from the known template targets into the model's inputs and learning\n",
            "objective, facilitating the learning of unified representation. Equipped with\n",
            "such designs, we construct two category-unified models SiamCUT and\n",
            "MoCUT.Extensive experiments demonstrate that SiamCUT and MoCUT exhibit strong\n",
            "generalization and training stability. Furthermore, our category-unified models\n",
            "outperform the category-specific counterparts by a significant margin (e.g., on\n",
            "KITTI dataset, 12% and 3% performance gains on the Siamese and motion\n",
            "paradigms). Our code will be available.\n",
            "\n",
            "671. Title: Learning Multi-Agent Communication from Graph Modeling Perspective\n",
            "   Abstract: This work focuses on leveraging and selecting from vast, unlabeled, open data\n",
            "to pre-fine-tune a pre-trained language model. The goal is to minimize the need\n",
            "for costly domain-specific data for subsequent fine-tuning while achieving\n",
            "desired performance levels. While many data selection algorithms have been\n",
            "designed for small-scale applications, rendering them unsuitable for our\n",
            "context, some emerging methods do cater to language data scales. However, they\n",
            "often prioritize data that aligns with the target distribution. While this\n",
            "strategy may be effective when training a model from scratch, it can yield\n",
            "limited results when the model has already been pre-trained on a different\n",
            "distribution. Differing from prior work, our key idea is to select data that\n",
            "nudges the pre-training distribution closer to the target distribution. We show\n",
            "the optimality of this approach for fine-tuning tasks under certain conditions.\n",
            "We demonstrate the efficacy of our methodology across a diverse array of tasks\n",
            "(NLU, NLG, zero-shot) with models up to 2.7B, showing that it consistently\n",
            "surpasses other selection methods. Moreover, our proposed method is\n",
            "significantly faster than existing techniques, scaling to millions of samples\n",
            "within a single GPU hour. Our code is open-sourced (Code repository:\n",
            "https://anonymous.4open.science/r/DV4LLM-D761/ ). While fine-tuning offers\n",
            "significant potential for enhancing performance across diverse tasks, its\n",
            "associated costs often limit its widespread adoption; with this work, we hope\n",
            "to lay the groundwork for cost-effective fine-tuning, making its benefits more\n",
            "accessible.\n",
            "\n",
            "672. Title: It's Never Too Late: Fusing Acoustic Information into Large Language Models for Automatic Speech Recognition\n",
            "   Abstract: The program performance on modern hardware is characterized by \\emph{locality\n",
            "of reference}, that is, it is faster to access data that is close in address\n",
            "space to data that has been accessed recently than data in a random location.\n",
            "This is due to many architectural features including caches, prefetching,\n",
            "virtual address translation and the physical properties of a hard disk drive;\n",
            "attempting to model all the components that constitute the performance of a\n",
            "modern machine is impossible, especially for general algorithm design purposes.\n",
            "What if one could prove an algorithm is asymptotically optimal on all systems\n",
            "that reward locality of reference, no matter how it manifests itself within\n",
            "reasonable limits? We show that this is possible, and that excluding some\n",
            "pathological cases, cache-oblivious algorithms that are asymptotically optimal\n",
            "in the ideal-cache model are asymptotically optimal in any reasonable setting\n",
            "that rewards locality of reference. This is surprising as the cache-oblivious\n",
            "framework envisions a particular architectural model involving blocked memory\n",
            "transfer into a multi-level hierarchy of caches of varying sizes, and was not\n",
            "designed to directly model locality-of-reference correlated performance.\n",
            "\n",
            "673. Title: Algorithms for Caching and MTS with reduced number of predictions\n",
            "   Abstract: We present Stable Video 4D (SV4D), a latent video diffusion model for\n",
            "multi-frame and multi-view consistent dynamic 3D content generation. Unlike\n",
            "previous methods that rely on separately trained generative models for video\n",
            "generation and novel view synthesis, we design a unified diffusion model to\n",
            "generate novel view videos of dynamic 3D objects. Specifically, given a\n",
            "monocular reference video, SV4D generates novel views for each video frame that\n",
            "are temporally consistent. We then use the generated novel view videos to\n",
            "optimize an implicit 4D representation (dynamic NeRF) efficiently, without the\n",
            "need for cumbersome SDS-based optimization used in most prior works. To train\n",
            "our unified novel view video generation model, we curated a dynamic 3D object\n",
            "dataset from the existing Objaverse dataset. Extensive experimental results on\n",
            "multiple datasets and user studies demonstrate SV4D's state-of-the-art\n",
            "performance on novel-view video synthesis as well as 4D generation compared to\n",
            "prior works.\n",
            "\n",
            "674. Title: Pseudo-Generalized Dynamic View Synthesis from a Video\n",
            "   Abstract: Graph research, the systematic study of interconnected data points\n",
            "represented as graphs, plays a vital role in capturing intricate relationships\n",
            "within networked systems. However, in the real world, as graphs scale up,\n",
            "concerns about data security among different data-owning agencies arise,\n",
            "hindering information sharing and, ultimately, the utilization of graph data.\n",
            "Therefore, establishing a mutual trust mechanism among graph agencies is\n",
            "crucial for unlocking the full potential of graphs. Here, we introduce a\n",
            "Cooperative Network Learning (CNL) framework to ensure secure graph computing\n",
            "for various graph tasks. Essentially, this CNL framework unifies the local and\n",
            "global perspectives of GNN computing with distributed data for an agency by\n",
            "virtually connecting all participating agencies as a global graph without a\n",
            "fixed central coordinator. Inter-agency computing is protected by various\n",
            "technologies inherent in our framework, including homomorphic encryption and\n",
            "secure transmission. Moreover, each agency has a fair right to design or employ\n",
            "various graph learning models from its local or global perspective. Thus, CNL\n",
            "can collaboratively train GNN models based on decentralized graphs inferred\n",
            "from local and global graphs. Experiments on contagion dynamics prediction and\n",
            "traditional graph tasks (i.e., node classification and link prediction)\n",
            "demonstrate that our CNL architecture outperforms state-of-the-art GNNs\n",
            "developed at individual sites, revealing that CNL can provide a reliable, fair,\n",
            "secure, privacy-preserving, and global perspective to build effective and\n",
            "personalized models for network applications. We hope this framework will\n",
            "address privacy concerns in graph-related research and integrate decentralized\n",
            "graph data structures to benefit the network research community in cooperation\n",
            "and innovation.\n",
            "\n",
            "675. Title: Is attention required for ICL? Exploring the Relationship Between Model Architecture and In-Context Learning Ability\n",
            "   Abstract: Spiking neural networks (SNNs) have garnered considerable attention owing to\n",
            "their ability to run on neuromorphic devices with super-high speeds and\n",
            "remarkable energy efficiencies. SNNs can be used in conventional neural\n",
            "network-based time- and energy-consuming applications. However, research on\n",
            "generative models within SNNs remains limited, despite their advantages. In\n",
            "particular, diffusion models are a powerful class of generative models, whose\n",
            "image generation quality surpass that of the other generative models, such as\n",
            "GANs. However, diffusion models are characterized by high computational costs\n",
            "and long inference times owing to their iterative denoising feature. Therefore,\n",
            "we propose a novel approach fully spiking denoising diffusion implicit model\n",
            "(FSDDIM) to construct a diffusion model within SNNs and leverage the high speed\n",
            "and low energy consumption features of SNNs via synaptic current learning\n",
            "(SCL). SCL fills the gap in that diffusion models use a neural network to\n",
            "estimate real-valued parameters of a predefined probabilistic distribution,\n",
            "whereas SNNs output binary spike trains. The SCL enables us to complete the\n",
            "entire generative process of diffusion models exclusively using SNNs. We\n",
            "demonstrate that the proposed method outperforms the state-of-the-art fully\n",
            "spiking generative model.\n",
            "\n",
            "676. Title: TorchRL: A data-driven decision-making library for PyTorch\n",
            "   Abstract: Large-scale neural language models exhibit a remarkable capacity for\n",
            "in-context learning (ICL): they can infer novel functions from datasets\n",
            "provided as input. Most of our current understanding of when and how ICL arises\n",
            "comes from LMs trained on extremely simple learning problems like linear\n",
            "regression and associative recall. There remains a significant gap between\n",
            "these model problems and the \"real\" ICL exhibited by LMs trained on large text\n",
            "corpora, which involves not just retrieval and function approximation but\n",
            "free-form generation of language and other structured outputs. In this paper,\n",
            "we study ICL through the lens of a new family of model problems we term in\n",
            "context language learning (ICLL). In ICLL, LMs are presented with a set of\n",
            "strings from a formal language, and must generate additional strings from the\n",
            "same language. We focus on in-context learning of regular languages generated\n",
            "by random finite automata. We evaluate a diverse set of neural sequence models\n",
            "(including several RNNs, Transformers, and state-space model variants) on\n",
            "regular ICLL tasks, aiming to answer three questions: (1) Which model classes\n",
            "are empirically capable of ICLL? (2) What algorithmic solutions do successful\n",
            "models implement to perform ICLL? (3) What architectural changes can improve\n",
            "ICLL in less performant models? We first show that Transformers significantly\n",
            "outperform neural sequence models with recurrent or convolutional\n",
            "representations on ICLL tasks. Next, we provide evidence that their ability to\n",
            "do so relies on specialized \"n-gram heads\" (higher-order variants of induction\n",
            "heads) that compute input-conditional next-token distributions. Finally, we\n",
            "show that hard-wiring these heads into neural models improves performance not\n",
            "just on ICLL, but natural language modeling -- improving the perplexity of\n",
            "340M-parameter models by up to 1.14 points (6.7%) on the SlimPajama dataset.\n",
            "\n",
            "677. Title: Spatio-Temporal Few-Shot Learning via Diffusive Neural Network Generation\n",
            "   Abstract: ML-augmented algorithms utilize predictions to achieve performance beyond\n",
            "their worst-case bounds. Producing these predictions might be a costly\n",
            "operation -- this motivated Im et al. '22 to introduce the study of algorithms\n",
            "which use predictions parsimoniously. We design parsimonious algorithms for\n",
            "caching and MTS with action predictions, proposed by Antoniadis et al. '20,\n",
            "focusing on the parameters of consistency (performance with perfect\n",
            "predictions) and smoothness (dependence of their performance on the prediction\n",
            "error). Our algorithm for caching is 1-consistent, robust, and its smoothness\n",
            "deteriorates with the decreasing number of available predictions. We propose an\n",
            "algorithm for general MTS whose consistency and smoothness both scale linearly\n",
            "with the decreasing number of predictions. Without the restriction on the\n",
            "number of available predictions, both algorithms match the earlier guarantees\n",
            "achieved by Antoniadis et al. '20.\n",
            "\n",
            "678. Title: Large Brain Model for Learning Generic Representations with Tremendous EEG Data in BCI\n",
            "   Abstract: Large language models (LLMs) have garnered significant attention due to their\n",
            "impressive natural language processing (NLP) capabilities. Recently, many\n",
            "studies have focused on the tool utilization ability of LLMs. They primarily\n",
            "investigated how LLMs effectively collaborate with given specific tools.\n",
            "However, in scenarios where LLMs serve as intelligent agents, as seen in\n",
            "applications like AutoGPT and MetaGPT, LLMs are expected to engage in intricate\n",
            "decision-making processes that involve deciding whether to employ a tool and\n",
            "selecting the most suitable tool(s) from a collection of available tools to\n",
            "fulfill user requests. Therefore, in this paper, we introduce MetaTool, a\n",
            "benchmark designed to evaluate whether LLMs have tool usage awareness and can\n",
            "correctly choose tools. Specifically, we create a dataset called ToolE within\n",
            "the benchmark. This dataset contains various types of user queries in the form\n",
            "of prompts that trigger LLMs to use tools, including both single-tool and\n",
            "multi-tool scenarios. Subsequently, we set the tasks for both tool usage\n",
            "awareness and tool selection. We define four subtasks from different\n",
            "perspectives in tool selection, including tool selection with similar choices,\n",
            "tool selection in specific scenarios, tool selection with possible reliability\n",
            "issues, and multi-tool selection. We conduct experiments involving eight\n",
            "popular LLMs and find that the majority of them still struggle to effectively\n",
            "select tools, highlighting the existing gaps between LLMs and genuine\n",
            "intelligent agents. However, through the error analysis, we found there is\n",
            "still significant room for improvement. Finally, we conclude with insights for\n",
            "tool developers -- we strongly recommend that tool developers choose an\n",
            "appropriate rewrite model for generating new descriptions based on the\n",
            "downstream LLM the tool will apply to. Our code is in\n",
            "https://github.com/HowieHwong/MetaTool.\n",
            "\n",
            "679. Title: MetaTool Benchmark for Large Language Models: Deciding Whether to Use Tools and Which to Use\n",
            "   Abstract: Recent studies have successfully shown that large language models (LLMs) can\n",
            "be successfully used for generative error correction (GER) on top of the\n",
            "automatic speech recognition (ASR) output. Specifically, an LLM is utilized to\n",
            "carry out a direct mapping from the N-best hypotheses list generated by an ASR\n",
            "system to the predicted output transcription. However, despite its\n",
            "effectiveness, GER introduces extra data uncertainty since the LLM is trained\n",
            "without taking into account acoustic information available in the speech\n",
            "signal. In this work, we aim to overcome such a limitation by infusing acoustic\n",
            "information before generating the predicted transcription through a novel late\n",
            "fusion solution termed Uncertainty-Aware Dynamic Fusion (UADF). UADF is a\n",
            "multimodal fusion approach implemented into an auto-regressive decoding process\n",
            "and works in two stages: (i) It first analyzes and calibrates the token-level\n",
            "LLM decision, and (ii) it then dynamically assimilates the information from the\n",
            "acoustic modality. Experimental evidence collected from various ASR tasks shows\n",
            "that UADF surpasses existing fusion mechanisms in several ways. It yields\n",
            "significant improvements in word error rate (WER) while mitigating data\n",
            "uncertainty issues in LLM and addressing the poor generalization relied with\n",
            "sole modality during fusion. We also demonstrate that UADF seamlessly adapts to\n",
            "audio-visual speech recognition.\n",
            "\n",
            "680. Title: Memory-Consistent Neural Networks for Imitation Learning\n",
            "   Abstract: The current electroencephalogram (EEG) based deep learning models are\n",
            "typically designed for specific datasets and applications in brain-computer\n",
            "interaction (BCI), limiting the scale of the models and thus diminishing their\n",
            "perceptual capabilities and generalizability. Recently, Large Language Models\n",
            "(LLMs) have achieved unprecedented success in text processing, prompting us to\n",
            "explore the capabilities of Large EEG Models (LEMs). We hope that LEMs can\n",
            "break through the limitations of different task types of EEG datasets, and\n",
            "obtain universal perceptual capabilities of EEG signals through unsupervised\n",
            "pre-training. Then the models can be fine-tuned for different downstream tasks.\n",
            "However, compared to text data, the volume of EEG datasets is generally small\n",
            "and the format varies widely. For example, there can be mismatched numbers of\n",
            "electrodes, unequal length data samples, varied task designs, and low\n",
            "signal-to-noise ratio. To overcome these challenges, we propose a unified\n",
            "foundation model for EEG called Large Brain Model (LaBraM). LaBraM enables\n",
            "cross-dataset learning by segmenting the EEG signals into EEG channel patches.\n",
            "Vector-quantized neural spectrum prediction is used to train a semantically\n",
            "rich neural tokenizer that encodes continuous raw EEG channel patches into\n",
            "compact neural codes. We then pre-train neural Transformers by predicting the\n",
            "original neural codes for the masked EEG channel patches. The LaBraMs were\n",
            "pre-trained on about 2,500 hours of various types of EEG signals from around 20\n",
            "datasets and validated on multiple different types of downstream tasks.\n",
            "Experiments on abnormal detection, event type classification, emotion\n",
            "recognition, and gait prediction show that our LaBraM outperforms all compared\n",
            "SOTA methods in their respective fields. Our code is available at\n",
            "https://github.com/935963004/LaBraM.\n",
            "\n",
            "681. Title: PAE: Reinforcement Learning from External Knowledge for Efficient Exploration\n",
            "   Abstract: In this paper we present a complete study of an end-to-end imitation learning\n",
            "system for speed control of a real car, based on a neural network with a Long\n",
            "Short Term Memory (LSTM). To achieve robustness and generalization from expert\n",
            "demonstrations, we propose data augmentation and label augmentation that are\n",
            "relevant for imitation learning in longitudinal control context. Based on front\n",
            "camera image only, our system is able to correctly control the speed of a car\n",
            "in simulation environment, and in a real car on a challenging test track. The\n",
            "system also shows promising results in open road context.\n",
            "\n",
            "682. Title: Demystifying Linear MDPs and Novel Dynamics Aggregation Framework\n",
            "   Abstract: As large language models (LLMs) are adopted as a fundamental component of\n",
            "language technologies, it is crucial to accurately characterize their\n",
            "performance. Because choices in prompt design can strongly influence model\n",
            "behavior, this design process is critical in effectively using any modern\n",
            "pre-trained generative language model. In this work, we focus on LLM\n",
            "sensitivity to a quintessential class of meaning-preserving design choices:\n",
            "prompt formatting. We find that several widely used open-source LLMs are\n",
            "extremely sensitive to subtle changes in prompt formatting in few-shot\n",
            "settings, with performance differences of up to 76 accuracy points when\n",
            "evaluated using LLaMA-2-13B. Sensitivity remains even when increasing model\n",
            "size, the number of few-shot examples, or performing instruction tuning. Our\n",
            "analysis suggests that work evaluating LLMs with prompting-based methods would\n",
            "benefit from reporting a range of performance across plausible prompt formats,\n",
            "instead of the currently-standard practice of reporting performance on a single\n",
            "format. We also show that format performance only weakly correlates between\n",
            "models, which puts into question the methodological validity of comparing\n",
            "models with an arbitrarily chosen, fixed prompt format. To facilitate\n",
            "systematic analysis we propose FormatSpread, an algorithm that rapidly\n",
            "evaluates a sampled set of plausible prompt formats for a given task, and\n",
            "reports the interval of expected performance without accessing model weights.\n",
            "Furthermore, we present a suite of analyses that characterize the nature of\n",
            "this sensitivity, including exploring the influence of particular atomic\n",
            "perturbations and the internal representation of particular formats.\n",
            "\n",
            "683. Title: Graph Generation with $K^2$-trees\n",
            "   Abstract: Interval and proper interval graphs are very well-known graph classes, for\n",
            "which there is a wide literature. As a consequence, some generalizations of\n",
            "interval graphs have been proposed, in which graphs in general are expressed in\n",
            "terms of $k$ interval graphs, by splitting the graph in some special way.\n",
            "  As a recent example of such an approach, the classes of $k$-thin and proper\n",
            "$k$-thin graphs have been introduced generalizing interval and proper interval\n",
            "graphs, respectively. The complexity of the recognition of each of these\n",
            "classes is still open, even for fixed $k \\geq 2$.\n",
            "  In this work, we introduce a subclass of $k$-thin graphs (resp. proper\n",
            "$k$-thin graphs), called precedence $k$-thin graphs (resp. precedence proper\n",
            "$k$-thin graphs). Concerning partitioned precedence $k$-thin graphs, we present\n",
            "a polynomial time recognition algorithm based on $PQ$-trees. With respect to\n",
            "partitioned precedence proper $k$-thin graphs, we prove that the related\n",
            "recognition problem is \\NP-complete for an arbitrary $k$ and polynomial-time\n",
            "solvable when $k$ is fixed. Moreover, we present a characterization for these\n",
            "classes based on threshold graphs.\n",
            "\n",
            "684. Title: Don't Judge by the Look: Towards Motion Coherent Video Representation\n",
            "   Abstract: In this work, we prove that, in linear MDPs, the feature dimension $d$ is\n",
            "lower bounded by $S/U$ in order to aptly represent transition probabilities,\n",
            "where $S$ is the size of the state space and $U$ is the maximum size of\n",
            "directly reachable states. Hence, $d$ can still scale with $S$ depending on the\n",
            "direct reachability of the environment. To address this limitation of linear\n",
            "MDPs, we propose a novel structural aggregation framework based on dynamics,\n",
            "named as the \"dynamics aggregation\". For this newly proposed framework, we\n",
            "design a provably efficient hierarchical reinforcement learning algorithm in\n",
            "linear function approximation that leverages aggregated sub-structures. Our\n",
            "proposed algorithm exhibits statistical efficiency, achieving a regret of $\n",
            "\\tilde{O} ( d_{\\psi}^{3/2} H^{3/2}\\sqrt{ N T} )$, where $d_{\\psi}$ represents\n",
            "the feature dimension of aggregated subMDPs and $N$ signifies the number of\n",
            "aggregated subMDPs. We establish that the condition $d_{\\psi}^3 N \\ll d^{3}$ is\n",
            "readily met in most real-world environments with hierarchical structures,\n",
            "enabling a substantial improvement in the regret bound compared to LSVI-UCB,\n",
            "which enjoys a regret of $ \\tilde{O} (d^{3/2} H^{3/2} \\sqrt{ T})$. To the best\n",
            "of our knowledge, this work presents the first HRL algorithm with linear\n",
            "function approximation that offers provable guarantees.\n",
            "\n",
            "685. Title: Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting\n",
            "   Abstract: The goal of acoustic (or sound) events detection (AED or SED) is to predict\n",
            "the temporal position of target events in given audio segments. This task plays\n",
            "a significant role in safety monitoring, acoustic early warning and other\n",
            "scenarios. However, the deficiency of data and diversity of acoustic event\n",
            "sources make the AED task a tough issue, especially for prevalent data-driven\n",
            "methods. In this paper, we start by analyzing acoustic events according to\n",
            "their time-frequency domain properties, showing that different acoustic events\n",
            "have different time-frequency scale characteristics. Inspired by the analysis,\n",
            "we propose an adaptive multi-scale detection (AdaMD) method. By taking\n",
            "advantage of the hourglass neural network and gated recurrent unit (GRU)\n",
            "module, our AdaMD produces multiple predictions at different temporal and\n",
            "frequency resolutions. An adaptive training algorithm is subsequently adopted\n",
            "to combine multi-scale predictions to enhance its overall capability.\n",
            "Experimental results on Detection and Classification of Acoustic Scenes and\n",
            "Events 2017 (DCASE 2017) Task 2, DCASE 2016 Task 3 and DCASE 2017 Task 3\n",
            "demonstrate that the AdaMD outperforms published state-of-the-art competitors\n",
            "in terms of the metrics of event error rate (ER) and F1-score. The verification\n",
            "experiment on our collected factory mechanical dataset also proves the\n",
            "noise-resistant capability of the AdaMD, providing the possibility for it to be\n",
            "deployed in the complex environment.\n",
            "\n",
            "686. Title: Weakly Supervised Virus Capsid Detection with Image-Level Annotations in Electron Microscopy Images\n",
            "   Abstract: Current training pipelines in object recognition neglect Hue Jittering when\n",
            "doing data augmentation as it not only brings appearance changes that are\n",
            "detrimental to classification, but also the implementation is inefficient in\n",
            "practice. In this study, we investigate the effect of hue variance in the\n",
            "context of video understanding and find this variance to be beneficial since\n",
            "static appearances are less important in videos that contain motion\n",
            "information. Based on this observation, we propose a data augmentation method\n",
            "for video understanding, named Motion Coherent Augmentation (MCA), that\n",
            "introduces appearance variation in videos and implicitly encourages the model\n",
            "to prioritize motion patterns, rather than static appearances. Concretely, we\n",
            "propose an operation SwapMix to efficiently modify the appearance of video\n",
            "samples, and introduce Variation Alignment (VA) to resolve the distribution\n",
            "shift caused by SwapMix, enforcing the model to learn appearance invariant\n",
            "representations. Comprehensive empirical evaluation across various\n",
            "architectures and different datasets solidly validates the effectiveness and\n",
            "generalization ability of MCA, and the application of VA in other augmentation\n",
            "methods. Code is available at https://github.com/BeSpontaneous/MCA-pytorch.\n",
            "\n",
            "687. Title: Generalized Neural Sorting Networks with Error-Free Differentiable Swap Functions\n",
            "   Abstract: Sorting is a fundamental operation of all computer systems, having been a\n",
            "long-standing significant research topic. Beyond the problem formulation of\n",
            "traditional sorting algorithms, we consider sorting problems for more abstract\n",
            "yet expressive inputs, e.g., multi-digit images and image fragments, through a\n",
            "neural sorting network. To learn a mapping from a high-dimensional input to an\n",
            "ordinal variable, the differentiability of sorting networks needs to be\n",
            "guaranteed. In this paper we define a softening error by a differentiable swap\n",
            "function, and develop an error-free swap function that holds a non-decreasing\n",
            "condition and differentiability. Furthermore, a permutation-equivariant\n",
            "Transformer network with multi-head attention is adopted to capture dependency\n",
            "between given inputs and also leverage its model capacity with self-attention.\n",
            "Experiments on diverse sorting benchmarks show that our methods perform better\n",
            "than or comparable to baseline methods.\n",
            "\n",
            "688. Title: Offline Data Enhanced On-Policy Policy Gradient with Provable Guarantees\n",
            "   Abstract: Hybrid RL is the setting where an RL agent has access to both offline data\n",
            "and online data by interacting with the real-world environment. In this work,\n",
            "we propose a new hybrid RL algorithm that combines an on-policy actor-critic\n",
            "method with offline data. On-policy methods such as policy gradient and natural\n",
            "policy gradient (NPG) have shown to be more robust to model misspecification,\n",
            "though sometimes it may not be as sample efficient as methods that rely on\n",
            "off-policy learning. On the other hand, offline methods that depend on\n",
            "off-policy training often require strong assumptions in theory and are less\n",
            "stable to train in practice. Our new approach integrates a procedure of\n",
            "off-policy training on the offline data into an on-policy NPG framework. We\n",
            "show that our approach, in theory, can obtain a best-of-both-worlds type of\n",
            "result -- it achieves the state-of-art theoretical guarantees of offline RL\n",
            "when offline RL-specific assumptions hold, while at the same time maintaining\n",
            "the theoretical guarantees of on-policy NPG regardless of the offline RL\n",
            "assumptions' validity. Experimentally, in challenging rich-observation\n",
            "environments, we show that our approach outperforms a state-of-the-art hybrid\n",
            "RL baseline which only relies on off-policy policy optimization, demonstrating\n",
            "the empirical benefit of combining on-policy and off-policy learning. Our code\n",
            "is publicly available at https://github.com/YifeiZhou02/HNPG.\n",
            "\n",
            "689. Title: Weaker MVI Condition: Extragradient Methods with Multi-Step Exploration\n",
            "   Abstract: State-of-the-art learning based boundary detection methods require extensive\n",
            "training data. Since labelling object boundaries is one of the most expensive\n",
            "types of annotations, there is a need to relax the requirement to carefully\n",
            "annotate images to make both the training more affordable and to extend the\n",
            "amount of training data. In this paper we propose a technique to generate\n",
            "weakly supervised annotations and show that bounding box annotations alone\n",
            "suffice to reach high-quality object boundaries without using any\n",
            "object-specific boundary annotations. With the proposed weak supervision\n",
            "techniques we achieve the top performance on the object boundary detection\n",
            "task, outperforming by a large margin the current fully supervised\n",
            "state-of-the-art methods.\n",
            "\n",
            "690. Title: Efficient Continual Finite-Sum Minimization\n",
            "   Abstract: In this report, two general concepts for proper efficiency in vector\n",
            "optimization are studied. Properly efficient elements can be defined as\n",
            "minimizers of functionals with certain monotonicity properties or as weakly\n",
            "efficient elements with respect to sets that contain the domination set.\n",
            "Interdependencies between both concepts are proved in topological vector spaces\n",
            "by means of Gerstewitz functionals. The investigation includes proper\n",
            "efficiency notions introduced by Henig and by Nehse and Iwanow. In contrary to\n",
            "Henig's notion, proper efficiency by Nehse and Iwanow is defined as efficiency\n",
            "with respect to certain convex sets which are not necessarily cones. For the\n",
            "finite-dimensional case, we turn to Geoffrion's proper efficiency as a special\n",
            "case of Henig's proper efficiency. It is characterized as efficiency with\n",
            "regard to subclasses of the set of polyhedral cones. Conditions for the\n",
            "existence of Geoffrion's properly efficient points are proved. For closed\n",
            "feasible point sets, Geoffrion's properly efficient point set is empty or\n",
            "coincides with that of Nehse and Iwanow. Properly efficient elements by Nehse\n",
            "and Iwanow are the minimizers of continuous convex functionals with certain\n",
            "monotonicity properties. Henig's proper efficiency can be described by means of\n",
            "minimizers of continuous sublinear functionals with certain monotonicity\n",
            "properties.\n",
            "\n",
            "691. Title: Ensemble Distillation for Unsupervised Constituency Parsing\n",
            "   Abstract: Neural Architecture Search (NAS) often trains and evaluates a large number of\n",
            "architectures. Recent predictor-based NAS approaches attempt to alleviate such\n",
            "heavy computation costs with two key steps: sampling some\n",
            "architecture-performance pairs and fitting a proxy accuracy predictor. Given\n",
            "limited samples, these predictors, however, are far from accurate to locate top\n",
            "architectures due to the difficulty of fitting the huge search space. This\n",
            "paper reflects on a simple yet crucial question: if our final goal is to find\n",
            "the best architecture, do we really need to model the whole space well?. We\n",
            "propose a paradigm shift from fitting the whole architecture space using one\n",
            "strong predictor, to progressively fitting a search path towards the\n",
            "high-performance sub-space through a set of weaker predictors. As a key\n",
            "property of the weak predictors, their probabilities of sampling better\n",
            "architectures keep increasing. Hence we only sample a few well-performed\n",
            "architectures guided by the previously learned predictor and estimate a new\n",
            "better weak predictor. This embarrassingly easy framework, dubbed WeakNAS,\n",
            "produces coarse-to-fine iteration to gradually refine the ranking of sampling\n",
            "space. Extensive experiments demonstrate that WeakNAS costs fewer samples to\n",
            "find top-performance architectures on NAS-Bench-101 and NAS-Bench-201. Compared\n",
            "to state-of-the-art (SOTA) predictor-based NAS methods, WeakNAS outperforms all\n",
            "with notable margins, e.g., requiring at least 7.5x less samples to find global\n",
            "optimal on NAS-Bench-101. WeakNAS can also absorb their ideas to boost\n",
            "performance more. Further, WeakNAS strikes the new SOTA result of 81.3% in the\n",
            "ImageNet MobileNet Search Space. The code is available at\n",
            "https://github.com/VITA-Group/WeakNAS.\n",
            "\n",
            "692. Title: Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed Graph Representation Learning\n",
            "   Abstract: The spectral excess theorem for distance-regular graphs states that a regular\n",
            "(connected) graph is distance-regular if and only if its spectral-excess equals\n",
            "its average excess. A bipartite graph is distance-biregular when it is\n",
            "distance-regular around each vertex and the intersection array only depends on\n",
            "the stable set such a vertex belongs to. In this note we derive a new version\n",
            "of the spectral excess theorem for bipartite distance-biregular graphs.\n",
            "\n",
            "693. Title: Zero-Mean Regularized Spectral Contrastive Learning: Implicitly Mitigating Wrong Connections in Positive-Pair Graphs\n",
            "   Abstract: We investigate the unsupervised constituency parsing task, which organizes\n",
            "words and phrases of a sentence into a hierarchical structure without using\n",
            "linguistically annotated data. We observe that existing unsupervised parsers\n",
            "capture differing aspects of parsing structures, which can be leveraged to\n",
            "enhance unsupervised parsing performance. To this end, we propose a notion of\n",
            "\"tree averaging,\" based on which we further propose a novel ensemble method for\n",
            "unsupervised parsing. To improve inference efficiency, we further distill the\n",
            "ensemble knowledge into a student model; such an ensemble-then-distill process\n",
            "is an effective approach to mitigate the over-smoothing problem existing in\n",
            "common multi-teacher distilling methods. Experiments show that our method\n",
            "surpasses all previous approaches, consistently demonstrating its effectiveness\n",
            "and robustness across various runs, with different ensemble components, and\n",
            "under domain-shift conditions.\n",
            "\n",
            "694. Title: Tree Search-Based Policy Optimization under Stochastic Execution Delay\n",
            "   Abstract: The standard formulation of Markov decision processes (MDPs) assumes that the\n",
            "agent's decisions are executed immediately. However, in numerous realistic\n",
            "applications such as robotics or healthcare, actions are performed with a delay\n",
            "whose value can even be stochastic. In this work, we introduce stochastic\n",
            "delayed execution MDPs, a new formalism addressing random delays without\n",
            "resorting to state augmentation. We show that given observed delay values, it\n",
            "is sufficient to perform a policy search in the class of Markov policies in\n",
            "order to reach optimal performance, thus extending the deterministic fixed\n",
            "delay case. Armed with this insight, we devise DEZ, a model-based algorithm\n",
            "that optimizes over the class of Markov policies. DEZ leverages Monte-Carlo\n",
            "tree search similar to its non-delayed variant EfficientZero to accurately\n",
            "infer future states from the action queue. Thus, it handles delayed execution\n",
            "while preserving the sample efficiency of EfficientZero. Through a series of\n",
            "experiments on the Atari suite, we demonstrate that although the previous\n",
            "baseline outperforms the naive method in scenarios with constant delay, it\n",
            "underperforms in the face of stochastic delays. In contrast, our approach\n",
            "significantly outperforms the baselines, for both constant and stochastic\n",
            "delays. The code is available at http://github.com/davidva1/Delayed-EZ .\n",
            "\n",
            "695. Title: RDesign: Hierarchical Data-efficient Representation Learning for Tertiary Structure-based RNA Design\n",
            "   Abstract: This paper studies semi-supervised graph classification, which is an\n",
            "important problem with various applications in social network analysis and\n",
            "bioinformatics. This problem is typically solved by using graph neural networks\n",
            "(GNNs), which yet rely on a large number of labeled graphs for training and are\n",
            "unable to leverage unlabeled graphs. We address the limitations by proposing\n",
            "the Kernel-based Graph Neural Network (KGNN). A KGNN consists of a GNN-based\n",
            "network as well as a kernel-based network parameterized by a memory network.\n",
            "The GNN-based network performs classification through learning graph\n",
            "representations to implicitly capture the similarity between query graphs and\n",
            "labeled graphs, while the kernel-based network uses graph kernels to explicitly\n",
            "compare each query graph with all the labeled graphs stored in a memory for\n",
            "prediction. The two networks are motivated from complementary perspectives, and\n",
            "thus combing them allows KGNN to use labeled graphs more effectively. We\n",
            "jointly train the two networks by maximizing their agreement on unlabeled\n",
            "graphs via posterior regularization, so that the unlabeled graphs serve as a\n",
            "bridge to let both networks mutually enhance each other. Experiments on a range\n",
            "of well-known benchmark datasets demonstrate that KGNN achieves impressive\n",
            "performance over competitive baselines.\n",
            "\n",
            "696. Title: Learning to Make Adherence-aware Advice\n",
            "   Abstract: Human-in-the-loop (HiL) reinforcement learning is gaining traction in domains\n",
            "with large action and state spaces, and sparse rewards by allowing the agent to\n",
            "take advice from HiL. Beyond advice accommodation, a sequential decision-making\n",
            "agent must be able to express the extent to which it was able to utilize the\n",
            "human advice. Subsequently, the agent should provide a means for the HiL to\n",
            "inspect parts of advice that it had to reject in favor of the overall\n",
            "environment objective. We introduce the problem of Advice-Conformance\n",
            "Verification which requires reinforcement learning (RL) agents to provide\n",
            "assurances to the human in the loop regarding how much of their advice is being\n",
            "conformed to. We then propose a Tree-based lingua-franca to support this\n",
            "communication, called a Preference Tree. We study two cases of good and bad\n",
            "advice scenarios in MuJoCo's Humanoid environment. Through our experiments, we\n",
            "show that our method can provide an interpretable means of solving the\n",
            "Advice-Conformance Verification problem by conveying whether or not the agent\n",
            "is using the human's advice. Finally, we present a human-user study with 20\n",
            "participants that validates our method.\n",
            "\n",
            "697. Title: Achieving Human Parity in Content-Grounded Datasets Generation\n",
            "   Abstract: We consider the regression problem where the dependence of the response Y on\n",
            "a set of predictors X is fully captured by the regression function E(Y |\n",
            "X)=g(B'X), for an unknown function g and low rank parameter B matrix. We\n",
            "combine neural networks with sufficient dimension reduction in order to remove\n",
            "the limitation of small p and n of the latter. We show in simulations that the\n",
            "proposed estimator is on par with competing sufficient dimension reduction\n",
            "methods in small p and n settings, such as minimum average variance estimation\n",
            "and conditional variance estimation. Among those, it is the only\n",
            "computationally applicable in large p and n problems.\n",
            "\n",
            "698. Title: MT-Ranker: Reference-free machine translation evaluation by inter-system ranking\n",
            "   Abstract: Scaling text-to-speech (TTS) to large-scale, multi-speaker, and in-the-wild\n",
            "datasets is important to capture the diversity in human speech such as speaker\n",
            "identities, prosodies, and styles (e.g., singing). Current large TTS systems\n",
            "usually quantize speech into discrete tokens and use language models to\n",
            "generate these tokens one by one, which suffer from unstable prosody, word\n",
            "skipping/repeating issue, and poor voice quality. In this paper, we develop\n",
            "NaturalSpeech 2, a TTS system that leverages a neural audio codec with residual\n",
            "vector quantizers to get the quantized latent vectors and uses a diffusion\n",
            "model to generate these latent vectors conditioned on text input. To enhance\n",
            "the zero-shot capability that is important to achieve diverse speech synthesis,\n",
            "we design a speech prompting mechanism to facilitate in-context learning in the\n",
            "diffusion model and the duration/pitch predictor. We scale NaturalSpeech 2 to\n",
            "large-scale datasets with 44K hours of speech and singing data and evaluate its\n",
            "voice quality on unseen speakers. NaturalSpeech 2 outperforms previous TTS\n",
            "systems by a large margin in terms of prosody/timbre similarity, robustness,\n",
            "and voice quality in a zero-shot setting, and performs novel zero-shot singing\n",
            "synthesis with only a speech prompt. Audio samples are available at\n",
            "https://speechresearch.github.io/naturalspeech2.\n",
            "\n",
            "699. Title: Optimal Sketching for Residual Error Estimation for Matrix and Vector Norms\n",
            "   Abstract: The lack of high-quality data for content-grounded generation tasks has been\n",
            "identified as a major obstacle to advancing these tasks. To address this gap,\n",
            "we propose Genie, a novel method for automatically generating high-quality\n",
            "content-grounded data. It consists of three stages: (a) Content Preparation,\n",
            "(b) Generation: creating task-specific examples from the content (e.g.,\n",
            "question-answer pairs or summaries). (c) Filtering mechanism aiming to ensure\n",
            "the quality and faithfulness of the generated data. We showcase this\n",
            "methodology by generating three large-scale synthetic data, making wishes, for\n",
            "Long-Form Question-Answering (LFQA), summarization, and information extraction.\n",
            "In a human evaluation, our generated data was found to be natural and of high\n",
            "quality. Furthermore, we compare models trained on our data with models trained\n",
            "on human-written data -- ELI5 and ASQA for LFQA and CNN-DailyMail for\n",
            "Summarization. We show that our models are on par with or outperforming models\n",
            "trained on human-generated data and consistently outperforming them in\n",
            "faithfulness. Finally, we applied our method to create LFQA data within the\n",
            "medical domain and compared a model trained on it with models trained on other\n",
            "domains.\n",
            "\n",
            "700. Title: On Error Propagation of Diffusion Models\n",
            "   Abstract: A simple design recipe for deep Transformers is to compose identical building\n",
            "blocks. But standard transformer blocks are far from simple, interweaving\n",
            "attention and MLP sub-blocks with skip connections & normalisation layers in\n",
            "precise arrangements. This complexity leads to brittle architectures, where\n",
            "seemingly minor changes can significantly reduce training speed, or render\n",
            "models untrainable.\n",
            "  In this work, we ask to what extent the standard transformer block can be\n",
            "simplified? Combining signal propagation theory and empirical observations, we\n",
            "motivate modifications that allow many block components to be removed with no\n",
            "loss of training speed, including skip connections, projection or value\n",
            "parameters, sequential sub-blocks and normalisation layers. In experiments on\n",
            "both autoregressive decoder-only and BERT encoder-only models, our simplified\n",
            "transformers emulate the per-update training speed and performance of standard\n",
            "transformers, while enjoying 15% faster training throughput, and using 15%\n",
            "fewer parameters.\n",
            "\n",
            "701. Title: Simplifying Transformer Blocks\n",
            "   Abstract: We study the problem of residual error estimation for matrix and vector norms\n",
            "using a linear sketch. Such estimates can be used, for example, to quickly\n",
            "assess how useful a more expensive low-rank approximation computation will be.\n",
            "The matrix case concerns the Frobenius norm and the task is to approximate the\n",
            "$k$-residual $\\|A - A_k\\|_F$ of the input matrix $A$ within a\n",
            "$(1+\\epsilon)$-factor, where $A_k$ is the optimal rank-$k$ approximation. We\n",
            "provide a tight bound of $\\Theta(k^2/\\epsilon^4)$ on the size of bilinear\n",
            "sketches, which have the form of a matrix product $SAT$. This improves the\n",
            "previous $O(k^2/\\epsilon^6)$ upper bound in (Andoni et al. SODA 2013) and gives\n",
            "the first non-trivial lower bound, to the best of our knowledge. In our\n",
            "algorithm, our sketching matrices $S$ and $T$ can both be sparse matrices,\n",
            "allowing for a very fast update time. We demonstrate that this gives a\n",
            "substantial advantage empirically, for roughly the same sketch size and\n",
            "accuracy as in previous work.\n",
            "  For the vector case, we consider the $\\ell_p$-norm for $p>2$, where the task\n",
            "is to approximate the $k$-residual $\\|x - x_k\\|_p$ up to a constant factor,\n",
            "where $x_k$ is the optimal $k$-sparse approximation to $x$. Such vector norms\n",
            "are frequently studied in the data stream literature and are useful for finding\n",
            "frequent items or so-called heavy hitters. We establish an upper bound of\n",
            "$O(k^{2/p}n^{1-2/p}\\operatorname{poly}(\\log n))$ for constant $\\epsilon$ on the\n",
            "dimension of a linear sketch for this problem. Our algorithm can be extended to\n",
            "the $\\ell_p$ sparse recovery problem with the same sketching dimension, which\n",
            "seems to be the first such bound for $p > 2$. We also show an\n",
            "$\\Omega(k^{2/p}n^{1-2/p})$ lower bound for the sparse recovery problem, which\n",
            "is tight up to a $\\mathrm{poly}(\\log n)$ factor.\n",
            "\n",
            "702. Title: Meta-Evolve: Continuous Robot Evolution for One-to-many Policy Transfer\n",
            "   Abstract: Although diffusion models (DMs) have shown promising performances in a number\n",
            "of tasks (e.g., speech synthesis and image generation), they might suffer from\n",
            "error propagation because of their sequential structure. However, this is not\n",
            "certain because some sequential models, such as Conditional Random Field (CRF),\n",
            "are free from this problem. To address this issue, we develop a theoretical\n",
            "framework to mathematically formulate error propagation in the architecture of\n",
            "DMs, The framework contains three elements, including modular error, cumulative\n",
            "error, and propagation equation. The modular and cumulative errors are related\n",
            "by the equation, which interprets that DMs are indeed affected by error\n",
            "propagation. Our theoretical study also suggests that the cumulative error is\n",
            "closely related to the generation quality of DMs. Based on this finding, we\n",
            "apply the cumulative error as a regularization term to reduce error\n",
            "propagation. Because the term is computationally intractable, we derive its\n",
            "upper bound and design a bootstrap algorithm to efficiently estimate the bound\n",
            "for optimization. We have conducted extensive experiments on multiple image\n",
            "datasets, showing that our proposed regularization reduces error propagation,\n",
            "significantly improves vanilla DMs, and outperforms previous baselines.\n",
            "\n",
            "703. Title: What's In My Big Data?\n",
            "   Abstract: Large text corpora are the backbone of language models. However, we have a\n",
            "limited understanding of the content of these corpora, including general\n",
            "statistics, quality, social factors, and inclusion of evaluation data\n",
            "(contamination). In this work, we propose What's In My Big Data? (WIMBD), a\n",
            "platform and a set of sixteen analyses that allow us to reveal and compare the\n",
            "contents of large text corpora. WIMBD builds on two basic capabilities -- count\n",
            "and search -- at scale, which allows us to analyze more than 35 terabytes on a\n",
            "standard compute node. We apply WIMBD to ten different corpora used to train\n",
            "popular language models, including C4, The Pile, and RedPajama. Our analysis\n",
            "uncovers several surprising and previously undocumented findings about these\n",
            "corpora, including the high prevalence of duplicate, synthetic, and low-quality\n",
            "content, personally identifiable information, toxic language, and benchmark\n",
            "contamination. For instance, we find that about 50% of the documents in\n",
            "RedPajama and LAION-2B-en are duplicates. In addition, several datasets used\n",
            "for benchmarking models trained on such corpora are contaminated with respect\n",
            "to important benchmarks, including the Winograd Schema Challenge and parts of\n",
            "GLUE and SuperGLUE. We open-source WIMBD's code and artifacts to provide a\n",
            "standard set of evaluations for new text-based corpora and to encourage more\n",
            "analyses and transparency around them.\n",
            "\n",
            "704. Title: BrainLM: A foundation model for brain activity recordings\n",
            "   Abstract: Contrastive Learning (CL) has emerged as one of the most successful paradigms\n",
            "for unsupervised visual representation learning, yet it often depends on\n",
            "intensive manual data augmentations. With the rise of generative models,\n",
            "especially diffusion models, the ability to generate realistic images close to\n",
            "the real data distribution has been well recognized. These generated\n",
            "high-equality images have been successfully applied to enhance contrastive\n",
            "representation learning, a technique termed ``data inflation''. However, we\n",
            "find that the generated data (even from a good diffusion model like DDPM) may\n",
            "sometimes even harm contrastive learning. We investigate the causes behind this\n",
            "failure from the perspective of both data inflation and data augmentation. For\n",
            "the first time, we reveal the complementary roles that stronger data inflation\n",
            "should be accompanied by weaker augmentations, and vice versa. We also provide\n",
            "rigorous theoretical explanations for these phenomena via deriving its\n",
            "generalization bounds under data inflation. Drawing from these insights, we\n",
            "propose Adaptive Inflation (AdaInf), a purely data-centric strategy without\n",
            "introducing any extra computation cost. On benchmark datasets, AdaInf can bring\n",
            "significant improvements for various contrastive learning methods. Notably,\n",
            "without using external data, AdaInf obtains 94.70% linear accuracy on CIFAR-10\n",
            "with SimCLR, setting a new record that surpasses many sophisticated methods.\n",
            "Code is available at https://github.com/PKU-ML/adainf.\n",
            "\n",
            "705. Title: A Stable, Fast, and Fully Automatic Learning Algorithm for Predictive Coding Networks\n",
            "   Abstract: Intersecting branes provide a useful mechanism to construct particle physics\n",
            "models from string theory with a wide variety of desirable characteristics. The\n",
            "landscape of such models can be enormous, and navigating towards regions which\n",
            "are most phenomenologically interesting is potentially challenging. Machine\n",
            "learning techniques can be used to efficiently construct large numbers of\n",
            "consistent and phenomenologically desirable models. In this work we phrase the\n",
            "problem of finding consistent intersecting D-brane models in terms of genetic\n",
            "algorithms, which mimic natural selection to evolve a population collectively\n",
            "towards optimal solutions. For a four-dimensional ${\\cal N}=1$ supersymmetric\n",
            "type IIA orientifold with intersecting D6-branes, we demonstrate that\n",
            "$\\mathcal{O}(10^6)$ unique, fully consistent models can be easily constructed,\n",
            "and, by a judicious choice of search environment and hyper-parameters,\n",
            "$\\mathcal{O}(30\\%)$ of the found models contain the desired Standard Model\n",
            "gauge group factor. Having a sizable sample allows us to draw some preliminary\n",
            "landscape statistics of intersecting brane models both with and without the\n",
            "restriction of having the Standard Model gauge factor.\n",
            "\n",
            "706. Title: $t^3$-Variational Autoencoder: Learning Heavy-tailed Data with Student's t and Power Divergence\n",
            "   Abstract: Predictive coding networks are neuroscience-inspired models with roots in\n",
            "both Bayesian statistics and neuroscience. Training such models, however, is\n",
            "quite inefficient and unstable. In this work, we show how by simply changing\n",
            "the temporal scheduling of the update rule for the synaptic weights leads to an\n",
            "algorithm that is much more efficient and stable than the original one, and has\n",
            "theoretical guarantees in terms of convergence. The proposed algorithm, that we\n",
            "call incremental predictive coding (iPC) is also more biologically plausible\n",
            "than the original one, as it it fully automatic. In an extensive set of\n",
            "experiments, we show that iPC constantly performs better than the original\n",
            "formulation on a large number of benchmarks for image classification, as well\n",
            "as for the training of both conditional and masked language models, in terms of\n",
            "test accuracy, efficiency, and convergence with respect to a large set of\n",
            "hyperparameters.\n",
            "\n",
            "707. Title: Kill Two Birds with One Stone: Rethinking Data Augmentation for Deep Long-tailed Learning\n",
            "   Abstract: We study the evolution of correlation functions of local fields in a\n",
            "two-dimensional quantum field theory under the $\\lambda T\\bar T$ deformation,\n",
            "suitably regularized. We show that this may be viewed in terms of the evolution\n",
            "of each field, with a Dirac-like string being attached at each infinitesimal\n",
            "step. The deformation then acts as a derivation on the whole operator algebra,\n",
            "satisfying the Leibniz rule. We derive an explicit equation which allows for\n",
            "the analysis of UV divergences, which may be absorbed into a non-local field\n",
            "renormalization to give correlation functions which are UV finite to all\n",
            "orders, satisfying a (deformed) operator product expansion and a\n",
            "Callan-Symanzik equation. We solve this in the case of a deformed CFT, showing\n",
            "that the Fourier-transformed renormalized two-point functions behave as\n",
            "$k^{2\\Delta+2\\lambda k^2}$, where $\\Delta$ is their IR conformal dimension. We\n",
            "discuss in detail deformed Noether currents, including the energy-momentum\n",
            "tensor, and show that, although they also become non-local, when suitably\n",
            "improved they remain finite, conserved and satisfy the expected Ward\n",
            "identities. Finally, we discuss how the equivalence of the $T\\bar T$\n",
            "deformation to a state-dependent coordinate transformation emerges in this\n",
            "picture.\n",
            "\n",
            "708. Title: Guiding Instruction-based Image Editing via Multimodal Large Language Models\n",
            "   Abstract: Instruction-based image editing improves the controllability and flexibility\n",
            "of image manipulation via natural commands without elaborate descriptions or\n",
            "regional masks. However, human instructions are sometimes too brief for current\n",
            "methods to capture and follow. Multimodal large language models (MLLMs) show\n",
            "promising capabilities in cross-modal understanding and visual-aware response\n",
            "generation via LMs. We investigate how MLLMs facilitate edit instructions and\n",
            "present MLLM-Guided Image Editing (MGIE). MGIE learns to derive expressive\n",
            "instructions and provides explicit guidance. The editing model jointly captures\n",
            "this visual imagination and performs manipulation through end-to-end training.\n",
            "We evaluate various aspects of Photoshop-style modification, global photo\n",
            "optimization, and local editing. Extensive experimental results demonstrate\n",
            "that expressive instructions are crucial to instruction-based image editing,\n",
            "and our MGIE can lead to a notable improvement in automatic metrics and human\n",
            "evaluation while maintaining competitive inference efficiency.\n",
            "\n",
            "709. Title: SmartPlay : A Benchmark for LLMs as Intelligent Agents\n",
            "   Abstract: The quasi-variational inequalities play a significant role in analyzing a\n",
            "wide range of real-world problems. However, these problems are more complicated\n",
            "to solve than variational inequalities as the constraint set is based on the\n",
            "current point. We study a class of quasi-variational inequality problems whose\n",
            "specific structure is beneficial in finding some of its solutions by solving a\n",
            "corresponding variational inequality problem. Based on the classical existence\n",
            "theorem for variational inequalities, our main results ensure the occurrence of\n",
            "solutions for the aforementioned class of quasi-variational inequalities in\n",
            "which the associated constraint maps are (possibly) unbounded. We employ a\n",
            "coercivity condition which plays a crucial role in obtaining these results.\n",
            "Finally, we apply our existence results to ensure the occurrence of equilibrium\n",
            "for the pure exchange economic problems and the jointly convex generalized Nash\n",
            "games.\n",
            "\n",
            "710. Title: A Variational Framework for Estimating Continuous Treatment Effects with Measurement Error\n",
            "   Abstract: QED in two-dimensional Minkowski space contains a single physical state as\n",
            "seen by an inertial observer or by a constantly accelerating Rindler observer.\n",
            "However in Feynman gauge if one takes a generic representative of the physical\n",
            "Minkowski state and traces over all left Rindler states, one does not arrive at\n",
            "a physical right Rindler state, but rather at a \"density matrix\" with negative\n",
            "eigenvalues for negative norm states corresponding intuitively to the radiation\n",
            "of uncorrelated temporal photons and ghosts. This reflects the fact that states\n",
            "that are exact under the Minkowski BRST operator are not necessarily exact or\n",
            "even closed under the Rindler BRST operator. Such situations are avoided when\n",
            "there are quantum corrections to the Hamiltonian that eliminate the horizons,\n",
            "which yield Mathurian fuzzball solutions.\n",
            "\n",
            "711. Title: Do Generated Data Always Help Contrastive Learning?\n",
            "   Abstract: The R-learner has been popular in causal inference as a flexible and\n",
            "efficient meta-learning approach for heterogeneous treatment effect estimation.\n",
            "In this article, we show the identifiability transition of the generalized\n",
            "R-learning framework from a binary treatment to continuous treatment. To\n",
            "resolve the non-identification issue with continuous treatment, we propose a\n",
            "novel identification strategy named T-identification, acknowledging the use of\n",
            "Tikhonov regularization rooted in the nonlinear functional analysis. Following\n",
            "the new identification strategy, we introduce an $\\ell_2$-penalized R-learner\n",
            "framework to estimate the conditional average treatment effect with continuous\n",
            "treatment. The new R-learner framework accommodates modern, flexible machine\n",
            "learning algorithms for both nuisance function and target estimand estimation.\n",
            "Asymptotic properties are studied when the target estimand is approximated by\n",
            "sieve approximation, including general error bounds, asymptotic normality, and\n",
            "inference. Simulations illustrate the superior performance of our proposed\n",
            "estimator. An application of the new method to the medical information mart for\n",
            "intensive care data reveals the heterogeneous treatment effect of oxygen\n",
            "saturation on survival in sepsis patients.\n",
            "\n",
            "712. Title: Complex priors and flexible inference in recurrent circuits with dendritic nonlinearities\n",
            "   Abstract: We give upper and lower bounds on the power of subsystems of the Ideal Proof\n",
            "System (IPS), the algebraic proof system recently proposed by Grochow and\n",
            "Pitassi, where the circuits comprising the proof come from various restricted\n",
            "algebraic circuit classes. This mimics an established research direction in the\n",
            "boolean setting for subsystems of Extended Frege proofs, where proof-lines are\n",
            "circuits from restricted boolean circuit classes. Except one, all of the\n",
            "subsystems considered in this paper can simulate the well-studied\n",
            "Nullstellensatz proof system, and prior to this work there were no known lower\n",
            "bounds when measuring proof size by the algebraic complexity of the polynomials\n",
            "(except with respect to degree, or to sparsity).\n",
            "  We give two general methods of converting certain algebraic lower bounds into\n",
            "proof complexity ones. Our methods require stronger notions of lower bounds,\n",
            "which lower bound a polynomial as well as an entire family of polynomials it\n",
            "defines. Our techniques are reminiscent of existing methods for converting\n",
            "boolean circuit lower bounds into related proof complexity results, such as\n",
            "feasible interpolation. We obtain the relevant types of lower bounds for a\n",
            "variety of classes (sparse polynomials, depth-3 powering formulas, read-once\n",
            "oblivious algebraic branching programs, and multilinear formulas), and infer\n",
            "the relevant proof complexity results. We complement our lower bounds by giving\n",
            "short refutations of the previously-studied subset-sum axiom using IPS\n",
            "subsystems, allowing us to conclude strict separations between some of these\n",
            "subsystems.\n",
            "\n",
            "713. Title: Understanding Transferable Representation Learning and Zero-shot Transfer in CLIP\n",
            "   Abstract: Multi-modal learning has become increasingly popular due to its ability to\n",
            "leverage information from different data sources (e.g., text and images) to\n",
            "improve the model performance. Recently, CLIP has emerged as an effective\n",
            "approach that employs vision-language contrastive pretraining to learn joint\n",
            "image and text representations and exhibits remarkable performance in zero-shot\n",
            "learning and text-guided natural image generation. Despite the huge practical\n",
            "success of CLIP, its theoretical understanding remains elusive. In this paper,\n",
            "we formally study transferrable representation learning underlying CLIP and\n",
            "demonstrate how features from different modalities get aligned. We also analyze\n",
            "its zero-shot transfer performance on the downstream tasks. Inspired by our\n",
            "analysis, we propose a new CLIP-type approach, which achieves better\n",
            "performance than CLIP and other state-of-the-art methods on benchmark datasets.\n",
            "\n",
            "714. Title: A Study of Bayesian Neural Network Surrogates for Bayesian Optimization\n",
            "   Abstract: As Large Language Models (LLMs) are deployed more widely, customization with\n",
            "respect to vocabulary, style, and character becomes more important. In this\n",
            "work, we introduce model arithmetic, a novel inference framework for composing\n",
            "and biasing LLMs without the need for model (re)training or highly specific\n",
            "datasets. In addition, the framework allows for more precise control of\n",
            "generated text than direct prompting and prior controlled text generation (CTG)\n",
            "techniques. Using model arithmetic, we can express prior CTG techniques as\n",
            "simple formulas and naturally extend them to new and more effective\n",
            "formulations. Further, we show that speculative sampling, a technique for\n",
            "efficient LLM sampling, extends to our setting. This enables highly efficient\n",
            "text generation with multiple composed models with only marginal overhead over\n",
            "a single model. Our empirical evaluation demonstrates that model arithmetic\n",
            "allows fine-grained control of generated text while outperforming\n",
            "state-of-the-art on the task of toxicity reduction. We release an open source\n",
            "easy-to-use implementation of our framework at\n",
            "https://github.com/eth-sri/language-model-arithmetic.\n",
            "\n",
            "715. Title: Demystifying Local & Global Fairness Trade-offs in Federated Learning Using Partial Information Decomposition\n",
            "   Abstract: Machine learning models are often used to decide who receives a loan, a job\n",
            "interview, or a public benefit. Models in such settings use features without\n",
            "considering their actionability. As a result, they can assign predictions that\n",
            "are fixed $-$ meaning that individuals who are denied loans and interviews are,\n",
            "in fact, precluded from access to credit and employment. In this work, we\n",
            "introduce a procedure called recourse verification to test if a model assigns\n",
            "fixed predictions to its decision subjects. We propose a model-agnostic\n",
            "approach for recourse verification with reachable sets $-$ i.e., the set of all\n",
            "points that a person can reach through their actions in feature space. We\n",
            "develop methods to construct reachable sets for discrete feature spaces, which\n",
            "can certify the responsiveness of any model by simply querying its predictions.\n",
            "We conduct a comprehensive empirical study on the infeasibility of recourse on\n",
            "datasets from consumer finance. Our results highlight how models can\n",
            "inadvertently preclude access by assigning fixed predictions and underscore the\n",
            "need to account for actionability in model development.\n",
            "\n",
            "716. Title: THOUGHT PROPAGATION: AN ANALOGICAL APPROACH TO COMPLEX REASONING WITH LARGE LANGUAGE MODELS\n",
            "   Abstract: The present work reports a comparative performance of artificial neurons\n",
            "obtained in terms of the real-valued Jaccard and coincidence similarity indices\n",
            "and respectively derived functionals. The interiority index and classic\n",
            "cross-correlation are also included for comparison purposes. After presenting\n",
            "the basic concepts related to real-valued multisets and the adopted similarity\n",
            "metrics, including the generalization of the real-valued Jaccard and\n",
            "coincidence indices to higher orders, we proceed to studying the response of a\n",
            "single neuron, not taking into account the output non-linearity (e.g.~sigmoid),\n",
            "respectively to the detection of gaussian two-dimensional stimulus in presence\n",
            "of displacement, magnification, intensity variation, noise and interference\n",
            "from additional patterns. It is shown that the real-valued Jaccard and\n",
            "coincidence approaches are substantially more robust and effective than the\n",
            "interiority index and the classic cross-correlation. The coincidence-based\n",
            "neurons are shown to have the best overall performance respectively to the\n",
            "considered type of data and perturbations. The potential of the multiset\n",
            "neurons is further illustrated with respect to the challenging problem of image\n",
            "segmentation, leading to impressive cost/benefit performance. The reported\n",
            "concepts, methods, and results, have substantial implications not only for\n",
            "pattern recognition and machine learning, but also regarding neurobiology and\n",
            "neuroscience.\n",
            "\n",
            "717. Title: Mask-Based Modeling for Neural Radiance Fields\n",
            "   Abstract: Bayesian optimization is a highly efficient approach to optimizing objective\n",
            "functions which are expensive to query. These objectives are typically\n",
            "represented by Gaussian process (GP) surrogate models which are easy to\n",
            "optimize and support exact inference. While standard GP surrogates have been\n",
            "well-established in Bayesian optimization, Bayesian neural networks (BNNs) have\n",
            "recently become practical function approximators, with many benefits over\n",
            "standard GPs such as the ability to naturally handle non-stationarity and learn\n",
            "representations for high-dimensional data. In this paper, we study BNNs as\n",
            "alternatives to standard GP surrogates for optimization. We consider a variety\n",
            "of approximate inference procedures for finite-width BNNs, including\n",
            "high-quality Hamiltonian Monte Carlo, low-cost stochastic MCMC, and heuristics\n",
            "such as deep ensembles. We also consider infinite-width BNNs, linearized\n",
            "Laplace approximations, and partially stochastic models such as deep kernel\n",
            "learning. We evaluate this collection of surrogate models on diverse problems\n",
            "with varying dimensionality, number of objectives, non-stationarity, and\n",
            "discrete and continuous inputs. We find: (i) the ranking of methods is highly\n",
            "problem dependent, suggesting the need for tailored inductive biases; (ii) HMC\n",
            "is the most successful approximate inference procedure for fully stochastic\n",
            "BNNs; (iii) full stochasticity may be unnecessary as deep kernel learning is\n",
            "relatively competitive; (iv) deep ensembles perform relatively poorly; (v)\n",
            "infinite-width BNNs are particularly promising, especially in high dimensions.\n",
            "\n",
            "718. Title: Label-Agnostic Forgetting: A Supervision-Free Unlearning in Deep Models\n",
            "   Abstract: We present Generalizable Wireless Radiance Fields (GWRF), a framework for\n",
            "modeling wireless signal propagation at arbitrary 3D transmitter and receiver\n",
            "positions. Unlike previous methods that adapt vanilla Neural Radiance Fields\n",
            "(NeRF) from the optical to the wireless signal domain, requiring extensive\n",
            "per-scene training, GWRF generalizes effectively across scenes. First, a\n",
            "geometry-aware Transformer encoder-based wireless scene representation module\n",
            "incorporates information from geographically proximate transmitters to learn a\n",
            "generalizable wireless radiance field. Second, a neural-driven ray tracing\n",
            "algorithm operates on this field to automatically compute signal reception at\n",
            "the receiver. Experimental results demonstrate that GWRF outperforms existing\n",
            "methods on single scenes and achieves state-of-the-art performance on unseen\n",
            "scenes.\n",
            "\n",
            "719. Title: Controlled Text Generation via Language Model Arithmetic\n",
            "   Abstract: Machine unlearning is a key requirement of many data protection regulations\n",
            "such as GDPR. Prior work on unlearning has mostly considered superficial\n",
            "unlearning tasks where a single or a few related pieces of information are\n",
            "required to be removed. However, the task of unlearning a fact is much more\n",
            "challenging in recent large language models (LLMs), because the facts in LLMs\n",
            "can be deduced from each other. In this work, we investigate whether current\n",
            "unlearning methods for LLMs succeed beyond superficial unlearning of facts.\n",
            "Specifically, we formally propose a framework and a definition for deep\n",
            "unlearning facts that are interrelated. We design the metric, recall, to\n",
            "quantify the extent of deep unlearning. To systematically evaluate deep\n",
            "unlearning, we construct a synthetic dataset EDU-RELAT, which consists of a\n",
            "synthetic knowledge base of family relationships and biographies, together with\n",
            "a realistic logical rule set that connects them. We use this dataset to test\n",
            "four unlearning methods in four LLMs at different sizes. Our findings reveal\n",
            "that in the task of deep unlearning only a single fact, they either fail to\n",
            "properly unlearn with high recall, or end up unlearning many other irrelevant\n",
            "facts. Our dataset and code are publicly available at:\n",
            "https://github.com/wrh14/deep_unlearning.\n",
            "\n",
            "720. Title: The Devil is in the Neurons: Interpreting and Mitigating Social Biases in Language Models\n",
            "   Abstract: Fair machine learning aims to prevent discrimination against individuals or\n",
            "sub-populations based on sensitive attributes such as gender and race. In\n",
            "recent years, causal inference methods have been increasingly used in fair\n",
            "machine learning to measure unfairness by causal effects. However, current\n",
            "methods assume that the true causal graph is given, which is often not true in\n",
            "real-world applications. To address this limitation, this paper proposes a\n",
            "framework for achieving causal fairness based on the notion of interventions\n",
            "when the true causal graph is partially known. The proposed approach involves\n",
            "modeling fair prediction using a Partially Directed Acyclic Graph (PDAG),\n",
            "specifically, a class of causal DAGs that can be learned from observational\n",
            "data combined with domain knowledge. The PDAG is used to measure causal\n",
            "fairness, and a constrained optimization problem is formulated to balance\n",
            "between fairness and accuracy. Results on both simulated and real-world\n",
            "datasets demonstrate the effectiveness of this method.\n",
            "\n",
            "721. Title: Social-Transmotion: Promptable Human Trajectory Prediction\n",
            "   Abstract: This work presents an information-theoretic perspective to group fairness\n",
            "trade-offs in federated learning (FL) with respect to sensitive attributes,\n",
            "such as gender, race, etc. Existing works often focus on either $\\textit{global\n",
            "fairness}$ (overall disparity of the model across all clients) or\n",
            "$\\textit{local fairness}$ (disparity of the model at each client), without\n",
            "always considering their trade-offs. There is a lack of understanding regarding\n",
            "the interplay between global and local fairness in FL, particularly under data\n",
            "heterogeneity, and if and when one implies the other. To address this gap, we\n",
            "leverage a body of work in information theory called partial information\n",
            "decomposition (PID), which first identifies three sources of unfairness in FL,\n",
            "namely, $\\textit{Unique Disparity}$, $\\textit{Redundant Disparity}$, and\n",
            "$\\textit{Masked Disparity}$. We demonstrate how these three disparities\n",
            "contribute to global and local fairness using canonical examples. This\n",
            "decomposition helps us derive fundamental limits on the trade-off between\n",
            "global and local fairness, highlighting where they agree or disagree. We\n",
            "introduce the $\\textit{Accuracy and Global-Local Fairness Optimality Problem\n",
            "(AGLFOP)}$, a convex optimization that defines the theoretical limits of\n",
            "accuracy and fairness trade-offs, identifying the best possible performance any\n",
            "FL strategy can attain given a dataset and client distribution. We also present\n",
            "experimental results on synthetic datasets and the ADULT dataset to support our\n",
            "theoretical findings.\n",
            "\n",
            "722. Title: CABINET: Content Relevance-based Noise Reduction for Table Question Answering\n",
            "   Abstract: Catastrophic overfitting (CO) in single-step adversarial training (AT)\n",
            "results in abrupt drops in the adversarial test accuracy (even down to 0%). For\n",
            "models trained with multi-step AT, it has been observed that the loss function\n",
            "behaves locally linearly with respect to the input, this is however lost in\n",
            "single-step AT. To address CO in single-step AT, several methods have been\n",
            "proposed to enforce local linearity of the loss via regularization. However,\n",
            "these regularization terms considerably slow down training due to Double\n",
            "Backpropagation. Instead, in this work, we introduce a regularization term,\n",
            "called ELLE, to mitigate CO effectively and efficiently in classical AT\n",
            "evaluations, as well as some more difficult regimes, e.g., large adversarial\n",
            "perturbations and long training schedules. Our regularization term can be\n",
            "theoretically linked to curvature of the loss function and is computationally\n",
            "cheaper than previous methods by avoiding Double Backpropagation. Our thorough\n",
            "experimental validation demonstrates that our work does not suffer from CO,\n",
            "even in challenging settings where previous works suffer from it. We also\n",
            "notice that adapting our regularization parameter during training (ELLE-A)\n",
            "greatly improves the performance, specially in large $\\epsilon$ setups. Our\n",
            "implementation is available in https://github.com/LIONS-EPFL/ELLE .\n",
            "\n",
            "723. Title: When can transformers reason with abstract symbols?\n",
            "   Abstract: Despite recent advancements in semantic segmentation, where and what pixels\n",
            "are hard to segment remains largely unexplored. Existing research only\n",
            "separates an image into easy and hard regions and empirically observes the\n",
            "latter are associated with object boundaries. In this paper, we conduct a\n",
            "comprehensive analysis of hard pixel errors, categorizing them into three\n",
            "types: false responses, merging mistakes, and displacements. Our findings\n",
            "reveal a quantitative association between hard pixels and aliasing, which is\n",
            "distortion caused by the overlapping of frequency components in the Fourier\n",
            "domain during downsampling. To identify the frequencies responsible for\n",
            "aliasing, we propose using the equivalent sampling rate to calculate the\n",
            "Nyquist frequency, which marks the threshold for aliasing. Then, we introduce\n",
            "the aliasing score as a metric to quantify the extent of aliasing. While\n",
            "positively correlated with the proposed aliasing score, three types of hard\n",
            "pixels exhibit different patterns. Here, we propose two novel de-aliasing\n",
            "filter (DAF) and frequency mixing (FreqMix) modules to alleviate aliasing\n",
            "degradation by accurately removing or adjusting frequencies higher than the\n",
            "Nyquist frequency. The DAF precisely removes the frequencies responsible for\n",
            "aliasing before downsampling, while the FreqMix dynamically selects\n",
            "high-frequency components within the encoder block. Experimental results\n",
            "demonstrate consistent improvements in semantic segmentation and low-light\n",
            "instance segmentation tasks. The code is available at:\n",
            "https://github.com/Linwei-Chen/Seg-Aliasing.\n",
            "\n",
            "724. Title: VCR-Graphormer: A Mini-batch Graph Transformer via Virtual Connections\n",
            "   Abstract: We investigate the capabilities of transformer models on relational reasoning\n",
            "tasks. In these tasks, models are trained on a set of strings encoding abstract\n",
            "relations, and are then tested out-of-distribution on data that contains\n",
            "symbols that did not appear in the training dataset. We prove that for any\n",
            "relational reasoning task in a large family of tasks, transformers learn the\n",
            "abstract relations and generalize to the test set when trained by gradient\n",
            "descent on sufficiently large quantities of training data. This is in contrast\n",
            "to classical fully-connected networks, which we prove fail to learn to reason.\n",
            "Our results inspire modifications of the transformer architecture that add only\n",
            "two trainable parameters per head, and that we empirically demonstrate improve\n",
            "data efficiency for learning to reason.\n",
            "\n",
            "725. Title: When Semantic Segmentation Meets Frequency Aliasing\n",
            "   Abstract: Large Language Models (LLMs) have revolutionized natural language processing\n",
            "and hold immense potential for advancing Artificial Intelligence. However, the\n",
            "core architecture of most mainstream LLMs -- the Transformer -- has inherent\n",
            "limitations in computational depth, rendering them theoretically incapable of\n",
            "solving many reasoning tasks that demand increasingly deep computations. Chain\n",
            "of Thought (CoT) prompting has emerged as a technique to address these\n",
            "architectural limitations, as evidenced by several theoretical studies. It\n",
            "offers a promising approach to solving complex reasoning tasks that were\n",
            "previously beyond the capabilities of these models. Despite its successes, CoT\n",
            "and its variants (such as Tree of Thought, Graph of Thought, etc.) rely on a\n",
            "\"one-prompt-for-all\" approach, using a single prompt structure (e.g., \"think\n",
            "step by step\") for a wide range of tasks -- from counting and sorting to\n",
            "solving mathematical and algorithmic problems. This approach poses significant\n",
            "challenges for models to generate the correct reasoning steps, as the model\n",
            "must navigate through a vast prompt template space to find the appropriate\n",
            "template for each task. In this work, we build upon previous theoretical\n",
            "analyses of CoT to demonstrate how the one-prompt-for-all approach can\n",
            "negatively affect the computability of LLMs. We partition the solution search\n",
            "space into two: the prompt space and the answer space. Our findings show that\n",
            "task-specific supervision is essential for navigating the prompt space\n",
            "accurately and achieving optimal performance. Through experiments with\n",
            "state-of-the-art LLMs, we reveal a gap in reasoning performance when\n",
            "supervision is applied versus when it is not.\n",
            "\n",
            "726. Title: Efficient local linearity regularization to overcome catastrophic overfitting\n",
            "   Abstract: Randomized smoothing-based certification is an effective approach for\n",
            "obtaining robustness certificates of deep neural networks (DNNs) against\n",
            "adversarial attacks. This method constructs a smoothed DNN model and certifies\n",
            "its robustness through statistical sampling, but it is computationally\n",
            "expensive, especially when certifying with a large number of samples.\n",
            "Furthermore, when the smoothed model is modified (e.g., quantized or pruned),\n",
            "certification guarantees may not hold for the modified DNN, and recertifying\n",
            "from scratch can be prohibitively expensive.\n",
            "  We present the first approach for incremental robustness certification for\n",
            "randomized smoothing, IRS. We show how to reuse the certification guarantees\n",
            "for the original smoothed model to certify an approximated model with very few\n",
            "samples. IRS significantly reduces the computational cost of certifying\n",
            "modified DNNs while maintaining strong robustness guarantees. We experimentally\n",
            "demonstrate the effectiveness of our approach, showing up to 3x certification\n",
            "speedup over the certification that applies randomized smoothing of the\n",
            "approximate model from scratch.\n",
            "\n",
            "727. Title: Horizon-Free Regret for Linear Markov Decision Processes\n",
            "   Abstract: Quantum mechanics has irked physicists ever since its conception more than\n",
            "100 years ago. While some of the misgivings, such as it being unintuitive, are\n",
            "merely aesthetic, quantum mechanics has one serious shortcoming: it lacks a\n",
            "physical description of the measurement process. This \"measurement problem\"\n",
            "indicates that quantum mechanics is at least an incomplete theory -- good as\n",
            "far as it goes, but missing a piece -- or, more radically, is in need of\n",
            "complete overhaul.\n",
            "  Here we describe an approach which may provide this sought-for completion or\n",
            "replacement: Superdeterminism. A superdeterministic theory is one which\n",
            "violates the assumption of Statistical Independence (that distributions of\n",
            "hidden variables are independent of measurement settings). Intuition suggests\n",
            "that Statistical Independence is an essential ingredient of any theory of\n",
            "science (never mind physics), and for this reason Superdeterminism is typically\n",
            "discarded swiftly in any discussion of quantum foundations.\n",
            "  The purpose of this paper is to explain why the existing objections to\n",
            "Superdeterminism are based on experience with classical physics and linear\n",
            "systems, but that this experience misleads us. Superdeterminism is a promising\n",
            "approach not only to solve the measurement problem, but also to understand the\n",
            "apparent nonlocality of quantum physics. Most importantly, we will discuss how\n",
            "it may be possible to test this hypothesis in an (almost) model independent\n",
            "way.\n",
            "\n",
            "728. Title: Incremental Randomized Smoothing Certification\n",
            "   Abstract: We study regret minimization in online episodic linear Markov Decision\n",
            "Processes, and obtain rate-optimal $\\widetilde O (\\sqrt K)$ regret where $K$\n",
            "denotes the number of episodes. Our work is the first to establish the optimal\n",
            "(w.r.t.~$K$) rate of convergence in the stochastic setting with bandit feedback\n",
            "using a policy optimization based approach, and the first to establish the\n",
            "optimal (w.r.t.~$K$) rate in the adversarial setup with full information\n",
            "feedback, for which no algorithm with an optimal rate guarantee is currently\n",
            "known.\n",
            "\n",
            "729. Title: OpenNeRF: Open Set 3D Neural Scene Segmentation with Pixel-Wise Features and Rendered Novel Views\n",
            "   Abstract: Predictive multiplicity refers to the phenomenon in which classification\n",
            "tasks may admit multiple competing models that achieve almost-equally-optimal\n",
            "performance, yet generate conflicting outputs for individual samples. This\n",
            "presents significant concerns, as it can potentially result in systemic\n",
            "exclusion, inexplicable discrimination, and unfairness in practical\n",
            "applications. Measuring and mitigating predictive multiplicity, however, is\n",
            "computationally challenging due to the need to explore all such\n",
            "almost-equally-optimal models, known as the Rashomon set, in potentially huge\n",
            "hypothesis spaces. To address this challenge, we propose a novel framework that\n",
            "utilizes dropout techniques for exploring models in the Rashomon set. We\n",
            "provide rigorous theoretical derivations to connect the dropout parameters to\n",
            "properties of the Rashomon set, and empirically evaluate our framework through\n",
            "extensive experimentation. Numerical results show that our technique\n",
            "consistently outperforms baselines in terms of the effectiveness of predictive\n",
            "multiplicity metric estimation, with runtime speedup up to $20\\times \\sim\n",
            "5000\\times$. With efficient Rashomon set exploration and metric estimation,\n",
            "mitigation of predictive multiplicity is then achieved through dropout ensemble\n",
            "and model selection.\n",
            "\n",
            "730. Title: R-EDL: Relaxing Nonessential Settings of Evidential Deep Learning\n",
            "   Abstract: Recent years have witnessed a burgeoning interest in federated learning (FL).\n",
            "However, the contexts in which clients engage in sequential learning remain\n",
            "under-explored. Bridging FL and continual learning (CL) gives rise to a\n",
            "challenging practical problem: federated continual learning (FCL). Existing\n",
            "research in FCL primarily focuses on mitigating the catastrophic forgetting\n",
            "issue of continual learning while collaborating with other clients. We argue\n",
            "that the forgetting phenomena are not invariably detrimental. In this paper, we\n",
            "consider a more practical and challenging FCL setting characterized by\n",
            "potentially unrelated or even antagonistic data/tasks across different clients.\n",
            "In the FL scenario, statistical heterogeneity and data noise among clients may\n",
            "exhibit spurious correlations which result in biased feature learning. While\n",
            "existing CL strategies focus on a complete utilization of previous knowledge,\n",
            "we found that forgetting biased information is beneficial in our study.\n",
            "Therefore, we propose a new concept accurate forgetting (AF) and develop a\n",
            "novel generative-replay method~\\method~which selectively utilizes previous\n",
            "knowledge in federated networks. We employ a probabilistic framework based on a\n",
            "normalizing flow model to quantify the credibility of previous knowledge.\n",
            "Comprehensive experiments affirm the superiority of our method over baselines.\n",
            "\n",
            "731. Title: IDEAL: Influence-Driven Selective Annotations Empower In-Context Learners in Large Language Models\n",
            "   Abstract: Pre-trained Language models (PLMs) have been acknowledged to contain harmful\n",
            "information, such as social biases, which may cause negative social impacts or\n",
            "even bring catastrophic results in application. Previous works on this problem\n",
            "mainly focused on using black-box methods such as probing to detect and\n",
            "quantify social biases in PLMs by observing model outputs. As a result,\n",
            "previous debiasing methods mainly finetune or even pre-train language models on\n",
            "newly constructed anti-stereotypical datasets, which are high-cost. In this\n",
            "work, we try to unveil the mystery of social bias inside language models by\n",
            "introducing the concept of {\\sc Social Bias Neurons}. Specifically, we propose\n",
            "{\\sc Integrated Gap Gradients (IG$^2$)} to accurately pinpoint units (i.e.,\n",
            "neurons) in a language model that can be attributed to undesirable behavior,\n",
            "such as social bias. By formalizing undesirable behavior as a distributional\n",
            "property of language, we employ sentiment-bearing prompts to elicit classes of\n",
            "sensitive words (demographics) correlated with such sentiments. Our IG$^2$ thus\n",
            "attributes the uneven distribution for different demographics to specific\n",
            "Social Bias Neurons, which track the trail of unwanted behavior inside PLM\n",
            "units to achieve interoperability. Moreover, derived from our interpretable\n",
            "technique, {\\sc Bias Neuron Suppression (BNS)} is further proposed to mitigate\n",
            "social biases. By studying BERT, RoBERTa, and their attributable differences\n",
            "from debiased FairBERTa, IG$^2$ allows us to locate and suppress identified\n",
            "neurons, and further mitigate undesired behaviors. As measured by prior metrics\n",
            "from StereoSet, our model achieves a higher degree of fairness while\n",
            "maintaining language modeling ability with low cost.\n",
            "\n",
            "732. Title: Provable Robust Watermarking for AI-Generated Text\n",
            "   Abstract: Seismic advances in generative AI algorithms for imagery, text, and other\n",
            "data types has led to the temptation to use synthetic data to train\n",
            "next-generation models. Repeating this process creates an autophagous\n",
            "(self-consuming) loop whose properties are poorly understood. We conduct a\n",
            "thorough analytical and empirical analysis using state-of-the-art generative\n",
            "image models of three families of autophagous loops that differ in how fixed or\n",
            "fresh real training data is available through the generations of training and\n",
            "in whether the samples from previous generation models have been biased to\n",
            "trade off data quality versus diversity. Our primary conclusion across all\n",
            "scenarios is that without enough fresh real data in each generation of an\n",
            "autophagous loop, future generative models are doomed to have their quality\n",
            "(precision) or diversity (recall) progressively decrease. We term this\n",
            "condition Model Autophagy Disorder (MAD), making analogy to mad cow disease.\n",
            "\n",
            "733. Title: Sliced Wasserstein Estimation with Control Variates\n",
            "   Abstract: Modern distribution matching algorithms for training diffusion or flow models\n",
            "directly prescribe the time evolution of the marginal distributions between two\n",
            "boundary distributions. In this work, we consider a generalized distribution\n",
            "matching setup, where these marginals are only implicitly described as a\n",
            "solution to some task-specific objective function. The problem setup, known as\n",
            "the Generalized Schr\\\"odinger Bridge (GSB), appears prevalently in many\n",
            "scientific areas both within and without machine learning. We propose\n",
            "Generalized Schr\\\"odinger Bridge Matching (GSBM), a new matching algorithm\n",
            "inspired by recent advances, generalizing them beyond kinetic energy\n",
            "minimization and to account for task-specific state costs. We show that such a\n",
            "generalization can be cast as solving conditional stochastic optimal control,\n",
            "for which efficient variational approximations can be used, and further\n",
            "debiased with the aid of path integral theory. Compared to prior methods for\n",
            "solving GSB problems, our GSBM algorithm better preserves a feasible transport\n",
            "map between the boundary distributions throughout training, thereby enabling\n",
            "stable convergence and significantly improved scalability. We empirically\n",
            "validate our claims on an extensive suite of experimental setups, including\n",
            "crowd navigation, opinion depolarization, LiDAR manifolds, and image domain\n",
            "transfer. Our work brings new algorithmic opportunities for training diffusion\n",
            "models enhanced with task-specific optimality structures. Code available at\n",
            "https://github.com/facebookresearch/generalized-schrodinger-bridge-matching\n",
            "\n",
            "734. Title: CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing\n",
            "   Abstract: We study the problem of watermarking large language models (LLMs) generated\n",
            "text -- one of the most promising approaches for addressing the safety\n",
            "challenges of LLM usage. In this paper, we propose a rigorous theoretical\n",
            "framework to quantify the effectiveness and robustness of LLM watermarks. We\n",
            "propose a robust and high-quality watermark method, Unigram-Watermark, by\n",
            "extending an existing approach with a simplified fixed grouping strategy. We\n",
            "prove that our watermark method enjoys guaranteed generation quality,\n",
            "correctness in watermark detection, and is robust against text editing and\n",
            "paraphrasing. Experiments on three varying LLMs and two datasets verify that\n",
            "our Unigram-Watermark achieves superior detection accuracy and comparable\n",
            "generation quality in perplexity, thus promoting the responsible use of LLMs.\n",
            "Code is available at https://github.com/XuandongZhao/Unigram-Watermark.\n",
            "\n",
            "735. Title: Emergent Communication with Conversational Repair\n",
            "   Abstract: Training generally capable agents that thoroughly explore their environment\n",
            "and learn new and diverse skills is a long-term goal of robot learning. Quality\n",
            "Diversity Reinforcement Learning (QD-RL) is an emerging research area that\n",
            "blends the best aspects of both fields -- Quality Diversity (QD) provides a\n",
            "principled form of exploration and produces collections of behaviorally diverse\n",
            "agents, while Reinforcement Learning (RL) provides a powerful performance\n",
            "improvement operator enabling generalization across tasks and dynamic\n",
            "environments. Existing QD-RL approaches have been constrained to sample\n",
            "efficient, deterministic off-policy RL algorithms and/or evolution strategies,\n",
            "and struggle with highly stochastic environments. In this work, we, for the\n",
            "first time, adapt on-policy RL, specifically Proximal Policy Optimization\n",
            "(PPO), to the Differentiable Quality Diversity (DQD) framework and propose\n",
            "additional improvements over prior work that enable efficient optimization and\n",
            "discovery of novel skills on challenging locomotion tasks. Our new algorithm,\n",
            "Proximal Policy Gradient Arborescence (PPGA), achieves state-of-the-art\n",
            "results, including a 4x improvement in best reward over baselines on the\n",
            "challenging humanoid domain.\n",
            "\n",
            "736. Title: Neur2RO: Neural Two-Stage Robust Optimization\n",
            "   Abstract: We introduce a deterministic variational formulation for training Bayesian\n",
            "last layer neural networks. This yields a sampling-free, single-pass model and\n",
            "loss that effectively improves uncertainty estimation. Our variational Bayesian\n",
            "last layer (VBLL) can be trained and evaluated with only quadratic complexity\n",
            "in last layer width, and is thus (nearly) computationally free to add to\n",
            "standard architectures. We experimentally investigate VBLLs, and show that they\n",
            "improve predictive accuracy, calibration, and out of distribution detection\n",
            "over baselines across both regression and classification. Finally, we\n",
            "investigate combining VBLL layers with variational Bayesian feature learning,\n",
            "yielding a lower variance collapsed variational inference method for Bayesian\n",
            "neural networks.\n",
            "\n",
            "737. Title: Neural Fine-Tuning Search for Few-Shot Learning\n",
            "   Abstract: The sliced Wasserstein (SW) distances between two probability measures are\n",
            "defined as the expectation of the Wasserstein distance between two\n",
            "one-dimensional projections of the two measures. The randomness comes from a\n",
            "projecting direction that is used to project the two input measures to one\n",
            "dimension. Due to the intractability of the expectation, Monte Carlo\n",
            "integration is performed to estimate the value of the SW distance. Despite\n",
            "having various variants, there has been no prior work that improves the Monte\n",
            "Carlo estimation scheme for the SW distance in terms of controlling its\n",
            "variance. To bridge the literature on variance reduction and the literature on\n",
            "the SW distance, we propose computationally efficient control variates to\n",
            "reduce the variance of the empirical estimation of the SW distance. The key\n",
            "idea is to first find Gaussian approximations of projected one-dimensional\n",
            "measures, then we utilize the closed-form of the Wasserstein-2 distance between\n",
            "two Gaussian distributions to design the control variates. In particular, we\n",
            "propose using a lower bound and an upper bound of the Wasserstein-2 distance\n",
            "between two fitted Gaussians as two computationally efficient control variates.\n",
            "We empirically show that the proposed control variate estimators can help to\n",
            "reduce the variance considerably when comparing measures over images and\n",
            "point-clouds. Finally, we demonstrate the favorable performance of the proposed\n",
            "control variate estimators in gradient flows to interpolate between two\n",
            "point-clouds and in deep generative modeling on standard image datasets, such\n",
            "as CIFAR10 and CelebA.\n",
            "\n",
            "738. Title: Proximal Policy Gradient Arborescence for Quality Diversity Reinforcement Learning\n",
            "   Abstract: Conversational repair is a mechanism used to detect and resolve\n",
            "miscommunication and misinformation problems when two or more agents interact.\n",
            "One particular and underexplored form of repair in emergent communication is\n",
            "the implicit repair mechanism, where the interlocutor purposely conveys the\n",
            "desired information in such a way as to prevent misinformation from any other\n",
            "interlocutor. This work explores how redundancy can modify the emergent\n",
            "communication protocol to continue conveying the necessary information to\n",
            "complete the underlying task, even with additional external environmental\n",
            "pressures such as noise. We focus on extending the signaling game, called the\n",
            "Lewis Game, by adding noise in the communication channel and inputs received by\n",
            "the agents. Our analysis shows that agents add redundancy to the transmitted\n",
            "messages as an outcome to prevent the negative impact of noise on the task\n",
            "success. Additionally, we observe that the emerging communication protocol's\n",
            "generalization capabilities remain equivalent to architectures employed in\n",
            "simpler games that are entirely deterministic. Additionally, our method is the\n",
            "only one suitable for producing robust communication protocols that can handle\n",
            "cases with and without noise while maintaining increased generalization\n",
            "performance levels.\n",
            "\n",
            "739. Title: Harnessing Density Ratios for Online Reinforcement Learning\n",
            "   Abstract: The theories of offline and online reinforcement learning, despite having\n",
            "evolved in parallel, have begun to show signs of the possibility for a\n",
            "unification, with algorithms and analysis techniques for one setting often\n",
            "having natural counterparts in the other. However, the notion of density ratio\n",
            "modeling, an emerging paradigm in offline RL, has been largely absent from\n",
            "online RL, perhaps for good reason: the very existence and boundedness of\n",
            "density ratios relies on access to an exploratory dataset with good coverage,\n",
            "but the core challenge in online RL is to collect such a dataset without having\n",
            "one to start. In this work we show -- perhaps surprisingly -- that density\n",
            "ratio-based algorithms have online counterparts. Assuming only the existence of\n",
            "an exploratory distribution with good coverage, a structural condition known as\n",
            "coverability (Xie et al., 2023), we give a new algorithm (GLOW) that uses\n",
            "density ratio realizability and value function realizability to perform\n",
            "sample-efficient online exploration. GLOW addresses unbounded density ratios\n",
            "via careful use of truncation, and combines this with optimism to guide\n",
            "exploration. GLOW is computationally inefficient; we complement it with a more\n",
            "efficient counterpart, HyGLOW, for the Hybrid RL setting (Song et al., 2022)\n",
            "wherein online RL is augmented with additional offline data. HyGLOW is derived\n",
            "as a special case of a more general meta-algorithm that provides a provable\n",
            "black-box reduction from hybrid RL to offline RL, which may be of independent\n",
            "interest.\n",
            "\n",
            "740. Title: Querying Easily Flip-flopped Samples for Deep Active Learning\n",
            "   Abstract: Performing analytical tasks over graph data has become increasingly\n",
            "interesting due to the ubiquity and large availability of relational\n",
            "information. However, unlike images or sentences, there is no notion of\n",
            "sequence in networks. Nodes (and edges) follow no absolute order, and it is\n",
            "hard for traditional machine learning (ML) algorithms to recognize a pattern\n",
            "and generalize their predictions on this type of data. Graph Neural Networks\n",
            "(GNN) successfully tackled this problem. They became popular after the\n",
            "generalization of the convolution concept to the graph domain. However, they\n",
            "possess a large number of hyperparameters and their design and optimization is\n",
            "currently hand-made, based on heuristics or empirical intuition. Neural\n",
            "Architecture Search (NAS) methods appear as an interesting solution to this\n",
            "problem. In this direction, this paper compares two NAS methods for optimizing\n",
            "GNN: one based on reinforcement learning and a second based on evolutionary\n",
            "algorithms. Results consider 7 datasets over two search spaces and show that\n",
            "both methods obtain similar accuracies to a random search, raising the question\n",
            "of how many of the search space dimensions are actually relevant to the\n",
            "problem.\n",
            "\n",
            "741. Title: Structural Inference with Dynamics Encoding and Partial Correlation Coefficients\n",
            "   Abstract: Active learning is a machine learning paradigm that aims to improve the\n",
            "performance of a model by strategically selecting and querying unlabeled data.\n",
            "One effective selection strategy is to base it on the model's predictive\n",
            "uncertainty, which can be interpreted as a measure of how informative a sample\n",
            "is. The sample's distance to the decision boundary is a natural measure of\n",
            "predictive uncertainty, but it is often intractable to compute, especially for\n",
            "complex decision boundaries formed in multiclass classification tasks. To\n",
            "address this issue, this paper proposes the {\\it least disagree metric} (LDM),\n",
            "defined as the smallest probability of disagreement of the predicted label, and\n",
            "an estimator for LDM proven to be asymptotically consistent under mild\n",
            "assumptions. The estimator is computationally efficient and can be easily\n",
            "implemented for deep learning models using parameter perturbation. The\n",
            "LDM-based active learning is performed by querying unlabeled data with the\n",
            "smallest LDM. Experimental results show that our LDM-based active learning\n",
            "algorithm obtains state-of-the-art overall performance on all considered\n",
            "datasets and deep architectures.\n",
            "\n",
            "742. Title: Modulate Your Spectrum in Self-Supervised Learning\n",
            "   Abstract: In this study, we delve into an emerging optimization challenge involving a\n",
            "black-box objective function that can only be gauged via a ranking oracle-a\n",
            "situation frequently encountered in real-world scenarios, especially when the\n",
            "function is evaluated by human judges. Such challenge is inspired from\n",
            "Reinforcement Learning with Human Feedback (RLHF), an approach recently\n",
            "employed to enhance the performance of Large Language Models (LLMs) using human\n",
            "guidance. We introduce ZO-RankSGD, an innovative zeroth-order optimization\n",
            "algorithm designed to tackle this optimization problem, accompanied by\n",
            "theoretical assurances. Our algorithm utilizes a novel rank-based random\n",
            "estimator to determine the descent direction and guarantees convergence to a\n",
            "stationary point. Moreover, ZO-RankSGD is readily applicable to policy\n",
            "optimization problems in Reinforcement Learning (RL), particularly when only\n",
            "ranking oracles for the episode reward are available. Last but not least, we\n",
            "demonstrate the effectiveness of ZO-RankSGD in a novel application: improving\n",
            "the quality of images generated by a diffusion generative model with human\n",
            "ranking feedback. Throughout experiments, we found that ZO-RankSGD can\n",
            "significantly enhance the detail of generated images with only a few rounds of\n",
            "human feedback. Overall, our work advances the field of zeroth-order\n",
            "optimization by addressing the problem of optimizing functions with only\n",
            "ranking feedback, and offers a new and effective approach for aligning\n",
            "Artificial Intelligence (AI) with human intentions.\n",
            "\n",
            "743. Title: TiC-CLIP: Continual Training of CLIP Models\n",
            "   Abstract: Recent advancements in Large Language Models (LLMs) have attracted\n",
            "considerable interest among researchers to leverage these models to enhance\n",
            "Recommender Systems (RSs). Existing work predominantly utilizes LLMs to\n",
            "generate knowledge-rich texts or utilizes LLM-derived embeddings as features to\n",
            "improve RSs. Although the extensive world knowledge embedded in LLMs generally\n",
            "benefits RSs, the application can only take limited number of users and items\n",
            "as inputs, without adequately exploiting collaborative filtering information.\n",
            "Considering its crucial role in RSs, one key challenge in enhancing RSs with\n",
            "LLMs lies in providing better collaborative filtering information through LLMs.\n",
            "In this paper, drawing inspiration from the in-context learning and chain of\n",
            "thought reasoning in LLMs, we propose the Large Language Models enhanced\n",
            "Collaborative Filtering (LLM-CF) framework, which distils the world knowledge\n",
            "and reasoning capabilities of LLMs into collaborative filtering. We also\n",
            "explored a concise and efficient instruction-tuning method, which improves the\n",
            "recommendation capabilities of LLMs while preserving their general\n",
            "functionalities (e.g., not decreasing on the LLM benchmark). Comprehensive\n",
            "experiments on three real-world datasets demonstrate that LLM-CF significantly\n",
            "enhances several backbone recommendation models and consistently outperforms\n",
            "competitive baselines, showcasing its effectiveness in distilling the world\n",
            "knowledge and reasoning capabilities of LLM into collaborative filtering.\n",
            "\n",
            "744. Title: Discovering Failure Modes of Text-guided Diffusion Models via Adversarial Search\n",
            "   Abstract: Submodular functions have applications throughout machine learning, but in\n",
            "many settings, we do not have direct access to the underlying function $f$. We\n",
            "focus on stochastic functions that are given as an expectation of functions\n",
            "over a distribution $P$. In practice, we often have only a limited set of\n",
            "samples $f_i$ from $P$. The standard approach indirectly optimizes $f$ by\n",
            "maximizing the sum of $f_i$. However, this ignores generalization to the true\n",
            "(unknown) distribution. In this paper, we achieve better performance on the\n",
            "actual underlying function $f$ by directly optimizing a combination of bias and\n",
            "variance. Algorithmically, we accomplish this by showing how to carry out\n",
            "distributionally robust optimization (DRO) for submodular functions, providing\n",
            "efficient algorithms backed by theoretical guarantees which leverage several\n",
            "novel contributions to the general theory of DRO. We also show compelling\n",
            "empirical evidence that DRO improves generalization to the unknown stochastic\n",
            "submodular function.\n",
            "\n",
            "745. Title: Test-time Adaptation against Multi-modal Reliability Bias\n",
            "   Abstract: Whitening loss offers a theoretical guarantee against feature collapse in\n",
            "self-supervised learning (SSL) with joint embedding architectures. Typically,\n",
            "it involves a hard whitening approach, transforming the embedding and applying\n",
            "loss to the whitened output. In this work, we introduce Spectral Transformation\n",
            "(ST), a framework to modulate the spectrum of embedding and to seek for\n",
            "functions beyond whitening that can avoid dimensional collapse. We show that\n",
            "whitening is a special instance of ST by definition, and our empirical\n",
            "investigations unveil other ST instances capable of preventing collapse.\n",
            "Additionally, we propose a novel ST instance named IterNorm with trace loss\n",
            "(INTL). Theoretical analysis confirms INTL's efficacy in preventing collapse\n",
            "and modulating the spectrum of embedding toward equal-eigenvalues during\n",
            "optimization. Our experiments on ImageNet classification and COCO object\n",
            "detection demonstrate INTL's potential in learning superior representations.\n",
            "The code is available at https://github.com/winci-ai/INTL.\n",
            "\n",
            "746. Title: Feature-aligned N-BEATS with Sinkhorn divergence\n",
            "   Abstract: Recent studies show that vision models pre-trained in generic visual learning\n",
            "tasks with large-scale data can provide useful feature representations for a\n",
            "wide range of visual perception problems. However, few attempts have been made\n",
            "to exploit pre-trained foundation models in visual place recognition (VPR). Due\n",
            "to the inherent difference in training objectives and data between the tasks of\n",
            "model pre-training and VPR, how to bridge the gap and fully unleash the\n",
            "capability of pre-trained models for VPR is still a key issue to address. To\n",
            "this end, we propose a novel method to realize seamless adaptation of\n",
            "pre-trained models for VPR. Specifically, to obtain both global and local\n",
            "features that focus on salient landmarks for discriminating places, we design a\n",
            "hybrid adaptation method to achieve both global and local adaptation\n",
            "efficiently, in which only lightweight adapters are tuned without adjusting the\n",
            "pre-trained model. Besides, to guide effective adaptation, we propose a mutual\n",
            "nearest neighbor local feature loss, which ensures proper dense local features\n",
            "are produced for local matching and avoids time-consuming spatial verification\n",
            "in re-ranking. Experimental results show that our method outperforms the\n",
            "state-of-the-art methods with less training data and training time, and uses\n",
            "about only 3% retrieval runtime of the two-stage VPR methods with RANSAC-based\n",
            "spatial verification. It ranks 1st on the MSLS challenge leaderboard (at the\n",
            "time of submission). The code is released at\n",
            "https://github.com/Lu-Feng/SelaVPR.\n",
            "\n",
            "747. Title: Fast Hyperboloid Decision Tree Algorithms\n",
            "   Abstract: We derive and systematically analyze scalar glueball correlation functions in\n",
            "both the hard-wall and dilaton soft-wall approximations to holographic QCD. The\n",
            "dynamical content of the holographic correlators is uncovered by examining\n",
            "their spectral density and by relating them to the operator product expansion,\n",
            "a dilatational low-energy theorem and a recently suggested two-dimensional\n",
            "power correction associated with the short-distance behavior of the heavy-quark\n",
            "potential. This approach provides holographic estimates for the three\n",
            "lowest-dimensional gluon condensates or alternatively their Wilson\n",
            "coefficients, the two leading moments of the instanton size distribution in the\n",
            "QCD vacuum and an effective UV gluon mass. A remarkable complementarity between\n",
            "the nonperturbative physics of the hard- and soft-wall correlators emerges, and\n",
            "their ability to describe detailed QCD results can be assessed quantitatively.\n",
            "We further provide the first holographic estimates for the decay constants of\n",
            "the 0++ glueball and its excitations. The hard-wall background turns out to\n",
            "encode more of the relevant QCD physics, and its prediction f ~ 0.8-0.9 GeV for\n",
            "the phenomenologically important ground state decay constant agrees inside\n",
            "errors with recent QCD sum rule and lattice results.\n",
            "\n",
            "748. Title: Distributionally Robust Optimization with Bias and Variance Reduction\n",
            "   Abstract: Text-guided diffusion models (TDMs) are widely applied but can fail\n",
            "unexpectedly. Common failures include: (i) natural-looking text prompts\n",
            "generating images with the wrong content, or (ii) different random samples of\n",
            "the latent variables that generate vastly different, and even unrelated,\n",
            "outputs despite being conditioned on the same text prompt. In this work, we aim\n",
            "to study and understand the failure modes of TDMs in more detail. To achieve\n",
            "this, we propose SAGE, the first adversarial search method on TDMs that\n",
            "systematically explores the discrete prompt space and the high-dimensional\n",
            "latent space, to automatically discover undesirable behaviors and failure cases\n",
            "in image generation. We use image classifiers as surrogate loss functions\n",
            "during searching, and employ human inspections to validate the identified\n",
            "failures. For the first time, our method enables efficient exploration of both\n",
            "the discrete and intricate human language space and the challenging latent\n",
            "space, overcoming the gradient vanishing problem. Then, we demonstrate the\n",
            "effectiveness of SAGE on five widely used generative models and reveal four\n",
            "typical failure modes: (1) We find a variety of natural text prompts that\n",
            "generate images failing to capture the semantics of input texts. We further\n",
            "discuss the underlying causes and potential solutions based on the results. (2)\n",
            "We find regions in the latent space that lead to distorted images independent\n",
            "of the text prompt, suggesting that parts of the latent space are not\n",
            "well-structured. (3) We also find latent samples that result in natural-looking\n",
            "images unrelated to the text prompt, implying a possible misalignment between\n",
            "the latent and prompt spaces. (4) By appending a single adversarial token\n",
            "embedding to any input prompts, we can generate a variety of specified target\n",
            "objects. Project page: https://sage-diffusion.github.io/\n",
            "\n",
            "749. Title: Zeroth-Order Optimization Meets Human Feedback: Provable Learning via Ranking Oracles\n",
            "   Abstract: Recent efforts in fine-tuning language models often rely on automatic data\n",
            "selection, commonly using Nearest Neighbors retrieval from large datasets.\n",
            "However, we theoretically show that this approach tends to select redundant\n",
            "data, limiting its effectiveness or even hurting performance. To address this,\n",
            "we introduce SIFT, a data selection algorithm designed to reduce uncertainty\n",
            "about the model's response given a prompt, which unifies ideas from retrieval\n",
            "and active learning. Whereas Nearest Neighbor retrieval typically fails in the\n",
            "presence of information duplication, SIFT accounts for information duplication\n",
            "and optimizes the overall information gain of the selected examples. We focus\n",
            "our evaluations on fine-tuning at test-time for prompt-specific language\n",
            "modeling on the Pile dataset, and show that SIFT consistently outperforms\n",
            "Nearest Neighbor retrieval, with minimal computational overhead. Moreover, we\n",
            "show that our uncertainty estimates can predict the performance gain of\n",
            "test-time fine-tuning, and use this to develop an adaptive algorithm that\n",
            "invests test-time compute proportional to realized performance gains. We\n",
            "provide the $\\texttt{activeft}$ (Active Fine-Tuning) library which can be used\n",
            "as a drop-in replacement for Nearest Neighbor retrieval.\n",
            "\n",
            "750. Title: Towards Seamless Adaptation of Pre-trained Models for Visual Place Recognition\n",
            "   Abstract: Hyperbolic geometry is gaining traction in machine learning for its\n",
            "effectiveness at capturing hierarchical structures in real-world data.\n",
            "Hyperbolic spaces, where neighborhoods grow exponentially, offer substantial\n",
            "advantages and consistently deliver state-of-the-art results across diverse\n",
            "applications. However, hyperbolic classifiers often grapple with computational\n",
            "challenges. Methods reliant on Riemannian optimization frequently exhibit\n",
            "sluggishness, stemming from the increased computational demands of operations\n",
            "on Riemannian manifolds. In response to these challenges, we present hyperDT, a\n",
            "novel extension of decision tree algorithms into hyperbolic space. Crucially,\n",
            "hyperDT eliminates the need for computationally intensive Riemannian\n",
            "optimization, numerically unstable exponential and logarithmic maps, or\n",
            "pairwise comparisons between points by leveraging inner products to adapt\n",
            "Euclidean decision tree algorithms to hyperbolic space. Our approach is\n",
            "conceptually straightforward and maintains constant-time decision complexity\n",
            "while mitigating the scalability issues inherent in high-dimensional Euclidean\n",
            "spaces. Building upon hyperDT we introduce hyperRF, a hyperbolic random forest\n",
            "model. Extensive benchmarking across diverse datasets underscores the superior\n",
            "performance of these models, providing a swift, precise, accurate, and\n",
            "user-friendly toolkit for hyperbolic data analysis.\n",
            "\n",
            "751. Title: Enhancing Human-AI Collaboration Through Logic-Guided Reasoning\n",
            "   Abstract: We propose Feature-aligned N-BEATS as a domain-generalized time series\n",
            "forecasting model. It is a nontrivial extension of N-BEATS with doubly residual\n",
            "stacking principle (Oreshkin et al. [45]) into a representation learning\n",
            "framework. In particular, it revolves around marginal feature probability\n",
            "measures induced by the intricate composition of residual and feature\n",
            "extracting operators of N-BEATS in each stack and aligns them stack-wise via an\n",
            "approximate of an optimal transport distance referred to as the Sinkhorn\n",
            "divergence. The training loss consists of an empirical risk minimization from\n",
            "multiple source domains, i.e., forecasting loss, and an alignment loss\n",
            "calculated with the Sinkhorn divergence, which allows the model to learn\n",
            "invariant features stack-wise across multiple source data sequences while\n",
            "retaining N-BEATS's interpretable design and forecasting power. Comprehensive\n",
            "experimental evaluations with ablation studies are provided and the\n",
            "corresponding results demonstrate the proposed model's forecasting and\n",
            "generalization capabilities.\n",
            "\n",
            "752. Title: Momentum Benefits Non-iid Federated Learning Simply and Provably\n",
            "   Abstract: Long-range time series forecasting is usually based on one of two existing\n",
            "forecasting strategies: Direct Forecasting and Iterative Forecasting, where the\n",
            "former provides low bias, high variance forecasts and the latter leads to low\n",
            "variance, high bias forecasts. In this paper, we propose a new forecasting\n",
            "strategy called Generative Forecasting (GenF), which generates synthetic data\n",
            "for the next few time steps and then makes long-range forecasts based on\n",
            "generated and observed data. We theoretically prove that GenF is able to better\n",
            "balance the forecasting variance and bias, leading to a much smaller\n",
            "forecasting error. We implement GenF via three components: (i) a novel\n",
            "conditional Wasserstein Generative Adversarial Network (GAN) based generator\n",
            "for synthetic time series data generation, called CWGAN-TS. (ii) a transformer\n",
            "based predictor, which makes long-range predictions using both generated and\n",
            "observed data. (iii) an information theoretic clustering algorithm to improve\n",
            "the training of both the CWGAN-TS and the transformer based predictor. The\n",
            "experimental results on five public datasets demonstrate that GenF\n",
            "significantly outperforms a diverse range of state-of-the-art benchmarks and\n",
            "classical approaches. Specifically, we find a 5% - 11% improvement in\n",
            "predictive performance (mean absolute error) while having a 15% - 50% reduction\n",
            "in parameters compared to the benchmarks. Lastly, we conduct an ablation study\n",
            "to further explore and demonstrate the effectiveness of the components\n",
            "comprising GenF.\n",
            "\n",
            "753. Title: Patched Denoising Diffusion Models For High-Resolution Image Synthesis\n",
            "   Abstract: Deep Metric Learning (DML) has long attracted the attention of the machine\n",
            "learning community as a key objective. Existing solutions concentrate on\n",
            "fine-tuning the pre-trained models on conventional image datasets. As a result\n",
            "of the success of recent pre-trained models trained from larger-scale datasets,\n",
            "it is challenging to adapt the model to the DML tasks in the local data domain\n",
            "while retaining the previously gained knowledge. In this paper, we investigate\n",
            "parameter-efficient methods for fine-tuning the pre-trained model for DML\n",
            "tasks. In particular, we propose a novel and effective framework based on\n",
            "learning Visual Prompts (VPT) in the pre-trained Vision Transformers (ViT).\n",
            "Based on the conventional proxy-based DML paradigm, we augment the proxy by\n",
            "incorporating the semantic information from the input image and the ViT, in\n",
            "which we optimize the visual prompts for each class. We demonstrate that our\n",
            "new approximations with semantic information are superior to representative\n",
            "capabilities, thereby improving metric learning performance. We conduct\n",
            "extensive experiments to demonstrate that our proposed framework is effective\n",
            "and efficient by evaluating popular DML benchmarks. In particular, we\n",
            "demonstrate that our fine-tuning method achieves comparable or even better\n",
            "performance than recent state-of-the-art full fine-tuning works of DML while\n",
            "tuning only a small percentage of total parameters.\n",
            "\n",
            "754. Title: Overthinking the Truth: Understanding how Language Models Process False Demonstrations\n",
            "   Abstract: Federated learning is a powerful paradigm for large-scale machine learning,\n",
            "but it faces significant challenges due to unreliable network connections, slow\n",
            "communication, and substantial data heterogeneity across clients. FedAvg and\n",
            "SCAFFOLD are two prominent algorithms to address these challenges. In\n",
            "particular, FedAvg employs multiple local updates before communicating with a\n",
            "central server, while SCAFFOLD maintains a control variable on each client to\n",
            "compensate for ``client drift'' in its local updates. Various methods have been\n",
            "proposed to enhance the convergence of these two algorithms, but they either\n",
            "make impractical adjustments to the algorithmic structure or rely on the\n",
            "assumption of bounded data heterogeneity.\n",
            "  This paper explores the utilization of momentum to enhance the performance of\n",
            "FedAvg and SCAFFOLD. When all clients participate in the training process, we\n",
            "demonstrate that incorporating momentum allows FedAvg to converge without\n",
            "relying on the assumption of bounded data heterogeneity even using a constant\n",
            "local learning rate. This is novel and fairly surprising as existing analyses\n",
            "for FedAvg require bounded data heterogeneity even with diminishing local\n",
            "learning rates. In partial client participation, we show that momentum enables\n",
            "SCAFFOLD to converge provably faster without imposing any additional\n",
            "assumptions. Furthermore, we use momentum to develop new variance-reduced\n",
            "extensions of FedAvg and SCAFFOLD, which exhibit state-of-the-art convergence\n",
            "rates. Our experimental results support all theoretical findings.\n",
            "\n",
            "755. Title: On the Foundations of Shortcut Learning\n",
            "   Abstract: Proximal causal learning is a promising framework for identifying the causal\n",
            "effect under the existence of unmeasured confounders. Within this framework,\n",
            "the doubly robust (DR) estimator was derived and has shown its effectiveness in\n",
            "estimation, especially when the model assumption is violated. However, the\n",
            "current form of the DR estimator is restricted to binary treatments, while the\n",
            "treatment can be continuous in many real-world applications. The primary\n",
            "obstacle to continuous treatments resides in the delta function present in the\n",
            "original DR estimator, making it infeasible in causal effect estimation and\n",
            "introducing a heavy computational burden in nuisance function estimation. To\n",
            "address these challenges, we propose a kernel-based DR estimator that can well\n",
            "handle continuous treatments. Equipped with its smoothness, we show that its\n",
            "oracle form is a consistent approximation of the influence function. Further,\n",
            "we propose a new approach to efficiently solve the nuisance functions. We then\n",
            "provide a comprehensive convergence analysis in terms of the mean square error.\n",
            "We demonstrate the utility of our estimator on synthetic datasets and\n",
            "real-world applications.\n",
            "\n",
            "756. Title: Doubly Robust Proximal Causal Learning for Continuous Treatments\n",
            "   Abstract: In the ever-evolving adversarial machine learning landscape, developing\n",
            "effective defenses against patch attacks has become a critical challenge,\n",
            "necessitating reliable solutions to safeguard real-world AI systems. Although\n",
            "diffusion models have shown remarkable capacity in image synthesis and have\n",
            "been recently utilized to counter $\\ell_p$-norm bounded attacks, their\n",
            "potential in mitigating localized patch attacks remains largely underexplored.\n",
            "In this work, we propose DiffPAD, a novel framework that harnesses the power of\n",
            "diffusion models for adversarial patch decontamination. DiffPAD first performs\n",
            "super-resolution restoration on downsampled input images, then adopts\n",
            "binarization, dynamic thresholding scheme and sliding window for effective\n",
            "localization of adversarial patches. Such a design is inspired by the\n",
            "theoretically derived correlation between patch size and diffusion restoration\n",
            "error that is generalized across diverse patch attack scenarios. Finally,\n",
            "DiffPAD applies inpainting techniques to the original input images with the\n",
            "estimated patch region being masked. By integrating closed-form solutions for\n",
            "super-resolution restoration and image inpainting into the conditional reverse\n",
            "sampling process of a pre-trained diffusion model, DiffPAD obviates the need\n",
            "for text guidance or fine-tuning. Through comprehensive experiments, we\n",
            "demonstrate that DiffPAD not only achieves state-of-the-art adversarial\n",
            "robustness against patch attacks but also excels in recovering naturalistic\n",
            "images without patch remnants. The source code is available at\n",
            "https://github.com/JasonFu1998/DiffPAD.\n",
            "\n",
            "757. Title: NOLA: Compressing LoRA using Linear Combination of Random Basis\n",
            "   Abstract: Message-passing graph neural networks (MPNNs) have emerged as a powerful\n",
            "paradigm for graph-based machine learning. Despite their effectiveness, MPNNs\n",
            "face challenges such as under-reaching and over-squashing, where limited\n",
            "receptive fields and structural bottlenecks hinder information flow in the\n",
            "graph. While graph transformers hold promise in addressing these issues, their\n",
            "scalability is limited due to quadratic complexity regarding the number of\n",
            "nodes, rendering them impractical for larger graphs. Here, we propose\n",
            "implicitly rewired message-passing neural networks (IPR-MPNNs), a novel\n",
            "approach that integrates implicit probabilistic graph rewiring into MPNNs. By\n",
            "introducing a small number of virtual nodes, i.e., adding additional nodes to a\n",
            "given graph and connecting them to existing nodes, in a differentiable,\n",
            "end-to-end manner, IPR-MPNNs enable long-distance message propagation,\n",
            "circumventing quadratic complexity. Theoretically, we demonstrate that\n",
            "IPR-MPNNs surpass the expressiveness of traditional MPNNs. Empirically, we\n",
            "validate our approach by showcasing its ability to mitigate under-reaching and\n",
            "over-squashing effects, achieving state-of-the-art performance across multiple\n",
            "graph datasets. Notably, IPR-MPNNs outperform graph transformers while\n",
            "maintaining significantly faster computational efficiency.\n",
            "\n",
            "758. Title: Mol-Instructions: A Large-Scale Biomolecular Instruction Dataset for Large Language Models\n",
            "   Abstract: For robots operating in the real world, it is desirable to learn reusable\n",
            "behaviours that can effectively be transferred and adapted to numerous tasks\n",
            "and scenarios. We propose an approach to learn abstract motor skills from data\n",
            "using a hierarchical mixture latent variable model. In contrast to existing\n",
            "work, our method exploits a three-level hierarchy of both discrete and\n",
            "continuous latent variables, to capture a set of high-level behaviours while\n",
            "allowing for variance in how they are executed. We demonstrate in manipulation\n",
            "domains that the method can effectively cluster offline data into distinct,\n",
            "executable behaviours, while retaining the flexibility of a continuous latent\n",
            "variable model. The resulting skills can be transferred and fine-tuned on new\n",
            "tasks, unseen objects, and from state to vision-based policies, yielding better\n",
            "sample efficiency and asymptotic performance compared to existing skill- and\n",
            "imitation-based methods. We further analyse how and when the skills are most\n",
            "beneficial: they encourage directed exploration to cover large regions of the\n",
            "state space relevant to the task, making them most effective in challenging\n",
            "sparse-reward settings.\n",
            "\n",
            "759. Title: Generative Adversarial Equilibrium Solvers\n",
            "   Abstract: Modern language models can imitate complex patterns through few-shot\n",
            "learning, enabling them to complete challenging tasks without fine-tuning.\n",
            "However, imitation can also lead models to reproduce inaccuracies or harmful\n",
            "content if present in the context. We study harmful imitation through the lens\n",
            "of a model's internal representations, and identify two related phenomena:\n",
            "\"overthinking\" and \"false induction heads\". The first phenomenon, overthinking,\n",
            "appears when we decode predictions from intermediate layers, given correct vs.\n",
            "incorrect few-shot demonstrations. At early layers, both demonstrations induce\n",
            "similar model behavior, but the behavior diverges sharply at some \"critical\n",
            "layer\", after which the accuracy given incorrect demonstrations progressively\n",
            "decreases. The second phenomenon, false induction heads, are a possible\n",
            "mechanistic cause of overthinking: these are heads in late layers that attend\n",
            "to and copy false information from previous demonstrations, and whose ablation\n",
            "reduces overthinking. Beyond scientific understanding, our results suggest that\n",
            "studying intermediate model computations could be a promising avenue for\n",
            "understanding and guarding against harmful model behaviors.\n",
            "\n",
            "760. Title: Meta Continual Learning Revisited: Implicitly Enhancing Online Hessian Approximation via Variance Reduction\n",
            "   Abstract: Machine learning models have been found to learn shortcuts -- unintended\n",
            "decision rules that are unable to generalize -- undermining models'\n",
            "reliability. Previous works address this problem under the tenuous assumption\n",
            "that only a single shortcut exists in the training data. Real-world images are\n",
            "rife with multiple visual cues from background to texture. Key to advancing the\n",
            "reliability of vision systems is understanding whether existing methods can\n",
            "overcome multiple shortcuts or struggle in a Whac-A-Mole game, i.e., where\n",
            "mitigating one shortcut amplifies reliance on others. To address this\n",
            "shortcoming, we propose two benchmarks: 1) UrbanCars, a dataset with precisely\n",
            "controlled spurious cues, and 2) ImageNet-W, an evaluation set based on\n",
            "ImageNet for watermark, a shortcut we discovered affects nearly every modern\n",
            "vision model. Along with texture and background, ImageNet-W allows us to study\n",
            "multiple shortcuts emerging from training on natural images. We find computer\n",
            "vision models, including large foundation models -- regardless of training set,\n",
            "architecture, and supervision -- struggle when multiple shortcuts are present.\n",
            "Even methods explicitly designed to combat shortcuts struggle in a Whac-A-Mole\n",
            "dilemma. To tackle this challenge, we propose Last Layer Ensemble, a\n",
            "simple-yet-effective method to mitigate multiple shortcuts without Whac-A-Mole\n",
            "behavior. Our results surface multi-shortcut mitigation as an overlooked\n",
            "challenge critical to advancing the reliability of vision systems. The datasets\n",
            "and code are released: https://github.com/facebookresearch/Whac-A-Mole.\n",
            "\n",
            "761. Title: LILO: Learning Interpretable Libraries by Compressing and Documenting Code\n",
            "   Abstract: This paper investigates different vector step-size adaptation approaches for\n",
            "non-stationary online, continual prediction problems. Vanilla stochastic\n",
            "gradient descent can be considerably improved by scaling the update with a\n",
            "vector of appropriately chosen step-sizes. Many methods, including AdaGrad,\n",
            "RMSProp, and AMSGrad, keep statistics about the learning process to approximate\n",
            "a second order update---a vector approximation of the inverse Hessian. Another\n",
            "family of approaches use meta-gradient descent to adapt the step-size\n",
            "parameters to minimize prediction error. These meta-descent strategies are\n",
            "promising for non-stationary problems, but have not been as extensively\n",
            "explored as quasi-second order methods. We first derive a general, incremental\n",
            "meta-descent algorithm, called AdaGain, designed to be applicable to a much\n",
            "broader range of algorithms, including those with semi-gradient updates or even\n",
            "those with accelerations, such as RMSProp. We provide an empirical comparison\n",
            "of methods from both families. We conclude that methods from both families can\n",
            "perform well, but in non-stationary prediction problems the meta-descent\n",
            "methods exhibit advantages. Our method is particularly robust across several\n",
            "prediction problems, and is competitive with the state-of-the-art method on a\n",
            "large-scale, time-series prediction problem on real data from a mobile robot.\n",
            "\n",
            "762. Title: Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models\n",
            "   Abstract: We introduce the use of generative adversarial learning to compute equilibria\n",
            "in general game-theoretic settings, specifically the generalized Nash\n",
            "equilibrium (GNE) in pseudo-games, and its specific instantiation as the\n",
            "competitive equilibrium (CE) in Arrow-Debreu competitive economies.\n",
            "Pseudo-games are a generalization of games in which players' actions affect not\n",
            "only the payoffs of other players but also their feasible action spaces.\n",
            "Although the computation of GNE and CE is intractable in the worst-case, i.e.,\n",
            "PPAD-hard, in practice, many applications only require solutions with high\n",
            "accuracy in expectation over a distribution of problem instances. We introduce\n",
            "Generative Adversarial Equilibrium Solvers (GAES): a family of generative\n",
            "adversarial neural networks that can learn GNE and CE from only a sample of\n",
            "problem instances. We provide computational and sample complexity bounds, and\n",
            "apply the framework to finding Nash equilibria in normal-form games, CE in\n",
            "Arrow-Debreu competitive economies, and GNE in an environmental economic model\n",
            "of the Kyoto mechanism.\n",
            "\n",
            "763. Title: Large Content And Behavior Models To Understand, Simulate, And Optimize Content And Behavior\n",
            "   Abstract: Pre-training on large-scale datasets and then fine-tuning on downstream tasks\n",
            "have become a standard practice in deep learning. However, pre-training data\n",
            "often contain label noise that may adversely affect the generalization of the\n",
            "model. This paper aims to understand the nature of noise in pre-training\n",
            "datasets and to mitigate its impact on downstream tasks. More specifically,\n",
            "through extensive experiments of supervised pre-training models on synthetic\n",
            "noisy ImageNet-1K and YFCC15M datasets, we demonstrate that while slight noise\n",
            "in pre-training can benefit in-domain (ID) transfer performance, where the\n",
            "training and testing data share the same distribution, it always deteriorates\n",
            "out-of-domain (OOD) performance, where training and testing data distribution\n",
            "are different. We empirically verify that the reason behind is noise in\n",
            "pre-training shapes the feature space differently. We then propose a\n",
            "light-weight black-box tuning method (NMTune) to affine the feature space to\n",
            "mitigate the malignant effect of noise and improve generalization on both ID\n",
            "and OOD tasks, considering one may not be able to fully fine-tune or even\n",
            "access the pre-trained models. We conduct practical experiments on popular\n",
            "vision and language models that are pre-trained on noisy data for evaluation of\n",
            "our approach. Our analysis and results show the importance of this interesting\n",
            "and novel research direction, which we term Noisy Model Learning.\n",
            "\n",
            "764. Title: TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series\n",
            "   Abstract: Recent advances in self-supervised learning have highlighted the efficacy of\n",
            "data augmentation in learning data representation from unlabeled data. Training\n",
            "a linear model atop these enhanced representations can yield an adept\n",
            "classifier. Despite the remarkable empirical performance, the underlying\n",
            "mechanisms that enable data augmentation to unravel nonlinear data structures\n",
            "into linearly separable representations remain elusive. This paper seeks to\n",
            "bridge this gap by investigating under what conditions learned representations\n",
            "can linearly separate manifolds when data is drawn from a multi-manifold model.\n",
            "Our investigation reveals that data augmentation offers additional information\n",
            "beyond observed data and can thus improve the information-theoretic optimal\n",
            "rate of linear separation capacity. In particular, we show that self-supervised\n",
            "learning can linearly separate manifolds with a smaller distance than\n",
            "unsupervised learning, underscoring the additional benefits of data\n",
            "augmentation. Our theoretical analysis further underscores that the performance\n",
            "of downstream linear classifiers primarily hinges on the linear separability of\n",
            "data representations rather than the size of the labeled data set, reaffirming\n",
            "the viability of constructing efficient classifiers with limited labeled data\n",
            "amid an expansive unlabeled data set.\n",
            "\n",
            "765. Title: Towards Reliable and Efficient Backdoor Trigger Inversion via Decoupling Benign Features\n",
            "   Abstract: Recently there has been a rising interest in training agents, embodied in\n",
            "virtual environments, to perform language-directed tasks by deep reinforcement\n",
            "learning. In this paper, we propose a simple but effective neural language\n",
            "grounding module for embodied agents that can be trained end to end from\n",
            "scratch taking raw pixels, unstructured linguistic commands, and sparse rewards\n",
            "as the inputs. We model the language grounding process as a language-guided\n",
            "transformation of visual features, where latent sentence embeddings are used as\n",
            "the transformation matrices. In several language-directed navigation tasks that\n",
            "feature challenging partial observability and require simple reasoning, our\n",
            "module significantly outperforms the state of the art. We also release\n",
            "XWorld3D, an easy-to-customize 3D environment that can potentially be modified\n",
            "to evaluate a variety of embodied agents.\n",
            "\n",
            "766. Title: Multi-task Learning with 3D-Aware Regularization\n",
            "   Abstract: Shannon and Weaver's seminal information theory divides communication into\n",
            "three levels: technical, semantic, and effectiveness. While the technical level\n",
            "deals with the accurate reconstruction of transmitted symbols, the semantic and\n",
            "effectiveness levels deal with the inferred meaning and its effect on the\n",
            "receiver. Large Language Models (LLMs), with their wide generalizability, make\n",
            "some progress towards the second level. However, LLMs and other communication\n",
            "models are not conventionally designed for predicting and optimizing\n",
            "communication for desired receiver behaviors and intents. As a result, the\n",
            "effectiveness level remains largely untouched by modern communication systems.\n",
            "In this paper, we introduce the receivers' \"behavior tokens,\" such as shares,\n",
            "likes, clicks, purchases, and retweets, in the LLM's training corpora to\n",
            "optimize content for the receivers and predict their behaviors. Other than\n",
            "showing similar performance to LLMs on content understanding tasks, our trained\n",
            "models show generalization capabilities on the behavior dimension for behavior\n",
            "simulation, content simulation, behavior understanding, and behavior domain\n",
            "adaptation. We show results on all these capabilities using a wide range of\n",
            "tasks on three corpora. We call these models Large Content and Behavior Models\n",
            "(LCBMs). Further, to spur more research on LCBMs, we release our new Content\n",
            "Behavior Corpus (CBC), a repository containing communicator, message, and\n",
            "corresponding receiver behavior (https://behavior-in-the-wild.github.io/LCBM).\n",
            "\n",
            "767. Title: FFB: A Fair Fairness Benchmark for In-Processing Group Fairness Methods\n",
            "   Abstract: Bayesian neural networks (BNNs) offer uncertainty quantification but come\n",
            "with the downside of substantially increased training and inference costs.\n",
            "Sparse BNNs have been investigated for efficient inference, typically by either\n",
            "slowly introducing sparsity throughout the training or by post-training\n",
            "compression of dense BNNs. The dilemma of how to cut down massive training\n",
            "costs remains, particularly given the requirement to learn about the\n",
            "uncertainty. To solve this challenge, we introduce Sparse Subspace Variational\n",
            "Inference (SSVI), the first fully sparse BNN framework that maintains a\n",
            "consistently highly sparse Bayesian model throughout the training and inference\n",
            "phases. Starting from a randomly initialized low-dimensional sparse subspace,\n",
            "our approach alternately optimizes the sparse subspace basis selection and its\n",
            "associated parameters. While basis selection is characterized as a\n",
            "non-differentiable problem, we approximate the optimal solution with a\n",
            "removal-and-addition strategy, guided by novel criteria based on weight\n",
            "distribution statistics. Our extensive experiments show that SSVI sets new\n",
            "benchmarks in crafting sparse BNNs, achieving, for instance, a 10-20x\n",
            "compression in model size with under 3\\% performance drop, and up to 20x FLOPs\n",
            "reduction during training compared with dense VI training. Remarkably, SSVI\n",
            "also demonstrates enhanced robustness to hyperparameters, reducing the need for\n",
            "intricate tuning in VI and occasionally even surpassing VI-trained dense BNNs\n",
            "on both accuracy and uncertainty metrics.\n",
            "\n",
            "768. Title: ODEFormer: Symbolic Regression of Dynamical Systems with Transformers\n",
            "   Abstract: The introduction of robust optimisation has pushed the state-of-the-art in\n",
            "defending against adversarial attacks. Notably, the state-of-the-art projected\n",
            "gradient descent (PGD)-based training method has been shown to be universally\n",
            "and reliably effective in defending against adversarial inputs. This robustness\n",
            "approach uses PGD as a reliable and universal \"first-order adversary\". However,\n",
            "the behaviour of such optimisation has not been studied in the light of a\n",
            "fundamentally different class of attacks called backdoors. In this paper, we\n",
            "study how to inject and defend against backdoor attacks for robust models\n",
            "trained using PGD-based robust optimisation. We demonstrate that these models\n",
            "are susceptible to backdoor attacks. Subsequently, we observe that backdoors\n",
            "are reflected in the feature representation of such models. Then, this\n",
            "observation is leveraged to detect such backdoor-infected models via a\n",
            "detection technique called AEGIS. Specifically, given a robust Deep Neural\n",
            "Network (DNN) that is trained using PGD-based first-order adversarial training\n",
            "approach, AEGIS uses feature clustering to effectively detect whether such DNNs\n",
            "are backdoor-infected or clean.\n",
            "  In our evaluation of several visible and hidden backdoor triggers on major\n",
            "classification tasks using CIFAR-10, MNIST and FMNIST datasets, AEGIS\n",
            "effectively detects PGD-trained robust DNNs infected with backdoors. AEGIS\n",
            "detects such backdoor-infected models with 91.6% accuracy (11 out of 12 tested\n",
            "models), without any false positives. Furthermore, AEGIS detects the targeted\n",
            "class in the backdoor-infected model with a reasonably low (11.1%) false\n",
            "positive rate. Our investigation reveals that salient features of adversarially\n",
            "robust DNNs could be promising to break the stealthy nature of backdoor\n",
            "attacks.\n",
            "\n",
            "769. Title: Sin3DM: Learning a Diffusion Model from a Single 3D Textured Shape\n",
            "   Abstract: Information extraction tasks require both accurate, efficient, and\n",
            "generalisable models. Classical supervised deep learning approaches can achieve\n",
            "the required performance, but they need large datasets and are limited in their\n",
            "ability to adapt to different tasks. On the other hand, large language models\n",
            "(LLMs) demonstrate good generalization, meaning that they can adapt to many\n",
            "different tasks based on user requests. However, LLMs are computationally\n",
            "expensive and tend to fail to generate structured outputs. In this article, we\n",
            "will introduce a new kind of GLiNER model that can be used for various\n",
            "information extraction tasks while being a small encoder model. Our model\n",
            "achieved SoTA performance on zero-shot NER benchmarks and leading performance\n",
            "on question-answering, summarization and relation extraction tasks.\n",
            "Additionally, in this article, we will cover experimental results on\n",
            "self-learning approaches for named entity recognition using GLiNER models.\n",
            "\n",
            "770. Title: Provable Benefits of Multi-task RL under Non-Markovian Decision Making Processes\n",
            "   Abstract: With the development of large language models (LLMs), striking a balance\n",
            "between the performance and safety of AI systems has never been more critical.\n",
            "However, the inherent tension between the objectives of helpfulness and\n",
            "harmlessness presents a significant challenge during LLM training. To address\n",
            "this issue, we propose Safe Reinforcement Learning from Human Feedback (Safe\n",
            "RLHF), a novel algorithm for human value alignment. Safe RLHF explicitly\n",
            "decouples human preferences regarding helpfulness and harmlessness, effectively\n",
            "avoiding the crowdworkers' confusion about the tension and allowing us to train\n",
            "separate reward and cost models. We formalize the safety concern of LLMs as an\n",
            "optimization task of maximizing the reward function while satisfying specified\n",
            "cost constraints. Leveraging the Lagrangian method to solve this constrained\n",
            "problem, Safe RLHF dynamically adjusts the balance between the two objectives\n",
            "during fine-tuning. Through a three-round fine-tuning using Safe RLHF, we\n",
            "demonstrate a superior ability to mitigate harmful responses while enhancing\n",
            "model performance compared to existing value-aligned algorithms.\n",
            "Experimentally, we fine-tuned the Alpaca-7B using Safe RLHF and aligned it with\n",
            "collected human preferences, significantly improving its helpfulness and\n",
            "harmlessness according to human evaluations.\n",
            "\n",
            "771. Title: 3D-Aware Hypothesis & Verification for Generalizable Relative Object Pose Estimation\n",
            "   Abstract: Learning arguably involves the discovery and memorization of abstract rules.\n",
            "The aim of this paper is to study associative memory mechanisms. Our model is\n",
            "based on high-dimensional matrices consisting of outer products of embeddings,\n",
            "which relates to the inner layers of transformer language models. We derive\n",
            "precise scaling laws with respect to sample size and parameter size, and\n",
            "discuss the statistical efficiency of different estimators, including\n",
            "optimization-based algorithms. We provide extensive numerical experiments to\n",
            "validate and interpret theoretical results, including fine-grained\n",
            "visualizations of the stored memory associations.\n",
            "\n",
            "772. Title: CALICO: Self-Supervised Camera-LiDAR Contrastive Pre-training for BEV Perception\n",
            "   Abstract: In multi-task reinforcement learning (RL) under Markov decision processes\n",
            "(MDPs), the presence of shared latent structures among multiple MDPs has been\n",
            "shown to yield significant benefits to the sample efficiency compared to\n",
            "single-task RL. In this paper, we investigate whether such a benefit can extend\n",
            "to more general sequential decision making problems, such as partially\n",
            "observable MDPs (POMDPs) and more general predictive state representations\n",
            "(PSRs). The main challenge here is that the large and complex model space makes\n",
            "it hard to identify what types of common latent structure of multi-task PSRs\n",
            "can reduce the model complexity and improve sample efficiency. To this end, we\n",
            "posit a joint model class for tasks and use the notion of $\\eta$-bracketing\n",
            "number to quantify its complexity; this number also serves as a general metric\n",
            "to capture the similarity of tasks and thus determines the benefit of\n",
            "multi-task over single-task RL. We first study upstream multi-task learning\n",
            "over PSRs, in which all tasks share the same observation and action spaces. We\n",
            "propose a provably efficient algorithm UMT-PSR for finding near-optimal\n",
            "policies for all PSRs, and demonstrate that the advantage of multi-task\n",
            "learning manifests if the joint model class of PSRs has a smaller\n",
            "$\\eta$-bracketing number compared to that of individual single-task learning.\n",
            "We also provide several example multi-task PSRs with small $\\eta$-bracketing\n",
            "numbers, which reap the benefits of multi-task learning. We further investigate\n",
            "downstream learning, in which the agent needs to learn a new target task that\n",
            "shares some commonalities with the upstream tasks via a similarity constraint.\n",
            "By exploiting the learned PSRs from the upstream, we develop a sample-efficient\n",
            "algorithm that provably finds a near-optimal policy.\n",
            "\n",
            "773. Title: CivRealm: A Learning and Reasoning Odyssey in Civilization for Decision-Making Agents\n",
            "   Abstract: Prior methods that tackle the problem of generalizable object pose estimation\n",
            "highly rely on having dense views of the unseen object. By contrast, we address\n",
            "the scenario where only a single reference view of the object is available. Our\n",
            "goal then is to estimate the relative object pose between this reference view\n",
            "and a query image that depicts the object in a different pose. In this\n",
            "scenario, robust generalization is imperative due to the presence of unseen\n",
            "objects during testing and the large-scale object pose variation between the\n",
            "reference and the query. To this end, we present a new\n",
            "hypothesis-and-verification framework, in which we generate and evaluate\n",
            "multiple pose hypotheses, ultimately selecting the most reliable one as the\n",
            "relative object pose. To measure reliability, we introduce a 3D-aware\n",
            "verification that explicitly applies 3D transformations to the 3D object\n",
            "representations learned from the two input images. Our comprehensive\n",
            "experiments on the Objaverse, LINEMOD, and CO3D datasets evidence the superior\n",
            "accuracy of our approach in relative pose estimation and its robustness in\n",
            "large-scale pose variations, when dealing with unseen objects.\n",
            "\n",
            "774. Title: CrossLoco: Human Motion Driven Control of Legged Robots via Guided Unsupervised Reinforcement Learning\n",
            "   Abstract: The truthfulness of existing explanation methods in authentically elucidating\n",
            "the underlying model's decision-making process has been questioned. Existing\n",
            "methods have deviated from faithfully representing the model, thus susceptible\n",
            "to adversarial attacks. To address this, we propose a novel eXplainable AI\n",
            "(XAI) method called SRD (Sharing Ratio Decomposition), which sincerely reflects\n",
            "the model's inference process, resulting in significantly enhanced robustness\n",
            "in our explanations. Different from the conventional emphasis on the neuronal\n",
            "level, we adopt a vector perspective to consider the intricate nonlinear\n",
            "interactions between filters. We also introduce an interesting observation\n",
            "termed Activation-Pattern-Only Prediction (APOP), letting us emphasize the\n",
            "importance of inactive neurons and redefine relevance encapsulating all\n",
            "relevant information including both active and inactive neurons. Our method,\n",
            "SRD, allows for the recursive decomposition of a Pointwise Feature Vector\n",
            "(PFV), providing a high-resolution Effective Receptive Field (ERF) at any\n",
            "layer.\n",
            "\n",
            "775. Title: To Grok or not to Grok: Disentangling Generalization and Memorization on Corrupted Algorithmic Datasets\n",
            "   Abstract: Text-to-3D generation has shown rapid progress in recent days with the advent\n",
            "of score distillation, a methodology of using pretrained text-to-2D diffusion\n",
            "models to optimize neural radiance field (NeRF) in the zero-shot setting.\n",
            "However, the lack of 3D awareness in the 2D diffusion models destabilizes score\n",
            "distillation-based methods from reconstructing a plausible 3D scene. To address\n",
            "this issue, we propose 3DFuse, a novel framework that incorporates 3D awareness\n",
            "into pretrained 2D diffusion models, enhancing the robustness and 3D\n",
            "consistency of score distillation-based methods. We realize this by first\n",
            "constructing a coarse 3D structure of a given text prompt and then utilizing\n",
            "projected, view-specific depth map as a condition for the diffusion model.\n",
            "Additionally, we introduce a training strategy that enables the 2D diffusion\n",
            "model learns to handle the errors and sparsity within the coarse 3D structure\n",
            "for robust generation, as well as a method for ensuring semantic consistency\n",
            "throughout all viewpoints of the scene. Our framework surpasses the limitations\n",
            "of prior arts, and has significant implications for 3D consistent generation of\n",
            "2D diffusion models.\n",
            "\n",
            "776. Title: A Precise Characterization of SGD Stability Using Loss Surface Geometry\n",
            "   Abstract: Accurate blind docking has the potential to lead to new biological\n",
            "breakthroughs, but for this promise to be realized, docking methods must\n",
            "generalize well across the proteome. Existing benchmarks, however, fail to\n",
            "rigorously assess generalizability. Therefore, we develop DockGen, a new\n",
            "benchmark based on the ligand-binding domains of proteins, and we show that\n",
            "existing machine learning-based docking models have very weak generalization\n",
            "abilities. We carefully analyze the scaling laws of ML-based docking and show\n",
            "that, by scaling data and model size, as well as integrating synthetic data\n",
            "strategies, we are able to significantly increase the generalization capacity\n",
            "and set new state-of-the-art performance across benchmarks. Further, we propose\n",
            "Confidence Bootstrapping, a new training paradigm that solely relies on the\n",
            "interaction between diffusion and confidence models and exploits the\n",
            "multi-resolution generation process of diffusion models. We demonstrate that\n",
            "Confidence Bootstrapping significantly improves the ability of ML-based docking\n",
            "methods to dock to unseen protein classes, edging closer to accurate and\n",
            "generalizable blind docking methods.\n",
            "\n",
            "777. Title: Source-Free and Image-Only Unsupervised Domain Adaptation for Category Level Object Pose Estimation\n",
            "   Abstract: The problem of speech separation, also known as the cocktail party problem,\n",
            "refers to the task of isolating a single speech signal from a mixture of speech\n",
            "signals. Previous work on source separation derived an upper bound for the\n",
            "source separation task in the domain of human speech. This bound is derived for\n",
            "deterministic models. Recent advancements in generative models challenge this\n",
            "bound. We show how the upper bound can be generalized to the case of random\n",
            "generative models. Applying a diffusion model Vocoder that was pretrained to\n",
            "model single-speaker voices on the output of a deterministic separation model\n",
            "leads to state-of-the-art separation results. It is shown that this requires\n",
            "one to combine the output of the separation model with that of the diffusion\n",
            "model. In our method, a linear combination is performed, in the frequency\n",
            "domain, using weights that are inferred by a learned model. We show\n",
            "state-of-the-art results on 2, 3, 5, 10, and 20 speakers on multiple\n",
            "benchmarks. In particular, for two speakers, our method is able to surpass what\n",
            "was previously considered the upper performance bound.\n",
            "\n",
            "778. Title: Deep Confident Steps to New Pockets: Strategies for Docking Generalization\n",
            "   Abstract: Test-time adaptation methods have been gaining attention recently as a\n",
            "practical solution for addressing source-to-target domain gaps by gradually\n",
            "updating the model without requiring labels on the target data. In this paper,\n",
            "we propose a method of test-time adaptation for category-level object pose\n",
            "estimation called TTA-COPE. We design a pose ensemble approach with a\n",
            "self-training loss using pose-aware confidence. Unlike previous unsupervised\n",
            "domain adaptation methods for category-level object pose estimation, our\n",
            "approach processes the test data in a sequential, online manner, and it does\n",
            "not require access to the source domain at runtime. Extensive experimental\n",
            "results demonstrate that the proposed pose ensemble and the self-training loss\n",
            "improve category-level object pose performance during test time under both\n",
            "semi-supervised and unsupervised settings. Project page:\n",
            "https://taeyeop.com/ttacope\n",
            "\n",
            "779. Title: EfficientDM: Efficient Quantization-Aware Fine-Tuning of Low-Bit Diffusion Models\n",
            "   Abstract: We propose a game-based formulation for learning dimensionality-reducing\n",
            "representations of feature vectors, when only a prior knowledge on future\n",
            "prediction tasks is available. In this game, the first player chooses a\n",
            "representation, and then the second player adversarially chooses a prediction\n",
            "task from a given class, representing the prior knowledge. The first player\n",
            "aims is to minimize, and the second player to maximize, the regret: The minimal\n",
            "prediction loss using the representation, compared to the same loss using the\n",
            "original features. For the canonical setting in which the representation, the\n",
            "response to predict and the predictors are all linear functions, and under the\n",
            "mean squared error loss function, we derive the theoretically optimal\n",
            "representation in pure strategies, which shows the effectiveness of the prior\n",
            "knowledge, and the optimal regret in mixed strategies, which shows the\n",
            "usefulness of randomizing the representation. For general representations and\n",
            "loss functions, we propose an efficient algorithm to optimize a randomized\n",
            "representation. The algorithm only requires the gradients of the loss function,\n",
            "and is based on incrementally adding a representation rule to a mixture of such\n",
            "rules.\n",
            "\n",
            "780. Title: VDT: General-purpose Video Diffusion Transformers via Mask Modeling\n",
            "   Abstract: Regularized reinforcement learning (RL), particularly the entropy-regularized\n",
            "kind, has gained traction in optimal control and inverse RL. While standard\n",
            "unregularized RL methods remain unaffected by changes in the number of actions,\n",
            "we show that it can severely impact their regularized counterparts. This paper\n",
            "demonstrates the importance of decoupling the regularizer from the action\n",
            "space: that is, to maintain a consistent level of regularization regardless of\n",
            "how many actions are involved to avoid over-regularization. Whereas the problem\n",
            "can be avoided by introducing a task-specific temperature parameter, it is\n",
            "often undesirable and cannot solve the problem when action spaces are\n",
            "state-dependent. In the state-dependent action context, different states with\n",
            "varying action spaces are regularized inconsistently. We introduce two\n",
            "solutions: a static temperature selection approach and a dynamic counterpart,\n",
            "universally applicable where this problem arises. Implementing these changes\n",
            "improves performance on the DeepMind control suite in static and dynamic\n",
            "temperature regimes and a biological sequence design task.\n",
            "\n",
            "781. Title: WizardCoder: Empowering Code Large Language Models with Evol-Instruct\n",
            "   Abstract: We propose to extract meaning representations from autoregressive language\n",
            "models by considering the distribution of all possible trajectories extending\n",
            "an input text. This strategy is prompt-free, does not require fine-tuning, and\n",
            "is applicable to any pre-trained autoregressive model. Moreover, unlike\n",
            "vector-based representations, distribution-based representations can also model\n",
            "asymmetric relations (e.g., direction of logical entailment, hypernym/hyponym\n",
            "relations) by using algebraic operations between likelihood functions. These\n",
            "ideas are grounded in distributional perspectives on semantics and are\n",
            "connected to standard constructions in automata theory, but to our knowledge\n",
            "they have not been applied to modern language models. We empirically show that\n",
            "the representations obtained from large models align well with human\n",
            "annotations, outperform other zero-shot and prompt-free methods on semantic\n",
            "similarity tasks, and can be used to solve more complex entailment and\n",
            "containment tasks that standard embeddings cannot handle. Finally, we extend\n",
            "our method to represent data from different modalities (e.g., image and text)\n",
            "using multimodal autoregressive models. Our code is available at:\n",
            "https://github.com/tianyu139/meaning-as-trajectories\n",
            "\n",
            "782. Title: Query-Policy Misalignment in Preference-Based Reinforcement Learning\n",
            "   Abstract: Stochastic Gradient Descent (SGD) stands as a cornerstone optimization\n",
            "algorithm with proven real-world empirical successes but relatively limited\n",
            "theoretical understanding. Recent research has illuminated a key factor\n",
            "contributing to its practical efficacy: the implicit regularization it\n",
            "instigates. Several studies have investigated the linear stability property of\n",
            "SGD in the vicinity of a stationary point as a predictive proxy for sharpness\n",
            "and generalization error in overparameterized neural networks (Wu et al., 2022;\n",
            "Jastrzebski et al., 2019; Cohen et al., 2021). In this paper, we delve deeper\n",
            "into the relationship between linear stability and sharpness. More\n",
            "specifically, we meticulously delineate the necessary and sufficient conditions\n",
            "for linear stability, contingent on hyperparameters of SGD and the sharpness at\n",
            "the optimum. Towards this end, we introduce a novel coherence measure of the\n",
            "loss Hessian that encapsulates pertinent geometric properties of the loss\n",
            "function that are relevant to the linear stability of SGD. It enables us to\n",
            "provide a simplified sufficient condition for identifying linear instability at\n",
            "an optimum. Notably, compared to previous works, our analysis relies on\n",
            "significantly milder assumptions and is applicable for a broader class of loss\n",
            "functions than known before, encompassing not only mean-squared error but also\n",
            "cross-entropy loss.\n",
            "\n",
            "783. Title: Fourier Transporter: Bi-Equivariant Robotic Manipulation in 3D\n",
            "   Abstract: Robust generalization is a major challenge in deep learning, particularly\n",
            "when the number of trainable parameters is very large. In general, it is very\n",
            "difficult to know if the network has memorized a particular set of examples or\n",
            "understood the underlying rule (or both). Motivated by this challenge, we study\n",
            "an interpretable model where generalizing representations are understood\n",
            "analytically, and are easily distinguishable from the memorizing ones. Namely,\n",
            "we consider multi-layer perceptron (MLP) and Transformer architectures trained\n",
            "on modular arithmetic tasks, where ($\\xi \\cdot 100\\%$) of labels are corrupted\n",
            "(\\emph{i.e.} some results of the modular operations in the training set are\n",
            "incorrect). We show that (i) it is possible for the network to memorize the\n",
            "corrupted labels \\emph{and} achieve $100\\%$ generalization at the same time;\n",
            "(ii) the memorizing neurons can be identified and pruned, lowering the accuracy\n",
            "on corrupted data and improving the accuracy on uncorrupted data; (iii)\n",
            "regularization methods such as weight decay, dropout and BatchNorm force the\n",
            "network to ignore the corrupted data during optimization, and achieve $100\\%$\n",
            "accuracy on the uncorrupted dataset; and (iv) the effect of these\n",
            "regularization methods is (``mechanistically'') interpretable: weight decay and\n",
            "dropout force all the neurons to learn generalizing representations, while\n",
            "BatchNorm de-amplifies the output of memorizing neurons and amplifies the\n",
            "output of the generalizing ones. Finally, we show that in the presence of\n",
            "regularization, the training dynamics involves two consecutive stages: first,\n",
            "the network undergoes \\emph{grokking} dynamics reaching high train \\emph{and}\n",
            "test accuracy; second, it unlearns the memorizing representations, where the\n",
            "train accuracy suddenly jumps from $100\\%$ to $100 (1-\\xi)\\%$.\n",
            "\n",
            "784. Title: A representation-learning game for classes of prediction tasks\n",
            "   Abstract: Many imitation learning (IL) algorithms employ inverse reinforcement learning\n",
            "(IRL) to infer the intrinsic reward function that an expert is implicitly\n",
            "optimizing for based on their demonstrated behaviors. However, in practice,\n",
            "IRL-based IL can fail to accomplish the underlying task due to a misalignment\n",
            "between the inferred reward and the objective of the task. In this paper, we\n",
            "address the susceptibility of IL to such misalignment by introducing a\n",
            "semi-supervised reward design paradigm called Protagonist Antagonist Guided\n",
            "Adversarial Reward (PAGAR). PAGAR-based IL trains a policy to perform well\n",
            "under mixed reward functions instead of a single reward function as in\n",
            "IRL-based IL. We identify the theoretical conditions under which PAGAR-based IL\n",
            "can avoid the task failures caused by reward misalignment. We also present a\n",
            "practical on-and-off policy approach to implementing PAGAR-based IL.\n",
            "Experimental results show that our algorithm outperforms standard IL baselines\n",
            "in complex tasks and challenging transfer settings.\n",
            "\n",
            "785. Title: DreamGaussian: Generative Gaussian Splatting for Efficient 3D Content Creation\n",
            "   Abstract: In order to oversee advanced AI systems, it is important to understand their\n",
            "underlying decision-making process. When prompted, large language models (LLMs)\n",
            "can provide natural language explanations or reasoning traces that sound\n",
            "plausible and receive high ratings from human annotators. However, it is\n",
            "unclear to what extent these explanations are faithful, i.e., truly capture the\n",
            "factors responsible for the model's predictions. In this work, we introduce\n",
            "Correlational Explanatory Faithfulness (CEF), a metric that can be used in\n",
            "faithfulness tests based on input interventions. Previous metrics used in such\n",
            "tests take into account only binary changes in the predictions. Our metric\n",
            "accounts for the total shift in the model's predicted label distribution, more\n",
            "accurately reflecting the explanations' faithfulness. We then introduce the\n",
            "Correlational Counterfactual Test (CCT) by instantiating CEF on the\n",
            "Counterfactual Test (CT) from Atanasova et al. (2023). We evaluate the\n",
            "faithfulness of free-text explanations generated by few-shot-prompted LLMs from\n",
            "the Llama2 family on three NLP tasks. We find that our metric measures aspects\n",
            "of faithfulness which the CT misses.\n",
            "\n",
            "786. Title: FedLoGe: Joint Local and Generic Federated Learning under Long-tailed Data\n",
            "   Abstract: Many complex robotic manipulation tasks can be decomposed as a sequence of\n",
            "pick and place actions. Training a robotic agent to learn this sequence over\n",
            "many different starting conditions typically requires many iterations or\n",
            "demonstrations, especially in 3D environments. In this work, we propose Fourier\n",
            "Transporter (FourTran) which leverages the two-fold SE(d)xSE(d) symmetry in the\n",
            "pick-place problem to achieve much higher sample efficiency. FourTran is an\n",
            "open-loop behavior cloning method trained using expert demonstrations to\n",
            "predict pick-place actions on new environments. FourTran is constrained to\n",
            "incorporate symmetries of the pick and place actions independently. Our method\n",
            "utilizes a fiber space Fourier transformation that allows for memory-efficient\n",
            "construction. We test our proposed network on the RLbench benchmark and achieve\n",
            "state-of-the-art results across various tasks.\n",
            "\n",
            "787. Title: GNNX-BENCH: Unravelling the Utility of Perturbation-based GNN Explainers through In-depth Benchmarking\n",
            "   Abstract: Learning neural operators for solving partial differential equations (PDEs)\n",
            "has attracted great attention due to its high inference efficiency. However,\n",
            "training such operators requires generating a substantial amount of labeled\n",
            "data, i.e., PDE problems together with their solutions. The data generation\n",
            "process is exceptionally time-consuming, as it involves solving numerous\n",
            "systems of linear equations to obtain numerical solutions to the PDEs. Many\n",
            "existing methods solve these systems independently without considering their\n",
            "inherent similarities, resulting in extremely redundant computations. To tackle\n",
            "this problem, we propose a novel method, namely Sorting Krylov Recycling (SKR),\n",
            "to boost the efficiency of solving these systems, thus significantly\n",
            "accelerating data generation for neural operators training. To the best of our\n",
            "knowledge, SKR is the first attempt to address the time-consuming nature of\n",
            "data generation for learning neural operators. The working horse of SKR is\n",
            "Krylov subspace recycling, a powerful technique for solving a series of\n",
            "interrelated systems by leveraging their inherent similarities. Specifically,\n",
            "SKR employs a sorting algorithm to arrange these systems in a sequence, where\n",
            "adjacent systems exhibit high similarities. Then it equips a solver with Krylov\n",
            "subspace recycling to solve the systems sequentially instead of independently,\n",
            "thus effectively enhancing the solving efficiency. Both theoretical analysis\n",
            "and extensive experiments demonstrate that SKR can significantly accelerate\n",
            "neural operator data generation, achieving a remarkable speedup of up to 13.9\n",
            "times.\n",
            "\n",
            "788. Title: SWE-bench: Can Language Models Resolve Real-world Github Issues?\n",
            "   Abstract: Stochastic differential equations (SDEs) are used to describe a wide variety\n",
            "of complex stochastic dynamical systems. Learning the hidden physics within\n",
            "SDEs is crucial for unraveling fundamental understanding of these systems'\n",
            "stochastic and nonlinear behavior. We propose a flexible and scalable framework\n",
            "for training artificial neural networks to learn constitutive equations that\n",
            "represent hidden physics within SDEs. The proposed stochastic physics-informed\n",
            "neural ordinary differential equation framework (SPINODE) propagates\n",
            "stochasticity through the known structure of the SDE (i.e., the known physics)\n",
            "to yield a set of deterministic ODEs that describe the time evolution of\n",
            "statistical moments of the stochastic states. SPINODE then uses ODE solvers to\n",
            "predict moment trajectories. SPINODE learns neural network representations of\n",
            "the hidden physics by matching the predicted moments to those estimated from\n",
            "data. Recent advances in automatic differentiation and mini-batch gradient\n",
            "descent with adjoint sensitivity are leveraged to establish the unknown\n",
            "parameters of the neural networks. We demonstrate SPINODE on three benchmark\n",
            "in-silico case studies and analyze the framework's numerical robustness and\n",
            "stability. SPINODE provides a promising new direction for systematically\n",
            "unraveling the hidden physics of multivariate stochastic dynamical systems with\n",
            "multiplicative noise.\n",
            "\n",
            "789. Title: Order-Preserving GFlowNets\n",
            "   Abstract: GFlowNets are a promising alternative to MCMC sampling for discrete\n",
            "compositional random variables. Training GFlowNets requires repeated\n",
            "evaluations of the unnormalized target distribution or reward function.\n",
            "However, for large-scale posterior sampling, this may be prohibitive since it\n",
            "incurs traversing the data several times. Moreover, if the data are distributed\n",
            "across clients, employing standard GFlowNets leads to intensive client-server\n",
            "communication. To alleviate both these issues, we propose embarrassingly\n",
            "parallel GFlowNet (EP-GFlowNet). EP-GFlowNet is a provably correct\n",
            "divide-and-conquer method to sample from product distributions of the form\n",
            "$R(\\cdot) \\propto R_1(\\cdot) ... R_N(\\cdot)$ -- e.g., in parallel or federated\n",
            "Bayes, where each $R_n$ is a local posterior defined on a data partition.\n",
            "First, in parallel, we train a local GFlowNet targeting each $R_n$ and send the\n",
            "resulting models to the server. Then, the server learns a global GFlowNet by\n",
            "enforcing our newly proposed \\emph{aggregating balance} condition, requiring a\n",
            "single communication step. Importantly, EP-GFlowNets can also be applied to\n",
            "multi-objective optimization and model reuse. Our experiments illustrate the\n",
            "EP-GFlowNets's effectiveness on many tasks, including parallel Bayesian\n",
            "phylogenetics, multi-objective multiset, sequence generation, and federated\n",
            "Bayesian structure learning.\n",
            "\n",
            "790. Title: Generative Sliced MMD Flows with Riesz Kernels\n",
            "   Abstract: The wide-ranging applications of large language models (LLMs), especially in\n",
            "safety-critical domains, necessitate the proper evaluation of the LLM's\n",
            "adversarial robustness. This paper proposes an efficient tool to audit the\n",
            "LLM's adversarial robustness via a prompt-based adversarial attack\n",
            "(PromptAttack). PromptAttack converts adversarial textual attacks into an\n",
            "attack prompt that can cause the victim LLM to output the adversarial sample to\n",
            "fool itself. The attack prompt is composed of three important components: (1)\n",
            "original input (OI) including the original sample and its ground-truth label,\n",
            "(2) attack objective (AO) illustrating a task description of generating a new\n",
            "sample that can fool itself without changing the semantic meaning, and (3)\n",
            "attack guidance (AG) containing the perturbation instructions to guide the LLM\n",
            "on how to complete the task by perturbing the original sample at character,\n",
            "word, and sentence levels, respectively. Besides, we use a fidelity filter to\n",
            "ensure that PromptAttack maintains the original semantic meanings of the\n",
            "adversarial examples. Further, we enhance the attack power of PromptAttack by\n",
            "ensembling adversarial examples at different perturbation levels. Comprehensive\n",
            "empirical results using Llama2 and GPT-3.5 validate that PromptAttack\n",
            "consistently yields a much higher attack success rate compared to AdvGLUE and\n",
            "AdvGLUE++. Interesting findings include that a simple emoji can easily mislead\n",
            "GPT-3.5 to make wrong predictions.\n",
            "\n",
            "791. Title: WildFusion: Learning 3D-Aware Latent Diffusion Models in View Space\n",
            "   Abstract: We introduce Tangent Attention Fine-Tuning (TAFT), a method for fine-tuning\n",
            "linearized transformers obtained by computing a First-order Taylor Expansion\n",
            "around a pre-trained initialization. We show that the Jacobian-Vector Product\n",
            "resulting from linearization can be computed efficiently in a single forward\n",
            "pass, reducing training and inference cost to the same order of magnitude as\n",
            "its original non-linear counterpart, while using the same number of parameters.\n",
            "Furthermore, we show that, when applied to various downstream visual\n",
            "classification tasks, the resulting Tangent Transformer fine-tuned with TAFT\n",
            "can perform comparably with fine-tuning the original non-linear network. Since\n",
            "Tangent Transformers are linear with respect to the new set of weights, and the\n",
            "resulting fine-tuning loss is convex, we show that TAFT enjoys several\n",
            "advantages compared to non-linear fine-tuning when it comes to model\n",
            "composition, parallel training, machine unlearning, and differential privacy.\n",
            "Our code is available at:\n",
            "https://github.com/tianyu139/tangent-model-composition\n",
            "\n",
            "792. Title: Deep Temporal Graph Clustering\n",
            "   Abstract: Deep graph clustering has recently received significant attention due to its\n",
            "ability to enhance the representation learning capabilities of models in\n",
            "unsupervised scenarios. Nevertheless, deep clustering for temporal graphs,\n",
            "which could capture crucial dynamic interaction information, has not been fully\n",
            "explored. It means that in many clustering-oriented real-world scenarios,\n",
            "temporal graphs can only be processed as static graphs. This not only causes\n",
            "the loss of dynamic information but also triggers huge computational\n",
            "consumption. To solve the problem, we propose a general framework for deep\n",
            "Temporal Graph Clustering called TGC, which introduces deep clustering\n",
            "techniques to suit the interaction sequence-based batch-processing pattern of\n",
            "temporal graphs. In addition, we discuss differences between temporal graph\n",
            "clustering and static graph clustering from several levels. To verify the\n",
            "superiority of the proposed framework TGC, we conduct extensive experiments.\n",
            "The experimental results show that temporal graph clustering enables more\n",
            "flexibility in finding a balance between time and space requirements, and our\n",
            "framework can effectively improve the performance of existing temporal graph\n",
            "learning methods. The code is released:\n",
            "https://github.com/MGitHubL/Deep-Temporal-Graph-Clustering.\n",
            "\n",
            "793. Title: T-MARS: Improving Visual Representations by Circumventing Text Feature Learning\n",
            "   Abstract: Maximum mean discrepancy (MMD) flows suffer from high computational costs in\n",
            "large scale computations. In this paper, we show that MMD flows with Riesz\n",
            "kernels $K(x,y) = - \\|x-y\\|^r$, $r \\in (0,2)$ have exceptional properties which\n",
            "allow their efficient computation. We prove that the MMD of Riesz kernels,\n",
            "which is also known as energy distance, coincides with the MMD of their sliced\n",
            "version. As a consequence, the computation of gradients of MMDs can be\n",
            "performed in the one-dimensional setting. Here, for $r=1$, a simple sorting\n",
            "algorithm can be applied to reduce the complexity from $O(MN+N^2)$ to\n",
            "$O((M+N)\\log(M+N))$ for two measures with $M$ and $N$ support points. As\n",
            "another interesting follow-up result, the MMD of compactly supported measures\n",
            "can be estimated from above and below by the Wasserstein-1 distance. For the\n",
            "implementations we approximate the gradient of the sliced MMD by using only a\n",
            "finite number $P$ of slices. We show that the resulting error has complexity\n",
            "$O(\\sqrt{d/P})$, where $d$ is the data dimension. These results enable us to\n",
            "train generative models by approximating MMD gradient flows by neural networks\n",
            "even for image applications. We demonstrate the efficiency of our model by\n",
            "image generation on MNIST, FashionMNIST and CIFAR10.\n",
            "\n",
            "794. Title: Towards Green AI in Fine-tuning Large Language Models via Adaptive Backpropagation\n",
            "   Abstract: Building a machine learning (ML) pipeline in an automated way is a crucial\n",
            "and complex task as it is constrained with the available time budget and\n",
            "resources. This encouraged the research community to introduce several\n",
            "solutions to utilize the available time and resources. A lot of work is done to\n",
            "suggest the most promising classifiers for a given dataset using sundry of\n",
            "techniques including meta-learning based techniques. This gives the autoML\n",
            "framework the chance to spend more time exploiting those classifiers and tuning\n",
            "their hyper-parameters. In this paper, we empirically study the hypothesis of\n",
            "improving the pipeline performance by exploiting the most promising classifiers\n",
            "within the limited time budget. We also study the effect of increasing the time\n",
            "budget over the pipeline performance. The empirical results across autoSKLearn,\n",
            "TPOT and ATM, show that exploiting the most promising classifiers does not\n",
            "achieve a statistically better performance than exploring the entire search\n",
            "space. The same conclusion is also applied for long time budgets.\n",
            "\n",
            "795. Title: RECOMBINER: Robust and Enhanced Compression with Bayesian Implicit Neural Representations\n",
            "   Abstract: Fine-tuning is the most effective way of adapting pre-trained large language\n",
            "models (LLMs) to downstream applications. With the fast growth of LLM-enabled\n",
            "AI applications and democratization of open-souced LLMs, fine-tuning has become\n",
            "possible for non-expert individuals, but intensively performed LLM fine-tuning\n",
            "worldwide could result in significantly high energy consumption and carbon\n",
            "footprint, which may bring large environmental impact. Mitigating such\n",
            "environmental impact towards Green AI directly correlates to reducing the FLOPs\n",
            "of fine-tuning, but existing techniques on efficient LLM fine-tuning can only\n",
            "achieve limited reduction of such FLOPs, due to their ignorance of the\n",
            "backpropagation cost in fine-tuning. To address this limitation, in this paper\n",
            "we present GreenTrainer, a new LLM fine-tuning technique that adaptively\n",
            "evaluates different tensors' backpropagation costs and contributions to the\n",
            "fine-tuned model accuracy, to minimize the fine-tuning cost by selecting the\n",
            "most appropriate set of tensors in training. Such selection in GreenTrainer is\n",
            "made based on a given objective of FLOPs reduction, which can flexibly adapt to\n",
            "the carbon footprint in energy supply and the need in Green AI. Experiment\n",
            "results over multiple open-sourced LLM models and abstractive summarization\n",
            "datasets show that, compared to fine-tuning the whole LLM model, GreenTrainer\n",
            "can save up to 64% FLOPs in fine-tuning without any noticeable model accuracy\n",
            "loss. Compared to the existing fine-tuning techniques such as LoRa,\n",
            "GreenTrainer can achieve up to 4% improvement on model accuracy with on-par\n",
            "FLOPs reduction.\n",
            "\n",
            "796. Title: Understanding Reconstruction Attacks with the Neural Tangent Kernel and Dataset Distillation\n",
            "   Abstract: When a small number of poisoned samples are injected into the training\n",
            "dataset of a deep neural network, the network can be induced to exhibit\n",
            "malicious behavior during inferences, which poses potential threats to\n",
            "real-world applications. While they have been intensively studied in\n",
            "classification, backdoor attacks on semantic segmentation have been largely\n",
            "overlooked. Unlike classification, semantic segmentation aims to classify every\n",
            "pixel within a given image. In this work, we explore backdoor attacks on\n",
            "segmentation models to misclassify all pixels of a victim class by injecting a\n",
            "specific trigger on non-victim pixels during inferences, which is dubbed\n",
            "Influencer Backdoor Attack (IBA). IBA is expected to maintain the\n",
            "classification accuracy of non-victim pixels and mislead classifications of all\n",
            "victim pixels in every single inference and could be easily applied to\n",
            "real-world scenes. Based on the context aggregation ability of segmentation\n",
            "models, we proposed a simple, yet effective, Nearest-Neighbor trigger injection\n",
            "strategy. We also introduce an innovative Pixel Random Labeling strategy which\n",
            "maintains optimal performance even when the trigger is placed far from the\n",
            "victim pixels. Our extensive experiments reveal that current segmentation\n",
            "models do suffer from backdoor attacks, demonstrate IBA real-world\n",
            "applicability, and show that our proposed techniques can further increase\n",
            "attack performance.\n",
            "\n",
            "797. Title: Tree Cross Attention\n",
            "   Abstract: Modern deep learning requires large volumes of data, which could contain\n",
            "sensitive or private information that cannot be leaked. Recent work has shown\n",
            "for homogeneous neural networks a large portion of this training data could be\n",
            "reconstructed with only access to the trained network parameters. While the\n",
            "attack was shown to work empirically, there exists little formal understanding\n",
            "of its effective regime which datapoints are susceptible to reconstruction. In\n",
            "this work, we first build a stronger version of the dataset reconstruction\n",
            "attack and show how it can provably recover the \\emph{entire training set} in\n",
            "the infinite width regime. We then empirically study the characteristics of\n",
            "this attack on two-layer networks and reveal that its success heavily depends\n",
            "on deviations from the frozen infinite-width Neural Tangent Kernel limit. Next,\n",
            "we study the nature of easily-reconstructed images. We show that both\n",
            "theoretically and empirically, reconstructed images tend to \"outliers\" in the\n",
            "dataset, and that these reconstruction attacks can be used for \\textit{dataset\n",
            "distillation}, that is, we can retrain on reconstructed images and obtain high\n",
            "predictive accuracy.\n",
            "\n",
            "798. Title: A Flexible Generative Model for Heterogeneous Tabular EHR with Missing Modality\n",
            "   Abstract: We lack a systematic understanding of the effects of fine-tuning (via methods\n",
            "such as instruction-tuning or reinforcement learning from human feedback),\n",
            "particularly on tasks outside the narrow fine-tuning distribution. In a\n",
            "simplified scenario, we demonstrate that improving performance on tasks within\n",
            "the fine-tuning data distribution comes at the expense of capabilities on other\n",
            "tasks. We hypothesize that language models implicitly infer the task of the\n",
            "prompt and that fine-tuning skews this inference towards tasks in the\n",
            "fine-tuning distribution. To test this, we propose Conjugate Prompting, which\n",
            "artificially makes the task look farther from the fine-tuning distribution\n",
            "while requiring the same capability, and we find that this recovers some of the\n",
            "pretraining capabilities in our synthetic setup. Since real-world fine-tuning\n",
            "distributions are predominantly English, we apply conjugate prompting to\n",
            "recover pretrained capabilities in LLMs by simply translating the prompts to\n",
            "different languages. This allows us to recover in-context learning abilities\n",
            "lost via instruction tuning, natural reasoning capability lost during code\n",
            "fine-tuning, and, more concerningly, harmful content generation suppressed by\n",
            "safety fine-tuning in chatbots like ChatGPT.\n",
            "\n",
            "799. Title: Modeling Boundedly Rational Agents with Latent Inference Budgets\n",
            "   Abstract: Cross Attention is a popular method for retrieving information from a set of\n",
            "context tokens for making predictions. At inference time, for each prediction,\n",
            "Cross Attention scans the full set of $\\mathcal{O}(N)$ tokens. In practice,\n",
            "however, often only a small subset of tokens are required for good performance.\n",
            "Methods such as Perceiver IO are cheap at inference as they distill the\n",
            "information to a smaller-sized set of latent tokens $L < N$ on which cross\n",
            "attention is then applied, resulting in only $\\mathcal{O}(L)$ complexity.\n",
            "However, in practice, as the number of input tokens and the amount of\n",
            "information to distill increases, the number of latent tokens needed also\n",
            "increases significantly. In this work, we propose Tree Cross Attention (TCA) -\n",
            "a module based on Cross Attention that only retrieves information from a\n",
            "logarithmic $\\mathcal{O}(\\log(N))$ number of tokens for performing inference.\n",
            "TCA organizes the data in a tree structure and performs a tree search at\n",
            "inference time to retrieve the relevant tokens for prediction. Leveraging TCA,\n",
            "we introduce ReTreever, a flexible architecture for token-efficient inference.\n",
            "We show empirically that Tree Cross Attention (TCA) performs comparable to\n",
            "Cross Attention across various classification and uncertainty regression tasks\n",
            "while being significantly more token-efficient. Furthermore, we compare\n",
            "ReTreever against Perceiver IO, showing significant gains while using the same\n",
            "number of tokens for inference.\n",
            "\n",
            "800. Title: Motion Guidance: Diffusion-Based Image Editing with Differentiable Motion Estimators\n",
            "   Abstract: We address reinforcement learning problems with finite state and action\n",
            "spaces where the underlying MDP has some known structure that could be\n",
            "potentially exploited to minimize the exploration rates of suboptimal (state,\n",
            "action) pairs. For any arbitrary structure, we derive problem-specific regret\n",
            "lower bounds satisfied by any learning algorithm. These lower bounds are made\n",
            "explicit for unstructured MDPs and for those whose transition probabilities and\n",
            "average reward functions are Lipschitz continuous w.r.t. the state and action.\n",
            "For Lipschitz MDPs, the bounds are shown not to scale with the sizes $S$ and\n",
            "$A$ of the state and action spaces, i.e., they are smaller than $c\\log T$ where\n",
            "$T$ is the time horizon and the constant $c$ only depends on the Lipschitz\n",
            "structure, the span of the bias function, and the minimal action sub-optimality\n",
            "gap. This contrasts with unstructured MDPs where the regret lower bound\n",
            "typically scales as $SA\\log T$. We devise DEL (Directed Exploration Learning),\n",
            "an algorithm that matches our regret lower bounds. We further simplify the\n",
            "algorithm for Lipschitz MDPs, and show that the simplified version is still\n",
            "able to efficiently exploit the structure.\n",
            "\n",
            "801. Title: GNNBoundary: Towards Explaining Graph Neural Networks through the Lens of Decision Boundaries\n",
            "   Abstract: We study the problem of modeling a population of agents pursuing unknown\n",
            "goals subject to unknown computational constraints. In standard models of\n",
            "bounded rationality, sub-optimal decision-making is simulated by adding\n",
            "homoscedastic noise to optimal decisions rather than explicitly simulating\n",
            "constrained inference. In this work, we introduce a latent inference budget\n",
            "model (L-IBM) that models agents' computational constraints explicitly, via a\n",
            "latent variable (inferred jointly with a model of agents' goals) that controls\n",
            "the runtime of an iterative inference algorithm. L-IBMs make it possible to\n",
            "learn agent models using data from diverse populations of suboptimal actors. In\n",
            "three modeling tasks -- inferring navigation goals from routes, inferring\n",
            "communicative intents from human utterances, and predicting next moves in human\n",
            "chess games -- we show that L-IBMs match or outperform Boltzmann models of\n",
            "decision-making under uncertainty. Inferred inference budgets are themselves\n",
            "meaningful, efficient to compute, and correlated with measures of player skill,\n",
            "partner skill and task difficulty.\n",
            "\n",
            "802. Title: Learning Optimal Contracts: How to Exploit Small Action Spaces\n",
            "   Abstract: Diffusion models are capable of generating impressive images conditioned on\n",
            "text descriptions, and extensions of these models allow users to edit images at\n",
            "a relatively coarse scale. However, the ability to precisely edit the layout,\n",
            "position, pose, and shape of objects in images with diffusion models is still\n",
            "difficult. To this end, we propose motion guidance, a zero-shot technique that\n",
            "allows a user to specify dense, complex motion fields that indicate where each\n",
            "pixel in an image should move. Motion guidance works by steering the diffusion\n",
            "sampling process with the gradients through an off-the-shelf optical flow\n",
            "network. Specifically, we design a guidance loss that encourages the sample to\n",
            "have the desired motion, as estimated by a flow network, while also being\n",
            "visually similar to the source image. By simultaneously sampling from a\n",
            "diffusion model and guiding the sample to have low guidance loss, we can obtain\n",
            "a motion-edited image. We demonstrate that our technique works on complex\n",
            "motions and produces high quality edits of real and generated images.\n",
            "\n",
            "803. Title: On the Role of Discrete Tokenization in Visual Representation Learning\n",
            "   Abstract: Diffusion models, which employ stochastic differential equations to sample\n",
            "images through integrals, have emerged as a dominant class of generative\n",
            "models. However, the rationality of the diffusion process itself receives\n",
            "limited attention, leaving the question of whether the problem is well-posed\n",
            "and well-conditioned. In this paper, we explore a perplexing tendency of\n",
            "diffusion models: they often display the infinite Lipschitz property of the\n",
            "network with respect to time variable near the zero point. We provide\n",
            "theoretical proofs to illustrate the presence of infinite Lipschitz constants\n",
            "and empirical results to confirm it. The Lipschitz singularities pose a threat\n",
            "to the stability and accuracy during both the training and inference processes\n",
            "of diffusion models. Therefore, the mitigation of Lipschitz singularities holds\n",
            "great potential for enhancing the performance of diffusion models. To address\n",
            "this challenge, we propose a novel approach, dubbed E-TSDM, which alleviates\n",
            "the Lipschitz singularities of the diffusion model near the zero point of\n",
            "timesteps. Remarkably, our technique yields a substantial improvement in\n",
            "performance. Moreover, as a byproduct of our method, we achieve a dramatic\n",
            "reduction in the Fr\\'echet Inception Distance of acceleration methods relying\n",
            "on network Lipschitz, including DDIM and DPM-Solver, by over 33%. Extensive\n",
            "experiments on diverse datasets validate our theory and method. Our work may\n",
            "advance the understanding of the general diffusion process, and also provide\n",
            "insights for the design of diffusion models.\n",
            "\n",
            "804. Title: AUGCAL: Improving Sim2Real Adaptation by Uncertainty Calibration on Augmented Synthetic Images\n",
            "   Abstract: The predictability of weather and climate is strongly state-dependent:\n",
            "special and extremely relevant atmospheric states like blockings are associated\n",
            "with anomalous instability. Indeed, typically, the instability of a chaotic\n",
            "dynamical system can vary considerably across its attractor. Such an attractor\n",
            "is in general densely populated by unstable periodic orbits that can be used to\n",
            "approximate any forward trajectory through the so-called shadowing. Dynamical\n",
            "heterogeneity can lead to the presence of unstable periodic orbits with\n",
            "different number of unstable dimensions. This phenomenon - unstable dimensions\n",
            "variability - implies a serious breakdown of hyperbolicity and has considerable\n",
            "implications in terms of the structural stability of the system and of the\n",
            "possibility to describe accurately its behaviour through numerical models. As a\n",
            "step in the direction of better understanding the properties of\n",
            "high-dimensional chaotic systems, we provide here an extensive numerical study\n",
            "of the dynamical heterogeneity of the Lorenz '96 model in a parametric\n",
            "configuration leading to chaotic dynamics. We show that the detected\n",
            "variability in the number of unstable dimensions is associated with the\n",
            "presence of many finite-time Lyapunov exponents that fluctuate about zero also\n",
            "when very long averaging times are considered. The transition between regions\n",
            "of the attractor with different degrees of instability comes with a significant\n",
            "drop of the quality of the shadowing. By performing a coarse graining based on\n",
            "the shadowing unstable periodic orbits, we can characterize the slow\n",
            "fluctuations of the system between regions featuring, on the average,\n",
            "anomalously high and anomalously low instability. In turn, such regions are\n",
            "associated, respectively, with states of anomalously high and low energy, thus\n",
            "providing a clear link between the microscopic and thermodynamical properties\n",
            "of the system.\n",
            "\n",
            "805. Title: Lipschitz Singularities in Diffusion Models\n",
            "   Abstract: The integration of diverse clinical modalities such as medical imaging and\n",
            "the tabular data extracted from patients' Electronic Health Records (EHRs) is a\n",
            "crucial aspect of modern healthcare. Integrative analysis of multiple sources\n",
            "can provide a comprehensive understanding of the clinical condition of a\n",
            "patient, improving diagnosis and treatment decision. Deep Neural Networks\n",
            "(DNNs) consistently demonstrate outstanding performance in a wide range of\n",
            "multimodal tasks in the medical domain. However, the complex endeavor of\n",
            "effectively merging medical imaging with clinical, demographic and genetic\n",
            "information represented as numerical tabular data remains a highly active and\n",
            "ongoing research pursuit.\n",
            "  We present a novel framework based on hypernetworks to fuse clinical imaging\n",
            "and tabular data by conditioning the image processing on the EHR's values and\n",
            "measurements. This approach aims to leverage the complementary information\n",
            "present in these modalities to enhance the accuracy of various medical\n",
            "applications. We demonstrate the strength and generality of our method on two\n",
            "different brain Magnetic Resonance Imaging (MRI) analysis tasks, namely, brain\n",
            "age prediction conditioned by subject's sex and multi-class Alzheimer's Disease\n",
            "(AD) classification conditioned by tabular data. We show that our framework\n",
            "outperforms both single-modality models and state-of-the-art MRI tabular data\n",
            "fusion methods. A link to our code can be found at\n",
            "https://github.com/daniel4725/HyperFusion\n",
            "\n",
            "806. Title: Improved Techniques for Training Consistency Models\n",
            "   Abstract: In this paper, we investigate the behavior of gradient descent algorithms in\n",
            "physics-informed machine learning methods like PINNs, which minimize residuals\n",
            "connected to partial differential equations (PDEs). Our key result is that the\n",
            "difficulty in training these models is closely related to the conditioning of a\n",
            "specific differential operator. This operator, in turn, is associated to the\n",
            "Hermitian square of the differential operator of the underlying PDE. If this\n",
            "operator is ill-conditioned, it results in slow or infeasible training.\n",
            "Therefore, preconditioning this operator is crucial. We employ both rigorous\n",
            "mathematical analysis and empirical evaluations to investigate various\n",
            "strategies, explaining how they better condition this critical operator, and\n",
            "consequently improve training.\n",
            "\n",
            "807. Title: TextField3D: Towards Enhancing Open-Vocabulary 3D Generation with Noisy Text Fields\n",
            "   Abstract: Consistency models are a nascent family of generative models that can sample\n",
            "high quality data in one step without the need for adversarial training.\n",
            "Current consistency models achieve optimal sample quality by distilling from\n",
            "pre-trained diffusion models and employing learned metrics such as LPIPS.\n",
            "However, distillation limits the quality of consistency models to that of the\n",
            "pre-trained diffusion model, and LPIPS causes undesirable bias in evaluation.\n",
            "To tackle these challenges, we present improved techniques for consistency\n",
            "training, where consistency models learn directly from data without\n",
            "distillation. We delve into the theory behind consistency training and identify\n",
            "a previously overlooked flaw, which we address by eliminating Exponential\n",
            "Moving Average from the teacher consistency model. To replace learned metrics\n",
            "like LPIPS, we adopt Pseudo-Huber losses from robust statistics. Additionally,\n",
            "we introduce a lognormal noise schedule for the consistency training objective,\n",
            "and propose to double total discretization steps every set number of training\n",
            "iterations. Combined with better hyperparameter tuning, these modifications\n",
            "enable consistency models to achieve FID scores of 2.51 and 3.25 on CIFAR-10\n",
            "and ImageNet $64\\times 64$ respectively in a single sampling step. These scores\n",
            "mark a 3.5$\\times$ and 4$\\times$ improvement compared to prior consistency\n",
            "training approaches. Through two-step sampling, we further reduce FID scores to\n",
            "2.24 and 2.77 on these two datasets, surpassing those obtained via distillation\n",
            "in both one-step and two-step settings, while narrowing the gap between\n",
            "consistency models and other state-of-the-art generative models.\n",
            "\n",
            "808. Title: Fine-Tuning Language Models for Factuality\n",
            "   Abstract: Large language models (LLMs) have been increasingly applied to various\n",
            "domains, which triggers increasing concerns about LLMs' safety on specialized\n",
            "domains, e.g. medicine. However, testing the domain-specific safety of LLMs is\n",
            "challenging due to the lack of domain knowledge-driven attacks in existing\n",
            "benchmarks. To bridge this gap, we propose a new task, knowledge-to-jailbreak,\n",
            "which aims to generate jailbreaks from domain knowledge to evaluate the safety\n",
            "of LLMs when applied to those domains. We collect a large-scale dataset with\n",
            "12,974 knowledge-jailbreak pairs and fine-tune a large language model as\n",
            "jailbreak-generator, to produce domain knowledge-specific jailbreaks.\n",
            "Experiments on 13 domains and 8 target LLMs demonstrate the effectiveness of\n",
            "jailbreak-generator in generating jailbreaks that are both relevant to the\n",
            "given knowledge and harmful to the target LLMs. We also apply our method to an\n",
            "out-of-domain knowledge base, showing that jailbreak-generator can generate\n",
            "jailbreaks that are comparable in harmfulness to those crafted by human\n",
            "experts. Data and code: https://github.com/THU-KEG/Knowledge-to-Jailbreak/.\n",
            "\n",
            "809. Title: Mind Your Augmentation: The Key to Decoupling Dense Self-Supervised Learning\n",
            "   Abstract: A semantic equivalence assessment is defined as a task that assesses semantic\n",
            "equivalence in a sentence pair by binary judgment (i.e., paraphrase\n",
            "identification) or grading (i.e., semantic textual similarity measurement). It\n",
            "constitutes a set of tasks crucial for research on natural language\n",
            "understanding. Recently, BERT realized a breakthrough in sentence\n",
            "representation learning (Devlin et al., 2019), which is broadly transferable to\n",
            "various NLP tasks. While BERT's performance improves by increasing its model\n",
            "size, the required computational power is an obstacle preventing practical\n",
            "applications from adopting the technology. Herein, we propose to inject phrasal\n",
            "paraphrase relations into BERT in order to generate suitable representations\n",
            "for semantic equivalence assessment instead of increasing the model size.\n",
            "Experiments on standard natural language understanding tasks confirm that our\n",
            "method effectively improves a smaller BERT model while maintaining the model\n",
            "size. The generated model exhibits superior performance compared to a larger\n",
            "BERT model on semantic equivalence assessment tasks. Furthermore, it achieves\n",
            "larger performance gains on tasks with limited training datasets for\n",
            "fine-tuning, which is a property desirable for transfer learning.\n",
            "\n",
            "810. Title: Modeling state-dependent communication between brain regions with switching nonlinear dynamical systems\n",
            "   Abstract: Pre-trained language models demonstrate general intelligence and common\n",
            "sense, but long inputs quickly become a bottleneck for memorizing information\n",
            "at inference time. We resurface a simple method, Memorizing Transformers (Wu et\n",
            "al., 2022), that gives the model access to a bank of pre-computed memories. We\n",
            "show that it is possible to fix many of the shortcomings of the original\n",
            "method, such as the need for fine-tuning, by critically assessing how\n",
            "positional encodings should be updated for the keys and values retrieved. This\n",
            "intuitive method uses the model's own key/query system to select and attend to\n",
            "the most relevant memories at each generation step, rather than using external\n",
            "embeddings. We demonstrate the importance of external information being\n",
            "retrieved in a majority of decoder layers, contrary to previous work. We open\n",
            "source a new counterfactual long-range retrieval benchmark, and show that\n",
            "Extended Mind Transformers outperform today's state of the art by 6% on\n",
            "average.\n",
            "\n",
            "811. Title: Learning to Embed Time Series Patches Independently\n",
            "   Abstract: Recent years have witnessed the rapid progress and broad application of\n",
            "diffusion probabilistic models (DPMs). Sampling from DPMs can be viewed as\n",
            "solving an ordinary differential equation (ODE). Despite the promising\n",
            "performance, the generation of DPMs usually consumes much time due to the large\n",
            "number of function evaluations (NFE). Though recent works have accelerated the\n",
            "sampling to around 20 steps with high-order solvers, the sample quality with\n",
            "less than 10 NFE can still be improved. In this paper, we propose a unified\n",
            "sampling framework (USF) to study the optional strategies for solver. Under\n",
            "this framework, we further reveal that taking different solving strategies at\n",
            "different timesteps may help further decrease the truncation error, and a\n",
            "carefully designed \\emph{solver schedule} has the potential to improve the\n",
            "sample quality by a large margin. Therefore, we propose a new sampling\n",
            "framework based on the exponential integral formulation that allows free\n",
            "choices of solver strategy at each step and design specific decisions for the\n",
            "framework. Moreover, we propose $S^3$, a predictor-based search method that\n",
            "automatically optimizes the solver schedule to get a better time-quality\n",
            "trade-off of sampling. We demonstrate that $S^3$ can find outstanding solver\n",
            "schedules which outperform the state-of-the-art sampling methods on CIFAR-10,\n",
            "CelebA, ImageNet, and LSUN-Bedroom datasets. Specifically, we achieve 2.69 FID\n",
            "with 10 NFE and 6.86 FID with 5 NFE on CIFAR-10 dataset, outperforming the SOTA\n",
            "method significantly. We further apply $S^3$ to Stable-Diffusion model and get\n",
            "an acceleration ratio of 2$\\times$, showing the feasibility of sampling in very\n",
            "few steps without retraining the neural network.\n",
            "\n",
            "812. Title: An operator preconditioning perspective on training in physics-informed machine learning\n",
            "   Abstract: Masked time series modeling has recently gained much attention as a\n",
            "self-supervised representation learning strategy for time series. Inspired by\n",
            "masked image modeling in computer vision, recent works first patchify and\n",
            "partially mask out time series, and then train Transformers to capture the\n",
            "dependencies between patches by predicting masked patches from unmasked\n",
            "patches. However, we argue that capturing such patch dependencies might not be\n",
            "an optimal strategy for time series representation learning; rather, learning\n",
            "to embed patches independently results in better time series representations.\n",
            "Specifically, we propose to use 1) the simple patch reconstruction task, which\n",
            "autoencode each patch without looking at other patches, and 2) the simple\n",
            "patch-wise MLP that embeds each patch independently. In addition, we introduce\n",
            "complementary contrastive learning to hierarchically capture adjacent time\n",
            "series information efficiently. Our proposed method improves time series\n",
            "forecasting and classification performance compared to state-of-the-art\n",
            "Transformer-based models, while it is more efficient in terms of the number of\n",
            "parameters and training/inference time. Code is available at this repository:\n",
            "https://github.com/seunghan96/pits.\n",
            "\n",
            "813. Title: Latent Intuitive Physics: Learning to Transfer Hidden Physics from A 3D Video\n",
            "   Abstract: We introduce latent intuitive physics, a transfer learning framework for\n",
            "physics simulation that can infer hidden properties of fluids from a single 3D\n",
            "video and simulate the observed fluid in novel scenes. Our key insight is to\n",
            "use latent features drawn from a learnable prior distribution conditioned on\n",
            "the underlying particle states to capture the invisible and complex physical\n",
            "properties. To achieve this, we train a parametrized prior learner given visual\n",
            "observations to approximate the visual posterior of inverse graphics, and both\n",
            "the particle states and the visual posterior are obtained from a learned neural\n",
            "renderer. The converged prior learner is embedded in our probabilistic physics\n",
            "engine, allowing us to perform novel simulations on unseen geometries,\n",
            "boundaries, and dynamics without knowledge of the true physical parameters. We\n",
            "validate our model in three ways: (i) novel scene simulation with the learned\n",
            "visual-world physics, (ii) future prediction of the observed fluid dynamics,\n",
            "and (iii) supervised particle simulation. Our model demonstrates strong\n",
            "performance in all three tasks.\n",
            "\n",
            "814. Title: Quasi-Monte Carlo for 3D Sliced Wasserstein\n",
            "   Abstract: The convergence of recent advances in optical fabrication and digital\n",
            "processing yields a new generation of imaging technology: light-field cameras,\n",
            "which bridge the realms of applied mathematics, optics, and high-performance\n",
            "computing. Herein for the first time, we introduce the paradigm of light-field\n",
            "imaging into laryngoscopy. The resultant probe can image the three-dimensional\n",
            "(3D) shape of vocal folds within a single camera exposure. Furthermore, to\n",
            "improve the spatial resolution, we developed an image fusion algorithm,\n",
            "providing a simple solution to a long-standing problem in light-field imaging.\n",
            "\n",
            "815. Title: Uni-RLHF: Universal Platform and Benchmark Suite for Reinforcement Learning with Diverse Human Feedback\n",
            "   Abstract: Monte Carlo (MC) integration has been employed as the standard approximation\n",
            "method for the Sliced Wasserstein (SW) distance, whose analytical expression\n",
            "involves an intractable expectation. However, MC integration is not optimal in\n",
            "terms of absolute approximation error. To provide a better class of empirical\n",
            "SW, we propose quasi-sliced Wasserstein (QSW) approximations that rely on\n",
            "Quasi-Monte Carlo (QMC) methods. For a comprehensive investigation of QMC for\n",
            "SW, we focus on the 3D setting, specifically computing the SW between\n",
            "probability measures in three dimensions. In greater detail, we empirically\n",
            "evaluate various methods to construct QMC point sets on the 3D\n",
            "unit-hypersphere, including the Gaussian-based and equal area mappings,\n",
            "generalized spiral points, and optimizing discrepancy energies. Furthermore, to\n",
            "obtain an unbiased estimator for stochastic optimization, we extend QSW to\n",
            "Randomized Quasi-Sliced Wasserstein (RQSW) by introducing randomness in the\n",
            "discussed point sets. Theoretically, we prove the asymptotic convergence of QSW\n",
            "and the unbiasedness of RQSW. Finally, we conduct experiments on various 3D\n",
            "tasks, such as point-cloud comparison, point-cloud interpolation, image style\n",
            "transfer, and training deep point-cloud autoencoders, to demonstrate the\n",
            "favorable performance of the proposed QSW and RQSW variants.\n",
            "\n",
            "816. Title: Light Schrödinger Bridge\n",
            "   Abstract: Teaching visualization design involve making students familiar and make them\n",
            "work with visualization models, framework and perspectives. Visualization\n",
            "research accommodates a plethora of perspectives emerging from researchers of\n",
            "varied backgrounds. These diverse range of perspectives give rise to multiples\n",
            "models, frameworks and perspectives to teach visualization design. In this\n",
            "paper, we look at an approach to visualization teaching by using\n",
            "improvisational techniques. The basic idea is to design a visualization without\n",
            "using an existing predefined model. Since improvisation, by definition, is not\n",
            "a model or a framework, this work presents a reflection on how improvisation\n",
            "can be a way of teaching visualization design.\n",
            "\n",
            "817. Title: Real-time Photorealistic Dynamic Scene Representation and Rendering with 4D Gaussian Splatting\n",
            "   Abstract: Variational Autoencoders (VAEs) have gained significant popularity among\n",
            "researchers as a powerful tool for understanding unknown distributions based on\n",
            "limited samples. This popularity stems partly from their impressive performance\n",
            "and partly from their ability to provide meaningful feature representations in\n",
            "the latent space. Wasserstein Autoencoders (WAEs), a variant of VAEs, aim to\n",
            "not only improve model efficiency but also interpretability. However, there has\n",
            "been limited focus on analyzing their statistical guarantees. The matter is\n",
            "further complicated by the fact that the data distributions to which WAEs are\n",
            "applied - such as natural images - are often presumed to possess an underlying\n",
            "low-dimensional structure within a high-dimensional feature space, which\n",
            "current theory does not adequately account for, rendering known bounds\n",
            "inefficient. To bridge the gap between the theory and practice of WAEs, in this\n",
            "paper, we show that WAEs can learn the data distributions when the network\n",
            "architectures are properly chosen. We show that the convergence rates of the\n",
            "expected excess risk in the number of samples for WAEs are independent of the\n",
            "high feature dimension, instead relying only on the intrinsic dimension of the\n",
            "data distribution.\n",
            "\n",
            "818. Title: Adaptive Self-training Framework for Fine-grained Scene Graph Generation\n",
            "   Abstract: The scene graph is a new data structure describing objects and their pairwise\n",
            "relationship within image scenes. As the size of scene graph in vision\n",
            "applications grows, how to losslessly and efficiently store such data on disks\n",
            "or transmit over the network becomes an inevitable problem. However, the\n",
            "compression of scene graph is seldom studied before because of the complicated\n",
            "data structures and distributions. Existing solutions usually involve\n",
            "general-purpose compressors or graph structure compression methods, which is\n",
            "weak at reducing redundancy for scene graph data. This paper introduces a new\n",
            "lossless compression framework with adaptive predictors for joint compression\n",
            "of objects and relations in scene graph data. The proposed framework consists\n",
            "of a unified prior extractor and specialized element predictors to adapt for\n",
            "different data elements. Furthermore, to exploit the context information within\n",
            "and between graph elements, Graph Context Convolution is proposed to support\n",
            "different graph context modeling schemes for different graph elements. Finally,\n",
            "a learned distribution model is devised to predict numerical data under\n",
            "complicated conditional constraints. Experiments conducted on labeled or\n",
            "generated scene graphs proves the effectiveness of the proposed framework in\n",
            "scene graph lossless compression task.\n",
            "\n",
            "819. Title: QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models\n",
            "   Abstract: Electrocardiograms (ECG) are widely employed as a diagnostic tool for\n",
            "monitoring electrical signals originating from a heart. Recent machine learning\n",
            "research efforts have focused on the application of screening various diseases\n",
            "using ECG signals. However, adapting to the application of screening disease is\n",
            "challenging in that labeled ECG data are limited. Achieving general\n",
            "representation through self-supervised learning (SSL) is a well-known approach\n",
            "to overcome the scarcity of labeled data; however, a naive application of SSL\n",
            "to ECG data, without considering the spatial-temporal relationships inherent in\n",
            "ECG signals, may yield suboptimal results. In this paper, we introduce ST-MEM\n",
            "(Spatio-Temporal Masked Electrocardiogram Modeling), designed to learn\n",
            "spatio-temporal features by reconstructing masked 12-lead ECG data. ST-MEM\n",
            "outperforms other SSL baseline methods in various experimental settings for\n",
            "arrhythmia classification tasks. Moreover, we demonstrate that ST-MEM is\n",
            "adaptable to various lead combinations. Through quantitative and qualitative\n",
            "analysis, we show a spatio-temporal relationship within ECG data. Our code is\n",
            "available at https://github.com/bakqui/ST-MEM.\n",
            "\n",
            "820. Title: Theoretical Understanding of Learning from Adversarial Perturbations\n",
            "   Abstract: It is not fully understood why adversarial examples can deceive neural\n",
            "networks and transfer between different networks. To elucidate this, several\n",
            "studies have hypothesized that adversarial perturbations, while appearing as\n",
            "noises, contain class features. This is supported by empirical evidence showing\n",
            "that networks trained on mislabeled adversarial examples can still generalize\n",
            "well to correctly labeled test samples. However, a theoretical understanding of\n",
            "how perturbations include class features and contribute to generalization is\n",
            "limited. In this study, we provide a theoretical framework for understanding\n",
            "learning from perturbations using a one-hidden-layer network trained on\n",
            "mutually orthogonal samples. Our results highlight that various adversarial\n",
            "perturbations, even perturbations of a few pixels, contain sufficient class\n",
            "features for generalization. Moreover, we reveal that the decision boundary\n",
            "when learning from perturbations matches that from standard samples except for\n",
            "specific regions under mild conditions. The code is available at\n",
            "https://github.com/s-kumano/learning-from-adversarial-perturbations.\n",
            "\n",
            "821. Title: Visual Data-Type Understanding does not emerge from scaling Vision-Language Models\n",
            "   Abstract: Enabling long-context understanding remains a key challenge in scaling\n",
            "existing sequence models -- a crucial component in developing generally\n",
            "intelligent models that can process and operate over long temporal horizons\n",
            "that potentially consist of millions of tokens. In this paper, we aim to\n",
            "address these challenges by providing a comprehensive exploration of the full\n",
            "development process for producing 1M context language models and video-language\n",
            "models, setting new benchmarks in language retrieval and new capabilities in\n",
            "long video understanding. We detail our long context data curation process,\n",
            "progressive context extension from 4K to 1M tokens, and present an efficient\n",
            "open-source implementation for scalable training on long sequences.\n",
            "Additionally, we open-source a family of 7B parameter models capable of\n",
            "processing long text documents and videos exceeding 1M tokens.\n",
            "\n",
            "822. Title: Training-free Multi-objective Diffusion Model for 3D Molecule Generation\n",
            "   Abstract: In the realm of self-supervised learning (SSL), masked image modeling (MIM)\n",
            "has gained popularity alongside contrastive learning methods. MIM involves\n",
            "reconstructing masked regions of input images using their unmasked portions. A\n",
            "notable subset of MIM methodologies employs discrete tokens as the\n",
            "reconstruction target, but the theoretical underpinnings of this choice remain\n",
            "underexplored. In this paper, we explore the role of these discrete tokens,\n",
            "aiming to unravel their benefits and limitations. Building upon the connection\n",
            "between MIM and contrastive learning, we provide a comprehensive theoretical\n",
            "understanding on how discrete tokenization affects the model's generalization\n",
            "capabilities. Furthermore, we propose a novel metric named TCAS, which is\n",
            "specifically designed to assess the effectiveness of discrete tokens within the\n",
            "MIM framework. Inspired by this metric, we contribute an innovative tokenizer\n",
            "design and propose a corresponding MIM method named ClusterMIM. It demonstrates\n",
            "superior performance on a variety of benchmark datasets and ViT backbones. Code\n",
            "is available at https://github.com/PKU-ML/ClusterMIM.\n",
            "\n",
            "823. Title: GRANDE: Gradient-Based Decision Tree Ensembles for Tabular Data\n",
            "   Abstract: 3D molecule generation is crucial for drug discovery and material design.\n",
            "While prior efforts focus on 3D diffusion models for their benefits in modeling\n",
            "continuous 3D conformers, they overlook the advantages of 1D SELFIES-based\n",
            "Language Models (LMs), which can generate 100% valid molecules and leverage the\n",
            "billion-scale 1D molecule datasets. To combine these advantages for 3D molecule\n",
            "generation, we propose a foundation model -- NExT-Mol: 3D Diffusion Meets 1D\n",
            "Language Modeling for 3D Molecule Generation. NExT-Mol uses an extensively\n",
            "pretrained molecule LM for 1D molecule generation, and subsequently predicts\n",
            "the generated molecule's 3D conformers with a 3D diffusion model. We enhance\n",
            "NExT-Mol's performance by scaling up the LM's model size, refining the\n",
            "diffusion neural architecture, and applying 1D to 3D transfer learning.\n",
            "Notably, our 1D molecule LM significantly outperforms baselines in\n",
            "distributional similarity while ensuring validity, and our 3D diffusion model\n",
            "achieves leading performances in conformer prediction. Given these improvements\n",
            "in 1D and 3D modeling, NExT-Mol achieves a 26% relative improvement in 3D FCD\n",
            "for de novo 3D generation on GEOM-DRUGS, and a 13% average relative gain for\n",
            "conditional 3D generation on QM9-2014. Our codes and pretrained checkpoints are\n",
            "available at https://github.com/acharkq/NExT-Mol.\n",
            "\n",
            "824. Title: Debiasing Algorithm through Model Adaptation\n",
            "   Abstract: Denoising diffusion models enable conditional generation and density modeling\n",
            "of complex relationships like images and text. However, the nature of the\n",
            "learned relationships is opaque making it difficult to understand precisely\n",
            "what relationships between words and parts of an image are captured, or to\n",
            "predict the effect of an intervention. We illuminate the fine-grained\n",
            "relationships learned by diffusion models by noticing a precise relationship\n",
            "between diffusion and information decomposition. Exact expressions for mutual\n",
            "information and conditional mutual information can be written in terms of the\n",
            "denoising model. Furthermore, pointwise estimates can be easily estimated as\n",
            "well, allowing us to ask questions about the relationships between specific\n",
            "images and captions. Decomposing information even further to understand which\n",
            "variables in a high-dimensional space carry information is a long-standing\n",
            "problem. For diffusion models, we show that a natural non-negative\n",
            "decomposition of mutual information emerges, allowing us to quantify\n",
            "informative relationships between words and pixels in an image. We exploit\n",
            "these new relations to measure the compositional understanding of diffusion\n",
            "models, to do unsupervised localization of objects in images, and to measure\n",
            "effects when selectively editing images through prompt interventions.\n",
            "\n",
            "825. Title: Idempotent Generative Network\n",
            "   Abstract: Dynamic scene reconstruction has garnered significant attention in recent\n",
            "years due to its capabilities in high-quality and real-time rendering. Among\n",
            "various methodologies, constructing a 4D spatial-temporal representation, such\n",
            "as 4D-GS, has gained popularity for its high-quality rendered images. However,\n",
            "these methods often produce suboptimal surfaces, as the discrete 3D Gaussian\n",
            "point clouds fail to align with the object's surface precisely. To address this\n",
            "problem, we propose DynaSurfGS to achieve both photorealistic rendering and\n",
            "high-fidelity surface reconstruction of dynamic scenarios. Specifically, the\n",
            "DynaSurfGS framework first incorporates Gaussian features from 4D neural voxels\n",
            "with the planar-based Gaussian Splatting to facilitate precise surface\n",
            "reconstruction. It leverages normal regularization to enforce the smoothness of\n",
            "the surface of dynamic objects. It also incorporates the as-rigid-as-possible\n",
            "(ARAP) constraint to maintain the approximate rigidity of local neighborhoods\n",
            "of 3D Gaussians between timesteps and ensure that adjacent 3D Gaussians remain\n",
            "closely aligned throughout. Extensive experiments demonstrate that DynaSurfGS\n",
            "surpasses state-of-the-art methods in both high-fidelity surface reconstruction\n",
            "and photorealistic rendering.\n",
            "\n",
            "826. Title: COLEP: Certifiably Robust Learning-Reasoning Conformal Prediction via Probabilistic Circuits\n",
            "   Abstract: Large language models are becoming the go-to solution for the ever-growing\n",
            "number of tasks. However, with growing capacity, models are prone to rely on\n",
            "spurious correlations stemming from biases and stereotypes present in the\n",
            "training data. This work proposes a novel method for detecting and mitigating\n",
            "gender bias in language models. We perform causal analysis to identify\n",
            "problematic model components and discover that mid-upper feed-forward layers\n",
            "are most prone to convey bias. Based on the analysis results, we intervene in\n",
            "the model by applying a linear projection to the weight matrices of these\n",
            "layers. Our titular method, DAMA, significantly decreases bias as measured by\n",
            "diverse metrics while maintaining the model's performance on downstream tasks.\n",
            "We release code for our method and models, which retrain LLaMA's\n",
            "state-of-the-art performance while being significantly less biased.\n",
            "\n",
            "827. Title: Abstractors and relational cross-attention: An inductive bias for explicit relational reasoning in Transformers\n",
            "   Abstract: We propose Conditional Idempotent Generative Networks (CIGN), a novel\n",
            "approach that expands upon Idempotent Generative Networks (IGN) to enable\n",
            "conditional generation. While IGNs offer efficient single-pass generation, they\n",
            "lack the ability to control the content of the generated data. CIGNs address\n",
            "this limitation by incorporating conditioning mechanisms, allowing users to\n",
            "steer the generation process towards specific types of data.\n",
            "  We establish the theoretical foundations for CIGNs, outlining their scope,\n",
            "loss function design, and evaluation metrics. We then present two potential\n",
            "architectures for implementing CIGNs: channel conditioning and filter\n",
            "conditioning. Finally, we discuss experimental results on the MNIST dataset,\n",
            "demonstrating the effectiveness of both approaches. Our findings pave the way\n",
            "for further exploration of CIGNs on larger datasets and with more powerful\n",
            "computing resources to determine the optimal implementation strategy.\n",
            "\n",
            "828. Title: Consistent algorithms for multi-label classification with macro-at-$k$ metrics\n",
            "   Abstract: Robotic systems that rely primarily on self-supervised learning have the\n",
            "potential to decrease the amount of human annotation and engineering effort\n",
            "required to learn control strategies. In the same way that prior robotic\n",
            "systems have leveraged self-supervised techniques from computer vision (CV) and\n",
            "natural language processing (NLP), our work builds on prior work showing that\n",
            "the reinforcement learning (RL) itself can be cast as a self-supervised\n",
            "problem: learning to reach any goal without human-specified rewards or labels.\n",
            "Despite the seeming appeal, little (if any) prior work has demonstrated how\n",
            "self-supervised RL methods can be practically deployed on robotic systems. By\n",
            "first studying a challenging simulated version of this task, we discover design\n",
            "decisions about architectures and hyperparameters that increase the success\n",
            "rate by $2 \\times$. These findings lay the groundwork for our main result: we\n",
            "demonstrate that a self-supervised RL algorithm based on contrastive learning\n",
            "can solve real-world, image-based robotic manipulation tasks, with tasks being\n",
            "specified by a single goal image provided after training.\n",
            "\n",
            "829. Title: Quantifying the Plausibility of Context Reliance in Neural Machine Translation\n",
            "   Abstract: Abstract visual reasoning is a characteristically human ability, allowing the\n",
            "identification of relational patterns that are abstracted away from object\n",
            "features, and the systematic generalization of those patterns to unseen\n",
            "problems. Recent work has demonstrated strong systematic generalization in\n",
            "visual reasoning tasks involving multi-object inputs, through the integration\n",
            "of slot-based methods used for extracting object-centric representations\n",
            "coupled with strong inductive biases for relational abstraction. However, this\n",
            "approach was limited to problems containing a single rule, and was not scalable\n",
            "to visual reasoning problems containing a large number of objects. Other recent\n",
            "work proposed Abstractors, an extension of Transformers that incorporates\n",
            "strong relational inductive biases, thereby inheriting the Transformer's\n",
            "scalability and multi-head architecture, but it has yet to be demonstrated how\n",
            "this approach might be applied to multi-object visual inputs. Here we combine\n",
            "the strengths of the above approaches and propose Slot Abstractors, an approach\n",
            "to abstract visual reasoning that can be scaled to problems involving a large\n",
            "number of objects and multiple relations among them. The approach displays\n",
            "state-of-the-art performance across four abstract visual reasoning tasks, as\n",
            "well as an abstract reasoning task involving real-world images.\n",
            "\n",
            "830. Title: Stack Attention: Improving the Ability of Transformers to Model Hierarchical Patterns\n",
            "   Abstract: Training machine learning models with differential privacy (DP) has received\n",
            "increasing interest in recent years. One of the most popular algorithms for\n",
            "training differentially private models is differentially private stochastic\n",
            "gradient descent (DPSGD) and its variants, where at each step gradients are\n",
            "clipped and combined with some noise. Given the increasing usage of DPSGD, we\n",
            "ask the question: is DPSGD alone sufficient to find a good minimizer for every\n",
            "dataset under privacy constraints? Towards answering this question, we show\n",
            "that even for the simple case of linear classification, unlike non-private\n",
            "optimization, (private) feature preprocessing is vital for differentially\n",
            "private optimization. In detail, we first show theoretically that there exists\n",
            "an example where without feature preprocessing, DPSGD incurs an optimality gap\n",
            "proportional to the maximum Euclidean norm of features over all samples. We\n",
            "then propose an algorithm called DPSGD-F, which combines DPSGD with feature\n",
            "preprocessing and prove that for classification tasks, it incurs an optimality\n",
            "gap proportional to the diameter of the features $\\max_{x, x' \\in D} \\|x -\n",
            "x'\\|_2$. We finally demonstrate the practicality of our algorithm on image\n",
            "classification benchmarks.\n",
            "\n",
            "831. Title: MogaNet: Multi-order Gated Aggregation Network\n",
            "   Abstract: Spiking neural network (SNN) has attracted great attention due to its\n",
            "characteristic of high efficiency and accuracy. Currently, the ANN-to-SNN\n",
            "conversion methods can obtain ANN on-par accuracy SNN with ultra-low latency (8\n",
            "time-steps) in CNN structure on computer vision (CV) tasks. However, as\n",
            "Transformer-based networks have achieved prevailing precision on both CV and\n",
            "natural language processing (NLP), the Transformer-based SNNs are still\n",
            "encounting the lower accuracy w.r.t the ANN counterparts. In this work, we\n",
            "introduce a novel ANN-to-SNN conversion method called SpikeZIP-TF, where ANN\n",
            "and SNN are exactly equivalent, thus incurring no accuracy degradation.\n",
            "SpikeZIP-TF achieves 83.82% accuracy on CV dataset (ImageNet) and 93.79%\n",
            "accuracy on NLP dataset (SST-2), which are higher than SOTA Transformer-based\n",
            "SNNs. The code is available in GitHub:\n",
            "https://github.com/Intelligent-Computing-Research-Group/SpikeZIP_transformer\n",
            "\n",
            "832. Title: Stabilizing Contrastive RL: Techniques for Robotic Goal Reaching from Offline Data\n",
            "   Abstract: Calibration measures and reliability diagrams are two fundamental tools for\n",
            "measuring and interpreting the calibration of probabilistic predictors.\n",
            "Calibration measures quantify the degree of miscalibration, and reliability\n",
            "diagrams visualize the structure of this miscalibration. However, the most\n",
            "common constructions of reliability diagrams and calibration measures --\n",
            "binning and ECE -- both suffer from well-known flaws (e.g. discontinuity). We\n",
            "show that a simple modification fixes both constructions: first smooth the\n",
            "observations using an RBF kernel, then compute the Expected Calibration Error\n",
            "(ECE) of this smoothed function. We prove that with a careful choice of\n",
            "bandwidth, this method yields a calibration measure that is well-behaved in the\n",
            "sense of (B{\\l}asiok, Gopalan, Hu, and Nakkiran 2023a) -- a consistent\n",
            "calibration measure. We call this measure the SmoothECE. Moreover, the\n",
            "reliability diagram obtained from this smoothed function visually encodes the\n",
            "SmoothECE, just as binned reliability diagrams encode the BinnedECE.\n",
            "  We also provide a Python package with simple, hyperparameter-free methods for\n",
            "measuring and plotting calibration: `pip install relplot\\`.\n",
            "\n",
            "833. Title: DNA-GPT: Divergent N-Gram Analysis for Training-Free Detection of GPT-Generated Text\n",
            "   Abstract: Deep learning models are known to be vulnerable to adversarial examples. A\n",
            "practical adversarial attack should require as little as possible knowledge of\n",
            "attacked models. Current substitute attacks need pre-trained models to generate\n",
            "adversarial examples and their attack success rates heavily rely on the\n",
            "transferability of adversarial examples. Current score-based and decision-based\n",
            "attacks require lots of queries for the attacked models. In this study, we\n",
            "propose a novel adversarial imitation attack. First, it produces a replica of\n",
            "the attacked model by a two-player game like the generative adversarial\n",
            "networks (GANs). The objective of the generative model is to generate examples\n",
            "that lead the imitation model returning different outputs with the attacked\n",
            "model. The objective of the imitation model is to output the same labels with\n",
            "the attacked model under the same inputs. Then, the adversarial examples\n",
            "generated by the imitation model are utilized to fool the attacked model.\n",
            "Compared with the current substitute attacks, imitation attacks can use less\n",
            "training data to produce a replica of the attacked model and improve the\n",
            "transferability of adversarial examples. Experiments demonstrate that our\n",
            "imitation attack requires less training data than the black-box substitute\n",
            "attacks, but achieves an attack success rate close to the white-box attack on\n",
            "unseen data with no query.\n",
            "\n",
            "834. Title: Generating Images with 3D Annotations Using Diffusion Models\n",
            "   Abstract: We construct a universally Bayes consistent learning rule that satisfies\n",
            "differential privacy (DP). We first handle the setting of binary classification\n",
            "and then extend our rule to the more general setting of density estimation\n",
            "(with respect to the total variation metric). The existence of a universally\n",
            "consistent DP learner reveals a stark difference with the distribution-free PAC\n",
            "model. Indeed, in the latter DP learning is extremely limited: even\n",
            "one-dimensional linear classifiers are not privately learnable in this\n",
            "stringent model. Our result thus demonstrates that by allowing the learning\n",
            "rate to depend on the target distribution, one can circumvent the\n",
            "above-mentioned impossibility result and in fact, learn \\emph{arbitrary}\n",
            "distributions by a single DP algorithm. As an application, we prove that any VC\n",
            "class can be privately learned in a semi-supervised setting with a near-optimal\n",
            "\\emph{labeled} sample complexity of $\\tilde{O}(d/\\varepsilon)$ labeled examples\n",
            "(and with an unlabeled sample complexity that can depend on the target\n",
            "distribution).\n",
            "\n",
            "835. Title: Dichotomy of Early and Late Phase Implicit Biases Can Provably Induce Grokking\n",
            "   Abstract: Multi-modal models have shown a promising capability to effectively integrate\n",
            "information from various sources, yet meanwhile, they are found vulnerable to\n",
            "pervasive perturbations, such as uni-modal attacks and missing conditions. To\n",
            "counter these perturbations, robust multi-modal representations are highly\n",
            "expected, which are positioned well away from the discriminative multi-modal\n",
            "decision boundary. In this paper, different from conventional empirical\n",
            "studies, we focus on a commonly used joint multi-modal framework and\n",
            "theoretically discover that larger uni-modal representation margins and more\n",
            "reliable integration for modalities are essential components for achieving\n",
            "higher robustness. This discovery can further explain the limitation of\n",
            "multi-modal robustness and the phenomenon that multi-modal models are often\n",
            "vulnerable to attacks on the specific modality. Moreover, our analysis reveals\n",
            "how the widespread issue, that the model has different preferences for\n",
            "modalities, limits the multi-modal robustness by influencing the essential\n",
            "components and could lead to attacks on the specific modality highly effective.\n",
            "Inspired by our theoretical finding, we introduce a training procedure called\n",
            "Certifiable Robust Multi-modal Training (CRMT), which can alleviate this\n",
            "influence from modality preference and explicitly regulate essential components\n",
            "to significantly improve robustness in a certifiable manner. Our method\n",
            "demonstrates substantial improvements in performance and robustness compared\n",
            "with existing methods. Furthermore, our training procedure can be easily\n",
            "extended to enhance other robust training strategies, highlighting its\n",
            "credibility and flexibility.\n",
            "\n",
            "836. Title: Continual Learning on a Diet: Learning from Sparsely Labeled Streams Under Constrained Computation\n",
            "   Abstract: The limited availability of 3D medical image datasets, due to privacy\n",
            "concerns and high collection or annotation costs, poses significant challenges\n",
            "in the field of medical imaging. While a promising alternative is the use of\n",
            "synthesized medical data, there are few solutions for realistic 3D medical\n",
            "image synthesis due to difficulties in backbone design and fewer 3D training\n",
            "samples compared to 2D counterparts. In this paper, we propose GEM-3D, a novel\n",
            "generative approach to the synthesis of 3D medical images and the enhancement\n",
            "of existing datasets using conditional diffusion models. Our method begins with\n",
            "a 2D slice, noted as the informed slice to serve the patient prior, and\n",
            "propagates the generation process using a 3D segmentation mask. By decomposing\n",
            "the 3D medical images into masks and patient prior information, GEM-3D offers a\n",
            "flexible yet effective solution for generating versatile 3D images from\n",
            "existing datasets. GEM-3D can enable dataset enhancement by combining informed\n",
            "slice selection and generation at random positions, along with editable mask\n",
            "volumes to introduce large variations in diffusion sampling. Moreover, as the\n",
            "informed slice contains patient-wise information, GEM-3D can also facilitate\n",
            "counterfactual image synthesis and dataset-level de-enhancement with desired\n",
            "control. Experiments on brain MRI and abdomen CT images demonstrate that GEM-3D\n",
            "is capable of synthesizing high-quality 3D medical images with volumetric\n",
            "consistency, offering a straightforward solution for dataset enhancement during\n",
            "inference. The code is available at https://github.com/HKU-MedAI/GEM-3D.\n",
            "\n",
            "837. Title: Smooth ECE: Principled Reliability Diagrams via Kernel Smoothing\n",
            "   Abstract: Establishing whether language models can use contextual information in a\n",
            "human-plausible way is important to ensure their trustworthiness in real-world\n",
            "settings. However, the questions of when and which parts of the context affect\n",
            "model generations are typically tackled separately, with current plausibility\n",
            "evaluations being practically limited to a handful of artificial benchmarks. To\n",
            "address this, we introduce Plausibility Evaluation of Context Reliance\n",
            "(PECoRe), an end-to-end interpretability framework designed to quantify context\n",
            "usage in language models' generations. Our approach leverages model internals\n",
            "to (i) contrastively identify context-sensitive target tokens in generated\n",
            "texts and (ii) link them to contextual cues justifying their prediction. We use\n",
            "\\pecore to quantify the plausibility of context-aware machine translation\n",
            "models, comparing model rationales with human annotations across several\n",
            "discourse-level phenomena. Finally, we apply our method to unannotated model\n",
            "translations to identify context-mediated predictions and highlight instances\n",
            "of (im)plausible context usage throughout generation.\n",
            "\n",
            "838. Title: Balancing Act: Constraining Disparate Impact in Sparse Models\n",
            "   Abstract: We propose and study a realistic Continual Learning (CL) setting where\n",
            "learning algorithms are granted a restricted computational budget per time step\n",
            "while training. We apply this setting to large-scale semi-supervised Continual\n",
            "Learning scenarios with sparse label rates. Previous proficient CL methods\n",
            "perform very poorly in this challenging setting. Overfitting to the sparse\n",
            "labeled data and insufficient computational budget are the two main culprits\n",
            "for such a poor performance. Our new setting encourages learning methods to\n",
            "effectively and efficiently utilize the unlabeled data during training. To that\n",
            "end, we propose a simple but highly effective baseline, DietCL, which utilizes\n",
            "both unlabeled and labeled data jointly. DietCL meticulously allocates\n",
            "computational budget for both types of data. We validate our baseline, at\n",
            "scale, on several datasets, e.g., CLOC, ImageNet10K, and CGLM, under constraint\n",
            "budget setups. DietCL outperforms, by a large margin, all existing supervised\n",
            "CL algorithms as well as more recent continual semi-supervised methods. Our\n",
            "extensive analysis and ablations demonstrate that DietCL is stable under a full\n",
            "spectrum of label sparsity, computational budget, and various other ablations.\n",
            "\n",
            "839. Title: DecompOpt: Controllable and Decomposed Diffusion Models for Structure-based Molecular Optimization\n",
            "   Abstract: Given the ever-increasing size of modern neural networks, the significance of\n",
            "sparse architectures has surged due to their accelerated inference speeds and\n",
            "minimal memory demands. When it comes to global pruning techniques, Iterative\n",
            "Magnitude Pruning (IMP) still stands as a state-of-the-art algorithm despite\n",
            "its simple nature, particularly in extremely sparse regimes. In light of the\n",
            "recent finding that the two successive matching IMP solutions are linearly\n",
            "connected without a loss barrier, we propose Sparse Weight Averaging with\n",
            "Multiple Particles (SWAMP), a straightforward modification of IMP that achieves\n",
            "performance comparable to an ensemble of two IMP solutions. For every\n",
            "iteration, we concurrently train multiple sparse models, referred to as\n",
            "particles, using different batch orders yet the same matching ticket, and then\n",
            "weight average such models to produce a single mask. We demonstrate that our\n",
            "method consistently outperforms existing baselines across different sparsities\n",
            "through extensive experiments on various data and neural network structures.\n",
            "\n",
            "840. Title: GoLLIE: Annotation Guidelines improve Zero-Shot Information-Extraction\n",
            "   Abstract: Generating differentially private (DP) synthetic data that closely resembles\n",
            "the original private data is a scalable way to mitigate privacy concerns in the\n",
            "current data-driven world. In contrast to current practices that train\n",
            "customized models for this task, we aim to generate DP Synthetic Data via APIs\n",
            "(DPSDA), where we treat foundation models as blackboxes and only utilize their\n",
            "inference APIs. Such API-based, training-free approaches are easier to deploy\n",
            "as exemplified by the recent surge in the number of API-based apps. These\n",
            "approaches can also leverage the power of large foundation models which are\n",
            "only accessible via their inference APIs. However, this comes with greater\n",
            "challenges due to strictly more restrictive model access and the need to\n",
            "protect privacy from the API provider.\n",
            "  In this paper, we present a new framework called Private Evolution (PE) to\n",
            "solve this problem and show its initial promise on synthetic images.\n",
            "Surprisingly, PE can match or even outperform state-of-the-art (SOTA) methods\n",
            "without any model training. For example, on CIFAR10 (with ImageNet as the\n",
            "public data), we achieve FID <= 7.9 with privacy cost {\\epsilon} = 0.67,\n",
            "significantly improving the previous SOTA from {\\epsilon} = 32. We further\n",
            "demonstrate the promise of applying PE on large foundation models such as\n",
            "Stable Diffusion to tackle challenging private datasets with a small number of\n",
            "high-resolution images. The code and data are released at\n",
            "https://github.com/microsoft/DPSDA.\n",
            "\n",
            "841. Title: Sparse Weight Averaging with Multiple Particles for Iterative Magnitude Pruning\n",
            "   Abstract: Model pruning is a popular approach to enable the deployment of large deep\n",
            "learning models on edge devices with restricted computational or storage\n",
            "capacities. Although sparse models achieve performance comparable to that of\n",
            "their dense counterparts at the level of the entire dataset, they exhibit high\n",
            "accuracy drops for some data sub-groups. Existing methods to mitigate this\n",
            "disparate impact induced by pruning (i) rely on surrogate metrics that address\n",
            "the problem indirectly and have limited interpretability; or (ii) scale poorly\n",
            "with the number of protected sub-groups in terms of computational cost. We\n",
            "propose a constrained optimization approach that directly addresses the\n",
            "disparate impact of pruning: our formulation bounds the accuracy change between\n",
            "the dense and sparse models, for each sub-group. This choice of constraints\n",
            "provides an interpretable success criterion to determine if a pruned model\n",
            "achieves acceptable disparity levels. Experimental results demonstrate that our\n",
            "technique scales reliably to problems involving large models and hundreds of\n",
            "protected sub-groups.\n",
            "\n",
            "842. Title: Leveraging augmented-Lagrangian techniques for differentiating over infeasible quadratic programs in machine learning\n",
            "   Abstract: Recent work by Power et al. (2022) highlighted a surprising \"grokking\"\n",
            "phenomenon in learning arithmetic tasks: a neural net first \"memorizes\" the\n",
            "training set, resulting in perfect training accuracy but near-random test\n",
            "accuracy, and after training for sufficiently longer, it suddenly transitions\n",
            "to perfect test accuracy. This paper studies the grokking phenomenon in\n",
            "theoretical setups and shows that it can be induced by a dichotomy of early and\n",
            "late phase implicit biases. Specifically, when training homogeneous neural nets\n",
            "with large initialization and small weight decay on both classification and\n",
            "regression tasks, we prove that the training process gets trapped at a solution\n",
            "corresponding to a kernel predictor for a long time, and then a very sharp\n",
            "transition to min-norm/max-margin predictors occurs, leading to a dramatic\n",
            "change in test accuracy.\n",
            "\n",
            "843. Title: Training Diffusion Models with Reinforcement Learning\n",
            "   Abstract: Inverse problems aim to determine parameters from observations, a crucial\n",
            "task in engineering and science. Lately, generative models, especially\n",
            "diffusion models, have gained popularity in this area for their ability to\n",
            "produce realistic solutions and their good mathematical properties. Despite\n",
            "their success, an important drawback of diffusion models is their sensitivity\n",
            "to the choice of variance schedule, which controls the dynamics of the\n",
            "diffusion process. Fine-tuning this schedule for specific applications is\n",
            "crucial but time-costly and does not guarantee an optimal result. We propose a\n",
            "novel approach for learning the schedule as part of the training process. Our\n",
            "method supports probabilistic conditioning on data, provides high-quality\n",
            "solutions, and is flexible, proving able to adapt to different applications\n",
            "with minimum overhead. This approach is tested in two unrelated inverse\n",
            "problems: super-resolution microscopy and quantitative phase imaging, yielding\n",
            "comparable or superior results to previous methods and fine-tuned diffusion\n",
            "models. We conclude that fine-tuning the schedule by experimentation should be\n",
            "avoided because it can be learned during training in a stable way that yields\n",
            "better results.\n",
            "\n",
            "844. Title: Implicit Gaussian process representation of vector fields over arbitrary latent manifolds\n",
            "   Abstract: Kernel methods are powerful tools to capture nonlinear patterns behind data.\n",
            "They implicitly learn high (even infinite) dimensional nonlinear features in\n",
            "the Reproducing Kernel Hilbert Space (RKHS) while making the computation\n",
            "tractable by leveraging the kernel trick. Classic kernel methods learn a single\n",
            "layer of nonlinear features, whose representational power may be limited.\n",
            "Motivated by recent success of deep neural networks (DNNs) that learn\n",
            "multi-layer hierarchical representations, we propose a Stacked Kernel Network\n",
            "(SKN) that learns a hierarchy of RKHS-based nonlinear features. SKN interleaves\n",
            "several layers of nonlinear transformations (from a linear space to a RKHS) and\n",
            "linear transformations (from a RKHS to a linear space). Similar to DNNs, a SKN\n",
            "is composed of multiple layers of hidden units, but each parameterized by a\n",
            "RKHS function rather than a finite-dimensional vector. We propose three ways to\n",
            "represent the RKHS functions in SKN: (1)nonparametric representation,\n",
            "(2)parametric representation and (3)random Fourier feature representation.\n",
            "Furthermore, we expand SKN into CNN architecture called Stacked Kernel\n",
            "Convolutional Network (SKCN). SKCN learning a hierarchy of RKHS-based nonlinear\n",
            "features by convolutional operation with each filter also parameterized by a\n",
            "RKHS function rather than a finite-dimensional matrix in CNN, which is suitable\n",
            "for image inputs. Experiments on various datasets demonstrate the effectiveness\n",
            "of SKN and SKCN, which outperform the competitive methods.\n",
            "\n",
            "845. Title: Differentially Private Synthetic Data via Foundation Model APIs 1: Images\n",
            "   Abstract: In this paper, we extend mean-field Langevin dynamics to minimax optimization\n",
            "over probability distributions for the first time with symmetric and provably\n",
            "convergent updates. We propose mean-field Langevin averaged gradient (MFL-AG),\n",
            "a single-loop algorithm that implements gradient descent ascent in the\n",
            "distribution spaces with a novel weighted averaging, and establish\n",
            "average-iterate convergence to the mixed Nash equilibrium. We also study both\n",
            "time and particle discretization regimes and prove a new uniform-in-time\n",
            "propagation of chaos result which accounts for the dependency of the particle\n",
            "interactions on all previous distributions. Furthermore, we propose mean-field\n",
            "Langevin anchored best response (MFL-ABR), a symmetric double-loop algorithm\n",
            "based on best response dynamics with linear last-iterate convergence. Finally,\n",
            "we study applications to zero-sum Markov games and conduct simulations\n",
            "demonstrating long-term optimality.\n",
            "\n",
            "846. Title: TEMPO: Prompt-based Generative Pre-trained Transformer for Time Series Forecasting\n",
            "   Abstract: Gaussian processes (GPs) are popular nonparametric statistical models for\n",
            "learning unknown functions and quantifying the spatiotemporal uncertainty in\n",
            "data. Recent works have extended GPs to model scalar and vector quantities\n",
            "distributed over non-Euclidean domains, including smooth manifolds appearing in\n",
            "numerous fields such as computer vision, dynamical systems, and neuroscience.\n",
            "However, these approaches assume that the manifold underlying the data is\n",
            "known, limiting their practical utility. We introduce RVGP, a generalisation of\n",
            "GPs for learning vector signals over latent Riemannian manifolds. Our method\n",
            "uses positional encoding with eigenfunctions of the connection Laplacian,\n",
            "associated with the tangent bundle, readily derived from common graph-based\n",
            "approximation of data. We demonstrate that RVGP possesses global regularity\n",
            "over the manifold, which allows it to super-resolve and inpaint vector fields\n",
            "while preserving singularities. Furthermore, we use RVGP to reconstruct\n",
            "high-density neural dynamics derived from low-density EEG recordings in healthy\n",
            "individuals and Alzheimer's patients. We show that vector field singularities\n",
            "are important disease markers and that their reconstruction leads to a\n",
            "comparable classification accuracy of disease states to high-density\n",
            "recordings. Thus, our method overcomes a significant practical limitation in\n",
            "experimental and clinical applications.\n",
            "\n",
            "847. Title: Active Test-Time Adaptation: Theoretical Analyses and An Algorithm\n",
            "   Abstract: Diffusion models are a class of flexible generative models trained with an\n",
            "approximation to the log-likelihood objective. However, most use cases of\n",
            "diffusion models are not concerned with likelihoods, but instead with\n",
            "downstream objectives such as human-perceived image quality or drug\n",
            "effectiveness. In this paper, we investigate reinforcement learning methods for\n",
            "directly optimizing diffusion models for such objectives. We describe how\n",
            "posing denoising as a multi-step decision-making problem enables a class of\n",
            "policy gradient algorithms, which we refer to as denoising diffusion policy\n",
            "optimization (DDPO), that are more effective than alternative reward-weighted\n",
            "likelihood approaches. Empirically, DDPO is able to adapt text-to-image\n",
            "diffusion models to objectives that are difficult to express via prompting,\n",
            "such as image compressibility, and those derived from human feedback, such as\n",
            "aesthetic quality. Finally, we show that DDPO can improve prompt-image\n",
            "alignment using feedback from a vision-language model without the need for\n",
            "additional data collection or human annotation. The project's website can be\n",
            "found at http://rl-diffusion.github.io .\n",
            "\n",
            "848. Title: Parsing neural dynamics with infinite recurrent switching linear dynamical systems\n",
            "   Abstract: Recent efforts in fine-tuning language models often rely on automatic data\n",
            "selection, commonly using Nearest Neighbors retrieval from large datasets.\n",
            "However, we theoretically show that this approach tends to select redundant\n",
            "data, limiting its effectiveness or even hurting performance. To address this,\n",
            "we introduce SIFT, a data selection algorithm designed to reduce uncertainty\n",
            "about the model's response given a prompt, which unifies ideas from retrieval\n",
            "and active learning. Whereas Nearest Neighbor retrieval typically fails in the\n",
            "presence of information duplication, SIFT accounts for information duplication\n",
            "and optimizes the overall information gain of the selected examples. We focus\n",
            "our evaluations on fine-tuning at test-time for prompt-specific language\n",
            "modeling on the Pile dataset, and show that SIFT consistently outperforms\n",
            "Nearest Neighbor retrieval, with minimal computational overhead. Moreover, we\n",
            "show that our uncertainty estimates can predict the performance gain of\n",
            "test-time fine-tuning, and use this to develop an adaptive algorithm that\n",
            "invests test-time compute proportional to realized performance gains. We\n",
            "provide the $\\texttt{activeft}$ (Active Fine-Tuning) library which can be used\n",
            "as a drop-in replacement for Nearest Neighbor retrieval.\n",
            "\n",
            "849. Title: Symmetric Mean-field Langevin Dynamics for Distributional Minimax Problems\n",
            "   Abstract: Binary classification involves predicting the label of an instance based on\n",
            "whether the model score for the positive class exceeds a threshold chosen based\n",
            "on the application requirements (e.g., maximizing recall for a precision\n",
            "bound). However, model scores are often not aligned with the true positivity\n",
            "rate. This is especially true when the training involves a differential\n",
            "sampling across classes or there is distributional drift between train and test\n",
            "settings. In this paper, we provide theoretical analysis and empirical evidence\n",
            "of the dependence of model score estimation bias on both uncertainty and score\n",
            "itself. Further, we formulate the decision boundary selection in terms of both\n",
            "model score and uncertainty, prove that it is NP-hard, and present algorithms\n",
            "based on dynamic programming and isotonic regression. Evaluation of the\n",
            "proposed algorithms on three real-world datasets yield 25%-40% gain in recall\n",
            "at high precision bounds over the traditional approach of using model score\n",
            "alone, highlighting the benefits of leveraging uncertainty.\n",
            "\n",
            "850. Title: Towards Poisoning Fair Representations\n",
            "   Abstract: Fair machine learning seeks to mitigate model prediction bias against certain\n",
            "demographic subgroups such as elder and female. Recently, fair representation\n",
            "learning (FRL) trained by deep neural networks has demonstrated superior\n",
            "performance, whereby representations containing no demographic information are\n",
            "inferred from the data and then used as the input to classification or other\n",
            "downstream tasks. Despite the development of FRL methods, their vulnerability\n",
            "under data poisoning attack, a popular protocol to benchmark model robustness\n",
            "under adversarial scenarios, is under-explored. Data poisoning attacks have\n",
            "been developed for classical fair machine learning methods which incorporate\n",
            "fairness constraints into shallow-model classifiers. Nonetheless, these attacks\n",
            "fall short in FRL due to notably different fairness goals and model\n",
            "architectures. This work proposes the first data poisoning framework attacking\n",
            "FRL. We induce the model to output unfair representations that contain as much\n",
            "demographic information as possible by injecting carefully crafted poisoning\n",
            "samples into the training data. This attack entails a prohibitive bilevel\n",
            "optimization, wherefore an effective approximated solution is proposed. A\n",
            "theoretical analysis on the needed number of poisoning samples is derived and\n",
            "sheds light on defending against the attack. Experiments on benchmark fairness\n",
            "datasets and state-of-the-art fair representation learning models demonstrate\n",
            "the superiority of our attack.\n",
            "\n",
            "851. Title: Sample Efficient Myopic Exploration Through Multitask Reinforcement Learning with Diverse Tasks\n",
            "   Abstract: In Causal Bayesian Optimization (CBO), an agent intervenes on an unknown\n",
            "structural causal model to maximize a downstream reward variable. In this\n",
            "paper, we consider the generalization where other agents or external events\n",
            "also intervene on the system, which is key for enabling adaptiveness to\n",
            "non-stationarities such as weather changes, market forces, or adversaries. We\n",
            "formalize this generalization of CBO as Adversarial Causal Bayesian\n",
            "Optimization (ACBO) and introduce the first algorithm for ACBO with bounded\n",
            "regret: Causal Bayesian Optimization with Multiplicative Weights (CBO-MW). Our\n",
            "approach combines a classical online learning strategy with causal modeling of\n",
            "the rewards. To achieve this, it computes optimistic counterfactual reward\n",
            "estimates by propagating uncertainty through the causal graph. We derive regret\n",
            "bounds for CBO-MW that naturally depend on graph-related quantities. We further\n",
            "propose a scalable implementation for the case of combinatorial interventions\n",
            "and submodular rewards. Empirically, CBO-MW outperforms non-causal and\n",
            "non-adversarial Bayesian optimization methods on synthetic environments and\n",
            "environments based on real-word data. Our experiments include a realistic\n",
            "demonstration of how CBO-MW can be used to learn users' demand patterns in a\n",
            "shared mobility system and reposition vehicles in strategic areas.\n",
            "\n",
            "852. Title: Composed Image Retrieval with Text Feedback via Multi-grained Uncertainty Regularization\n",
            "   Abstract: Multitask Reinforcement Learning (MTRL) approaches have gained increasing\n",
            "attention for its wide applications in many important Reinforcement Learning\n",
            "(RL) tasks. However, while recent advancements in MTRL theory have focused on\n",
            "the improved statistical efficiency by assuming a shared structure across\n",
            "tasks, exploration--a crucial aspect of RL--has been largely overlooked. This\n",
            "paper addresses this gap by showing that when an agent is trained on a\n",
            "sufficiently diverse set of tasks, a generic policy-sharing algorithm with\n",
            "myopic exploration design like $\\epsilon$-greedy that are inefficient in\n",
            "general can be sample-efficient for MTRL. To the best of our knowledge, this is\n",
            "the first theoretical demonstration of the \"exploration benefits\" of MTRL. It\n",
            "may also shed light on the enigmatic success of the wide applications of myopic\n",
            "exploration in practice. To validate the role of diversity, we conduct\n",
            "experiments on synthetic robotic control environments, where the diverse task\n",
            "set aligns with the task selection by automatic curriculum learning, which is\n",
            "empirically shown to improve sample-efficiency.\n",
            "\n",
            "853. Title: Improving Intrinsic Exploration by Creating Stationary Objectives\n",
            "   Abstract: Large language models (LLMs) exploit in-context learning (ICL) to solve tasks\n",
            "with only a few demonstrations, but its mechanisms are not yet well-understood.\n",
            "Some works suggest that LLMs only recall already learned concepts from\n",
            "pre-training, while others hint that ICL performs implicit learning over\n",
            "demonstrations. We characterize two ways through which ICL leverages\n",
            "demonstrations. Task recognition (TR) captures the extent to which LLMs can\n",
            "recognize a task through demonstrations -- even without ground-truth labels --\n",
            "and apply their pre-trained priors, whereas task learning (TL) is the ability\n",
            "to capture new input-label mappings unseen in pre-training. Using a wide range\n",
            "of classification datasets and three LLM families (GPT-3, LLaMA and OPT), we\n",
            "design controlled experiments to disentangle the roles of TR and TL in ICL. We\n",
            "show that (1) models can achieve non-trivial performance with only TR, and TR\n",
            "does not further improve with larger models or more demonstrations; (2) LLMs\n",
            "acquire TL as the model scales, and TL's performance consistently improves with\n",
            "more demonstrations in context. Our findings unravel two different forces\n",
            "behind ICL and we advocate for discriminating them in future ICL research due\n",
            "to their distinct nature.\n",
            "\n",
            "854. Title: Adversarial Causal Bayesian Optimization\n",
            "   Abstract: Self-supervised learning has unlocked the potential of scaling up pretraining\n",
            "to billions of images, since annotation is unnecessary. But are we making the\n",
            "best use of data? How more economical can we be? In this work, we attempt to\n",
            "answer this question by making two contributions. First, we investigate\n",
            "first-person videos and introduce a \"Walking Tours\" dataset. These videos are\n",
            "high-resolution, hours-long, captured in a single uninterrupted take, depicting\n",
            "a large number of objects and actions with natural scene transitions. They are\n",
            "unlabeled and uncurated, thus realistic for self-supervision and comparable\n",
            "with human learning.\n",
            "  Second, we introduce a novel self-supervised image pretraining method\n",
            "tailored for learning from continuous videos. Existing methods typically adapt\n",
            "image-based pretraining approaches to incorporate more frames. Instead, we\n",
            "advocate a \"tracking to learn to recognize\" approach. Our method called DoRA,\n",
            "leads to attention maps that Discover and tRAck objects over time in an\n",
            "end-to-end manner, using transformer cross-attention. We derive multiple views\n",
            "from the tracks and use them in a classical self-supervised distillation loss.\n",
            "Using our novel approach, a single Walking Tours video remarkably becomes a\n",
            "strong competitor to ImageNet for several image and video downstream tasks.\n",
            "\n",
            "855. Title: Is ImageNet worth 1 video? Learning strong image encoders from 1 long unlabelled video\n",
            "   Abstract: Exploration bonuses in reinforcement learning guide long-horizon exploration\n",
            "by defining custom intrinsic objectives. Several exploration objectives like\n",
            "count-based bonuses, pseudo-counts, and state-entropy maximization are\n",
            "non-stationary and hence are difficult to optimize for the agent. While this\n",
            "issue is generally known, it is usually omitted and solutions remain\n",
            "under-explored. The key contribution of our work lies in transforming the\n",
            "original non-stationary rewards into stationary rewards through an augmented\n",
            "state representation. For this purpose, we introduce the Stationary Objectives\n",
            "For Exploration (SOFE) framework. SOFE requires identifying sufficient\n",
            "statistics for different exploration bonuses and finding an efficient encoding\n",
            "of these statistics to use as input to a deep network. SOFE is based on\n",
            "proposing state augmentations that expand the state space but hold the promise\n",
            "of simplifying the optimization of the agent's objective. We show that SOFE\n",
            "improves the performance of several exploration objectives, including\n",
            "count-based bonuses, pseudo-counts, and state-entropy maximization. Moreover,\n",
            "SOFE outperforms prior methods that attempt to stabilize the optimization of\n",
            "intrinsic objectives. We demonstrate the efficacy of SOFE in hard-exploration\n",
            "problems, including sparse-reward tasks, pixel-based observations, 3D\n",
            "navigation, and procedurally generated environments.\n",
            "\n",
            "856. Title: EmerDiff: Emerging Pixel-level Semantic Knowledge in Diffusion Models\n",
            "   Abstract: Reinforcement Learning with Human Feedback (RLHF) is the most prominent\n",
            "method for Language Model (LM) alignment. However, RLHF is an unstable and\n",
            "data-hungry process that continually requires new high-quality LM-generated\n",
            "data for finetuning. We introduce Advantage-Leftover Lunch RL (A-LoL), a new\n",
            "class of offline policy gradient algorithms that enable RL training on any\n",
            "pre-existing data. By assuming the entire LM output sequence as a single\n",
            "action, A-LoL allows incorporating sequence-level classifiers or human-designed\n",
            "scoring functions as rewards. Subsequently, by using LM's value estimate, A-LoL\n",
            "only trains on positive advantage (leftover) data points, making it resilient\n",
            "to noise. Overall, A-LoL is an easy-to-implement, sample-efficient, and stable\n",
            "LM training recipe.\n",
            "  We demonstrate the effectiveness of A-LoL and its variants with a set of four\n",
            "different language generation tasks. We compare against both online RL (PPO)\n",
            "and recent preference-based (DPO, PRO) and reward-based (GOLD) offline RL\n",
            "baselines. On the commonly-used RLHF benchmark, Helpful and Harmless Assistant\n",
            "(HHA), LMs trained with A-LoL methods achieve the highest diversity while also\n",
            "being rated more safe and helpful than the baselines according to humans.\n",
            "Additionally, in the remaining three tasks, A-LoL could optimize multiple\n",
            "distinct reward functions even when using noisy or suboptimal training data.\n",
            "  We also release our experimental code. https://github.com/abaheti95/LoL-RL\n",
            "\n",
            "857. Title: An Agnostic View on the Cost of Overfitting in (Kernel) Ridge Regression\n",
            "   Abstract: We investigate composed image retrieval with text feedback. Users gradually\n",
            "look for the target of interest by moving from coarse to fine-grained feedback.\n",
            "However, existing methods merely focus on the latter, i.e., fine-grained\n",
            "search, by harnessing positive and negative pairs during training. This\n",
            "pair-based paradigm only considers the one-to-one distance between a pair of\n",
            "specific points, which is not aligned with the one-to-many coarse-grained\n",
            "retrieval process and compromises the recall rate. In an attempt to fill this\n",
            "gap, we introduce a unified learning approach to simultaneously modeling the\n",
            "coarse- and fine-grained retrieval by considering the multi-grained\n",
            "uncertainty. The key idea underpinning the proposed method is to integrate\n",
            "fine- and coarse-grained retrieval as matching data points with small and large\n",
            "fluctuations, respectively. Specifically, our method contains two modules:\n",
            "uncertainty modeling and uncertainty regularization. (1) The uncertainty\n",
            "modeling simulates the multi-grained queries by introducing identically\n",
            "distributed fluctuations in the feature space. (2) Based on the uncertainty\n",
            "modeling, we further introduce uncertainty regularization to adapt the matching\n",
            "objective according to the fluctuation range. Compared with existing methods,\n",
            "the proposed strategy explicitly prevents the model from pushing away potential\n",
            "candidates in the early stage, and thus improves the recall rate. On the three\n",
            "public datasets, i.e., FashionIQ, Fashion200k, and Shoes, the proposed method\n",
            "has achieved +4.03%, +3.38%, and +2.40% Recall@50 accuracy over a strong\n",
            "baseline, respectively.\n",
            "\n",
            "858. Title: Posterior Sampling Based on Gradient Flows of the MMD with Negative Distance Kernel\n",
            "   Abstract: Learning from off-policy data is essential for sample-efficient reinforcement\n",
            "learning. In the present work, we build on the insight that the advantage\n",
            "function can be understood as the causal effect of an action on the return, and\n",
            "show that this allows us to decompose the return of a trajectory into parts\n",
            "caused by the agent's actions (skill) and parts outside of the agent's control\n",
            "(luck). Furthermore, this decomposition enables us to naturally extend Direct\n",
            "Advantage Estimation (DAE) to off-policy settings (Off-policy DAE). The\n",
            "resulting method can learn from off-policy trajectories without relying on\n",
            "importance sampling techniques or truncating off-policy actions. We draw\n",
            "connections between Off-policy DAE and previous methods to demonstrate how it\n",
            "can speed up learning and when the proposed off-policy corrections are\n",
            "important. Finally, we use the MinAtar environments to illustrate how ignoring\n",
            "off-policy corrections can lead to suboptimal policy optimization performance.\n",
            "\n",
            "859. Title: Trajeglish: Traffic Modeling as Next-Token Prediction\n",
            "   Abstract: This paper introduces an efficient strategy to transform Large Language\n",
            "Models (LLMs) into Multi-Modal Large Language Models (MLLMs). By\n",
            "conceptualizing this transformation as a domain adaptation process, i.e.,\n",
            "transitioning from text understanding to embracing multiple modalities, we\n",
            "intriguingly note that, within each attention block, tuning LayerNorm suffices\n",
            "to yield strong performance. Moreover, when benchmarked against other tuning\n",
            "approaches like full parameter finetuning or LoRA, its benefits on efficiency\n",
            "are substantial. For example, when compared to LoRA on a 13B model scale,\n",
            "performance can be enhanced by an average of over 20% across five multi-modal\n",
            "tasks, and meanwhile, results in a significant reduction of trainable\n",
            "parameters by 41.9% and a decrease in GPU memory usage by 17.6%. On top of this\n",
            "LayerNorm strategy, we showcase that selectively tuning only with\n",
            "conversational data can improve efficiency further. Beyond these empirical\n",
            "outcomes, we provide a comprehensive analysis to explore the role of LayerNorm\n",
            "in adapting LLMs to the multi-modal domain and improving the expressive power\n",
            "of the model.\n",
            "\n",
            "860. Title: Addressing Signal Delay in Deep Reinforcement Learning\n",
            "   Abstract: We study the cost of overfitting in noisy kernel ridge regression (KRR),\n",
            "which we define as the ratio between the test error of the interpolating\n",
            "ridgeless model and the test error of the optimally-tuned model. We take an\n",
            "\"agnostic\" view in the following sense: we consider the cost as a function of\n",
            "sample size for any target function, even if the sample size is not large\n",
            "enough for consistency or the target is outside the RKHS. We analyze the cost\n",
            "of overfitting under a Gaussian universality ansatz using recently derived\n",
            "(non-rigorous) risk estimates in terms of the task eigenstructure. Our analysis\n",
            "provides a more refined characterization of benign, tempered and catastrophic\n",
            "overfitting (cf. Mallinar et al. 2022).\n",
            "\n",
            "861. Title: Neuro-Inspired Information-Theoretic Hierarchical Perception for Multimodal Learning\n",
            "   Abstract: This paper describes a parsing model that combines the exact dynamic\n",
            "programming of CRF parsing with the rich nonlinear featurization of neural net\n",
            "approaches. Our model is structurally a CRF that factors over anchored rule\n",
            "productions, but instead of linear potential functions based on sparse\n",
            "features, we use nonlinear potentials computed via a feedforward neural\n",
            "network. Because potentials are still local to anchored rules, structured\n",
            "inference (CKY) is unchanged from the sparse case. Computing gradients during\n",
            "learning involves backpropagating an error signal formed from standard CRF\n",
            "sufficient statistics (expected rule counts). Using only dense features, our\n",
            "neural CRF already exceeds a strong baseline CRF model (Hall et al., 2014). In\n",
            "combination with sparse features, our system achieves 91.1 F1 on section 23 of\n",
            "the Penn Treebank, and more generally outperforms the best prior single parser\n",
            "results on a range of languages.\n",
            "\n",
            "862. Title: Skill or Luck? Return Decomposition via Advantage Functions\n",
            "   Abstract: Large Language Models (LLMs) excel in various tasks, but they rely on\n",
            "carefully crafted prompts that often demand substantial human effort. To\n",
            "automate this process, in this paper, we propose a novel framework for discrete\n",
            "prompt optimization, called EvoPrompt, which borrows the idea of evolutionary\n",
            "algorithms (EAs) as they exhibit good performance and fast convergence. To\n",
            "enable EAs to work on discrete prompts, which are natural language expressions\n",
            "that need to be coherent and human-readable, we connect LLMs with EAs. This\n",
            "approach allows us to simultaneously leverage the powerful language processing\n",
            "capabilities of LLMs and the efficient optimization performance of EAs.\n",
            "Specifically, abstaining from any gradients or parameters, EvoPrompt starts\n",
            "from a population of prompts and iteratively generates new prompts with LLMs\n",
            "based on the evolutionary operators, improving the population based on the\n",
            "development set. We optimize prompts for both closed- and open-source LLMs\n",
            "including GPT-3.5 and Alpaca, on 31 datasets covering language understanding,\n",
            "generation tasks, as well as BIG-Bench Hard (BBH) tasks. EvoPrompt\n",
            "significantly outperforms human-engineered prompts and existing methods for\n",
            "automatic prompt generation (e.g., up to 25% on BBH). Furthermore, EvoPrompt\n",
            "demonstrates that connecting LLMs with EAs creates synergies, which could\n",
            "inspire further research on the combination of LLMs and conventional\n",
            "algorithms.\n",
            "\n",
            "863. Title: Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers\n",
            "   Abstract: In value-based reinforcement learning methods such as deep Q-learning,\n",
            "function approximation errors are known to lead to overestimated value\n",
            "estimates and suboptimal policies. We show that this problem persists in an\n",
            "actor-critic setting and propose novel mechanisms to minimize its effects on\n",
            "both the actor and the critic. Our algorithm builds on Double Q-learning, by\n",
            "taking the minimum value between a pair of critics to limit overestimation. We\n",
            "draw the connection between target networks and overestimation bias, and\n",
            "suggest delaying policy updates to reduce per-update error and further improve\n",
            "performance. We evaluate our method on the suite of OpenAI gym tasks,\n",
            "outperforming the state of the art in every environment tested.\n",
            "\n",
            "864. Title: Reasoning on Graphs: Faithful and Interpretable Large Language Model Reasoning\n",
            "   Abstract: This work addresses the problem of sensing the world: how to learn a\n",
            "multimodal representation of a reinforcement learning agent's environment that\n",
            "allows the execution of tasks under incomplete perceptual conditions. To\n",
            "address such problem, we argue for hierarchy in the design of representation\n",
            "models and contribute with a novel multimodal representation model, MUSE. The\n",
            "proposed model learns hierarchical representations: low-level modality-specific\n",
            "representations, encoded from raw observation data, and a high-level multimodal\n",
            "representation, encoding joint-modality information to allow robust state\n",
            "estimation. We employ MUSE as the sensory representation model of deep\n",
            "reinforcement learning agents provided with multimodal observations in Atari\n",
            "games. We perform a comparative study over different designs of reinforcement\n",
            "learning agents, showing that MUSE allows agents to perform tasks under\n",
            "incomplete perceptual experience with minimal performance loss. Finally, we\n",
            "evaluate the performance of MUSE in literature-standard multimodal scenarios\n",
            "with higher number and more complex modalities, showing that it outperforms\n",
            "state-of-the-art multimodal variational autoencoders in single and\n",
            "cross-modality generation.\n",
            "\n",
            "865. Title: A Lightweight Method for Tackling Unknown Participation Statistics in Federated Averaging\n",
            "   Abstract: Generating step-by-step \"chain-of-thought\" rationales improves language model\n",
            "performance on complex reasoning tasks like mathematics or commonsense\n",
            "question-answering. However, inducing language model rationale generation\n",
            "currently requires either constructing massive rationale datasets or\n",
            "sacrificing accuracy by using only few-shot inference. We propose a technique\n",
            "to iteratively leverage a small number of rationale examples and a large\n",
            "dataset without rationales, to bootstrap the ability to perform successively\n",
            "more complex reasoning. This technique, the \"Self-Taught Reasoner\" (STaR),\n",
            "relies on a simple loop: generate rationales to answer many questions, prompted\n",
            "with a few rationale examples; if the generated answers are wrong, try again to\n",
            "generate a rationale given the correct answer; fine-tune on all the rationales\n",
            "that ultimately yielded correct answers; repeat. We show that STaR\n",
            "significantly improves performance on multiple datasets compared to a model\n",
            "fine-tuned to directly predict final answers, and performs comparably to\n",
            "fine-tuning a 30$\\times$ larger state-of-the-art language model on\n",
            "CommensenseQA. Thus, STaR lets a model improve itself by learning from its own\n",
            "generated reasoning.\n",
            "\n",
            "866. Title: Accurate and Scalable Estimation of Epistemic Uncertainty for Graph Neural Networks\n",
            "   Abstract: In federated learning (FL), clients usually have diverse participation\n",
            "statistics that are unknown a priori, which can significantly harm the\n",
            "performance of FL if not handled properly. Existing works aiming at addressing\n",
            "this problem are usually based on global variance reduction, which requires a\n",
            "substantial amount of additional memory in a multiplicative factor equal to the\n",
            "total number of clients. An important open problem is to find a lightweight\n",
            "method for FL in the presence of clients with unknown participation rates. In\n",
            "this paper, we address this problem by adapting the aggregation weights in\n",
            "federated averaging (FedAvg) based on the participation history of each client.\n",
            "We first show that, with heterogeneous participation statistics, FedAvg with\n",
            "non-optimal aggregation weights can diverge from the optimal solution of the\n",
            "original FL objective, indicating the need of finding optimal aggregation\n",
            "weights. However, it is difficult to compute the optimal weights when the\n",
            "participation statistics are unknown. To address this problem, we present a new\n",
            "algorithm called FedAU, which improves FedAvg by adaptively weighting the\n",
            "client updates based on online estimates of the optimal weights without knowing\n",
            "the statistics of client participation. We provide a theoretical convergence\n",
            "analysis of FedAU using a novel methodology to connect the estimation error and\n",
            "convergence. Our theoretical results reveal important and interesting insights,\n",
            "while showing that FedAU converges to an optimal solution of the original\n",
            "objective and has desirable properties such as linear speedup. Our experimental\n",
            "results also verify the advantage of FedAU over baseline methods with various\n",
            "participation patterns.\n",
            "\n",
            "867. Title: Learning semilinear neural operators: A unified recursive framework for prediction and data assimilation.\n",
            "   Abstract: Recent advances in the theory of Neural Operators (NOs) have enabled fast and\n",
            "accurate computation of the solutions to complex systems described by partial\n",
            "differential equations (PDEs). Despite their great success, current NO-based\n",
            "solutions face important challenges when dealing with spatio-temporal PDEs over\n",
            "long time scales. Specifically, the current theory of NOs does not present a\n",
            "systematic framework to perform data assimilation and efficiently correct the\n",
            "evolution of PDE solutions over time based on sparsely sampled noisy\n",
            "measurements. In this paper, we propose a learning-based state-space approach\n",
            "to compute the solution operators to infinite-dimensional semilinear PDEs.\n",
            "Exploiting the structure of semilinear PDEs and the theory of nonlinear\n",
            "observers in function spaces, we develop a flexible recursive method that\n",
            "allows for both prediction and data assimilation by combining prediction and\n",
            "correction operations. The proposed framework is capable of producing fast and\n",
            "accurate predictions over long time horizons, dealing with irregularly sampled\n",
            "noisy measurements to correct the solution, and benefits from the decoupling\n",
            "between the spatial and temporal dynamics of this class of PDEs. We show\n",
            "through experiments on the Kuramoto-Sivashinsky, Navier-Stokes and Korteweg-de\n",
            "Vries equations that the proposed model is robust to noise and can leverage\n",
            "arbitrary amounts of measurements to correct its prediction over a long time\n",
            "horizon with little computational overhead.\n",
            "\n",
            "868. Title: PanoDiffusion: 360-degree Panorama Outpainting via Diffusion\n",
            "   Abstract: We propose conditional flows of the maximum mean discrepancy (MMD) with the\n",
            "negative distance kernel for posterior sampling and conditional generative\n",
            "modeling. This MMD, which is also known as energy distance, has several\n",
            "advantageous properties like efficient computation via slicing and sorting. We\n",
            "approximate the joint distribution of the ground truth and the observations\n",
            "using discrete Wasserstein gradient flows and establish an error bound for the\n",
            "posterior distributions. Further, we prove that our particle flow is indeed a\n",
            "Wasserstein gradient flow of an appropriate functional. The power of our method\n",
            "is demonstrated by numerical examples including conditional image generation\n",
            "and inverse problems like superresolution, inpainting and computed tomography\n",
            "in low-dose and limited-angle settings.\n",
            "\n",
            "869. Title: Alleviating Exposure Bias in Diffusion Models through Sampling with Shifted Time Steps\n",
            "   Abstract: Retrieval-augmented language models (RALMs) hold promise to produce language\n",
            "understanding systems that are are factual, efficient, and up-to-date. An\n",
            "important desideratum of RALMs, is that retrieved information helps model\n",
            "performance when it is relevant, and does not harm performance when it is not.\n",
            "This is particularly important in multi-hop reasoning scenarios, where misuse\n",
            "of irrelevant evidence can lead to cascading errors. However, recent work has\n",
            "shown that retrieval augmentation can sometimes have a negative effect on\n",
            "performance. In this work, we present a thorough analysis on five open-domain\n",
            "question answering benchmarks, characterizing cases when retrieval reduces\n",
            "accuracy. We then propose two methods to mitigate this issue. First, a simple\n",
            "baseline that filters out retrieved passages that do not entail question-answer\n",
            "pairs according to a natural language inference (NLI) model. This is effective\n",
            "in preventing performance reduction, but at a cost of also discarding relevant\n",
            "passages. Thus, we propose a method for automatically generating data to\n",
            "fine-tune the language model to properly leverage retrieved passages, using a\n",
            "mix of relevant and irrelevant contexts at training time. We empirically show\n",
            "that even 1,000 examples suffice to train the model to be robust to irrelevant\n",
            "contexts while maintaining high performance on examples with relevant ones.\n",
            "\n",
            "870. Title: How to Fine-Tune Vision Models with SGD\n",
            "   Abstract: In this paper, we explore the application of mean field theory, a technique\n",
            "from statistical physics, to deep metric learning and address the high training\n",
            "complexity commonly associated with conventional metric learning loss\n",
            "functions. By adapting mean field theory for deep metric learning, we develop\n",
            "an approach to design classification-based loss functions from pair-based ones,\n",
            "which can be considered complementary to the proxy-based approach. Applying the\n",
            "mean field theory to two pair-based loss functions, we derive two new loss\n",
            "functions, MeanFieldContrastive and MeanFieldClassWiseMultiSimilarity losses,\n",
            "with reduced training complexity. We extensively evaluate these derived loss\n",
            "functions on three image-retrieval datasets and demonstrate that our loss\n",
            "functions outperform baseline methods in two out of the three datasets.\n",
            "\n",
            "871. Title: DMBP: Diffusion model-based predictor for robust offline reinforcement learning against state observation perturbations\n",
            "   Abstract: Tucker decomposition is a powerful tensor model to handle multi-aspect data.\n",
            "It demonstrates the low-rank property by decomposing the grid-structured data\n",
            "as interactions between a core tensor and a set of object representations\n",
            "(factors). A fundamental assumption of such decomposition is that there are\n",
            "finite objects in each aspect or mode, corresponding to discrete indexes of\n",
            "data entries. However, real-world data is often not naturally posed in this\n",
            "setting. For example, geographic data is represented as continuous indexes of\n",
            "latitude and longitude coordinates, and cannot fit tensor models directly. To\n",
            "generalize Tucker decomposition to such scenarios, we propose Functional\n",
            "Bayesian Tucker Decomposition (FunBaT). We treat the continuous-indexed data as\n",
            "the interaction between the Tucker core and a group of latent functions. We use\n",
            "Gaussian processes (GP) as functional priors to model the latent functions.\n",
            "Then, we convert each GP into a state-space prior by constructing an equivalent\n",
            "stochastic differential equation (SDE) to reduce computational cost. An\n",
            "efficient inference algorithm is developed for scalable posterior approximation\n",
            "based on advanced message-passing techniques. The advantage of our method is\n",
            "shown in both synthetic data and several real-world applications. We release\n",
            "the code of FunBaT at\n",
            "\\url{https://github.com/xuangu-fang/Functional-Bayesian-Tucker-Decomposition}.\n",
            "\n",
            "872. Title: Effective Data Augmentation With Diffusion Models\n",
            "   Abstract: Large pretrained models such as GPT-3 have had tremendous impact on modern\n",
            "natural language processing by leveraging self-supervised learning to learn\n",
            "salient representations that can be used to readily finetune on a wide variety\n",
            "of downstream tasks. We investigate the possibility of transferring such\n",
            "advances to molecular machine learning by building a chemical foundation model,\n",
            "ChemBERTa-2, using the language of SMILES. While labeled data for molecular\n",
            "prediction tasks is typically scarce, libraries of SMILES strings are readily\n",
            "available. In this work, we build upon ChemBERTa by optimizing the pretraining\n",
            "process. We compare multi-task and self-supervised pretraining by varying\n",
            "hyperparameters and pretraining dataset size, up to 77M compounds from PubChem.\n",
            "To our knowledge, the 77M set constitutes one of the largest datasets used for\n",
            "molecular pretraining to date. We find that with these pretraining\n",
            "improvements, we are competitive with existing state-of-the-art architectures\n",
            "on the MoleculeNet benchmark suite. We analyze the degree to which improvements\n",
            "in pretraining translate to improvement on downstream tasks.\n",
            "\n",
            "873. Title: Bayesian Bi-clustering of Neural Spiking Activity with Latent Structures\n",
            "   Abstract: We characterize the statistical efficiency of knowledge transfer through $n$\n",
            "samples from a teacher to a probabilistic student classifier with input space\n",
            "$\\mathcal S$ over labels $\\mathcal A$. We show that privileged information at\n",
            "three progressive levels accelerates the transfer. At the first level, only\n",
            "samples with hard labels are known, via which the maximum likelihood estimator\n",
            "attains the minimax rate $\\sqrt{{|{\\mathcal S}||{\\mathcal A}|}/{n}}$. The\n",
            "second level has the teacher probabilities of sampled labels available in\n",
            "addition, which turns out to boost the convergence rate lower bound to\n",
            "${{|{\\mathcal S}||{\\mathcal A}|}/{n}}$. However, under this second data\n",
            "acquisition protocol, minimizing a naive adaptation of the cross-entropy loss\n",
            "results in an asymptotically biased student. We overcome this limitation and\n",
            "achieve the fundamental limit by using a novel empirical variant of the squared\n",
            "error logit loss. The third level further equips the student with the soft\n",
            "labels (complete logits) on ${\\mathcal A}$ given every sampled input, thereby\n",
            "provably enables the student to enjoy a rate ${|{\\mathcal S}|}/{n}$ free of\n",
            "$|{\\mathcal A}|$. We find any Kullback-Leibler divergence minimizer to be\n",
            "optimal in the last case. Numerical simulations distinguish the four learners\n",
            "and corroborate our theory.\n",
            "\n",
            "874. Title: LipVoicer: Generating Speech from Silent Videos Guided by Lip Reading\n",
            "   Abstract: SGD and AdamW are the two most used optimizers for fine-tuning large neural\n",
            "networks in computer vision. When the two methods perform the same, SGD is\n",
            "preferable because it uses less memory (12 bytes/parameter with momentum and 8\n",
            "bytes/parameter without) than AdamW (16 bytes/parameter). However, on a suite\n",
            "of downstream tasks, especially those with distribution shifts, we find that\n",
            "fine-tuning with AdamW performs substantially better than SGD on modern Vision\n",
            "Transformer and ConvNeXt models. We find that large gaps in performance between\n",
            "SGD and AdamW occur when the fine-tuning gradients in the first \"embedding\"\n",
            "layer are much larger than in the rest of the model. Our analysis suggests an\n",
            "easy fix that works consistently across datasets and models: freezing the\n",
            "embedding layer (less than 1% of the parameters) leads to SGD with or without\n",
            "momentum performing slightly better than AdamW while using less memory (e.g.,\n",
            "on ViT-L, SGD uses 33% less GPU memory). Our insights result in\n",
            "state-of-the-art accuracies on five popular distribution shift benchmarks:\n",
            "WILDS-FMoW, WILDS-Camelyon, BREEDS-Living-17, Waterbirds, and DomainNet.\n",
            "\n",
            "875. Title: BENO: Boundary-embedded Neural Operators for Elliptic PDEs\n",
            "   Abstract: Modern neural recording techniques allow neuroscientists to obtain spiking\n",
            "activity of multiple neurons from different brain regions over long time\n",
            "periods, which requires new statistical methods to be developed for\n",
            "understanding structure of the large-scale data. In this paper, we develop a\n",
            "bi-clustering method to cluster the neural spiking activity spatially and\n",
            "temporally, according to their low-dimensional latent structures. The spatial\n",
            "(neuron) clusters are defined by the latent trajectories within each neural\n",
            "population, while the temporal (state) clusters are defined by (populationally)\n",
            "synchronous local linear dynamics shared with different periods. To flexibly\n",
            "extract the bi-clustering structure, we build the model non-parametrically, and\n",
            "develop an efficient Markov chain Monte Carlo (MCMC) algorithm to sample the\n",
            "posterior distributions of model parameters. Validating our proposed MCMC\n",
            "algorithm through simulations, we find the method can recover unknown\n",
            "parameters and true bi-clustering structures successfully. We then apply the\n",
            "proposed bi-clustering method to multi-regional neural recordings under\n",
            "different experiment settings, where we find that simultaneously considering\n",
            "latent trajectories and spatial-temporal clustering structures can provide us\n",
            "with a more accurate and interpretable result. Overall, the proposed method\n",
            "provides scientific insights for large-scale (counting) time series with\n",
            "elongated recording periods, and it can potentially have application beyond\n",
            "neuroscience.\n",
            "\n",
            "876. Title: Towards Foundational Models for Molecular Learning on Large-Scale Multi-Task Datasets\n",
            "   Abstract: Diffusion Probabilistic Models (DPM) have shown remarkable efficacy in the\n",
            "synthesis of high-quality images. However, their inference process\n",
            "characteristically requires numerous, potentially hundreds, of iterative steps,\n",
            "which could exaggerate the problem of exposure bias due to the training and\n",
            "inference discrepancy. Previous work has attempted to mitigate this issue by\n",
            "perturbing inputs during training, which consequently mandates the retraining\n",
            "of the DPM. In this work, we conduct a systematic study of exposure bias in DPM\n",
            "and, intriguingly, we find that the exposure bias could be alleviated with a\n",
            "novel sampling method that we propose, without retraining the model. We\n",
            "empirically and theoretically show that, during inference, for each backward\n",
            "time step $t$ and corresponding state $\\hat{x}_t$, there might exist another\n",
            "time step $t_s$ which exhibits superior coupling with $\\hat{x}_t$. Based on\n",
            "this finding, we introduce a sampling method named Time-Shift Sampler. Our\n",
            "framework can be seamlessly integrated to existing sampling algorithms, such as\n",
            "DDPM, DDIM and other high-order solvers, inducing merely minimal additional\n",
            "computations. Experimental results show our method brings significant and\n",
            "consistent improvements in FID scores on different datasets and sampling\n",
            "methods. For example, integrating Time-Shift Sampler to F-PNDM yields a\n",
            "FID=3.88, achieving 44.49\\% improvements as compared to F-PNDM, on CIFAR-10\n",
            "with 10 sampling steps, which is more performant than the vanilla DDIM with 100\n",
            "sampling steps. Our code is available at https://github.com/Mingxiao-Li/TS-DPM.\n",
            "\n",
            "877. Title: Towards the Fundamental Limits of Knowledge Transfer over Finite Domains\n",
            "   Abstract: Data augmentation is one of the most prevalent tools in deep learning,\n",
            "underpinning many recent advances, including those from classification,\n",
            "generative models, and representation learning. The standard approach to data\n",
            "augmentation combines simple transformations like rotations and flips to\n",
            "generate new images from existing ones. However, these new images lack\n",
            "diversity along key semantic axes present in the data. Current augmentations\n",
            "cannot alter the high-level semantic attributes, such as animal species present\n",
            "in a scene, to enhance the diversity of data. We address the lack of diversity\n",
            "in data augmentation with image-to-image transformations parameterized by\n",
            "pre-trained text-to-image diffusion models. Our method edits images to change\n",
            "their semantics using an off-the-shelf diffusion model, and generalizes to\n",
            "novel visual concepts from a few labelled examples. We evaluate our approach on\n",
            "few-shot image classification tasks, and on a real-world weed recognition task,\n",
            "and observe an improvement in accuracy in tested domains.\n",
            "\n",
            "878. Title: From Zero to Turbulence: Generative Modeling for 3D Flow Simulation\n",
            "   Abstract: While graph neural networks (GNNs) are widely used for node and graph\n",
            "representation learning tasks, the reliability of GNN uncertainty estimates\n",
            "under distribution shifts remains relatively under-explored. Indeed, while\n",
            "post-hoc calibration strategies can be used to improve in-distribution\n",
            "calibration, they need not also improve calibration under distribution shift.\n",
            "However, techniques which produce GNNs with better intrinsic uncertainty\n",
            "estimates are particularly valuable, as they can always be combined with\n",
            "post-hoc strategies later. Therefore, in this work, we propose G-$\\Delta$UQ, a\n",
            "novel training framework designed to improve intrinsic GNN uncertainty\n",
            "estimates. Our framework adapts the principle of stochastic data centering to\n",
            "graph data through novel graph anchoring strategies, and is able to support\n",
            "partially stochastic GNNs. While, the prevalent wisdom is that fully stochastic\n",
            "networks are necessary to obtain reliable estimates, we find that the\n",
            "functional diversity induced by our anchoring strategies when sampling\n",
            "hypotheses renders this unnecessary and allows us to support G-$\\Delta$UQ on\n",
            "pretrained models. Indeed, through extensive evaluation under covariate,\n",
            "concept and graph size shifts, we show that G-$\\Delta$UQ leads to better\n",
            "calibrated GNNs for node and graph classification. Further, it also improves\n",
            "performance on the uncertainty-based tasks of out-of-distribution detection and\n",
            "generalization gap estimation. Overall, our work provides insights into\n",
            "uncertainty estimation for GNNs, and demonstrates the utility of G-$\\Delta$UQ\n",
            "in obtaining reliable estimates.\n",
            "\n",
            "879. Title: INSIDE: LLMs' Internal States Retain the Power of Hallucination Detection\n",
            "   Abstract: Simulations of turbulent flows in 3D are one of the most expensive\n",
            "simulations in computational fluid dynamics (CFD). Many works have been written\n",
            "on surrogate models to replace numerical solvers for fluid flows with faster,\n",
            "learned, autoregressive models. However, the intricacies of turbulence in three\n",
            "dimensions necessitate training these models with very small time steps, while\n",
            "generating realistic flow states requires either long roll-outs with many steps\n",
            "and significant error accumulation or starting from a known, realistic flow\n",
            "state - something we aimed to avoid in the first place. Instead, we propose to\n",
            "approach turbulent flow simulation as a generative task directly learning the\n",
            "manifold of all possible turbulent flow states without relying on any initial\n",
            "flow state. For our experiments, we introduce a challenging 3D turbulence\n",
            "dataset of high-resolution flows and detailed vortex structures caused by\n",
            "various objects and derive two novel sample evaluation metrics for turbulent\n",
            "flows. On this dataset, we show that our generative model captures the\n",
            "distribution of turbulent flows caused by unseen objects and generates\n",
            "high-quality, realistic samples amenable for downstream applications without\n",
            "access to any initial state.\n",
            "\n",
            "880. Title: Learning dynamic representations of the functional connectome in neurobiological networks\n",
            "   Abstract: Negation and uncertainty modeling are long-standing tasks in natural language\n",
            "processing. Linguistic theory postulates that expressions of negation and\n",
            "uncertainty are semantically independent from each other and the content they\n",
            "modify. However, previous works on representation learning do not explicitly\n",
            "model this independence. We therefore attempt to disentangle the\n",
            "representations of negation, uncertainty, and content using a Variational\n",
            "Autoencoder. We find that simply supervising the latent representations results\n",
            "in good disentanglement, but auxiliary objectives based on adversarial learning\n",
            "and mutual information minimization can provide additional disentanglement\n",
            "gains.\n",
            "\n",
            "881. Title: Clifford Group Equivariant Simplicial Message Passing Networks\n",
            "   Abstract: The objective for establishing dense correspondence between paired images\n",
            "consists of two terms: a data term and a prior term. While conventional\n",
            "techniques focused on defining hand-designed prior terms, which are difficult\n",
            "to formulate, recent approaches have focused on learning the data term with\n",
            "deep neural networks without explicitly modeling the prior, assuming that the\n",
            "model itself has the capacity to learn an optimal prior from a large-scale\n",
            "dataset. The performance improvement was obvious, however, they often fail to\n",
            "address inherent ambiguities of matching, such as textureless regions,\n",
            "repetitive patterns, and large displacements. To address this, we propose\n",
            "DiffMatch, a novel conditional diffusion-based framework designed to explicitly\n",
            "model both the data and prior terms. Unlike previous approaches, this is\n",
            "accomplished by leveraging a conditional denoising diffusion model. DiffMatch\n",
            "consists of two main components: conditional denoising diffusion module and\n",
            "cost injection module. We stabilize the training process and reduce memory\n",
            "usage with a stage-wise training strategy. Furthermore, to boost performance,\n",
            "we introduce an inference technique that finds a better path to the accurate\n",
            "matching field. Our experimental results demonstrate significant performance\n",
            "improvements of our method over existing approaches, and the ablation studies\n",
            "validate our design choices along with the effectiveness of each component.\n",
            "Project page is available at https://ku-cvlab.github.io/DiffMatch/.\n",
            "\n",
            "882. Title: Kalman Filter for Online Classification of Non-Stationary Data\n",
            "   Abstract: We introduce Clifford Group Equivariant Simplicial Message Passing Networks,\n",
            "a method for steerable E(n)-equivariant message passing on simplicial\n",
            "complexes. Our method integrates the expressivity of Clifford group-equivariant\n",
            "layers with simplicial message passing, which is topologically more intricate\n",
            "than regular graph message passing. Clifford algebras include higher-order\n",
            "objects such as bivectors and trivectors, which express geometric features\n",
            "(e.g., areas, volumes) derived from vectors. Using this knowledge, we represent\n",
            "simplex features through geometric products of their vertices. To achieve\n",
            "efficient simplicial message passing, we share the parameters of the message\n",
            "network across different dimensions. Additionally, we restrict the final\n",
            "message to an aggregation of the incoming messages from different dimensions,\n",
            "leading to what we term shared simplicial message passing. Experimental results\n",
            "show that our method is able to outperform both equivariant and simplicial\n",
            "graph neural networks on a variety of geometric tasks.\n",
            "\n",
            "883. Title: StructComp: Substituting propagation with Structural Compression in Training Graph Contrastive Learning\n",
            "   Abstract: The static synaptic connectivity of neuronal circuits stands in direct\n",
            "contrast to the dynamics of their function. As in changing community\n",
            "interactions, different neurons can participate actively in various\n",
            "combinations to effect behaviors at different times. We introduce an\n",
            "unsupervised approach to learn the dynamic affinities between neurons in live,\n",
            "behaving animals, and to reveal which communities form among neurons at\n",
            "different times. The inference occurs in two major steps. First, pairwise\n",
            "non-linear affinities between neuronal traces from brain-wide calcium activity\n",
            "are organized by non-negative tensor factorization (NTF). Each factor specifies\n",
            "which groups of neurons are most likely interacting for an inferred interval in\n",
            "time, and for which animals. Finally, a generative model that allows for\n",
            "weighted community detection is applied to the functional motifs produced by\n",
            "NTF to reveal a dynamic functional connectome. Since time codes the different\n",
            "experimental variables (e.g., application of chemical stimuli), this provides\n",
            "an atlas of neural motifs active during separate stages of an experiment (e.g.,\n",
            "stimulus application or spontaneous behaviors). Results from our analysis are\n",
            "experimentally validated, confirming that our method is able to robustly\n",
            "predict causal interactions between neurons to generate behavior. Code is\n",
            "available at https://github.com/dyballa/dynamic-connectomes.\n",
            "\n",
            "884. Title: Understanding prompt engineering may not require rethinking generalization\n",
            "   Abstract: In reinforcement learning, agents often learn policies for specific tasks\n",
            "without the ability to generalize this knowledge to related tasks. This paper\n",
            "introduces an algorithm that attempts to address this limitation by decomposing\n",
            "neural networks encoding policies for Markov Decision Processes into reusable\n",
            "sub-policies, which are used to synthesize temporally extended actions, or\n",
            "options. We consider neural networks with piecewise linear activation\n",
            "functions, so that they can be mapped to an equivalent tree that is similar to\n",
            "oblique decision trees. Since each node in such a tree serves as a function of\n",
            "the input of the tree, each sub-tree is a sub-policy of the main policy. We\n",
            "turn each of these sub-policies into options by wrapping it with while-loops of\n",
            "varied number of iterations. Given the large number of options, we propose a\n",
            "selection mechanism based on minimizing the Levin loss for a uniform policy on\n",
            "these options. Empirical results in two grid-world domains where exploration\n",
            "can be difficult confirm that our method can identify useful options, thereby\n",
            "accelerating the learning process on similar but different tasks.\n",
            "\n",
            "885. Title: Unveiling Options with Neural Network Decomposition\n",
            "   Abstract: Zero-shot learning in prompted vision-language models, the practice of\n",
            "crafting prompts to build classifiers without an explicit training process, has\n",
            "achieved impressive performance in many settings. This success presents a\n",
            "seemingly surprising observation: these methods suffer relatively little from\n",
            "overfitting, i.e., when a prompt is manually engineered to achieve low error on\n",
            "a given training set (thus rendering the method no longer actually zero-shot),\n",
            "the approach still performs well on held-out test data. In this paper, we show\n",
            "that we can explain such performance well via recourse to classical PAC-Bayes\n",
            "bounds. Specifically, we show that the discrete nature of prompts, combined\n",
            "with a PAC-Bayes prior given by a language model, results in generalization\n",
            "bounds that are remarkably tight by the standards of the literature: for\n",
            "instance, the generalization bound of an ImageNet classifier is often within a\n",
            "few percentage points of the true test error. We demonstrate empirically that\n",
            "this holds for existing handcrafted prompts and prompts generated through\n",
            "simple greedy search. Furthermore, the resulting bound is well-suited for model\n",
            "selection: the models with the best bound typically also have the best test\n",
            "performance. This work thus provides a possible justification for the\n",
            "widespread practice of prompt engineering, even if it seems that such methods\n",
            "could potentially overfit the training data.\n",
            "\n",
            "886. Title: Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake Analysis\n",
            "   Abstract: The rapid development of large language models (LLMs) has not only provided\n",
            "numerous opportunities but also presented significant challenges. This becomes\n",
            "particularly evident when LLMs inadvertently generate harmful or toxic content,\n",
            "either unintentionally or because of intentional inducement. Existing alignment\n",
            "methods usually direct LLMs toward the favorable outcomes by utilizing\n",
            "human-annotated, flawless instruction-response pairs. Conversely, this study\n",
            "proposes a novel alignment technique based on mistake analysis, which\n",
            "deliberately exposes LLMs to erroneous content to learn the reasons for\n",
            "mistakes and how to avoid them. In this case, mistakes are repurposed into\n",
            "valuable data for alignment, effectively helping to avoid the production of\n",
            "erroneous responses. Without external models or human annotations, our method\n",
            "leverages a model's intrinsic ability to discern undesirable mistakes and\n",
            "improves the safety of its generated responses. Experimental results reveal\n",
            "that our method outperforms existing alignment approaches in enhancing model\n",
            "safety while maintaining the overall utility.\n",
            "\n",
            "887. Title: Interpretable Sparse System Identification: Beyond Recent Deep Learning Techniques on Time-Series Prediction\n",
            "   Abstract: In recent years, usage and applications of Autonomous Underwater Vehicles has\n",
            "grown rapidly. Interaction of divers with the AUVs remains an integral part of\n",
            "the usage of AUVs for various applications and makes building robust and\n",
            "efficient underwater gesture recognition systems extremely important. In this\n",
            "paper, we propose an Underwater Gesture Recognition system trained on the\n",
            "Cognitive Autonomous Diving Buddy Underwater gesture dataset using deep\n",
            "learning that achieves 98.01\\% accuracy on the dataset, which to the best of\n",
            "our knowledge is the best performance achieved on this dataset at the time of\n",
            "writing this paper. We also improve the Gesture Recognition System\n",
            "Interpretability by using XAI techniques to visualize the model's predictions.\n",
            "\n",
            "888. Title: Multi-modal Gaussian Process Variational Autoencoders for Neural and Behavioral Data\n",
            "   Abstract: Object-centric architectures usually apply a differentiable module to the\n",
            "entire feature map to decompose it into sets of entity representations called\n",
            "slots. Some of these methods structurally resemble clustering algorithms, where\n",
            "the cluster's center in latent space serves as a slot representation. Slot\n",
            "Attention is an example of such a method, acting as a learnable analog of the\n",
            "soft k-means algorithm. Our work employs a learnable clustering method based on\n",
            "the Gaussian Mixture Model. Unlike other approaches, we represent slots not\n",
            "only as centers of clusters but also incorporate information about the distance\n",
            "between clusters and assigned vectors, leading to more expressive slot\n",
            "representations. Our experiments demonstrate that using this approach instead\n",
            "of Slot Attention improves performance in object-centric scenarios, achieving\n",
            "state-of-the-art results in the set property prediction task.\n",
            "\n",
            "889. Title: LLMCarbon: Modeling the End-to-End Carbon Footprint of Large Language Models\n",
            "   Abstract: Multi-modal image fusion (MMIF) integrates valuable information from\n",
            "different modality images into a fused one. However, the fusion of multiple\n",
            "visible images with different focal regions and infrared images is a\n",
            "unprecedented challenge in real MMIF applications. This is because of the\n",
            "limited depth of the focus of visible optical lenses, which impedes the\n",
            "simultaneous capture of the focal information within the same scene. To address\n",
            "this issue, in this paper, we propose a MMIF framework for joint focused\n",
            "integration and modalities information extraction. Specifically, a\n",
            "semi-sparsity-based smoothing filter is introduced to decompose the images into\n",
            "structure and texture components. Subsequently, a novel multi-scale operator is\n",
            "proposed to fuse the texture components, capable of detecting significant\n",
            "information by considering the pixel focus attributes and relevant data from\n",
            "various modal images. Additionally, to achieve an effective capture of scene\n",
            "luminance and reasonable contrast maintenance, we consider the distribution of\n",
            "energy information in the structural components in terms of multi-directional\n",
            "frequency variance and information entropy. Extensive experiments on existing\n",
            "MMIF datasets, as well as the object detection and depth estimation tasks,\n",
            "consistently demonstrate that the proposed algorithm can surpass the\n",
            "state-of-the-art methods in visual perception and quantitative evaluation. The\n",
            "code is available at https://github.com/ixilai/MFIF-MMIF.\n",
            "\n",
            "890. Title: Approximately Piecewise E(3) Equivariant Point Networks\n",
            "   Abstract: In this manuscript we introduce numerical Gaussian process Kalman filtering\n",
            "(GPKF). Numerical Gaussian processes have recently been developed to simulate\n",
            "spatiotemporal models. The contribution of this paper is to embed numerical\n",
            "Gaussian processes into the recursive Kalman filter equations. This embedding\n",
            "enables us to do Kalman filtering on infinite-dimensional systems using\n",
            "Gaussian processes. This is possible because i) we are obtaining a linear model\n",
            "from numerical Gaussian processes, and ii) the states of this model are by\n",
            "definition Gaussian distributed random variables. Convenient properties of the\n",
            "numerical GPKF are that no spatial discretization of the model is necessary,\n",
            "and manual setting up of the Kalman filter, that is fine-tuning the process and\n",
            "measurement noise levels by hand is not required, as they are learned online\n",
            "from the data stream. We showcase the capability of the numerical GPKF in a\n",
            "simulation study of the advection equation.\n",
            "\n",
            "891. Title: The mechanistic basis of data dependence and abrupt learning in an in-context classification task\n",
            "   Abstract: Integrating a notion of symmetry into point cloud neural networks is a\n",
            "provably effective way to improve their generalization capability. Of\n",
            "particular interest are $E(3)$ equivariant point cloud networks where Euclidean\n",
            "transformations applied to the inputs are preserved in the outputs. Recent\n",
            "efforts aim to extend networks that are $E(3)$ equivariant, to accommodate\n",
            "inputs made of multiple parts, each of which exhibits local $E(3)$ symmetry. In\n",
            "practical settings, however, the partitioning into individually transforming\n",
            "regions is unknown a priori. Errors in the partition prediction would\n",
            "unavoidably map to errors in respecting the true input symmetry. Past works\n",
            "have proposed different ways to predict the partition, which may exhibit\n",
            "uncontrolled errors in their ability to maintain equivariance to the actual\n",
            "partition. To this end, we introduce APEN: a general framework for constructing\n",
            "approximate piecewise-$E(3)$ equivariant point networks. Our primary insight is\n",
            "that functions that are equivariant with respect to a finer partition will also\n",
            "maintain equivariance in relation to the true partition. Leveraging this\n",
            "observation, we propose a design where the equivariance approximation error at\n",
            "each layers can be bounded solely in terms of (i) uncertainty quantification of\n",
            "the partition prediction, and (ii) bounds on the probability of failing to\n",
            "suggest a proper subpartition of the ground truth one. We demonstrate the\n",
            "effectiveness of APEN using two data types exemplifying part-based symmetry:\n",
            "(i) real-world scans of room scenes containing multiple furniture-type objects;\n",
            "and, (ii) human motions, characterized by articulated parts exhibiting rigid\n",
            "movement. Our empirical results demonstrate the advantage of integrating\n",
            "piecewise $E(3)$ symmetry into network design, showing a distinct improvement\n",
            "in generalization compared to prior works for both classification and\n",
            "segmentation tasks.\n",
            "\n",
            "892. Title: Symbol as Points: Panoptic Symbol Spotting via Point-based Representation\n",
            "   Abstract: This work studies the problem of panoptic symbol spotting, which is to spot\n",
            "and parse both countable object instances (windows, doors, tables, etc.) and\n",
            "uncountable stuff (wall, railing, etc.) from computer-aided design (CAD)\n",
            "drawings. Existing methods typically involve either rasterizing the vector\n",
            "graphics into images and using image-based methods for symbol spotting, or\n",
            "directly building graphs and using graph neural networks for symbol\n",
            "recognition. In this paper, we take a different approach, which treats graphic\n",
            "primitives as a set of 2D points that are locally connected and use point cloud\n",
            "segmentation methods to tackle it. Specifically, we utilize a point transformer\n",
            "to extract the primitive features and append a mask2former-like spotting head\n",
            "to predict the final output. To better use the local connection information of\n",
            "primitives and enhance their discriminability, we further propose the attention\n",
            "with connection module (ACM) and contrastive connection learning scheme (CCL).\n",
            "Finally, we propose a KNN interpolation mechanism for the mask attention module\n",
            "of the spotting head to better handle primitive mask downsampling, which is\n",
            "primitive-level in contrast to pixel-level for the image. Our approach, named\n",
            "SymPoint, is simple yet effective, outperforming recent state-of-the-art method\n",
            "GAT-CADNet by an absolute increase of 9.6% PQ and 10.4% RQ on the FloorPlanCAD\n",
            "dataset. The source code and models will be available at\n",
            "https://github.com/nicehuster/SymPoint.\n",
            "\n",
            "893. Title: Horizon-free Reinforcement Learning in Adversarial Linear Mixture MDPs\n",
            "   Abstract: We study reinforcement learning with linear function approximation, unknown\n",
            "transition, and adversarial losses in the bandit feedback setting.\n",
            "Specifically, we focus on linear mixture MDPs whose transition kernel is a\n",
            "linear mixture model. We propose a new algorithm that attains an\n",
            "$\\widetilde{O}(d\\sqrt{HS^3K} + \\sqrt{HSAK})$ regret with high probability,\n",
            "where $d$ is the dimension of feature mappings, $S$ is the size of state space,\n",
            "$A$ is the size of action space, $H$ is the episode length and $K$ is the\n",
            "number of episodes. Our result strictly improves the previous best-known\n",
            "$\\widetilde{O}(dS^2 \\sqrt{K} + \\sqrt{HSAK})$ result in Zhao et al. (2023a)\n",
            "since $H \\leq S$ holds by the layered MDP structure. Our advancements are\n",
            "primarily attributed to (i) a new least square estimator for the transition\n",
            "parameter that leverages the visit information of all states, as opposed to\n",
            "only one state in prior work, and (ii) a new self-normalized concentration\n",
            "tailored specifically to handle non-independent noises, originally proposed in\n",
            "the dynamic assortment area and firstly applied in reinforcement learning to\n",
            "handle correlations between different states.\n",
            "\n",
            "894. Title: Multi-Task Reinforcement Learning with Mixture of Orthogonal Experts\n",
            "   Abstract: Information extraction tasks require both accurate, efficient, and\n",
            "generalisable models. Classical supervised deep learning approaches can achieve\n",
            "the required performance, but they need large datasets and are limited in their\n",
            "ability to adapt to different tasks. On the other hand, large language models\n",
            "(LLMs) demonstrate good generalization, meaning that they can adapt to many\n",
            "different tasks based on user requests. However, LLMs are computationally\n",
            "expensive and tend to fail to generate structured outputs. In this article, we\n",
            "will introduce a new kind of GLiNER model that can be used for various\n",
            "information extraction tasks while being a small encoder model. Our model\n",
            "achieved SoTA performance on zero-shot NER benchmarks and leading performance\n",
            "on question-answering, summarization and relation extraction tasks.\n",
            "Additionally, in this article, we will cover experimental results on\n",
            "self-learning approaches for named entity recognition using GLiNER models.\n",
            "\n",
            "895. Title: Soft Mixture Denoising: Beyond the Expressive Bottleneck of Diffusion Models\n",
            "   Abstract: We present shuffle coding, a general method for optimal compression of\n",
            "sequences of unordered objects using bits-back coding. Data structures that can\n",
            "be compressed using shuffle coding include multisets, graphs, hypergraphs, and\n",
            "others. We release an implementation that can easily be adapted to different\n",
            "data types and statistical models, and demonstrate that our implementation\n",
            "achieves state-of-the-art compression rates on a range of graph datasets\n",
            "including molecular data.\n",
            "\n",
            "896. Title: SCHEMA: State CHangEs MAtter for Procedure Planning in Instructional Videos\n",
            "   Abstract: Denoisers play a central role in many applications, from noise suppression in\n",
            "low-grade imaging sensors, to empowering score-based generative models. The\n",
            "latter category of methods makes use of Tweedie's formula, which links the\n",
            "posterior mean in Gaussian denoising (\\ie the minimum MSE denoiser) with the\n",
            "score of the data distribution. Here, we derive a fundamental relation between\n",
            "the higher-order central moments of the posterior distribution, and the\n",
            "higher-order derivatives of the posterior mean. We harness this result for\n",
            "uncertainty quantification of pre-trained denoisers. Particularly, we show how\n",
            "to efficiently compute the principal components of the posterior distribution\n",
            "for any desired region of an image, as well as to approximate the full marginal\n",
            "distribution along those (or any other) one-dimensional directions. Our method\n",
            "is fast and memory-efficient, as it does not explicitly compute or store the\n",
            "high-order moment tensors and it requires no training or fine tuning of the\n",
            "denoiser. Code and examples are available on the project webpage in\n",
            "https://hilamanor.github.io/GaussianDenoisingPosterior/ .\n",
            "\n",
            "897. Title: On the Posterior Distribution in Denoising: Application to Uncertainty Quantification\n",
            "   Abstract: Transformer models exhibit in-context learning: the ability to accurately\n",
            "predict the response to a novel query based on illustrative examples in the\n",
            "input sequence. In-context learning contrasts with traditional in-weights\n",
            "learning of query-output relationships. What aspects of the training data\n",
            "distribution and architecture favor in-context vs in-weights learning? Recent\n",
            "work has shown that specific distributional properties inherent in language,\n",
            "such as burstiness, large dictionaries and skewed rank-frequency distributions,\n",
            "control the trade-off or simultaneous appearance of these two forms of\n",
            "learning. We first show that these results are recapitulated in a minimal\n",
            "attention-only network trained on a simplified dataset. In-context learning\n",
            "(ICL) is driven by the abrupt emergence of an induction head, which\n",
            "subsequently competes with in-weights learning. By identifying progress\n",
            "measures that precede in-context learning and targeted experiments, we\n",
            "construct a two-parameter model of an induction head which emulates the full\n",
            "data distributional dependencies displayed by the attention-based network. A\n",
            "phenomenological model of induction head formation traces its abrupt emergence\n",
            "to the sequential learning of three nested logits enabled by an intrinsic\n",
            "curriculum. We propose that the sharp transitions in attention-based networks\n",
            "arise due to a specific chain of multi-layer operations necessary to achieve\n",
            "ICL, which is implemented by nested nonlinearities sequentially learned during\n",
            "training.\n",
            "\n",
            "898. Title: Decision ConvFormer: Local Filtering in MetaFormer is Sufficient for Decision Making\n",
            "   Abstract: Because diffusion models have shown impressive performances in a number of\n",
            "tasks, such as image synthesis, there is a trend in recent works to prove (with\n",
            "certain assumptions) that these models have strong approximation capabilities.\n",
            "In this paper, we show that current diffusion models actually have an\n",
            "expressive bottleneck in backward denoising and some assumption made by\n",
            "existing theoretical guarantees is too strong. Based on this finding, we prove\n",
            "that diffusion models have unbounded errors in both local and global denoising.\n",
            "In light of our theoretical studies, we introduce soft mixture denoising (SMD),\n",
            "an expressive and efficient model for backward denoising. SMD not only permits\n",
            "diffusion models to well approximate any Gaussian mixture distributions in\n",
            "theory, but also is simple and efficient for implementation. Our experiments on\n",
            "multiple image datasets show that SMD significantly improves different types of\n",
            "diffusion models (e.g., DDPM), espeically in the situation of few backward\n",
            "iterations.\n",
            "\n",
            "899. Title: Entropy Coding of Unordered Data Structures\n",
            "   Abstract: Text-to-image generative models can produce photo-realistic images for an\n",
            "extremely broad range of concepts, and their usage has proliferated widely\n",
            "among the general public. On the flip side, these models have numerous\n",
            "drawbacks, including their potential to generate images featuring sexually\n",
            "explicit content, mirror artistic styles without permission, or even\n",
            "hallucinate (or deepfake) the likenesses of celebrities. Consequently, various\n",
            "methods have been proposed in order to \"erase\" sensitive concepts from\n",
            "text-to-image models. In this work, we examine five recently proposed concept\n",
            "erasure methods, and show that targeted concepts are not fully excised from any\n",
            "of these methods. Specifically, we leverage the existence of special learned\n",
            "word embeddings that can retrieve \"erased\" concepts from the sanitized models\n",
            "with no alterations to their weights. Our results highlight the brittleness of\n",
            "post hoc concept erasure methods, and call into question their use in the\n",
            "algorithmic toolkit for AI safety.\n",
            "\n",
            "900. Title: Circumventing Concept Erasure Methods For Text-To-Image Generative Models\n",
            "   Abstract: The recent success of Transformer in natural language processing has sparked\n",
            "its use in various domains. In offline reinforcement learning (RL), Decision\n",
            "Transformer (DT) is emerging as a promising model based on Transformer.\n",
            "However, we discovered that the attention module of DT is not appropriate to\n",
            "capture the inherent local dependence pattern in trajectories of RL modeled as\n",
            "a Markov decision process. To overcome the limitations of DT, we propose a\n",
            "novel action sequence predictor, named Decision ConvFormer (DC), based on the\n",
            "architecture of MetaFormer, which is a general structure to process multiple\n",
            "entities in parallel and understand the interrelationship among the multiple\n",
            "entities. DC employs local convolution filtering as the token mixer and can\n",
            "effectively capture the inherent local associations of the RL dataset. In\n",
            "extensive experiments, DC achieved state-of-the-art performance across various\n",
            "standard RL benchmarks while requiring fewer resources. Furthermore, we show\n",
            "that DC better understands the underlying meaning in data and exhibits enhanced\n",
            "generalization capability.\n",
            "\n",
            "901. Title: Predictive auxiliary objectives in deep RL mimic learning in the brain\n",
            "   Abstract: Bayesian filtering approximates the true underlying behavior of a\n",
            "time-varying system by inverting an explicit generative model to convert noisy\n",
            "measurements into state estimates. This process typically requires either\n",
            "storage, inversion, and multiplication of large matrices or Monte Carlo\n",
            "estimation, neither of which are practical in high-dimensional state spaces\n",
            "such as the weight spaces of artificial neural networks. Here, we frame the\n",
            "standard Bayesian filtering problem as optimization over a time-varying\n",
            "objective. Instead of maintaining matrices for the filtering equations or\n",
            "simulating particles, we specify an optimizer that defines the Bayesian filter\n",
            "implicitly. In the linear-Gaussian setting, we show that every Kalman filter\n",
            "has an equivalent formulation using K steps of gradient descent. In the\n",
            "nonlinear setting, our experiments demonstrate that our framework results in\n",
            "filters that are effective, robust, and scalable to high-dimensional systems,\n",
            "comparing well against the standard toolbox of Bayesian filtering solutions. We\n",
            "suggest that it is easier to fine-tune an optimizer than it is to specify the\n",
            "correct filtering equations, making our framework an attractive option for\n",
            "high-dimensional filtering problems.\n",
            "\n",
            "902. Title: Self-Supervised Speech Quality Estimation and Enhancement Using Only Clean Speech\n",
            "   Abstract: Representation rank is an important concept for understanding the role of\n",
            "Neural Networks (NNs) in Deep Reinforcement learning (DRL), which measures the\n",
            "expressive capacity of value networks. Existing studies focus on unboundedly\n",
            "maximizing this rank; nevertheless, that approach would introduce overly\n",
            "complex models in the learning, thus undermining performance. Hence,\n",
            "fine-tuning representation rank presents a challenging and crucial optimization\n",
            "problem. To address this issue, we find a guiding principle for adaptive\n",
            "control of the representation rank. We employ the Bellman equation as a\n",
            "theoretical foundation and derive an upper bound on the cosine similarity of\n",
            "consecutive state-action pairs representations of value networks. We then\n",
            "leverage this upper bound to propose a novel regularizer, namely BEllman\n",
            "Equation-based automatic rank Regularizer (BEER). This regularizer adaptively\n",
            "regularizes the representation rank, thus improving the DRL agent's\n",
            "performance. We first validate the effectiveness of automatic control of rank\n",
            "on illustrative experiments. Then, we scale up BEER to complex continuous\n",
            "control tasks by combining it with the deterministic policy gradient method.\n",
            "Among 12 challenging DeepMind control tasks, BEER outperforms the baselines by\n",
            "a large margin. Besides, BEER demonstrates significant advantages in Q-value\n",
            "approximation. Our code is available at\n",
            "https://github.com/sweetice/BEER-ICLR2024.\n",
            "\n",
            "903. Title: Making Pre-trained Language Models Great on Tabular Prediction\n",
            "   Abstract: By providing external information to large language models (LLMs), tool\n",
            "augmentation (including retrieval augmentation) has emerged as a promising\n",
            "solution for addressing the limitations of LLMs' static parametric memory.\n",
            "However, how receptive are LLMs to such external evidence, especially when the\n",
            "evidence conflicts with their parametric memory? We present the first\n",
            "comprehensive and controlled investigation into the behavior of LLMs when\n",
            "encountering knowledge conflicts. We propose a systematic framework to elicit\n",
            "high-quality parametric memory from LLMs and construct the corresponding\n",
            "counter-memory, which enables us to conduct a series of controlled experiments.\n",
            "Our investigation reveals seemingly contradicting behaviors of LLMs. On the one\n",
            "hand, different from prior wisdom, we find that LLMs can be highly receptive to\n",
            "external evidence even when that conflicts with their parametric memory, given\n",
            "that the external evidence is coherent and convincing. On the other hand, LLMs\n",
            "also demonstrate a strong confirmation bias when the external evidence contains\n",
            "some information that is consistent with their parametric memory, despite being\n",
            "presented with conflicting evidence at the same time. These results pose\n",
            "important implications that are worth careful consideration for the further\n",
            "development and deployment of tool- and retrieval-augmented LLMs. Resources are\n",
            "available at https://github.com/OSU-NLP-Group/LLM-Knowledge-Conflict.\n",
            "\n",
            "904. Title: Unveiling and Manipulating Prompt Influence in Large Language Models\n",
            "   Abstract: Text-to-image diffusion models have demonstrated an unparalleled ability to\n",
            "generate high-quality, diverse images from a textual prompt. However, the\n",
            "internal representations learned by these models remain an enigma. In this\n",
            "work, we present Conceptor, a novel method to interpret the internal\n",
            "representation of a textual concept by a diffusion model. This interpretation\n",
            "is obtained by decomposing the concept into a small set of human-interpretable\n",
            "textual elements. Applied over the state-of-the-art Stable Diffusion model,\n",
            "Conceptor reveals non-trivial structures in the representations of concepts.\n",
            "For example, we find surprising visual connections between concepts, that\n",
            "transcend their textual semantics. We additionally discover concepts that rely\n",
            "on mixtures of exemplars, biases, renowned artistic styles, or a simultaneous\n",
            "fusion of multiple meanings of the concept. Through a large battery of\n",
            "experiments, we demonstrate Conceptor's ability to provide meaningful, robust,\n",
            "and faithful decompositions for a wide variety of abstract, concrete, and\n",
            "complex textual concepts, while allowing to naturally connect each\n",
            "decomposition element to its corresponding visual impact on the generated\n",
            "images. Our code will be available at: https://hila-chefer.github.io/Conceptor/\n",
            "\n",
            "905. Title: A Topological Perspective on Demystifying GNN-Based Link Prediction Performance\n",
            "   Abstract: A non-destructive technique for obtaining voltage contrast information with\n",
            "photoelectron emission microscopy (PEEM) is described. Samples consisting of\n",
            "electrically isolated metal lines were used to quantify voltage contrast in\n",
            "PEEM. The voltage contrast behavior is characterized by comparing measured\n",
            "voltage contrast with calculated voltage contrast from two electrostatic\n",
            "models. Measured voltage contrast was found to agree closely with the\n",
            "calculated voltage contrast, demonstrating that voltage contrast in PEEM can be\n",
            "used to probe local voltage information in microelectronic devices in a\n",
            "non-intrusive fashion.\n",
            "\n",
            "906. Title: Adaptive Chameleon or Stubborn Sloth: Revealing the Behavior of Large Language Models in Knowledge Conflicts\n",
            "   Abstract: Graph Neural Networks (GNNs) have shown great promise in learning node\n",
            "embeddings for link prediction (LP). While numerous studies aim to improve the\n",
            "overall LP performance of GNNs, none have explored its varying performance\n",
            "across different nodes and its underlying reasons. To this end, we aim to\n",
            "demystify which nodes will perform better from the perspective of their local\n",
            "topology. Despite the widespread belief that low-degree nodes exhibit poorer LP\n",
            "performance, our empirical findings provide nuances to this viewpoint and\n",
            "prompt us to propose a better metric, Topological Concentration (TC), based on\n",
            "the intersection of the local subgraph of each node with the ones of its\n",
            "neighbors. We empirically demonstrate that TC has a higher correlation with LP\n",
            "performance than other node-level topological metrics like degree and subgraph\n",
            "density, offering a better way to identify low-performing nodes than using\n",
            "cold-start. With TC, we discover a novel topological distribution shift issue\n",
            "in which newly joined neighbors of a node tend to become less interactive with\n",
            "that node's existing neighbors, compromising the generalizability of node\n",
            "embeddings for LP at testing time. To make the computation of TC scalable, We\n",
            "further propose Approximated Topological Concentration (ATC) and\n",
            "theoretically/empirically justify its efficacy in approximating TC and reducing\n",
            "the computation complexity. Given the positive correlation between node TC and\n",
            "its LP performance, we explore the potential of boosting LP performance via\n",
            "enhancing TC by re-weighting edges in the message-passing and discuss its\n",
            "effectiveness with limitations. Our code is publicly available at\n",
            "https://github.com/YuWVandy/Topo_LP_GNN.\n",
            "\n",
            "907. Title: Implicit Maximum a Posteriori Filtering via Adaptive Optimization\n",
            "   Abstract: The ability to predict upcoming events has been hypothesized to comprise a\n",
            "key aspect of natural and machine cognition. This is supported by trends in\n",
            "deep reinforcement learning (RL), where self-supervised auxiliary objectives\n",
            "such as prediction are widely used to support representation learning and\n",
            "improve task performance. Here, we study the effects predictive auxiliary\n",
            "objectives have on representation learning across different modules of an RL\n",
            "system and how these mimic representational changes observed in the brain. We\n",
            "find that predictive objectives improve and stabilize learning particularly in\n",
            "resource-limited architectures, and we identify settings where longer\n",
            "predictive horizons better support representational transfer. Furthermore, we\n",
            "find that representational changes in this RL system bear a striking\n",
            "resemblance to changes in neural activity observed in the brain across various\n",
            "experiments. Specifically, we draw a connection between the auxiliary\n",
            "predictive model of the RL system and hippocampus, an area thought to learn a\n",
            "predictive model to support memory-guided behavior. We also connect the encoder\n",
            "network and the value learning network of the RL system to visual cortex and\n",
            "striatum in the brain, respectively. This work demonstrates how representation\n",
            "learning in deep RL systems can provide an interpretable framework for modeling\n",
            "multi-region interactions in the brain. The deep RL perspective taken here also\n",
            "suggests an additional role of the hippocampus in the brain -- that of an\n",
            "auxiliary learning system that benefits representation learning in other\n",
            "regions.\n",
            "\n",
            "908. Title: Expressivity of ReLU-Networks under Convex Relaxations\n",
            "   Abstract: Convex relaxations are a key component of training and certifying provably\n",
            "safe neural networks. However, despite substantial progress, a wide and poorly\n",
            "understood accuracy gap to standard networks remains, raising the question of\n",
            "whether this is due to fundamental limitations of convex relaxations. Initial\n",
            "work investigating this question focused on the simple and widely used IBP\n",
            "relaxation. It revealed that some univariate, convex, continuous piecewise\n",
            "linear (CPWL) functions cannot be encoded by any ReLU network such that its\n",
            "IBP-analysis is precise. To explore whether this limitation is shared by more\n",
            "advanced convex relaxations, we conduct the first in-depth study on the\n",
            "expressive power of ReLU networks across all commonly used convex relaxations.\n",
            "We show that: (i) more advanced relaxations allow a larger class of univariate\n",
            "functions to be expressed as precisely analyzable ReLU networks, (ii) more\n",
            "precise relaxations can allow exponentially larger solution spaces of ReLU\n",
            "networks encoding the same functions, and (iii) even using the most precise\n",
            "single-neuron relaxations, it is impossible to construct precisely analyzable\n",
            "ReLU networks that express multivariate, convex, monotone CPWL functions.\n",
            "\n",
            "909. Title: The Hidden Language of Diffusion Models\n",
            "   Abstract: The transferability of deep neural networks (DNNs) has made significant\n",
            "progress in image and language processing. However, due to the heterogeneity\n",
            "among tables, such DNN bonus is still far from being well exploited on tabular\n",
            "data prediction (e.g., regression or classification tasks). Condensing\n",
            "knowledge from diverse domains, language models (LMs) possess the capability to\n",
            "comprehend feature names from various tables, potentially serving as versatile\n",
            "learners in transferring knowledge across distinct tables and diverse\n",
            "prediction tasks, but their discrete text representation space is inherently\n",
            "incompatible with numerical feature values in tables. In this paper, we present\n",
            "TP-BERTa, a specifically pre-trained LM for tabular data prediction.\n",
            "Concretely, a novel relative magnitude tokenization converts scalar numerical\n",
            "feature values to finely discrete, high-dimensional tokens, and an\n",
            "intra-feature attention approach integrates feature values with the\n",
            "corresponding feature names. Comprehensive experiments demonstrate that our\n",
            "pre-trained TP-BERTa leads the performance among tabular DNNs and is\n",
            "competitive with Gradient Boosted Decision Tree models in typical tabular data\n",
            "regime.\n",
            "\n",
            "910. Title: Meta-Learning Priors Using Unrolled Proximal Networks\n",
            "   Abstract: Humans and animals can learn complex predictive models that allow them to\n",
            "accurately and reliably reason about real-world phenomena, and they can adapt\n",
            "such models extremely quickly in the face of unexpected changes. Deep neural\n",
            "network models allow us to represent very complex functions, but lack this\n",
            "capacity for rapid online adaptation. The goal in this paper is to develop a\n",
            "method for continual online learning from an incoming stream of data, using\n",
            "deep neural network models. We formulate an online learning procedure that uses\n",
            "stochastic gradient descent to update model parameters, and an expectation\n",
            "maximization algorithm with a Chinese restaurant process prior to develop and\n",
            "maintain a mixture of models to handle non-stationary task distributions. This\n",
            "allows for all models to be adapted as necessary, with new models instantiated\n",
            "for task changes and old models recalled when previously seen tasks are\n",
            "encountered again. Furthermore, we observe that meta-learning can be used to\n",
            "meta-train a model such that this direct online adaptation with SGD is\n",
            "effective, which is otherwise not the case for large function approximators. In\n",
            "this work, we apply our meta-learning for online learning (MOLe) approach to\n",
            "model-based reinforcement learning, where adapting the predictive model is\n",
            "critical for control; we demonstrate that MOLe outperforms alternative prior\n",
            "methods, and enables effective continuous adaptation in non-stationary task\n",
            "distributions such as varying terrains, motor failures, and unexpected\n",
            "disturbances.\n",
            "\n",
            "911. Title: KITAB: Evaluating LLMs on Constraint Satisfaction for Information Retrieval\n",
            "   Abstract: In real-world environments, autonomous agents rely on their egocentric\n",
            "observations. They must learn adaptive strategies to interact with others who\n",
            "possess mixed motivations, discernible only through visible cues. Several\n",
            "Multi-Agent Reinforcement Learning (MARL) methods adopt centralized approaches\n",
            "that involve either centralized training or reward-sharing, often violating the\n",
            "realistic ways in which living organisms, like animals or humans, process\n",
            "information and interact. MARL strategies deploying decentralized training with\n",
            "intrinsic motivation offer a self-supervised approach, enable agents to develop\n",
            "flexible social strategies through the interaction of autonomous agents.\n",
            "However, by contrasting the self-supervised and centralized methods, we reveal\n",
            "that populations trained with reward-sharing methods surpass those using\n",
            "self-supervised methods in a mixed-motive environment. We link this superiority\n",
            "to specialized role emergence and an agent's expertise in its role.\n",
            "Interestingly, this gap shrinks in pure-motive settings, emphasizing the need\n",
            "for evaluations in more complex, realistic environments (mixed-motive). Our\n",
            "preliminary results suggest a gap in population performance that can be closed\n",
            "by improving self-supervised methods and thereby pushing MARL closer to\n",
            "real-world readiness.\n",
            "\n",
            "912. Title: Representation Deficiency in Masked Language Modeling\n",
            "   Abstract: In recent studies, collaborative intelligence (CI) has emerged as a promising\n",
            "framework for deployment of Artificial Intelligence (AI)-based services on\n",
            "mobile/edge devices. In CI, the AI model (a deep neural network) is split\n",
            "between the edge and the cloud, and intermediate features are sent from the\n",
            "edge sub-model to the cloud sub-model. In this paper, we study bit allocation\n",
            "for feature coding in multi-stream CI systems. We model task distortion as a\n",
            "function of rate using convex surfaces similar to those found in\n",
            "distortion-rate theory. Using such models, we are able to provide closed-form\n",
            "bit allocation solutions for single-task systems and scalarized multi-task\n",
            "systems. Moreover, we provide analytical characterization of the full Pareto\n",
            "set for 2-stream k-task systems, and bounds on the Pareto set for 3-stream\n",
            "2-task systems. Analytical results are examined on a variety of DNN models from\n",
            "the literature to demonstrate wide applicability of the results\n",
            "\n",
            "913. Title: Pareto Deep Long-Tailed Recognition: A Conflict-Averse Solution\n",
            "   Abstract: Large Language Models (LLMs) exhibit emerging in-context learning abilities\n",
            "through prompt engineering. The recent progress in large-scale generative\n",
            "models has further expanded their use in real-world language applications.\n",
            "However, the critical challenge of improving the generalizability and\n",
            "factuality of LLMs in natural language understanding and question answering\n",
            "remains under-explored. While previous in-context learning research has focused\n",
            "on enhancing models to adhere to users' specific instructions and quality\n",
            "expectations, and to avoid undesired outputs, little to no work has explored\n",
            "the use of task-Specific fine-tuned Language Models (SLMs) to improve LLMs'\n",
            "in-context learning during the inference stage. Our primary contribution is the\n",
            "establishment of a simple yet effective framework that enhances the reliability\n",
            "of LLMs as it: 1) generalizes out-of-distribution data, 2) elucidates how LLMs\n",
            "benefit from discriminative models, and 3) minimizes hallucinations in\n",
            "generative tasks. Using our proposed plug-in method, enhanced versions of Llama\n",
            "2 and ChatGPT surpass their original versions regarding generalizability and\n",
            "factuality. We offer a comprehensive suite of resources, including 16 curated\n",
            "datasets, prompts, model checkpoints, and LLM outputs across 9 distinct tasks.\n",
            "The code and data are released at:\n",
            "https://github.com/YangLinyi/Supervised-Knowledge-Makes-Large-Language-Models-Better-In-context-Learners.\n",
            "Our empirical analysis sheds light on the advantages of incorporating\n",
            "discriminative models into LLMs and highlights the potential of our methodology\n",
            "in fostering more reliable LLMs.\n",
            "\n",
            "914. Title: Learning model uncertainty as variance-minimizing instance weights\n",
            "   Abstract: Despite the substantial progress of active learning for image recognition,\n",
            "there still lacks an instance-level active learning method specified for object\n",
            "detection. In this paper, we propose Multiple Instance Active Object Detection\n",
            "(MI-AOD), to select the most informative images for detector training by\n",
            "observing instance-level uncertainty. MI-AOD defines an instance uncertainty\n",
            "learning module, which leverages the discrepancy of two adversarial instance\n",
            "classifiers trained on the labeled set to predict instance uncertainty of the\n",
            "unlabeled set. MI-AOD treats unlabeled images as instance bags and feature\n",
            "anchors in images as instances, and estimates the image uncertainty by\n",
            "re-weighting instances in a multiple instance learning (MIL) fashion. Iterative\n",
            "instance uncertainty learning and re-weighting facilitate suppressing noisy\n",
            "instances, toward bridging the gap between instance uncertainty and image-level\n",
            "uncertainty. Experiments validate that MI-AOD sets a solid baseline for\n",
            "instance-level active learning. On commonly used object detection datasets,\n",
            "MI-AOD outperforms state-of-the-art methods with significant margins,\n",
            "particularly when the labeled sets are small. Code is available at\n",
            "https://github.com/yuantn/MI-AOD.\n",
            "\n",
            "915. Title: Understanding In-Context Learning from Repetitions\n",
            "   Abstract: This paper explores the elusive mechanism underpinning in-context learning in\n",
            "Large Language Models (LLMs). Our work provides a novel perspective by\n",
            "examining in-context learning via the lens of surface repetitions. We\n",
            "quantitatively investigate the role of surface features in text generation, and\n",
            "empirically establish the existence of \\emph{token co-occurrence\n",
            "reinforcement}, a principle that strengthens the relationship between two\n",
            "tokens based on their contextual co-occurrences. By investigating the dual\n",
            "impacts of these features, our research illuminates the internal workings of\n",
            "in-context learning and expounds on the reasons for its failures. This paper\n",
            "provides an essential contribution to the understanding of in-context learning\n",
            "and its potential limitations, providing a fresh perspective on this exciting\n",
            "capability.\n",
            "\n",
            "916. Title: Latent Representation and Simulation of Markov Processes via Time-Lagged Information Bottleneck\n",
            "   Abstract: Prompts play a crucial role in guiding the responses of Large Language Models\n",
            "(LLMs). However, the intricate role of individual tokens in prompts, known as\n",
            "input saliency, in shaping the responses remains largely underexplored.\n",
            "Existing saliency methods either misalign with LLM generation objectives or\n",
            "rely heavily on linearity assumptions, leading to potential inaccuracies. To\n",
            "address this, we propose Token Distribution Dynamics (TDD), a\n",
            "\\textcolor{black}{simple yet effective} approach to unveil and manipulate the\n",
            "role of prompts in generating LLM outputs. TDD leverages the robust\n",
            "interpreting capabilities of the language model head (LM head) to assess input\n",
            "saliency. It projects input tokens into the embedding space and then estimates\n",
            "their significance based on distribution dynamics over the vocabulary. We\n",
            "introduce three TDD variants: forward, backward, and bidirectional, each\n",
            "offering unique insights into token relevance. Extensive experiments reveal\n",
            "that the TDD surpasses state-of-the-art baselines with a big margin in\n",
            "elucidating the causal relationships between prompts and LLM outputs. Beyond\n",
            "mere interpretation, we apply TDD to two prompt manipulation tasks for\n",
            "controlled text generation: zero-shot toxic language suppression and sentiment\n",
            "steering. Empirical results underscore TDD's proficiency in identifying both\n",
            "toxic and sentimental cues in prompts, subsequently mitigating toxicity or\n",
            "modulating sentiment in the generated content.\n",
            "\n",
            "917. Title: Meta Inverse Constrained Reinforcement Learning: Convergence Guarantee and Generalization Analysis\n",
            "   Abstract: Masked Language Modeling (MLM) has been one of the most prominent approaches\n",
            "for pretraining bidirectional text encoders due to its simplicity and\n",
            "effectiveness. One notable concern about MLM is that the special\n",
            "$\\texttt{[MASK]}$ symbol causes a discrepancy between pretraining data and\n",
            "downstream data as it is present only in pretraining but not in fine-tuning. In\n",
            "this work, we offer a new perspective on the consequence of such a discrepancy:\n",
            "We demonstrate empirically and theoretically that MLM pretraining allocates\n",
            "some model dimensions exclusively for representing $\\texttt{[MASK]}$ tokens,\n",
            "resulting in a representation deficiency for real tokens and limiting the\n",
            "pretrained model's expressiveness when it is adapted to downstream data without\n",
            "$\\texttt{[MASK]}$ tokens. Motivated by the identified issue, we propose MAE-LM,\n",
            "which pretrains the Masked Autoencoder architecture with MLM where\n",
            "$\\texttt{[MASK]}$ tokens are excluded from the encoder. Empirically, we show\n",
            "that MAE-LM improves the utilization of model dimensions for real token\n",
            "representations, and MAE-LM consistently outperforms MLM-pretrained models\n",
            "across different pretraining settings and model sizes when fine-tuned on the\n",
            "GLUE and SQuAD benchmarks.\n",
            "\n",
            "918. Title: Facing the Elephant in the Room: Visual Prompt Tuning or Full finetuning?\n",
            "   Abstract: Parameter-transfer is a well-known and versatile approach for meta-learning,\n",
            "with applications including few-shot learning, federated learning, and\n",
            "reinforcement learning. However, parameter-transfer algorithms often require\n",
            "sharing models that have been trained on the samples from specific tasks, thus\n",
            "leaving the task-owners susceptible to breaches of privacy. We conduct the\n",
            "first formal study of privacy in this setting and formalize the notion of\n",
            "task-global differential privacy as a practical relaxation of more commonly\n",
            "studied threat models. We then propose a new differentially private algorithm\n",
            "for gradient-based parameter transfer that not only satisfies this privacy\n",
            "requirement but also retains provable transfer learning guarantees in convex\n",
            "settings. Empirically, we apply our analysis to the problems of federated\n",
            "learning with personalization and few-shot classification, showing that\n",
            "allowing the relaxation to task-global privacy from the more commonly studied\n",
            "notion of local privacy leads to dramatically increased performance in\n",
            "recurrent neural language modeling and image classification.\n",
            "\n",
            "919. Title: Boundary Denoising for Video Activity Localization\n",
            "   Abstract: Markov processes are widely used mathematical models for describing dynamic\n",
            "systems in various fields. However, accurately simulating large-scale systems\n",
            "at long time scales is computationally expensive due to the short time steps\n",
            "required for accurate integration. In this paper, we introduce an inference\n",
            "process that maps complex systems into a simplified representational space and\n",
            "models large jumps in time. To achieve this, we propose Time-lagged Information\n",
            "Bottleneck (T-IB), a principled objective rooted in information theory, which\n",
            "aims to capture relevant temporal features while discarding high-frequency\n",
            "information to simplify the simulation task and minimize the inference error.\n",
            "Our experiments demonstrate that T-IB learns information-optimal\n",
            "representations for accurately modeling the statistical properties and dynamics\n",
            "of the original process at a selected time lag, outperforming existing\n",
            "time-lagged dimensionality reduction methods.\n",
            "\n",
            "920. Title: Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement\n",
            "   Abstract: Video activity localization aims at understanding the semantic content in\n",
            "long untrimmed videos and retrieving actions of interest. The retrieved action\n",
            "with its start and end locations can be used for highlight generation, temporal\n",
            "action detection, etc. Unfortunately, learning the exact boundary location of\n",
            "activities is highly challenging because temporal activities are continuous in\n",
            "time, and there are often no clear-cut transitions between actions. Moreover,\n",
            "the definition of the start and end of events is subjective, which may confuse\n",
            "the model. To alleviate the boundary ambiguity, we propose to study the video\n",
            "activity localization problem from a denoising perspective. Specifically, we\n",
            "propose an encoder-decoder model named DenoiseLoc. During training, a set of\n",
            "action spans is randomly generated from the ground truth with a controlled\n",
            "noise scale. Then we attempt to reverse this process by boundary denoising,\n",
            "allowing the localizer to predict activities with precise boundaries and\n",
            "resulting in faster convergence speed. Experiments show that DenoiseLoc\n",
            "advances %in several video activity understanding tasks. For example, we\n",
            "observe a gain of +12.36% average mAP on QV-Highlights dataset and +1.64%\n",
            "mAP@0.5 on THUMOS'14 dataset over the baseline. Moreover, DenoiseLoc achieves\n",
            "state-of-the-art performance on TACoS and MAD datasets, but with much fewer\n",
            "predictions compared to other current methods.\n",
            "\n",
            "921. Title: Future Language Modeling from Temporal Document History\n",
            "   Abstract: As the scale of vision models continues to grow, the emergence of Visual\n",
            "Prompt Tuning (VPT) as a parameter-efficient transfer learning technique has\n",
            "gained attention due to its superior performance compared to traditional\n",
            "full-finetuning. However, the conditions favoring VPT (the ``when\") and the\n",
            "underlying rationale (the ``why\") remain unclear. In this paper, we conduct a\n",
            "comprehensive analysis across 19 distinct datasets and tasks. To understand the\n",
            "``when\" aspect, we identify the scenarios where VPT proves favorable by two\n",
            "dimensions: task objectives and data distributions. We find that VPT is\n",
            "preferrable when there is 1) a substantial disparity between the original and\n",
            "the downstream task objectives (e.g., transitioning from classification to\n",
            "counting), or 2) a similarity in data distributions between the two tasks\n",
            "(e.g., both involve natural images). In exploring the ``why\" dimension, our\n",
            "results indicate VPT's success cannot be attributed solely to overfitting and\n",
            "optimization considerations. The unique way VPT preserves original features and\n",
            "adds parameters appears to be a pivotal factor. Our study provides insights\n",
            "into VPT's mechanisms, and offers guidance for its optimal utilization.\n",
            "\n",
            "922. Title: Latent Trajectory Learning for Limited Timestamps under Distribution Shift over Time\n",
            "   Abstract: The ability to derive underlying principles from a handful of observations\n",
            "and then generalize to novel situations -- known as inductive reasoning -- is\n",
            "central to human intelligence. Prior work suggests that language models (LMs)\n",
            "often fall short on inductive reasoning, despite achieving impressive success\n",
            "on research benchmarks. In this work, we conduct a systematic study of the\n",
            "inductive reasoning capabilities of LMs through iterative hypothesis\n",
            "refinement, a technique that more closely mirrors the human inductive process\n",
            "than standard input-output prompting. Iterative hypothesis refinement employs a\n",
            "three-step process: proposing, selecting, and refining hypotheses in the form\n",
            "of textual rules. By examining the intermediate rules, we observe that LMs are\n",
            "phenomenal hypothesis proposers (i.e., generating candidate rules), and when\n",
            "coupled with a (task-specific) symbolic interpreter that is able to\n",
            "systematically filter the proposed set of rules, this hybrid approach achieves\n",
            "strong results across inductive reasoning benchmarks that require inducing\n",
            "causal relations, language-like instructions, and symbolic concepts. However,\n",
            "they also behave as puzzling inductive reasoners, showing notable performance\n",
            "gaps between rule induction (i.e., identifying plausible rules) and rule\n",
            "application (i.e., applying proposed rules to instances), suggesting that LMs\n",
            "are proposing hypotheses without being able to actually apply the rules.\n",
            "Through empirical and human analyses, we further reveal several discrepancies\n",
            "between the inductive reasoning processes of LMs and humans, shedding light on\n",
            "both the potentials and limitations of using LMs in inductive reasoning tasks.\n",
            "\n",
            "923. Title: Transferring Learning Trajectories of Neural Networks\n",
            "   Abstract: Predicting the future is of great interest across many aspects of human\n",
            "activity. Businesses are interested in future trends, traders are interested in\n",
            "future stock prices, and companies are highly interested in future\n",
            "technological breakthroughs. While there are many automated systems for\n",
            "predicting future numerical data, such as weather, stock prices, and demand for\n",
            "products, there is relatively little work in automatically predicting textual\n",
            "data. Humans are interested in textual data predictions because it is a natural\n",
            "format for our consumption, and experts routinely make predictions in a textual\n",
            "format (Christensen et al., 2004; Tetlock & Gardner, 2015; Frick, 2015).\n",
            "However, there has been relatively little formalization of this general problem\n",
            "in the machine learning or natural language processing communities. To address\n",
            "this gap, we introduce the task of future language modeling: probabilistic\n",
            "modeling of texts in the future based on a temporal history of texts. To our\n",
            "knowledge, our work is the first work to formalize the task of predicting the\n",
            "future in this way. We show that it is indeed possible to build future language\n",
            "models that improve upon strong non-temporal language model baselines, opening\n",
            "the door to working on this important, and widely applicable problem.\n",
            "\n",
            "924. Title: FITS: Modeling Time Series with $10k$ Parameters\n",
            "   Abstract: Small integration time steps limit molecular dynamics (MD) simulations to\n",
            "millisecond time scales. Markov state models (MSMs) and equation-free\n",
            "approaches learn low-dimensional kinetic models from MD simulation data by\n",
            "performing configurational or dynamical coarse-graining of the state space. The\n",
            "learned kinetic models enable the efficient generation of dynamical\n",
            "trajectories over vastly longer time scales than are accessible by MD, but the\n",
            "discretization of configurational space and/or absence of a means to\n",
            "reconstruct molecular configurations precludes the generation of continuous\n",
            "all-atom molecular trajectories. We propose latent space simulators (LSS) to\n",
            "learn kinetic models for continuous all-atom simulation trajectories by\n",
            "training three deep learning networks to (i) learn the slow collective\n",
            "variables of the molecular system, (ii) propagate the system dynamics within\n",
            "this slow latent space, and (iii) generatively reconstruct molecular\n",
            "configurations. We demonstrate the approach in an application to Trp-cage\n",
            "miniprotein to produce novel ultra-long synthetic folding trajectories that\n",
            "accurately reproduce all-atom molecular structure, thermodynamics, and kinetics\n",
            "at six orders of magnitude lower cost than MD. The dramatically lower cost of\n",
            "trajectory generation enables greatly improved sampling and greatly reduced\n",
            "statistical uncertainties in estimated thermodynamic averages and kinetic\n",
            "rates.\n",
            "\n",
            "925. Title: R-MAE: Regions Meet Masked Autoencoders\n",
            "   Abstract: Representation learning is all about discovering the hidden modular\n",
            "attributes that generate the data faithfully. We explore the potential of\n",
            "Denoising Diffusion Probabilistic Model (DM) in unsupervised learning of the\n",
            "modular attributes. We build a theoretical framework that connects the\n",
            "diffusion time-steps and the hidden attributes, which serves as an effective\n",
            "inductive bias for unsupervised learning. Specifically, the forward diffusion\n",
            "process incrementally adds Gaussian noise to samples at each time-step, which\n",
            "essentially collapses different samples into similar ones by losing attributes,\n",
            "e.g., fine-grained attributes such as texture are lost with less noise added\n",
            "(i.e., early time-steps), while coarse-grained ones such as shape are lost by\n",
            "adding more noise (i.e., late time-steps). To disentangle the modular\n",
            "attributes, at each time-step t, we learn a t-specific feature to compensate\n",
            "for the newly lost attribute, and the set of all 1,...,t-specific features,\n",
            "corresponding to the cumulative set of lost attributes, are trained to make up\n",
            "for the reconstruction error of a pre-trained DM at time-step t. On CelebA,\n",
            "FFHQ, and Bedroom datasets, the learned feature significantly improves\n",
            "attribute classification and enables faithful counterfactual generation, e.g.,\n",
            "interpolating only one specified attribute between two images, validating the\n",
            "disentanglement quality. Codes are in https://github.com/yue-zhongqi/diti.\n",
            "\n",
            "926. Title: Multilinear Operator Networks\n",
            "   Abstract: Style Transfer has been proposed in a number of fields: fine arts, natural\n",
            "language processing, and fixed trajectories. We scale this concept up to\n",
            "control policies within a Deep Reinforcement Learning infrastructure. Each\n",
            "network is trained to maximize the expected reward, which typically encodes the\n",
            "goal of an action, and can be described as the content. The expressive power of\n",
            "deep neural networks enables encoding a secondary task, which can be described\n",
            "as the style. The Neural Policy Style Transfer (NPST) algorithm is proposed to\n",
            "transfer the style of one policy to another, while maintaining the content of\n",
            "the latter. Different policies are defined via Deep Q-Network architectures.\n",
            "These models are trained using demonstrations through Inverse Reinforcement\n",
            "Learning. Two different sets of user demonstrations are performed, one for\n",
            "content and other for style. Different styles are encoded as defined by user\n",
            "demonstrations. The generated policy is the result of feeding a content policy\n",
            "and a style policy to the NPST algorithm. Experiments are performed in a\n",
            "catch-ball game inspired by the Deep Reinforcement Learning classical Atari\n",
            "games; and a real-world painting scenario with a full-sized humanoid robot,\n",
            "based on previous works of the authors. The implementation of three different\n",
            "Q-Network architectures (Shallow, Deep and Deep Recurrent Q-Network) to encode\n",
            "the policies within the NPST framework is proposed and the results obtained in\n",
            "the experiments with each of these architectures compared.\n",
            "\n",
            "927. Title: MBR and QE Finetuning: Training-time Distillation of the Best and Most Expensive Decoding Methods\n",
            "   Abstract: Of all the vector fields surrounding the minima of recurrent learning setups,\n",
            "the gradient field with its exploding and vanishing updates appears a poor\n",
            "choice for optimization, offering little beyond efficient computability. We\n",
            "seek to improve this suboptimal practice in the context of physics simulations,\n",
            "where backpropagating feedback through many unrolled time steps is considered\n",
            "crucial to acquiring temporally coherent behavior. The alternative vector field\n",
            "we propose follows from two principles: physics simulators, unlike neural\n",
            "networks, have a balanced gradient flow, and certain modifications to the\n",
            "backpropagation pass leave the positions of the original minima unchanged. As\n",
            "any modification of backpropagation decouples forward and backward pass, the\n",
            "rotation-free character of the gradient field is lost. Therefore, we discuss\n",
            "the negative implications of using such a rotational vector field for\n",
            "optimization and how to counteract them. Our final procedure is easily\n",
            "implementable via a sequence of gradient stopping and component-wise comparison\n",
            "operations, which do not negatively affect scalability. Our experiments on\n",
            "three control problems show that especially as we increase the complexity of\n",
            "each task, the unbalanced updates from the gradient can no longer provide the\n",
            "precise control signals necessary while our method still solves the tasks. Our\n",
            "code can be found at https://github.com/tum-pbs/StableBPTT.\n",
            "\n",
            "928. Title: Adaptive Stochastic Gradient Algorithm for Black-box Multi-Objective Learning\n",
            "   Abstract: Despite the remarkable capabilities of deep neural networks in image\n",
            "recognition, the dependence on activation functions remains a largely\n",
            "unexplored area and has yet to be eliminated. On the other hand, Polynomial\n",
            "Networks is a class of models that does not require activation functions, but\n",
            "have yet to perform on par with modern architectures. In this work, we aim\n",
            "close this gap and propose MONet, which relies solely on multilinear operators.\n",
            "The core layer of MONet, called Mu-Layer, captures multiplicative interactions\n",
            "of the elements of the input token. MONet captures high-degree interactions of\n",
            "the input elements and we demonstrate the efficacy of our approach on a series\n",
            "of image recognition and scientific computing benchmarks. The proposed model\n",
            "outperforms prior polynomial networks and performs on par with modern\n",
            "architectures. We believe that MONet can inspire further research on models\n",
            "that use entirely multilinear operations.\n",
            "\n",
            "929. Title: Stabilizing Backpropagation Through Time to Learn Complex Physics\n",
            "   Abstract: A recent paper by Farina & Pipis (2023) established the existence of\n",
            "uncoupled no-linear-swap regret dynamics with polynomial-time iterations in\n",
            "extensive-form games. The equilibrium points reached by these dynamics, known\n",
            "as linear correlated equilibria, are currently the tightest known relaxation of\n",
            "correlated equilibrium that can be learned in polynomial time in any finite\n",
            "extensive-form game. However, their properties remain vastly unexplored, and\n",
            "their computation is onerous. In this paper, we provide several contributions\n",
            "shedding light on the fundamental nature of linear-swap regret. First, we show\n",
            "a connection between linear deviations and a generalization of communication\n",
            "deviations in which the player can make queries to a \"mediator\" who replies\n",
            "with action recommendations, and, critically, the player is not constrained to\n",
            "match the timing of the game as would be the case for communication deviations.\n",
            "We coin this latter set the untimed communication (UTC) deviations. We show\n",
            "that the UTC deviations coincide precisely with the linear deviations, and\n",
            "therefore that any player minimizing UTC regret also minimizes linear-swap\n",
            "regret. We then leverage this connection to develop state-of-the-art no-regret\n",
            "algorithms for computing linear correlated equilibria, both in theory and in\n",
            "practice. In theory, our algorithms achieve polynomially better per-iteration\n",
            "runtimes; in practice, our algorithms represent the state of the art by several\n",
            "orders of magnitude.\n",
            "\n",
            "930. Title: RAPPER: Reinforced Rationale-Prompted Paradigm for Natural Language Explanation in Visual Question Answering\n",
            "   Abstract: Recent research in decoding methods for Natural Language Generation (NLG)\n",
            "tasks has shown that MAP decoding is not optimal, because model probabilities\n",
            "do not always align with human preferences. Stronger decoding methods,\n",
            "including Quality Estimation (QE) reranking and Minimum Bayes' Risk (MBR)\n",
            "decoding, have since been proposed to mitigate the model-perplexity-vs-quality\n",
            "mismatch. While these decoding methods achieve state-of-the-art performance,\n",
            "they are prohibitively expensive to compute. In this work, we propose MBR\n",
            "finetuning and QE finetuning which distill the quality gains from these\n",
            "decoding methods at training time, while using an efficient decoding algorithm\n",
            "at inference time. Using the canonical NLG task of Neural Machine Translation\n",
            "(NMT), we show that even with self-training, these finetuning methods\n",
            "significantly outperform the base model. Moreover, when using an external LLM\n",
            "as a teacher model, these finetuning methods outperform finetuning on\n",
            "human-generated references. These findings suggest new ways to leverage\n",
            "monolingual data to achieve improvements in model quality that are on par with,\n",
            "or even exceed, improvements from human-curated data, while maintaining maximum\n",
            "efficiency during decoding.\n",
            "\n",
            "931. Title: Efficient Planning with Latent Diffusion\n",
            "   Abstract: A framework previously introduced in [3] for solving a sequence of stochastic\n",
            "optimization problems with bounded changes in the minimizers is extended and\n",
            "applied to machine learning problems such as regression and classification. The\n",
            "stochastic optimization problems arising in these machine learning problems is\n",
            "solved using algorithms such as stochastic gradient descent (SGD). A method\n",
            "based on estimates of the change in the minimizers and properties of the\n",
            "optimization algorithm is introduced for adaptively selecting the number of\n",
            "samples at each time step to ensure that the excess risk, i.e., the expected\n",
            "gap between the loss achieved by the approximate minimizer produced by the\n",
            "optimization algorithm and the exact minimizer, does not exceed a target level.\n",
            "A bound is developed to show that the estimate of the change in the minimizers\n",
            "is non-trivial provided that the excess risk is small enough. Extensions\n",
            "relevant to the machine learning setting are considered, including a cost-based\n",
            "approach to select the number of samples with a cost budget over a fixed\n",
            "horizon, and an approach to applying cross-validation for model selection.\n",
            "Finally, experiments with synthetic and real data are used to validate the\n",
            "algorithms.\n",
            "\n",
            "932. Title: AnomalyCLIP: Object-agnostic Prompt Learning for Zero-shot Anomaly Detection\n",
            "   Abstract: We present a novel usage of Transformers to make image classification\n",
            "interpretable. Unlike mainstream classifiers that wait until the last fully\n",
            "connected layer to incorporate class information to make predictions, we\n",
            "investigate a proactive approach, asking each class to search for itself in an\n",
            "image. We realize this idea via a Transformer encoder-decoder inspired by\n",
            "DEtection TRansformer (DETR). We learn \"class-specific\" queries (one for each\n",
            "class) as input to the decoder, enabling each class to localize its patterns in\n",
            "an image via cross-attention. We name our approach INterpretable TRansformer\n",
            "(INTR), which is fairly easy to implement and exhibits several compelling\n",
            "properties. We show that INTR intrinsically encourages each class to attend\n",
            "distinctively; the cross-attention weights thus provide a faithful\n",
            "interpretation of the prediction. Interestingly, via \"multi-head\"\n",
            "cross-attention, INTR could identify different \"attributes\" of a class, making\n",
            "it particularly suitable for fine-grained classification and analysis, which we\n",
            "demonstrate on eight datasets. Our code and pre-trained models are publicly\n",
            "accessible at the Imageomics Institute GitHub site:\n",
            "https://github.com/Imageomics/INTR.\n",
            "\n",
            "933. Title: AlignDiff: Aligning Diverse Human Preferences via Behavior-Customisable Diffusion Model\n",
            "   Abstract: Temporal abstraction and efficient planning pose significant challenges in\n",
            "offline reinforcement learning, mainly when dealing with domains that involve\n",
            "temporally extended tasks and delayed sparse rewards. Existing methods\n",
            "typically plan in the raw action space and can be inefficient and inflexible.\n",
            "Latent action spaces offer a more flexible paradigm, capturing only possible\n",
            "actions within the behavior policy support and decoupling the temporal\n",
            "structure between planning and modeling. However, current latent-action-based\n",
            "methods are limited to discrete spaces and require expensive planning. This\n",
            "paper presents a unified framework for continuous latent action space\n",
            "representation learning and planning by leveraging latent, score-based\n",
            "diffusion models. We establish the theoretical equivalence between planning in\n",
            "the latent action space and energy-guided sampling with a pretrained diffusion\n",
            "model and incorporate a novel sequence-level exact sampling method. Our\n",
            "proposed method, $\\texttt{LatentDiffuser}$, demonstrates competitive\n",
            "performance on low-dimensional locomotion control tasks and surpasses existing\n",
            "methods in higher-dimensional tasks.\n",
            "\n",
            "934. Title: Prioritized Soft Q-Decomposition for Lexicographic Reinforcement Learning\n",
            "   Abstract: Denoising diffusions are state-of-the-art generative models exhibiting\n",
            "remarkable empirical performance. They work by diffusing the data distribution\n",
            "into a Gaussian distribution and then learning to reverse this noising process\n",
            "to obtain synthetic datapoints. The denoising diffusion relies on\n",
            "approximations of the logarithmic derivatives of the noised data densities\n",
            "using score matching. Such models can also be used to perform approximate\n",
            "posterior simulation when one can only sample from the prior and likelihood. We\n",
            "propose a unifying framework generalising this approach to a wide class of\n",
            "spaces and leading to an original extension of score matching. We illustrate\n",
            "the resulting models on various applications.\n",
            "\n",
            "935. Title: Zero-Shot Robotic Manipulation with Pre-Trained Image-Editing Diffusion Models\n",
            "   Abstract: Reinforcement learning (RL) for complex tasks remains a challenge, primarily\n",
            "due to the difficulties of engineering scalar reward functions and the inherent\n",
            "inefficiency of training models from scratch. Instead, it would be better to\n",
            "specify complex tasks in terms of elementary subtasks and to reuse subtask\n",
            "solutions whenever possible. In this work, we address continuous space\n",
            "lexicographic multi-objective RL problems, consisting of prioritized subtasks,\n",
            "which are notoriously difficult to solve. We show that these can be scalarized\n",
            "with a subtask transformation and then solved incrementally using value\n",
            "decomposition. Exploiting this insight, we propose prioritized soft\n",
            "Q-decomposition (PSQD), a novel algorithm for learning and adapting subtask\n",
            "solutions under lexicographic priorities in continuous state-action spaces.\n",
            "PSQD offers the ability to reuse previously learned subtask solutions in a\n",
            "zero-shot composition, followed by an adaptation step. Its ability to use\n",
            "retained subtask training data for offline learning eliminates the need for new\n",
            "environment interaction during adaptation. We demonstrate the efficacy of our\n",
            "approach by presenting successful learning, reuse, and adaptation results for\n",
            "both low- and high-dimensional simulated robot control tasks, as well as\n",
            "offline learning results. In contrast to baseline approaches, PSQD does not\n",
            "trade off between conflicting subtasks or priority constraints and satisfies\n",
            "subtask priorities during learning. PSQD provides an intuitive framework for\n",
            "tackling complex RL problems, offering insights into the inner workings of the\n",
            "subtask composition.\n",
            "\n",
            "936. Title: The All-Seeing Project: Towards Panoptic Visual Recognition and Understanding of the Open World\n",
            "   Abstract: Data poisoning attacks aim to manipulate the model produced by a learning\n",
            "algorithm by adversarially modifying the training set. We consider differential\n",
            "privacy as a defensive measure against this type of attack. We show that such\n",
            "learners are resistant to data poisoning attacks when the adversary is only\n",
            "able to poison a small number of items. However, this protection degrades as\n",
            "the adversary poisons more data. To illustrate, we design attack algorithms\n",
            "targeting objective and output perturbation learners, two standard approaches\n",
            "to differentially-private machine learning. Experiments show that our methods\n",
            "are effective when the attacker is allowed to poison sufficiently many training\n",
            "items.\n",
            "\n",
            "937. Title: Denoising Diffusion Step-aware Models\n",
            "   Abstract: Graph Neural Networks (GNNs) have seen significant success in tasks such as\n",
            "node classification, largely contingent upon the availability of sufficient\n",
            "labeled nodes. Yet, the excessive cost of labeling large-scale graphs led to a\n",
            "focus on active learning on graphs, which aims for effective data selection to\n",
            "maximize downstream model performance. Notably, most existing methods assume\n",
            "reliable graph topology, while real-world scenarios often present noisy graphs.\n",
            "Given this, designing a successful active learning framework for noisy graphs\n",
            "is highly needed but challenging, as selecting data for labeling and obtaining\n",
            "a clean graph are two tasks naturally interdependent: selecting high-quality\n",
            "data requires clean graph structure while cleaning noisy graph structure\n",
            "requires sufficient labeled data. Considering the complexity mentioned above,\n",
            "we propose an active learning framework, GALClean, which has been specifically\n",
            "designed to adopt an iterative approach for conducting both data selection and\n",
            "graph purification simultaneously with best information learned from the prior\n",
            "iteration. Importantly, we summarize GALClean as an instance of the\n",
            "Expectation-Maximization algorithm, which provides a theoretical understanding\n",
            "of its design and mechanisms. This theory naturally leads to an enhanced\n",
            "version, GALClean+. Extensive experiments have demonstrated the effectiveness\n",
            "and robustness of our proposed method across various types and levels of noisy\n",
            "graphs.\n",
            "\n",
            "938. Title: GAFormer: Enhancing Timeseries Transformers Through Group-Aware Embeddings\n",
            "   Abstract: Novelty Detection (ND) plays a crucial role in machine learning by\n",
            "identifying new or unseen data during model inference. This capability is\n",
            "especially important for the safe and reliable operation of automated systems.\n",
            "Despite advances in this field, existing techniques often fail to maintain\n",
            "their performance when subject to adversarial attacks. Our research addresses\n",
            "this gap by marrying the merits of nearest-neighbor algorithms with robust\n",
            "features obtained from models pretrained on ImageNet. We focus on enhancing the\n",
            "robustness and performance of ND algorithms. Experimental results demonstrate\n",
            "that our approach significantly outperforms current state-of-the-art methods\n",
            "across various benchmarks, particularly under adversarial conditions. By\n",
            "incorporating robust pretrained features into the k-NN algorithm, we establish\n",
            "a new standard for performance and robustness in the field of robust ND. This\n",
            "work opens up new avenues for research aimed at fortifying machine learning\n",
            "systems against adversarial vulnerabilities. Our implementation is publicly\n",
            "available at https://github.com/rohban-lab/ZARND.\n",
            "\n",
            "939. Title: Decoupling Weighing and Selecting for Integrating Multiple Graph Pre-training Tasks\n",
            "   Abstract: Classic computer vision algorithms, instance segmentation, and semantic\n",
            "segmentation can not provide a holistic understanding of the surroundings for\n",
            "the visually impaired. In this paper, we utilize panoptic segmentation to\n",
            "assist the navigation of visually impaired people by offering both things and\n",
            "stuff awareness in the proximity of the visually impaired efficiently. To this\n",
            "end, we propose an efficient Attention module -- Lintention which can model\n",
            "long-range interactions in linear time using linear space. Based on Lintention,\n",
            "we then devise a novel panoptic segmentation model which we term Panoptic\n",
            "Lintention Net. Experiments on the COCO dataset indicate that the Panoptic\n",
            "Lintention Net raises the Panoptic Quality (PQ) from 39.39 to 41.42 with 4.6\\%\n",
            "performance gain while only requiring 10\\% fewer GFLOPs and 25\\% fewer\n",
            "parameters in the semantic branch. Furthermore, a real-world test via our\n",
            "designed compact wearable panoptic segmentation system, indicates that our\n",
            "system based on the Panoptic Lintention Net accomplishes a relatively stable\n",
            "and exceptionally remarkable panoptic segmentation in real-world scenes.\n",
            "\n",
            "940. Title: Label-Focused Inductive Bias over Latent Object Features in Visual Classification\n",
            "   Abstract: Identifying subordinate-level categories from images is a longstanding task\n",
            "in computer vision and is referred to as fine-grained visual recognition\n",
            "(FGVR). It has tremendous significance in real-world applications since an\n",
            "average layperson does not excel at differentiating species of birds or\n",
            "mushrooms due to subtle differences among the species. A major bottleneck in\n",
            "developing FGVR systems is caused by the need of high-quality paired expert\n",
            "annotations. To circumvent the need of expert knowledge we propose Fine-grained\n",
            "Semantic Category Reasoning (FineR) that internally leverages the world\n",
            "knowledge of large language models (LLMs) as a proxy in order to reason about\n",
            "fine-grained category names. In detail, to bridge the modality gap between\n",
            "images and LLM, we extract part-level visual attributes from images as text and\n",
            "feed that information to a LLM. Based on the visual attributes and its internal\n",
            "world knowledge the LLM reasons about the subordinate-level category names. Our\n",
            "training-free FineR outperforms several state-of-the-art FGVR and language and\n",
            "vision assistant models and shows promise in working in the wild and in new\n",
            "domains where gathering expert annotation is arduous.\n",
            "\n",
            "941. Title: Aux-NAS: Exploiting Auxiliary Labels with Negligibly Extra Inference Cost\n",
            "   Abstract: Deep neural networks are valuable assets considering their commercial\n",
            "benefits and huge demands for costly annotation and computation resources. To\n",
            "protect the copyright of DNNs, backdoor-based ownership verification becomes\n",
            "popular recently, in which the model owner can watermark the model by embedding\n",
            "a specific backdoor behavior before releasing it. The defenders (usually the\n",
            "model owners) can identify whether a suspicious third-party model is ``stolen''\n",
            "from them based on the presence of the behavior. Unfortunately, these\n",
            "watermarks are proven to be vulnerable to removal attacks even like\n",
            "fine-tuning. To further explore this vulnerability, we investigate the\n",
            "parameter space and find there exist many watermark-removed models in the\n",
            "vicinity of the watermarked one, which may be easily used by removal attacks.\n",
            "Inspired by this finding, we propose a mini-max formulation to find these\n",
            "watermark-removed models and recover their watermark behavior. Extensive\n",
            "experiments demonstrate that our method improves the robustness of the model\n",
            "watermarking against parametric changes and numerous watermark-removal attacks.\n",
            "The codes for reproducing our main experiments are available at\n",
            "\\url{https://github.com/GuanhaoGan/robust-model-watermarking}.\n",
            "\n",
            "942. Title: Deep Orthogonal Hypersphere Compression for Anomaly Detection\n",
            "   Abstract: Neuroprostheses show potential in restoring lost sensory function and\n",
            "enhancing human capabilities, but the sensations produced by current devices\n",
            "often seem unnatural or distorted. Exact placement of implants and differences\n",
            "in individual perception lead to significant variations in stimulus response,\n",
            "making personalized stimulus optimization a key challenge. Bayesian\n",
            "optimization could be used to optimize patient-specific stimulation parameters\n",
            "with limited noisy observations, but is not feasible for high-dimensional\n",
            "stimuli. Alternatively, deep learning models can optimize stimulus encoding\n",
            "strategies, but typically assume perfect knowledge of patient-specific\n",
            "variations. Here we propose a novel, practically feasible approach that\n",
            "overcomes both of these fundamental limitations. First, a deep encoder network\n",
            "is trained to produce optimal stimuli for any individual patient by inverting a\n",
            "forward model mapping electrical stimuli to visual percepts. Second, a\n",
            "preferential Bayesian optimization strategy utilizes this encoder to optimize\n",
            "patient-specific parameters for a new patient, using a minimal number of\n",
            "pairwise comparisons between candidate stimuli. We demonstrate the viability of\n",
            "this approach on a novel, state-of-the-art visual prosthesis model. We show\n",
            "that our approach quickly learns a personalized stimulus encoder, leads to\n",
            "dramatic improvements in the quality of restored vision, and is robust to noisy\n",
            "patient feedback and misspecifications in the underlying forward model.\n",
            "Overall, our results suggest that combining the strengths of deep learning and\n",
            "Bayesian optimization could significantly improve the perceptual experience of\n",
            "patients fitted with visual prostheses and may prove a viable solution for a\n",
            "range of neuroprosthetic technologies.\n",
            "\n",
            "943. Title: Exploring Effective Stimulus Encoding via Vision System Modeling for Visual Prostheses\n",
            "   Abstract: Vision transformers have shown great potential in various computer vision\n",
            "tasks owing to their strong capability to model long-range dependency using the\n",
            "self-attention mechanism. Nevertheless, they treat an image as a 1D sequence of\n",
            "visual tokens, lacking an intrinsic inductive bias (IB) in modeling local\n",
            "visual structures and dealing with scale variance, which is instead learned\n",
            "implicitly from large-scale training data with longer training schedules. In\n",
            "this paper, we propose a Vision Transformer Advanced by Exploring intrinsic IB\n",
            "from convolutions, i.e., ViTAE. Technically, ViTAE has several spatial pyramid\n",
            "reduction modules to downsample and embed the input image into tokens with rich\n",
            "multi-scale context using multiple convolutions with different dilation rates.\n",
            "In this way, it acquires an intrinsic scale invariance IB and can learn robust\n",
            "feature representation for objects at various scales. Moreover, in each\n",
            "transformer layer, ViTAE has a convolution block parallel to the multi-head\n",
            "self-attention module, whose features are fused and fed into the feed-forward\n",
            "network. Consequently, it has the intrinsic locality IB and is able to learn\n",
            "local features and global dependencies collaboratively. The proposed two kinds\n",
            "of cells are stacked in both isotropic and multi-stage manners to formulate two\n",
            "families of ViTAE models, i.e., the vanilla ViTAE and ViTAEv2. Experiments on\n",
            "the ImageNet dataset as well as downstream tasks on the MS COCO, ADE20K, and\n",
            "AP10K datasets validate the superiority of our models over the baseline\n",
            "transformer models and concurrent works. Besides, we scale up our ViTAE model\n",
            "to 644M parameters and obtain the state-of-the-art classification performance,\n",
            "i.e., 88.5% Top-1 classification accuracy on ImageNet validation set and the\n",
            "best 91.2% Top-1 accuracy on ImageNet real validation set, without using extra\n",
            "private data.\n",
            "\n",
            "944. Title: ADDP: Learning General Representations for Image Recognition and Generation with Alternating Denoising Diffusion Process\n",
            "   Abstract: As neural networks grow in scale, their training becomes both computationally\n",
            "demanding and rich in dynamics. Amidst the flourishing interest in these\n",
            "training dynamics, we present a novel observation: Parameters during training\n",
            "exhibit intrinsic correlations over time. Capitalizing on this, we introduce\n",
            "Correlation Mode Decomposition (CMD). This algorithm clusters the parameter\n",
            "space into groups, termed modes, that display synchronized behavior across\n",
            "epochs. This enables CMD to efficiently represent the training dynamics of\n",
            "complex networks, like ResNets and Transformers, using only a few modes.\n",
            "Moreover, test set generalization is enhanced. We introduce an efficient CMD\n",
            "variant, designed to run concurrently with training. Our experiments indicate\n",
            "that CMD surpasses the state-of-the-art method for compactly modeled dynamics\n",
            "on image classification. Our modeling can improve training efficiency and lower\n",
            "communication overhead, as shown by our preliminary experiments in the context\n",
            "of federated learning.\n",
            "\n",
            "945. Title: Towards Faithful XAI Evaluation via Generalization-Limited Backdoor Watermark\n",
            "   Abstract: Many well-known and effective anomaly detection methods assume that a\n",
            "reasonable decision boundary has a hypersphere shape, which however is\n",
            "difficult to obtain in practice and is not sufficiently compact, especially\n",
            "when the data are in high-dimensional spaces. In this paper, we first propose a\n",
            "novel deep anomaly detection model that improves the original hypersphere\n",
            "learning through an orthogonal projection layer, which ensures that the\n",
            "training data distribution is consistent with the hypersphere hypothesis,\n",
            "thereby increasing the true positive rate and decreasing the false negative\n",
            "rate. Moreover, we propose a bi-hypersphere compression method to obtain a\n",
            "hyperspherical shell that yields a more compact decision region than a\n",
            "hyperball, which is demonstrated theoretically and numerically. The proposed\n",
            "methods are not confined to common datasets such as image and tabular data, but\n",
            "are also extended to a more challenging but promising scenario, graph-level\n",
            "anomaly detection, which learns graph representation with maximum mutual\n",
            "information between the substructure and global structure features while\n",
            "exploring orthogonal single- or bi-hypersphere anomaly decision boundaries. The\n",
            "numerical and visualization results on benchmark datasets demonstrate the\n",
            "superiority of our methods in comparison to many baselines and state-of-the-art\n",
            "methods.\n",
            "\n",
            "946. Title: Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources\n",
            "   Abstract: Recent progress in large language models (LLMs) like GPT-4 and PaLM-2 has\n",
            "brought significant advancements in addressing math reasoning problems. In\n",
            "particular, OpenAI's latest version of GPT-4, known as GPT-4 Code Interpreter,\n",
            "shows remarkable performance on challenging math datasets. In this paper, we\n",
            "explore the effect of code on enhancing LLMs' reasoning capability by\n",
            "introducing different constraints on the \\textit{Code Usage Frequency} of GPT-4\n",
            "Code Interpreter. We found that its success can be largely attributed to its\n",
            "powerful skills in generating and executing code, evaluating the output of code\n",
            "execution, and rectifying its solution when receiving unreasonable outputs.\n",
            "Based on this insight, we propose a novel and effective prompting method,\n",
            "explicit \\uline{c}ode-based \\uline{s}elf-\\uline{v}erification~(CSV), to further\n",
            "boost the mathematical reasoning potential of GPT-4 Code Interpreter. This\n",
            "method employs a zero-shot prompt on GPT-4 Code Interpreter to encourage it to\n",
            "use code to self-verify its answers. In instances where the verification state\n",
            "registers as ``False'', the model shall automatically amend its solution,\n",
            "analogous to our approach of rectifying errors during a mathematics\n",
            "examination. Furthermore, we recognize that the states of the verification\n",
            "result indicate the confidence of a solution, which can improve the\n",
            "effectiveness of majority voting. With GPT-4 Code Interpreter and CSV, we\n",
            "achieve an impressive zero-shot accuracy on MATH dataset \\textbf{(53.9\\% $\\to$\n",
            "84.3\\%)}.\n",
            "\n",
            "947. Title: GlucoBench: Curated List of Continuous Glucose Monitoring Datasets with Prediction Benchmarks\n",
            "   Abstract: We introduce a new framework for molecular graph generation with 3D molecular\n",
            "generative models. Our Synthetic Coordinate Embedding (SyCo) framework maps\n",
            "molecular graphs to Euclidean point clouds via synthetic conformer coordinates\n",
            "and learns the inverse map using an E(n)-Equivariant Graph Neural Network\n",
            "(EGNN). The induced point cloud-structured latent space is well-suited to apply\n",
            "existing 3D molecular generative models. This approach simplifies the graph\n",
            "generation problem - without relying on molecular fragments nor autoregressive\n",
            "decoding - into a point cloud generation problem followed by node and edge\n",
            "classification tasks. Further, we propose a novel similarity-constrained\n",
            "optimization scheme for 3D diffusion models based on inpainting and guidance.\n",
            "As a concrete implementation of our framework, we develop EDM-SyCo based on the\n",
            "E(3) Equivariant Diffusion Model (EDM). EDM-SyCo achieves state-of-the-art\n",
            "performance in distribution learning of molecular graphs, outperforming the\n",
            "best non-autoregressive methods by more than 30% on ZINC250K and 16% on the\n",
            "large-scale GuacaMol dataset while improving conditional generation by up to\n",
            "3.9 times.\n",
            "\n",
            "948. Title: Improved Analysis of Sparse Linear Regression in Local Differential Privacy Model\n",
            "   Abstract: We propose a new approach for propagating stable probability distributions\n",
            "through neural networks. Our method is based on local linearization, which we\n",
            "show to be an optimal approximation in terms of total variation distance for\n",
            "the ReLU non-linearity. This allows propagating Gaussian and Cauchy input\n",
            "uncertainties through neural networks to quantify their output uncertainties.\n",
            "To demonstrate the utility of propagating distributions, we apply the proposed\n",
            "method to predicting calibrated confidence intervals and selective prediction\n",
            "on out-of-distribution data. The results demonstrate a broad applicability of\n",
            "propagating distributions and show the advantages of our method over other\n",
            "approaches such as moment matching.\n",
            "\n",
            "949. Title: State Representation Learning Using an Unbalanced Atlas\n",
            "   Abstract: Recent years have witnessed the great success of graph pre-training for graph\n",
            "representation learning. With hundreds of graph pre-training tasks proposed,\n",
            "integrating knowledge acquired from multiple pre-training tasks has become a\n",
            "popular research topic. In this paper, we identify two important collaborative\n",
            "processes for this topic: (1) select: how to select an optimal task combination\n",
            "from a given task pool based on their compatibility, and (2) weigh: how to\n",
            "weigh the selected tasks based on their importance. While there currently has\n",
            "been a lot of work focused on weighing, comparatively little effort has been\n",
            "devoted to selecting. This paper proposes a novel instance-level framework for\n",
            "integrating multiple graph pre-training tasks, Weigh And Select (WAS), where\n",
            "the two collaborative processes, weighing and selecting, are combined by\n",
            "decoupled siamese networks. Specifically, it first adaptively learns an optimal\n",
            "combination of tasks for each instance from a given task pool, based on which a\n",
            "customized instance-level task weighing strategy is learned. Extensive\n",
            "experiments on 16 graph datasets across node-level and graph-level downstream\n",
            "tasks have demonstrated that by combining a few simple but classical tasks, WAS\n",
            "can achieve comparable performance to other leading counterparts. The code is\n",
            "available at https://github.com/TianyuFan0504/WAS.\n",
            "\n",
            "950. Title: Few-Shot Detection of Machine-Generated Text using Style Representations\n",
            "   Abstract: The manifold hypothesis posits that high-dimensional data often lies on a\n",
            "lower-dimensional manifold and that utilizing this manifold as the target space\n",
            "yields more efficient representations. While numerous traditional\n",
            "manifold-based techniques exist for dimensionality reduction, their application\n",
            "in self-supervised learning has witnessed slow progress. The recent MSimCLR\n",
            "method combines manifold encoding with SimCLR but requires extremely low target\n",
            "encoding dimensions to outperform SimCLR, limiting its applicability. This\n",
            "paper introduces a novel learning paradigm using an unbalanced atlas (UA),\n",
            "capable of surpassing state-of-the-art self-supervised learning approaches. We\n",
            "investigated and engineered the DeepInfomax with an unbalanced atlas (DIM-UA)\n",
            "method by adapting the Spatiotemporal DeepInfomax (ST-DIM) framework to align\n",
            "with our proposed UA paradigm. The efficacy of DIM-UA is demonstrated through\n",
            "training and evaluation on the Atari Annotated RAM Interface (AtariARI)\n",
            "benchmark, a modified version of the Atari 2600 framework that produces\n",
            "annotated image samples for representation learning. The UA paradigm improves\n",
            "existing algorithms significantly as the number of target encoding dimensions\n",
            "grows. For instance, the mean F1 score averaged over categories of DIM-UA is\n",
            "~75% compared to ~70% of ST-DIM when using 16384 hidden units.\n",
            "\n",
            "951. Title: Latent 3D Graph Diffusion\n",
            "   Abstract: Recent advancements in language representation learning primarily emphasize\n",
            "language modeling for deriving meaningful representations, often neglecting\n",
            "style-specific considerations. This study addresses this gap by creating\n",
            "generic, sentence-level style embeddings crucial for style-centric tasks. Our\n",
            "approach is grounded on the premise that low-level text style changes can\n",
            "compose any high-level style. We hypothesize that applying this concept to\n",
            "representation learning enables the development of versatile text style\n",
            "embeddings. By fine-tuning a general-purpose text encoder using contrastive\n",
            "learning and standard cross-entropy loss, we aim to capture these low-level\n",
            "style shifts, anticipating that they offer insights applicable to high-level\n",
            "text styles. The outcomes prompt us to reconsider the underlying assumptions as\n",
            "the results do not always show that the learned style representations capture\n",
            "high-level text styles.\n",
            "\n",
            "952. Title: Uncertainty Quantification via Stable Distribution Propagation\n",
            "   Abstract: In this paper, we revisit the problem of sparse linear regression in the\n",
            "local differential privacy (LDP) model. Existing research in the\n",
            "non-interactive and sequentially local models has focused on obtaining the\n",
            "lower bounds for the case where the underlying parameter is $1$-sparse, and\n",
            "extending such bounds to the more general $k$-sparse case has proven to be\n",
            "challenging. Moreover, it is unclear whether efficient non-interactive LDP\n",
            "(NLDP) algorithms exist. To address these issues, we first consider the problem\n",
            "in the $\\epsilon$ non-interactive LDP model and provide a lower bound of\n",
            "$\\Omega(\\frac{\\sqrt{dk\\log d}}{\\sqrt{n}\\epsilon})$ on the $\\ell_2$-norm\n",
            "estimation error for sub-Gaussian data, where $n$ is the sample size and $d$ is\n",
            "the dimension of the space. We propose an innovative NLDP algorithm, the very\n",
            "first of its kind for the problem. As a remarkable outcome, this algorithm also\n",
            "yields a novel and highly efficient estimator as a valuable by-product. Our\n",
            "algorithm achieves an upper bound of\n",
            "$\\tilde{O}({\\frac{d\\sqrt{k}}{\\sqrt{n}\\epsilon}})$ for the estimation error when\n",
            "the data is sub-Gaussian, which can be further improved by a factor of\n",
            "$O(\\sqrt{d})$ if the server has additional public but unlabeled data. For the\n",
            "sequentially interactive LDP model, we show a similar lower bound of\n",
            "$\\Omega({\\frac{\\sqrt{dk}}{\\sqrt{n}\\epsilon}})$. As for the upper bound, we\n",
            "rectify a previous method and show that it is possible to achieve a bound of\n",
            "$\\tilde{O}(\\frac{k\\sqrt{d}}{\\sqrt{n}\\epsilon})$. Our findings reveal\n",
            "fundamental differences between the non-private case, central DP model, and\n",
            "local DP model in the sparse linear regression problem.\n",
            "\n",
            "953. Title: Beyond Accuracy: Evaluating Self-Consistency of Code Large Language Models with IdentityChain\n",
            "   Abstract: Code Large Language Models (Code LLMs) are being increasingly employed in\n",
            "real-life applications, so evaluating them is critical. While the conventional\n",
            "accuracy evaluates the performance of Code LLMs on a set of individual tasks,\n",
            "their self-consistency across different tasks is overlooked. Intuitively, a\n",
            "trustworthy model should be self-consistent when generating natural language\n",
            "specifications for its own code and generating code for its own specifications.\n",
            "Failure to preserve self-consistency reveals a lack of understanding of the\n",
            "shared semantics underlying natural language and programming language, and\n",
            "therefore undermines the trustworthiness of a model. In this paper, we first\n",
            "formally define the self-consistency of Code LLMs and then design a framework,\n",
            "IdentityChain, which effectively and efficiently evaluates the self-consistency\n",
            "and conventional accuracy of a model at the same time. We study eleven Code\n",
            "LLMs and show that they fail to preserve self-consistency, which is indeed a\n",
            "distinct aspect from conventional accuracy. Furthermore, we show that\n",
            "IdentityChain can be used as a model debugging tool to expose weaknesses of\n",
            "Code LLMs by demonstrating three major weaknesses that we identify in current\n",
            "models using IdentityChain. Our code is available at\n",
            "https://github.com/marcusm117/IdentityChain.\n",
            "\n",
            "954. Title: Diffusion Models for Multi-Task Generative Modeling\n",
            "   Abstract: We propose the first loss function for approximate Nash equilibria of\n",
            "normal-form games that is amenable to unbiased Monte Carlo estimation. This\n",
            "construction allows us to deploy standard non-convex stochastic optimization\n",
            "techniques for approximating Nash equilibria, resulting in novel algorithms\n",
            "with provable guarantees. We complement our theoretical analysis with\n",
            "experiments demonstrating that stochastic gradient descent can outperform\n",
            "previous state-of-the-art approaches.\n",
            "\n",
            "955. Title: Approximating Nash Equilibria in Normal-Form Games via Stochastic Optimization\n",
            "   Abstract: The advent of Large Language Models (LLMs) has made a transformative impact.\n",
            "However, the potential that LLMs such as ChatGPT can be exploited to generate\n",
            "misinformation has posed a serious concern to online safety and public trust. A\n",
            "fundamental research question is: will LLM-generated misinformation cause more\n",
            "harm than human-written misinformation? We propose to tackle this question from\n",
            "the perspective of detection difficulty. We first build a taxonomy of\n",
            "LLM-generated misinformation. Then we categorize and validate the potential\n",
            "real-world methods for generating misinformation with LLMs. Then, through\n",
            "extensive empirical investigation, we discover that LLM-generated\n",
            "misinformation can be harder to detect for humans and detectors compared to\n",
            "human-written misinformation with the same semantics, which suggests it can\n",
            "have more deceptive styles and potentially cause more harm. We also discuss the\n",
            "implications of our discovery on combating misinformation in the age of LLMs\n",
            "and the countermeasures.\n",
            "\n",
            "956. Title: Can LLM-Generated Misinformation Be Detected?\n",
            "   Abstract: The predictions of small transformers, trained to calculate the greatest\n",
            "common divisor (GCD) of two positive integers, can be fully characterized by\n",
            "looking at model inputs and outputs. As training proceeds, the model learns a\n",
            "list $\\mathcal D$ of integers, products of divisors of the base used to\n",
            "represent integers and small primes, and predicts the largest element of\n",
            "$\\mathcal D$ that divides both inputs. Training distributions impact\n",
            "performance. Models trained from uniform operands only learn a handful of GCD\n",
            "(up to $38$ GCD $\\leq100$). Log-uniform operands boost performance to $73$ GCD\n",
            "$\\leq 100$, and a log-uniform distribution of outcomes (i.e. GCD) to $91$.\n",
            "However, training from uniform (balanced) GCD breaks explainability.\n",
            "\n",
            "957. Title: Robust NAS under adversarial training: benchmark, theory, and beyond\n",
            "   Abstract: I explored adapting Stable Diffusion v1.5 for generating domain-specific\n",
            "satellite and aerial images in remote sensing. Recognizing the limitations of\n",
            "existing models like Midjourney and Stable Diffusion, trained primarily on\n",
            "natural RGB images and lacking context for remote sensing, I used the RSICD\n",
            "dataset to train a Stable Diffusion model with a loss of 0.2. I incorporated\n",
            "descriptive captions from the dataset for text-conditioning. Additionally, I\n",
            "created a synthetic dataset for a Land Use Land Classification (LULC) task,\n",
            "employing prompting techniques with RAG and ChatGPT and fine-tuning a\n",
            "specialized remote sensing LLM. However, I faced challenges with prompt quality\n",
            "and model performance. I trained a classification model (ResNet18) on the\n",
            "synthetic dataset achieving 49.48% test accuracy in TorchGeo to create a\n",
            "baseline. Quantitative evaluation through FID scores and qualitative feedback\n",
            "from domain experts assessed the realism and quality of the generated images\n",
            "and dataset. Despite extensive fine-tuning and dataset iterations, results\n",
            "indicated subpar image quality and realism, as indicated by high FID scores and\n",
            "domain-expert evaluation. These findings call attention to the potential of\n",
            "diffusion models in remote sensing while highlighting significant challenges\n",
            "related to insufficient pretraining data and computational resources.\n",
            "\n",
            "958. Title: Large Language Models are Efficient Learners of Noise-Robust Speech Recognition\n",
            "   Abstract: We consider cross-silo federated linear contextual bandit (LCB) problem under\n",
            "differential privacy, where multiple silos (agents) interact with the local\n",
            "users and communicate via a central server to realize collaboration while\n",
            "without sacrificing each user's privacy. We identify three issues in the\n",
            "state-of-the-art: (i) failure of claimed privacy protection and (ii) incorrect\n",
            "regret bound due to noise miscalculation and (iii) ungrounded communication\n",
            "cost. To resolve these issues, we take a two-step principled approach. First,\n",
            "we design an algorithmic framework consisting of a generic federated LCB\n",
            "algorithm and flexible privacy protocols. Then, leveraging the proposed\n",
            "framework, we study federated LCBs under two different privacy constraints. We\n",
            "first establish privacy and regret guarantees under silo-level local\n",
            "differential privacy, which fix the issues present in state-of-the-art\n",
            "algorithm. To further improve the regret performance, we next consider shuffle\n",
            "model of differential privacy, under which we show that our algorithm can\n",
            "achieve nearly ``optimal'' regret without a trusted server. We accomplish this\n",
            "via two different schemes -- one relies on a new result on privacy\n",
            "amplification via shuffling for DP mechanisms and another one leverages the\n",
            "integration of a shuffle protocol for vector sum into the tree-based mechanism,\n",
            "both of which might be of independent interest. Finally, we support our\n",
            "theoretical results with numerical evaluations over contextual bandit instances\n",
            "generated from both synthetic and real-life data.\n",
            "\n",
            "959. Title: Decongestion by Representation: Learning to Improve Economic Welfare in Marketplaces\n",
            "   Abstract: Evolution strategies (ES), as a family of black-box optimization algorithms,\n",
            "recently emerge as a scalable alternative to reinforcement learning (RL)\n",
            "approaches such as Q-learning or policy gradient, and are much faster when many\n",
            "central processing units (CPUs) are available due to better parallelization. In\n",
            "this paper, we propose a systematic incremental learning method for ES in\n",
            "dynamic environments. The goal is to adjust previously learned policy to a new\n",
            "one incrementally whenever the environment changes. We incorporate an instance\n",
            "weighting mechanism with ES to facilitate its learning adaptation, while\n",
            "retaining scalability of ES. During parameter updating, higher weights are\n",
            "assigned to instances that contain more new knowledge, thus encouraging the\n",
            "search distribution to move towards new promising areas of parameter space. We\n",
            "propose two easy-to-implement metrics to calculate the weights: instance\n",
            "novelty and instance quality. Instance novelty measures an instance's\n",
            "difference from the previous optimum in the original environment, while\n",
            "instance quality corresponds to how well an instance performs in the new\n",
            "environment. The resulting algorithm, Instance Weighted Incremental Evolution\n",
            "Strategies (IW-IES), is verified to achieve significantly improved performance\n",
            "on challenging RL tasks ranging from robot navigation to locomotion. This paper\n",
            "thus introduces a family of scalable ES algorithms for RL domains that enables\n",
            "rapid learning adaptation to dynamic environments.\n",
            "\n",
            "960. Title: End-to-End (Instance)-Image Goal Navigation through Correspondence as an Emergent Phenomenon\n",
            "   Abstract: We reveal and address the frequently overlooked yet important issue of\n",
            "disguised procedural unfairness, namely, the potentially inadvertent\n",
            "alterations on the behavior of neutral (i.e., not problematic) aspects of data\n",
            "generating process, and/or the lack of procedural assurance of the greatest\n",
            "benefit of the least advantaged individuals. Inspired by John Rawls's advocacy\n",
            "for pure procedural justice, we view automated decision-making as a microcosm\n",
            "of social institutions, and consider how the data generating process itself can\n",
            "satisfy the requirements of procedural fairness. We propose a framework that\n",
            "decouples the objectionable data generating components from the neutral ones by\n",
            "utilizing reference points and the associated value instantiation rule. Our\n",
            "findings highlight the necessity of preventing disguised procedural unfairness,\n",
            "drawing attention not only to the objectionable data generating components that\n",
            "we aim to mitigate, but also more importantly, to the neutral components that\n",
            "we intend to keep unaffected.\n",
            "\n",
            "961. Title: Generalized Policy Iteration using Tensor Approximation for Hybrid Control\n",
            "   Abstract: Recent developments in neural architecture search (NAS) emphasize the\n",
            "significance of considering robust architectures against malicious data.\n",
            "However, there is a notable absence of benchmark evaluations and theoretical\n",
            "guarantees for searching these robust architectures, especially when\n",
            "adversarial training is considered. In this work, we aim to address these two\n",
            "challenges, making twofold contributions. First, we release a comprehensive\n",
            "data set that encompasses both clean accuracy and robust accuracy for a vast\n",
            "array of adversarially trained networks from the NAS-Bench-201 search space on\n",
            "image datasets. Then, leveraging the neural tangent kernel (NTK) tool from deep\n",
            "learning theory, we establish a generalization theory for searching\n",
            "architecture in terms of clean accuracy and robust accuracy under\n",
            "multi-objective adversarial training. We firmly believe that our benchmark and\n",
            "theoretical insights will significantly benefit the NAS community through\n",
            "reliable reproducibility, efficient assessment, and theoretical foundation,\n",
            "particularly in the pursuit of robust architectures.\n",
            "\n",
            "962. Title: On Differentially Private Federated Linear Contextual Bandits\n",
            "   Abstract: Sampling from probability densities is a common challenge in fields such as\n",
            "Uncertainty Quantification (UQ) and Generative Modelling (GM). In GM in\n",
            "particular, the use of reverse-time diffusion processes depending on the\n",
            "log-densities of Ornstein-Uhlenbeck forward processes are a popular sampling\n",
            "tool. In Berner et al. [2022] the authors point out that these log-densities\n",
            "can be obtained by solution of a \\textit{Hamilton-Jacobi-Bellman} (HJB)\n",
            "equation known from stochastic optimal control. While this HJB equation is\n",
            "usually treated with indirect methods such as policy iteration and unsupervised\n",
            "training of black-box architectures like Neural Networks, we propose instead to\n",
            "solve the HJB equation by direct time integration, using compressed polynomials\n",
            "represented in the Tensor Train (TT) format for spatial discretization.\n",
            "Crucially, this method is sample-free, agnostic to normalization constants and\n",
            "can avoid the curse of dimensionality due to the TT compression. We provide a\n",
            "complete derivation of the HJB equation's action on Tensor Train polynomials\n",
            "and demonstrate the performance of the proposed time-step-, rank- and\n",
            "degree-adaptive integration method on a nonlinear sampling task in 20\n",
            "dimensions.\n",
            "\n",
            "963. Title: Procedural Fairness Through Decoupling Objectionable Data Generating Components\n",
            "   Abstract: In the pursuit of reducing the number of trainable parameters in deep\n",
            "transformer networks, we employ Reinforcement Learning to dynamically select\n",
            "layers during training and tie them together. Every few iterations, the RL\n",
            "agent is asked whether to train each layer $i$ independently or to copy the\n",
            "weights of a previous layer $j<i$. This facilitates weight sharing, reduces the\n",
            "number of trainable parameters, and also serves as an effective regularization\n",
            "technique. Experimental evaluations validate that our model modestly\n",
            "outperforms the baseline transformer model with regard to perplexity and\n",
            "drastically reduces the number of trainable parameters. In particular, the\n",
            "memory consumption during training is up to one order of magnitude less than\n",
            "the conventional training method.\n",
            "\n",
            "964. Title: Bounds on Representation-Induced Confounding Bias for Treatment Effect Estimation\n",
            "   Abstract: Optimal transport (OT) serves as a natural framework for comparing\n",
            "probability measures, with applications in statistics, machine learning, and\n",
            "applied mathematics. Alas, statistical estimation and exact computation of the\n",
            "OT distances suffer from the curse of dimensionality. To circumvent these\n",
            "issues, entropic regularization has emerged as a remedy that enables parametric\n",
            "estimation rates via plug-in and efficient computation using Sinkhorn\n",
            "iterations. Motivated by further scaling up entropic OT (EOT) to data\n",
            "dimensions and sample sizes that appear in modern machine learning\n",
            "applications, we propose a novel neural estimation approach. Our estimator\n",
            "parametrizes a semi-dual representation of the EOT distance by a neural\n",
            "network, approximates expectations by sample means, and optimizes the resulting\n",
            "empirical objective over parameter space. We establish non-asymptotic error\n",
            "bounds on the EOT neural estimator of the cost and optimal plan. Our bounds\n",
            "characterize the effective error in terms of neural network size and the number\n",
            "of samples, revealing optimal scaling laws that guarantee parametric\n",
            "convergence. The bounds hold for compactly supported distributions and imply\n",
            "that the proposed estimator is minimax-rate optimal over that class. Numerical\n",
            "experiments validating our theory are also provided.\n",
            "\n",
            "965. Title: Spurious Feature Diversification Improves Out-of-distribution Generalization\n",
            "   Abstract: Reward models play a key role in aligning language model applications towards\n",
            "human preferences. However, this setup creates an incentive for the language\n",
            "model to exploit errors in the reward model to achieve high estimated reward, a\n",
            "phenomenon often termed \\emph{reward hacking}. A natural mitigation is to train\n",
            "an ensemble of reward models, aggregating over model outputs to obtain a more\n",
            "robust reward estimate. We explore the application of reward ensembles to\n",
            "alignment at both training time (through reinforcement learning) and inference\n",
            "time (through reranking). First, we show that reward models are\n",
            "\\emph{underspecified}: reward models that perform similarly in-distribution can\n",
            "yield very different rewards when used in alignment, due to distribution shift.\n",
            "Second, underspecification results in overoptimization, where alignment to one\n",
            "reward model does not improve reward as measured by another reward model\n",
            "trained on the same data. Third, overoptimization is mitigated by the use of\n",
            "reward ensembles, and ensembles that vary by their \\emph{pretraining} seeds\n",
            "lead to better generalization than ensembles that differ only by their\n",
            "\\emph{fine-tuning} seeds, with both outperforming individual reward models.\n",
            "However, even pretrain reward ensembles do not eliminate reward hacking: we\n",
            "show several qualitative reward hacking phenomena that are not mitigated by\n",
            "ensembling because all reward models in the ensemble exhibit similar error\n",
            "patterns.\n",
            "\n",
            "966. Title: Energy-guided Entropic Neural Optimal Transport\n",
            "   Abstract: The substantial success of Vision Transformer (ViT) in computer vision tasks\n",
            "is largely attributed to the architecture design. This underscores the\n",
            "necessity of efficient architecture search for designing better ViTs\n",
            "automatically. As training-based architecture search methods are\n",
            "computationally intensive, there is a growing interest in training-free methods\n",
            "that use zero-cost proxies to score ViTs. However, existing training-free\n",
            "approaches require expert knowledge to manually design specific zero-cost\n",
            "proxies. Moreover, these zero-cost proxies exhibit limitations to generalize\n",
            "across diverse domains. In this paper, we introduce Auto-Prox, an automatic\n",
            "proxy discovery framework, to address the problem. First, we build the\n",
            "ViT-Bench-101, which involves different ViT candidates and their actual\n",
            "performance on multiple datasets. Utilizing ViT-Bench-101, we can evaluate\n",
            "zero-cost proxies based on their score-accuracy correlation. Then, we represent\n",
            "zero-cost proxies with computation graphs and organize the zero-cost proxy\n",
            "search space with ViT statistics and primitive operations. To discover generic\n",
            "zero-cost proxies, we propose a joint correlation metric to evolve and mutate\n",
            "different zero-cost proxy candidates. We introduce an elitism-preserve strategy\n",
            "for search efficiency to achieve a better trade-off between exploitation and\n",
            "exploration. Based on the discovered zero-cost proxy, we conduct a ViT\n",
            "architecture search in a training-free manner. Extensive experiments\n",
            "demonstrate that our method generalizes well to different datasets and achieves\n",
            "state-of-the-art results both in ranking correlation and final accuracy. Codes\n",
            "can be found at https://github.com/lilujunai/Auto-Prox-AAAI24.\n",
            "\n",
            "967. Title: Small-scale proxies for large-scale Transformer training instabilities\n",
            "   Abstract: Generalization to out-of-distribution (OOD) data is a critical challenge in\n",
            "machine learning. Ensemble-based methods, like weight space ensembles that\n",
            "interpolate model parameters, have been shown to achieve superior OOD\n",
            "performance. However, the underlying mechanism for their effectiveness remains\n",
            "unclear. In this study, we closely examine WiSE-FT, a popular weight space\n",
            "ensemble method that interpolates between a pre-trained and a fine-tuned model.\n",
            "We observe an unexpected ``FalseFalseTrue\" phenomenon, in which WiSE-FT\n",
            "successfully corrects many cases where each individual model makes incorrect\n",
            "predictions, which contributes significantly to its OOD effectiveness. To gain\n",
            "further insights, we conduct theoretical analysis in a multi-class setting with\n",
            "a large number of spurious features. Our analysis predicts the above phenomenon\n",
            "and it further shows that ensemble-based models reduce prediction errors in the\n",
            "OOD settings by utilizing a more diverse set of spurious features. Contrary to\n",
            "the conventional wisdom that focuses on learning invariant features for better\n",
            "OOD performance, our findings suggest that incorporating a large number of\n",
            "diverse spurious features weakens their individual contributions, leading to\n",
            "improved overall OOD generalization performance. Additionally, our findings\n",
            "provide the first explanation for the mysterious phenomenon of weight space\n",
            "ensembles outperforming output space ensembles in OOD. Empirically we\n",
            "demonstrate the effectiveness of utilizing diverse spurious features on a\n",
            "MultiColorMNIST dataset, and our experimental results are consistent with the\n",
            "theoretical analysis. Building upon the new theoretical insights into the\n",
            "efficacy of ensemble methods, we further propose a novel averaging method\n",
            "called BAlaNced averaGing (BANG) which significantly enhances the OOD\n",
            "performance of WiSE-FT.\n",
            "\n",
            "968. Title: Learning to Reject with a Fixed Predictor: Application to Decontextualization\n",
            "   Abstract: Multimodal large language models (MLLMs) have recently become a focal point\n",
            "of research due to their formidable multimodal understanding capabilities. For\n",
            "example, in the audio and speech domains, an LLM can be equipped with\n",
            "(automatic) speech recognition (ASR) abilities by just concatenating the audio\n",
            "tokens, computed with an audio encoder, and the text tokens to achieve\n",
            "state-of-the-art results. On the contrary, tasks like visual and audio-visual\n",
            "speech recognition (VSR/AVSR), which also exploit noise-invariant lip movement\n",
            "information, have received little or no attention. To bridge this gap, we\n",
            "propose Llama-AVSR, a new MLLM with strong audio-visual speech recognition\n",
            "capabilities. It leverages pre-trained audio and video encoders to produce\n",
            "modality-specific tokens which, together with the text tokens, are processed by\n",
            "a pre-trained LLM (e.g., Llama3.1-8B) to yield the resulting response in an\n",
            "auto-regressive fashion. Llama-AVSR requires a small number of trainable\n",
            "parameters as only modality-specific projectors and LoRA modules are trained\n",
            "whereas the multi-modal encoders and LLM are kept frozen. We evaluate our\n",
            "proposed approach on LRS3, the largest public AVSR benchmark, and we achieve\n",
            "new state-of-the-art results for the tasks of ASR and AVSR with a WER of 0.81%\n",
            "and 0.77%, respectively. To bolster our results, we investigate the key factors\n",
            "that underpin the effectiveness of Llama-AVSR: the choice of the pre-trained\n",
            "encoders and LLM, the efficient integration of LoRA modules, and the optimal\n",
            "performance-efficiency trade-off obtained via modality-aware compression rates.\n",
            "\n",
            "969. Title: Soft Robust MDPs and Risk-Sensitive MDPs: Equivalence, Policy Gradient, and Sample Complexity\n",
            "   Abstract: Robust Markov Decision Processes (MDPs) and risk-sensitive MDPs are both\n",
            "powerful tools for making decisions in the presence of uncertainties. Previous\n",
            "efforts have aimed to establish their connections, revealing equivalences in\n",
            "specific formulations. This paper introduces a new formulation for\n",
            "risk-sensitive MDPs, which assesses risk in a slightly different manner\n",
            "compared to the classical Markov risk measure (Ruszczy\\'nski 2010), and\n",
            "establishes its equivalence with a class of soft robust MDP (RMDP) problems,\n",
            "including the standard RMDP as a special case. Leveraging this equivalence, we\n",
            "further derive the policy gradient theorem for both problems, proving gradient\n",
            "domination and global convergence of the exact policy gradient method under the\n",
            "tabular setting with direct parameterization. This forms a sharp contrast to\n",
            "the Markov risk measure, known to be potentially non-gradient-dominant (Huang\n",
            "et al. 2021). We also propose a sample-based offline learning algorithm, namely\n",
            "the robust fitted-Z iteration (RFZI), for a specific soft RMDP problem with a\n",
            "KL-divergence regularization term (or equivalently the risk-sensitive MDP with\n",
            "an entropy risk measure). We showcase its streamlined design and less stringent\n",
            "assumptions due to the equivalence and analyze its sample complexity\n",
            "\n",
            "970. Title: PlaSma: Procedural Knowledge Models for Language-based Planning and Re-Planning\n",
            "   Abstract: State-of-the-art methods for conditional average treatment effect (CATE)\n",
            "estimation make widespread use of representation learning. Here, the idea is to\n",
            "reduce the variance of the low-sample CATE estimation by a (potentially\n",
            "constrained) low-dimensional representation. However, low-dimensional\n",
            "representations can lose information about the observed confounders and thus\n",
            "lead to bias, because of which the validity of representation learning for CATE\n",
            "estimation is typically violated. In this paper, we propose a new,\n",
            "representation-agnostic refutation framework for estimating bounds on the\n",
            "representation-induced confounding bias that comes from dimensionality\n",
            "reduction (or other constraints on the representations) in CATE estimation.\n",
            "First, we establish theoretically under which conditions CATE is\n",
            "non-identifiable given low-dimensional (constrained) representations. Second,\n",
            "as our remedy, we propose a neural refutation framework which performs partial\n",
            "identification of CATE or, equivalently, aims at estimating lower and upper\n",
            "bounds of the representation-induced confounding bias. We demonstrate the\n",
            "effectiveness of our bounds in a series of experiments. In sum, our refutation\n",
            "framework is of direct relevance in practice where the validity of CATE\n",
            "estimation is of importance.\n",
            "\n",
            "971. Title: Peering Through Preferences: Unraveling Feedback Acquisition for Aligning Large Language Models\n",
            "   Abstract: Prompt learning has emerged as an effective and data-efficient technique in\n",
            "large Vision-Language Models (VLMs). However, when adapting VLMs to specialized\n",
            "domains such as remote sensing and medical imaging, domain prompt learning\n",
            "remains underexplored. While large-scale domain-specific foundation models can\n",
            "help tackle this challenge, their concentration on a single vision level makes\n",
            "it challenging to prompt both vision and language modalities. To overcome this,\n",
            "we propose to leverage domain-specific knowledge from domain-specific\n",
            "foundation models to transfer the robust recognition ability of VLMs from\n",
            "generalized to specialized domains, using quaternion networks. Specifically,\n",
            "the proposed method involves using domain-specific vision features from\n",
            "domain-specific foundation models to guide the transformation of generalized\n",
            "contextual embeddings from the language branch into a specialized space within\n",
            "the quaternion networks. Moreover, we present a hierarchical approach that\n",
            "generates vision prompt features by analyzing intermodal relationships between\n",
            "hierarchical language prompt features and domain-specific vision features. In\n",
            "this way, quaternion networks can effectively mine the intermodal relationships\n",
            "in the specific domain, facilitating domain-specific vision-language\n",
            "contrastive learning. Extensive experiments on domain-specific datasets show\n",
            "that our proposed method achieves new state-of-the-art results in prompt\n",
            "learning.\n",
            "\n",
            "972. Title: Prompt Learning with Quaternion Networks\n",
            "   Abstract: We study the problem of classification with a reject option for a fixed\n",
            "predictor, applicable in natural language processing. We introduce a new\n",
            "problem formulation for this scenario, and an algorithm minimizing a new\n",
            "surrogate loss function. We provide a complete theoretical analysis of the\n",
            "surrogate loss function with a strong $H$-consistency guarantee. For\n",
            "evaluation, we choose the decontextualization task, and provide a\n",
            "manually-labelled dataset of $2\\mathord,000$ examples. Our algorithm\n",
            "significantly outperforms the baselines considered, with a $\\sim\\!\\!25\\%$\n",
            "improvement in coverage when halving the error rate, which is only $\\sim\\!\\! 3\n",
            "\\%$ away from the theoretical limit.\n",
            "\n",
            "973. Title: Robustness of AI-Image Detectors: Fundamental Limits and Practical Attacks\n",
            "   Abstract: Universal Adversarial Perturbations (UAPs) are imperceptible, image-agnostic\n",
            "vectors that cause deep neural networks (DNNs) to misclassify inputs with high\n",
            "probability. In practical attack scenarios, adversarial perturbations may\n",
            "undergo transformations such as changes in pixel intensity, scaling, etc.\n",
            "before being added to DNN inputs. Existing methods do not create UAPs robust to\n",
            "these real-world transformations, thereby limiting their applicability in\n",
            "practical attack scenarios. In this work, we introduce and formulate UAPs\n",
            "robust against real-world transformations. We build an iterative algorithm\n",
            "using probabilistic robustness bounds and construct such UAPs robust to\n",
            "transformations generated by composing arbitrary sub-differentiable\n",
            "transformation functions. We perform an extensive evaluation on the popular\n",
            "CIFAR-10 and ILSVRC 2012 datasets measuring our UAPs' robustness under a wide\n",
            "range common, real-world transformations such as rotation, contrast changes,\n",
            "etc. We further show that by using a set of primitive transformations our\n",
            "method can generalize well to unseen transformations such as fog, JPEG\n",
            "compression, etc. Our results show that our method can generate UAPs up to 23%\n",
            "more robust than state-of-the-art baselines.\n",
            "\n",
            "974. Title: Topological data analysis on noisy quantum computers\n",
            "   Abstract: Topological methods, including persistent homology, are powerful tools for\n",
            "analysis of high-dimensional data sets but these methods rely almost\n",
            "exclusively on thresholding techniques. In noisy data sets, thresholding does\n",
            "not always allow for the recovery of topological information. We present an\n",
            "easy to implement, computationally efficient pre-processing algorithm to\n",
            "prepare noisy point cloud data sets for topological data analysis. The\n",
            "topological de-noising algorithm allows for the recovery of topological\n",
            "information that is inaccessible by thresholding methods. We apply the\n",
            "algorithm to synthetically-generated noisy data sets and show the recovery of\n",
            "topological information which is impossible to obtain by thresholding. We also\n",
            "apply the algorithm to natural image data in R^8 and show a very clean recovery\n",
            "of topological information previously only available with large amounts of\n",
            "thresholding. Finally, we discuss future directions for improving this\n",
            "algorithm using zig-zag persistence methods.\n",
            "\n",
            "975. Title: TUVF: Learning Generalizable Texture UV Radiance Fields\n",
            "   Abstract: Congestion is a common failure mode of markets, where consumers compete\n",
            "inefficiently on the same subset of goods (e.g., chasing the same small set of\n",
            "properties on a vacation rental platform). The typical economic story is that\n",
            "prices decongest by balancing supply and demand. But in modern online\n",
            "marketplaces, prices are typically set in a decentralized way by sellers, and\n",
            "the information about items is inevitably partial. The power of a platform is\n",
            "limited to controlling representations -- the subset of information about items\n",
            "presented by default to users. This motivates the present study of decongestion\n",
            "by representation, where a platform seeks to learn representations that reduce\n",
            "congestion and thus improve social welfare. The technical challenge is twofold:\n",
            "relying only on revealed preferences from the choices of consumers, rather than\n",
            "true preferences; and the combinatorial problem associated with representations\n",
            "that determine the features to reveal in the default view. We tackle both\n",
            "challenges by proposing a differentiable proxy of welfare that can be trained\n",
            "end-to-end on consumer choice data. We develop sufficient conditions for when\n",
            "decongestion promotes welfare, and present the results of extensive experiments\n",
            "on both synthetic and real data that demonstrate the utility of our approach.\n",
            "\n",
            "976. Title: Dual-Encoders for Extreme Multi-label Classification\n",
            "   Abstract: Video diffusion models have recently made great progress in generation\n",
            "quality, but are still limited by the high memory and computational\n",
            "requirements. This is because current video diffusion models often attempt to\n",
            "process high-dimensional videos directly. To tackle this issue, we propose\n",
            "content-motion latent diffusion model (CMD), a novel efficient extension of\n",
            "pretrained image diffusion models for video generation. Specifically, we\n",
            "propose an autoencoder that succinctly encodes a video as a combination of a\n",
            "content frame (like an image) and a low-dimensional motion latent\n",
            "representation. The former represents the common content, and the latter\n",
            "represents the underlying motion in the video, respectively. We generate the\n",
            "content frame by fine-tuning a pretrained image diffusion model, and we\n",
            "generate the motion latent representation by training a new lightweight\n",
            "diffusion model. A key innovation here is the design of a compact latent space\n",
            "that can directly utilizes a pretrained image diffusion model, which has not\n",
            "been done in previous latent video diffusion models. This leads to considerably\n",
            "better quality generation and reduced computational costs. For instance, CMD\n",
            "can sample a video 7.7$\\times$ faster than prior approaches by generating a\n",
            "video of 512$\\times$1024 resolution and length 16 in 3.1 seconds. Moreover, CMD\n",
            "achieves an FVD score of 212.7 on WebVid-10M, 27.3% better than the previous\n",
            "state-of-the-art of 292.4.\n",
            "\n",
            "977. Title: Closing the Curious Case of Neural Text Degeneration\n",
            "   Abstract: In this paper we classify all extremal and $s$-extremal binary self-dual\n",
            "codes of length 38. There are exactly 2744 extremal $[38,19,8]$ self-dual\n",
            "codes, two $s$-extremal $[38,19,6]$ codes, and 1730 $s$-extremal $[38,19,8]$\n",
            "codes. We obtain our results from the use of a recursive algorithm used in the\n",
            "recent classification of all extremal self-dual codes of length 36, and from a\n",
            "generalization of this recursive algorithm for the shadow. The classification\n",
            "of $s$-extremal $[38,19,6]$ codes permits to achieve the classification of all\n",
            "$s$-extremal codes with d=6.\n",
            "\n",
            "978. Title: Fast, Expressive $\\mathrm{SE}(n)$ Equivariant Networks through Weight-Sharing in Position-Orientation Space\n",
            "   Abstract: Implicit neural representations (INRs) have recently advanced numerous\n",
            "vision-related areas. INR performance depends strongly on the choice of the\n",
            "nonlinear activation function employed in its multilayer perceptron (MLP)\n",
            "network. A wide range of nonlinearities have been explored, but, unfortunately,\n",
            "current INRs designed to have high accuracy also suffer from poor robustness\n",
            "(to signal noise, parameter variation, etc.). Inspired by harmonic analysis, we\n",
            "develop a new, highly accurate and robust INR that does not exhibit this\n",
            "tradeoff. Wavelet Implicit neural REpresentation (WIRE) uses a continuous\n",
            "complex Gabor wavelet activation function that is well-known to be optimally\n",
            "concentrated in space-frequency and to have excellent biases for representing\n",
            "images. A wide range of experiments (image denoising, image inpainting,\n",
            "super-resolution, computed tomography reconstruction, image overfitting, and\n",
            "novel view synthesis with neural radiance fields) demonstrate that WIRE defines\n",
            "the new state of the art in INR accuracy, training time, and robustness.\n",
            "\n",
            "979. Title: Efficient Video Diffusion Models via Content-Frame Motion-Latent Decomposition\n",
            "   Abstract: Aligning large language models (LLMs) with human values and intents\n",
            "critically involves the use of human or AI feedback. While dense feedback\n",
            "annotations are expensive to acquire and integrate, sparse feedback presents a\n",
            "structural design choice between ratings (e.g., score Response A on a scale of\n",
            "1-7) and rankings (e.g., is Response A better than Response B?). In this work,\n",
            "we analyze the effect of this design choice for the alignment and evaluation of\n",
            "LLMs. We uncover an inconsistency problem wherein the preferences inferred from\n",
            "ratings and rankings significantly disagree 60% for both human and AI\n",
            "annotators. Our subsequent analysis identifies various facets of annotator\n",
            "biases that explain this phenomena, such as human annotators would rate denser\n",
            "responses higher while preferring accuracy during pairwise judgments. To our\n",
            "surprise, we also observe that the choice of feedback protocol also has a\n",
            "significant effect on the evaluation of aligned LLMs. In particular, we find\n",
            "that LLMs that leverage rankings data for alignment (say model X) are preferred\n",
            "over those that leverage ratings data (say model Y), with a rank-based\n",
            "evaluation protocol (is X/Y's response better than reference response?) but not\n",
            "with a rating-based evaluation protocol (score Rank X/Y's response on a scale\n",
            "of 1-7). Our findings thus shed light on critical gaps in methods for\n",
            "evaluating the real-world utility of language models and their strong\n",
            "dependence on the feedback protocol used for alignment. Our code and data are\n",
            "available at https://github.com/Hritikbansal/sparse_feedback.\n",
            "\n",
            "980. Title: Neural Polynomial Gabor Fields for Macro Motion Analysis\n",
            "   Abstract: Despite their ubiquity in language generation, it remains unknown why\n",
            "truncation sampling heuristics like nucleus sampling are so effective. We\n",
            "provide a theoretical explanation for the effectiveness of the truncation\n",
            "sampling by proving that truncation methods that discard tokens below some\n",
            "probability threshold (the most common type of truncation) can guarantee that\n",
            "all sampled tokens have nonzero true probability. However, thresholds are a\n",
            "coarse heuristic, and necessarily discard some tokens with nonzero true\n",
            "probability as well. In pursuit of a more precise sampling strategy, we show\n",
            "that we can leverage a known source of model errors, the softmax bottleneck, to\n",
            "prove that certain tokens have nonzero true probability, without relying on a\n",
            "threshold. Based on our findings, we develop an experimental truncation\n",
            "strategy and the present pilot studies demonstrating the promise of this type\n",
            "of algorithm. Our evaluations show that our method outperforms its\n",
            "threshold-based counterparts under automatic and human evaluation metrics for\n",
            "low-entropy (i.e., close to greedy) open-ended text generation. Our theoretical\n",
            "findings and pilot experiments provide both insight into why truncation\n",
            "sampling works, and make progress toward more expressive sampling algorithms\n",
            "that better surface the generative capabilities of large language models.\n",
            "\n",
            "981. Title: Human Motion Diffusion as a Generative Prior\n",
            "   Abstract: Human motion stylization aims to revise the style of an input motion while\n",
            "keeping its content unaltered. Unlike existing works that operate directly in\n",
            "pose space, we leverage the latent space of pretrained autoencoders as a more\n",
            "expressive and robust representation for motion extraction and infusion.\n",
            "Building upon this, we present a novel generative model that produces diverse\n",
            "stylization results of a single motion (latent) code. During training, a motion\n",
            "code is decomposed into two coding components: a deterministic content code,\n",
            "and a probabilistic style code adhering to a prior distribution; then a\n",
            "generator massages the random combination of content and style codes to\n",
            "reconstruct the corresponding motion codes. Our approach is versatile, allowing\n",
            "the learning of probabilistic style space from either style labeled or\n",
            "unlabeled motions, providing notable flexibility in stylization as well. In\n",
            "inference, users can opt to stylize a motion using style cues from a reference\n",
            "motion or a label. Even in the absence of explicit style input, our model\n",
            "facilitates novel re-stylization by sampling from the unconditional style prior\n",
            "distribution. Experimental results show that our proposed stylization models,\n",
            "despite their lightweight design, outperform the state-of-the-art in style\n",
            "reenactment, content preservation, and generalization across various\n",
            "applications and settings. Project Page: https://murrol.github.io/GenMoStyle\n",
            "\n",
            "982. Title: Generative Human Motion Stylization in Latent Space\n",
            "   Abstract: Given the success of Large Language Models (LLMs), there has been\n",
            "considerable interest in studying the properties of model activations. The\n",
            "literature overwhelmingly agrees that LLM representations are dominated by a\n",
            "few \"outlier dimensions\" with exceedingly high variance and magnitude. Several\n",
            "studies in Natural Language Processing (NLP) have sought to mitigate the impact\n",
            "of such outlier dimensions and force LLMs to be isotropic (i.e., have uniform\n",
            "variance across all dimensions in embedding space). Isotropy is thought to be a\n",
            "desirable property for LLMs that improves model performance and more closely\n",
            "aligns textual representations with human intuition. However, many of the\n",
            "claims regarding isotropy in NLP have been based on the average cosine\n",
            "similarity of embeddings, which has recently been shown to be a flawed measure\n",
            "of isotropy. In this paper, we propose I-STAR: IsoScore*-based STable\n",
            "Anisotropic Regularization, a novel regularization method that can be used to\n",
            "increase or decrease levels of isotropy in embedding space during training.\n",
            "I-STAR uses IsoScore*, the first accurate measure of isotropy that is both\n",
            "differentiable and stable on mini-batch computations. In contrast to several\n",
            "previous works, we find that decreasing isotropy in contextualized embeddings\n",
            "improves performance on the majority of tasks and models considered in this\n",
            "paper.\n",
            "\n",
            "983. Title: Reward Model Ensembles Help Mitigate Overoptimization\n",
            "   Abstract: Reinforcement learning from human feedback (RLHF) is a standard approach for\n",
            "fine-tuning large language models to follow instructions. As part of this\n",
            "process, learned reward models are used to approximately model human\n",
            "preferences. However, as imperfect representations of the \"true\" reward, these\n",
            "learned reward models are susceptible to overoptimization. Gao et al. (2023)\n",
            "studied this phenomenon in a synthetic human feedback setup with a\n",
            "significantly larger \"gold\" reward model acting as the true reward (instead of\n",
            "humans) and showed that overoptimization remains a persistent problem\n",
            "regardless of the size of the proxy reward model and training data used. Using\n",
            "a similar setup, we conduct a systematic study to evaluate the efficacy of\n",
            "using ensemble-based conservative optimization objectives, specifically\n",
            "worst-case optimization (WCO) and uncertainty-weighted optimization (UWO), for\n",
            "mitigating reward model overoptimization when using two optimization methods:\n",
            "(a) best-of-n sampling (BoN) (b) proximal policy optimization (PPO). We\n",
            "additionally extend the setup of Gao et al. (2023) to include 25% label noise\n",
            "to better mirror real-world conditions. Both with and without label noise, we\n",
            "find that conservative optimization practically eliminates overoptimization and\n",
            "improves performance by up to 70% for BoN sampling. For PPO, ensemble-based\n",
            "conservative optimization always reduces overoptimization and outperforms\n",
            "single reward model optimization. Moreover, combining it with a small KL\n",
            "penalty successfully prevents overoptimization at no performance cost. Overall,\n",
            "our results demonstrate that ensemble-based conservative optimization can\n",
            "effectively counter overoptimization.\n",
            "\n",
            "984. Title: SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis\n",
            "   Abstract: Bayesian Optimization (BO) is typically used to optimize an unknown function\n",
            "$f$ that is noisy and costly to evaluate, by exploiting an acquisition function\n",
            "that must be maximized at each optimization step. Even if provably\n",
            "asymptotically optimal BO algorithms are efficient at optimizing\n",
            "low-dimensional functions, scaling them to high-dimensional spaces remains an\n",
            "open problem, often tackled by assuming an additive structure for $f$. By doing\n",
            "so, BO algorithms typically introduce additional restrictive assumptions on the\n",
            "additive structure that reduce their applicability domain. This paper contains\n",
            "two main contributions: (i) we relax the restrictive assumptions on the\n",
            "additive structure of $f$ without weakening the maximization guarantees of the\n",
            "acquisition function, and (ii) we address the over-exploration problem for\n",
            "decentralized BO algorithms. To these ends, we propose DuMBO, an asymptotically\n",
            "optimal decentralized BO algorithm that achieves very competitive performance\n",
            "against state-of-the-art BO algorithms, especially when the additive structure\n",
            "of $f$ comprises high-dimensional factors.\n",
            "\n",
            "985. Title: Neural Snowflakes: Universal Latent Graph Inference via Trainable Latent Geometries\n",
            "   Abstract: Recent work has demonstrated the significant potential of denoising diffusion\n",
            "models for generating human motion, including text-to-motion capabilities.\n",
            "However, these methods are restricted by the paucity of annotated motion data,\n",
            "a focus on single-person motions, and a lack of detailed control. In this\n",
            "paper, we introduce three forms of composition based on diffusion priors:\n",
            "sequential, parallel, and model composition. Using sequential composition, we\n",
            "tackle the challenge of long sequence generation. We introduce DoubleTake, an\n",
            "inference-time method with which we generate long animations consisting of\n",
            "sequences of prompted intervals and their transitions, using a prior trained\n",
            "only for short clips. Using parallel composition, we show promising steps\n",
            "toward two-person generation. Beginning with two fixed priors as well as a few\n",
            "two-person training examples, we learn a slim communication block, ComMDM, to\n",
            "coordinate interaction between the two resulting motions. Lastly, using model\n",
            "composition, we first train individual priors to complete motions that realize\n",
            "a prescribed motion for a given joint. We then introduce DiffusionBlending, an\n",
            "interpolation mechanism to effectively blend several such models to enable\n",
            "flexible and efficient fine-grained joint and trajectory-level control and\n",
            "editing. We evaluate the composition methods using an off-the-shelf motion\n",
            "diffusion model, and further compare the results to dedicated models trained\n",
            "for these specific tasks.\n",
            "\n",
            "986. Title: Retro-fallback: retrosynthetic planning in an uncertain world\n",
            "   Abstract: Electroencephalography (EEG) signals, known for convenient non-invasive\n",
            "acquisition but low signal-to-noise ratio, have recently gained substantial\n",
            "attention due to the potential to decode natural images. This paper presents a\n",
            "self-supervised framework to demonstrate the feasibility of learning image\n",
            "representations from EEG signals, particularly for object recognition. The\n",
            "framework utilizes image and EEG encoders to extract features from paired image\n",
            "stimuli and EEG responses. Contrastive learning aligns these two modalities by\n",
            "constraining their similarity. With the framework, we attain significantly\n",
            "above-chance results on a comprehensive EEG-image dataset, achieving a top-1\n",
            "accuracy of 15.6% and a top-5 accuracy of 42.8% in challenging 200-way\n",
            "zero-shot tasks. Moreover, we perform extensive experiments to explore the\n",
            "biological plausibility by resolving the temporal, spatial, spectral, and\n",
            "semantic aspects of EEG signals. Besides, we introduce attention modules to\n",
            "capture spatial correlations, providing implicit evidence of the brain activity\n",
            "perceived from EEG data. These findings yield valuable insights for neural\n",
            "decoding and brain-computer interfaces in real-world scenarios. The code will\n",
            "be released on https://github.com/eeyhsong/NICE-EEG.\n",
            "\n",
            "987. Title: Noise-free Score Distillation\n",
            "   Abstract: The inductive bias of a graph neural network (GNN) is largely encoded in its\n",
            "specified graph. Latent graph inference relies on latent geometric\n",
            "representations to dynamically rewire or infer a GNN's graph to maximize the\n",
            "GNN's predictive downstream performance, but it lacks solid theoretical\n",
            "foundations in terms of embedding-based representation guarantees. This paper\n",
            "addresses this issue by introducing a trainable deep learning architecture,\n",
            "coined neural snowflake, that can adaptively implement fractal-like metrics on\n",
            "$\\mathbb{R}^d$. We prove that any given finite weights graph can be\n",
            "isometrically embedded by a standard MLP encoder. Furthermore, when the latent\n",
            "graph can be represented in the feature space of a sufficiently regular kernel,\n",
            "we show that the combined neural snowflake and MLP encoder do not succumb to\n",
            "the curse of dimensionality by using only a low-degree polynomial number of\n",
            "parameters in the number of nodes. This implementation enables a\n",
            "low-dimensional isometric embedding of the latent graph. We conduct synthetic\n",
            "experiments to demonstrate the superior metric learning capabilities of neural\n",
            "snowflakes when compared to more familiar spaces like Euclidean space.\n",
            "Additionally, we carry out latent graph inference experiments on graph\n",
            "benchmarks. Consistently, the neural snowflake model achieves predictive\n",
            "performance that either matches or surpasses that of the state-of-the-art\n",
            "latent graph inference models. Importantly, this performance improvement is\n",
            "achieved without requiring random search for optimal latent geometry. Instead,\n",
            "the neural snowflake model achieves this enhancement in a differentiable\n",
            "manner.\n",
            "\n",
            "988. Title: Towards Optimal Feature-Shaping Methods for Out-of-Distribution Detection\n",
            "   Abstract: Existing score distillation methods are sensitive to classifier-free guidance\n",
            "(CFG) scale: manifested as over-smoothness or instability at small CFG scales,\n",
            "while over-saturation at large ones. To explain and analyze these issues, we\n",
            "revisit the derivation of Score Distillation Sampling (SDS) and decipher\n",
            "existing score distillation with the Wasserstein Generative Adversarial Network\n",
            "(WGAN) paradigm. With the WGAN paradigm, we find that existing score\n",
            "distillation either employs a fixed sub-optimal discriminator or conducts\n",
            "incomplete discriminator optimization, resulting in the scale-sensitive issue.\n",
            "We propose the Adversarial Score Distillation (ASD), which maintains an\n",
            "optimizable discriminator and updates it using the complete optimization\n",
            "objective. Experiments show that the proposed ASD performs favorably in 2D\n",
            "distillation and text-to-3D tasks against existing methods. Furthermore, to\n",
            "explore the generalization ability of our WGAN paradigm, we extend ASD to the\n",
            "image editing task, which achieves competitive results. The project page and\n",
            "code are at https://github.com/2y7c3/ASD.\n",
            "\n",
            "989. Title: Minimum width for universal approximation using ReLU networks on compact domain\n",
            "   Abstract: Embodied perception refers to the ability of an autonomous agent to perceive\n",
            "its environment so that it can (re)act. The responsiveness of the agent is\n",
            "largely governed by latency of its processing pipeline. While past work has\n",
            "studied the algorithmic trade-off between latency and accuracy, there has not\n",
            "been a clear metric to compare different methods along the Pareto optimal\n",
            "latency-accuracy curve. We point out a discrepancy between standard offline\n",
            "evaluation and real-time applications: by the time an algorithm finishes\n",
            "processing a particular frame, the surrounding world has changed. To these\n",
            "ends, we present an approach that coherently integrates latency and accuracy\n",
            "into a single metric for real-time online perception, which we refer to as\n",
            "\"streaming accuracy\". The key insight behind this metric is to jointly evaluate\n",
            "the output of the entire perception stack at every time instant, forcing the\n",
            "stack to consider the amount of streaming data that should be ignored while\n",
            "computation is occurring. More broadly, building upon this metric, we introduce\n",
            "a meta-benchmark that systematically converts any single-frame task into a\n",
            "streaming perception task. We focus on the illustrative tasks of object\n",
            "detection and instance segmentation in urban video streams, and contribute a\n",
            "novel dataset with high-quality and temporally-dense annotations. Our proposed\n",
            "solutions and their empirical analysis demonstrate a number of surprising\n",
            "conclusions: (1) there exists an optimal \"sweet spot\" that maximizes streaming\n",
            "accuracy along the Pareto optimal latency-accuracy curve, (2) asynchronous\n",
            "tracking and future forecasting naturally emerge as internal representations\n",
            "that enable streaming perception, and (3) dynamic scheduling can be used to\n",
            "overcome temporal aliasing, yielding the paradoxical result that latency is\n",
            "sometimes minimized by sitting idle and \"doing nothing\".\n",
            "\n",
            "990. Title: Object-Aware Inversion and Reassembly for Image Editing\n",
            "   Abstract: Periodic time series (PTS) forecasting plays a crucial role in a variety of\n",
            "industries to foster critical tasks, such as early warning, pre-planning,\n",
            "resource scheduling, etc. However, the complicated dependencies of the PTS\n",
            "signal on its inherent periodicity as well as the sophisticated composition of\n",
            "various periods hinder the performance of PTS forecasting. In this paper, we\n",
            "introduce a deep expansion learning framework, DEPTS, for PTS forecasting.\n",
            "DEPTS starts with a decoupled formulation by introducing the periodic state as\n",
            "a hidden variable, which stimulates us to make two dedicated modules to tackle\n",
            "the aforementioned two challenges. First, we develop an expansion module on top\n",
            "of residual learning to perform a layer-by-layer expansion of those complicated\n",
            "dependencies. Second, we introduce a periodicity module with a parameterized\n",
            "periodic function that holds sufficient capacity to capture diversified\n",
            "periods. Moreover, our two customized modules also have certain interpretable\n",
            "capabilities, such as attributing the forecasts to either local momenta or\n",
            "global periodicity and characterizing certain core periodic properties, e.g.,\n",
            "amplitudes and frequencies. Extensive experiments on both synthetic data and\n",
            "real-world data demonstrate the effectiveness of DEPTS on handling PTS. In most\n",
            "cases, DEPTS achieves significant improvements over the best baseline.\n",
            "Specifically, the error reduction can even reach up to 20% for a few cases.\n",
            "Finally, all codes are publicly available.\n",
            "\n",
            "991. Title: Teaching Arithmetic to Small Transformers\n",
            "   Abstract: Large language models like GPT-4 exhibit emergent capabilities across\n",
            "general-purpose tasks, such as basic arithmetic, when trained on extensive text\n",
            "data, even though these tasks are not explicitly encoded by the unsupervised,\n",
            "next-token prediction objective. This study investigates how small\n",
            "transformers, trained from random initialization, can efficiently learn\n",
            "arithmetic operations such as addition, multiplication, and elementary\n",
            "functions like square root, using the next-token prediction objective. We first\n",
            "demonstrate that conventional training data is not the most effective for\n",
            "arithmetic learning, and simple formatting changes can significantly improve\n",
            "accuracy. This leads to sharp phase transitions as a function of training data\n",
            "scale, which, in some cases, can be explained through connections to low-rank\n",
            "matrix completion. Building on prior work, we then train on chain-of-thought\n",
            "style data that includes intermediate step results. Even in the complete\n",
            "absence of pretraining, this approach significantly and simultaneously improves\n",
            "accuracy, sample complexity, and convergence speed. We also study the interplay\n",
            "between arithmetic and text data during training and examine the effects of\n",
            "few-shot prompting, pretraining, and model scale. Additionally, we discuss\n",
            "length generalization challenges. Our work highlights the importance of\n",
            "high-quality, instructive data that considers the particular characteristics of\n",
            "the next-word prediction objective for rapidly eliciting arithmetic\n",
            "capabilities.\n",
            "\n",
            "992. Title: BadEdit: Backdooring Large Language Models by Model Editing\n",
            "   Abstract: It has been shown that deep neural networks of a large enough width are\n",
            "universal approximators but they are not if the width is too small. There were\n",
            "several attempts to characterize the minimum width $w_{\\min}$ enabling the\n",
            "universal approximation property; however, only a few of them found the exact\n",
            "values. In this work, we show that the minimum width for $L^p$ approximation of\n",
            "$L^p$ functions from $[0,1]^{d_x}$ to $\\mathbb R^{d_y}$ is exactly\n",
            "$\\max\\{d_x,d_y,2\\}$ if an activation function is ReLU-Like (e.g., ReLU, GELU,\n",
            "Softplus). Compared to the known result for ReLU networks,\n",
            "$w_{\\min}=\\max\\{d_x+1,d_y\\}$ when the domain is $\\smash{\\mathbb R^{d_x}}$, our\n",
            "result first shows that approximation on a compact domain requires smaller\n",
            "width than on $\\smash{\\mathbb R^{d_x}}$. We next prove a lower bound on\n",
            "$w_{\\min}$ for uniform approximation using general activation functions\n",
            "including ReLU: $w_{\\min}\\ge d_y+1$ if $d_x<d_y\\le2d_x$. Together with our\n",
            "first result, this shows a dichotomy between $L^p$ and uniform approximations\n",
            "for general activation functions and input/output dimensions.\n",
            "\n",
            "993. Title: HyperHuman: Hyper-Realistic Human Generation with Latent Structural Diffusion\n",
            "   Abstract: By comparing the original and target prompts, we can obtain numerous editing\n",
            "pairs, each comprising an object and its corresponding editing target. To allow\n",
            "editability while maintaining fidelity to the input image, existing editing\n",
            "methods typically involve a fixed number of inversion steps that project the\n",
            "whole input image to its noisier latent representation, followed by a denoising\n",
            "process guided by the target prompt. However, we find that the optimal number\n",
            "of inversion steps for achieving ideal editing results varies significantly\n",
            "among different editing pairs, owing to varying editing difficulties.\n",
            "Therefore, the current literature, which relies on a fixed number of inversion\n",
            "steps, produces sub-optimal generation quality, especially when handling\n",
            "multiple editing pairs in a natural image. To this end, we propose a new image\n",
            "editing paradigm, dubbed Object-aware Inversion and Reassembly (OIR), to enable\n",
            "object-level fine-grained editing. Specifically, we design a new search metric,\n",
            "which determines the optimal inversion steps for each editing pair, by jointly\n",
            "considering the editability of the target and the fidelity of the non-editing\n",
            "region. We use our search metric to find the optimal inversion step for each\n",
            "editing pair when editing an image. We then edit these editing pairs separately\n",
            "to avoid concept mismatch. Subsequently, we propose an additional reassembly\n",
            "step to seamlessly integrate the respective editing results and the non-editing\n",
            "region to obtain the final edited image. To systematically evaluate the\n",
            "effectiveness of our method, we collect two datasets called OIRBench for\n",
            "benchmarking single- and multi-object editing, respectively. Experiments\n",
            "demonstrate that our method achieves superior performance in editing object\n",
            "shapes, colors, materials, categories, etc., especially in multi-object editing\n",
            "scenarios.\n",
            "\n",
            "994. Title: The Unreasonable Effectiveness of Linear Prediction as a Perceptual Metric\n",
            "   Abstract: Few neural architectures lend themselves to provable learning with gradient\n",
            "based methods. One popular model is the single-index model, in which labels are\n",
            "produced by composing an unknown linear projection with a possibly unknown\n",
            "scalar link function. Learning this model with SGD is relatively\n",
            "well-understood, whereby the so-called information exponent of the link\n",
            "function governs a polynomial sample complexity rate. However, extending this\n",
            "analysis to deeper or more complicated architectures remains challenging.\n",
            "  In this work, we consider single index learning in the setting of symmetric\n",
            "neural networks. Under analytic assumptions on the activation and maximum\n",
            "degree assumptions on the link function, we prove that gradient flow recovers\n",
            "the hidden planted direction, represented as a finitely supported vector in the\n",
            "feature space of power sum polynomials. We characterize a notion of information\n",
            "exponent adapted to our setting that controls the efficiency of learning.\n",
            "\n",
            "995. Title: Lion Secretly Solves a Constrained Optimization: As Lyapunov Predicts\n",
            "   Abstract: The Lipschitz constant plays a crucial role in certifying the robustness of\n",
            "neural networks to input perturbations. Since calculating the exact Lipschitz\n",
            "constant is NP-hard, efforts have been made to obtain tight upper bounds on the\n",
            "Lipschitz constant. Typically, this involves solving a large matrix\n",
            "verification problem, the computational cost of which grows significantly for\n",
            "both deeper and wider networks. In this paper, we provide a compositional\n",
            "approach to estimate Lipschitz constants for deep feed-forward neural networks.\n",
            "We first obtain an exact decomposition of the large matrix verification problem\n",
            "into smaller sub-problems. Then, leveraging the underlying cascade structure of\n",
            "the network, we develop two algorithms. The first algorithm explores the\n",
            "geometric features of the problem and enables us to provide Lipschitz estimates\n",
            "that are comparable to existing methods by solving small semidefinite programs\n",
            "(SDPs) that are only as large as the size of each layer. The second algorithm\n",
            "relaxes these sub-problems and provides a closed-form solution to each\n",
            "sub-problem for extremely fast estimation, altogether eliminating the need to\n",
            "solve SDPs. The two algorithms represent different levels of trade-offs between\n",
            "efficiency and accuracy. Finally, we demonstrate that our approach provides a\n",
            "steep reduction in computation time (as much as several thousand times faster,\n",
            "depending on the algorithm for deeper networks) while yielding Lipschitz bounds\n",
            "that are very close to or even better than those achieved by state-of-the-art\n",
            "approaches in a broad range of experiments. In summary, our approach\n",
            "considerably advances the scalability and efficiency of certifying neural\n",
            "network robustness, making it particularly attractive for online learning\n",
            "tasks.\n",
            "\n",
            "996. Title: PixArt-$\\alpha$: Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis\n",
            "   Abstract: Weight initialization plays an important role in neural network training.\n",
            "Widely used initialization methods are proposed and evaluated for networks that\n",
            "are trained from scratch. However, the growing number of pretrained models now\n",
            "offers new opportunities for tackling this classical problem of weight\n",
            "initialization. In this work, we introduce weight selection, a method for\n",
            "initializing smaller models by selecting a subset of weights from a pretrained\n",
            "larger model. This enables the transfer of knowledge from pretrained weights to\n",
            "smaller models. Our experiments demonstrate that weight selection can\n",
            "significantly enhance the performance of small models and reduce their training\n",
            "time. Notably, it can also be used together with knowledge distillation. Weight\n",
            "selection offers a new approach to leverage the power of pretrained models in\n",
            "resource-constrained settings, and we hope it can be a useful tool for training\n",
            "small models in the large-model era. Code is available at\n",
            "https://github.com/OscarXZQ/weight-selection.\n",
            "\n",
            "997. Title: PORF: POSE RESIDUAL FIELD FOR ACCURATE NEURAL SURFACE RECONSTRUCTION\n",
            "   Abstract: There has been a recent surge of interest in developing generally-capable\n",
            "agents that can adapt to new tasks without additional training in the\n",
            "environment. Learning world models from reward-free exploration is a promising\n",
            "approach, and enables policies to be trained using imagined experience for new\n",
            "tasks. However, achieving a general agent requires robustness across different\n",
            "environments. In this work, we address the novel problem of generating\n",
            "curricula in the reward-free setting to train robust world models. We consider\n",
            "robustness in terms of minimax regret over all environment instantiations and\n",
            "show that the minimax regret can be connected to minimising the maximum error\n",
            "in the world model across environment instances. This result informs our\n",
            "algorithm, WAKER: Weighted Acquisition of Knowledge across Environments for\n",
            "Robustness. WAKER selects environments for data collection based on the\n",
            "estimated error of the world model for each environment. Our experiments\n",
            "demonstrate that WAKER outperforms several baselines, resulting in improved\n",
            "robustness, efficiency, and generalisation.\n",
            "\n",
            "998. Title: Reward-Free Curricula for Training Robust World Models\n",
            "   Abstract: We show how perceptual embeddings of the visual system can be constructed at\n",
            "inference-time with no training data or deep neural network features. Our\n",
            "perceptual embeddings are solutions to a weighted least squares (WLS) problem,\n",
            "defined at the pixel-level, and solved at inference-time, that can capture\n",
            "global and local image characteristics. The distance in embedding space is used\n",
            "to define a perceptual similarity metric which we call LASI: Linear\n",
            "Autoregressive Similarity Index. Experiments on full-reference image quality\n",
            "assessment datasets show LASI performs competitively with learned deep feature\n",
            "based methods like LPIPS (Zhang et al., 2018) and PIM (Bhardwaj et al., 2020),\n",
            "at a similar computational cost to hand-crafted methods such as MS-SSIM (Wang\n",
            "et al., 2003). We found that increasing the dimensionality of the embedding\n",
            "space consistently reduces the WLS loss while increasing performance on\n",
            "perceptual tasks, at the cost of increasing the computational complexity. LASI\n",
            "is fully differentiable, scales cubically with the number of embedding\n",
            "dimensions, and can be parallelized at the pixel-level. A Maximum\n",
            "Differentiation (MAD) competition (Wang & Simoncelli, 2008) between LASI and\n",
            "LPIPS shows that both methods are capable of finding failure points for the\n",
            "other, suggesting these metrics can be combined.\n",
            "\n",
            "999. Title: Merge, Then Compress: Demystify Efficient SMoE with Hints from Its Routing Policy\n",
            "   Abstract: While reinforcement learning (RL) has shown promising performance, its sample\n",
            "complexity continues to be a substantial hurdle, restricting its broader\n",
            "application across a variety of domains. Imitation learning (IL) utilizes\n",
            "oracles to improve sample efficiency, yet it is often constrained by the\n",
            "quality of the oracles deployed. which actively interleaves between IL and RL\n",
            "based on an online estimate of their performance. RPI draws on the strengths of\n",
            "IL, using oracle queries to facilitate exploration, an aspect that is notably\n",
            "challenging in sparse-reward RL, particularly during the early stages of\n",
            "learning. As learning unfolds, RPI gradually transitions to RL, effectively\n",
            "treating the learned policy as an improved oracle. This algorithm is capable of\n",
            "learning from and improving upon a diverse set of black-box oracles. Integral\n",
            "to RPI are Robust Active Policy Selection (RAPS) and Robust Policy Gradient\n",
            "(RPG), both of which reason over whether to perform state-wise imitation from\n",
            "the oracles or learn from its own value function when the learner's performance\n",
            "surpasses that of the oracles in a specific state. Empirical evaluations and\n",
            "theoretical analysis validate that RPI excels in comparison to existing\n",
            "state-of-the-art methodologies, demonstrating superior performance across\n",
            "various benchmark domains.\n",
            "\n",
            "1000. Title: DIFFTACTILE: A Physics-based Differentiable Tactile Simulator for Contact-rich Robotic Manipulation\n",
            "   Abstract: We consider a combinatorial multi-armed bandit problem for maximum value\n",
            "reward function under maximum value and index feedback. This is a new feedback\n",
            "structure that lies in between commonly studied semi-bandit and full-bandit\n",
            "feedback structures. We propose an algorithm and provide a regret bound for\n",
            "problem instances with stochastic arm outcomes according to arbitrary\n",
            "distributions with finite supports. The regret analysis rests on considering an\n",
            "extended set of arms, associated with values and probabilities of arm outcomes,\n",
            "and applying a smoothness condition. Our algorithm achieves a\n",
            "$O((k/\\Delta)\\log(T))$ distribution-dependent and a $\\tilde{O}(\\sqrt{T})$\n",
            "distribution-independent regret where $k$ is the number of arms selected in\n",
            "each round, $\\Delta$ is a distribution-dependent reward gap and $T$ is the\n",
            "horizon time. Perhaps surprisingly, the regret bound is comparable to\n",
            "previously-known bound under more informative semi-bandit feedback. We\n",
            "demonstrate the effectiveness of our algorithm through experimental results.\n",
            "\n",
            "1001. Title: Combinatorial Bandits for Maximum Value Reward Function under Value-Index Feedback\n",
            "   Abstract: Sparsely activated Mixture-of-Experts (SMoE) has shown promise to scale up\n",
            "the learning capacity of neural networks, however, they have issues like (a)\n",
            "High Memory Usage, due to duplication of the network layers into multiple\n",
            "copies as experts; and (b) Redundancy in Experts, as common learning-based\n",
            "routing policies suffer from representational collapse. Therefore, vanilla SMoE\n",
            "models are memory inefficient and non-scalable, especially for\n",
            "resource-constrained downstream scenarios. In this paper, we ask: Can we craft\n",
            "a compact SMoE model by consolidating expert information? What is the best\n",
            "recipe to merge multiple experts into fewer but more knowledgeable experts? Our\n",
            "pilot investigation reveals that conventional model merging methods fail to be\n",
            "effective in such expert merging for SMoE. The potential reasons are: (1)\n",
            "redundant information overshadows critical experts; (2) appropriate neuron\n",
            "permutation for each expert is missing to bring all of them in alignment. To\n",
            "address this, we propose M-SMoE, which leverages routing statistics to guide\n",
            "expert merging. Specifically, it starts with neuron permutation alignment for\n",
            "experts; then, dominant experts and their \"group members\" are formed; lastly,\n",
            "every expert group is merged into a single expert by utilizing each expert's\n",
            "activation frequency as their weight for merging, thus diminishing the impact\n",
            "of insignificant experts. Moreover, we observed that our proposed merging\n",
            "promotes a low dimensionality in the merged expert's weight space, naturally\n",
            "paving the way for additional compression. Hence, our final method, MC-SMoE\n",
            "(i.e., Merge, then Compress SMoE), further decomposes the merged experts into\n",
            "low-rank and structural sparse alternatives. Extensive experiments across 8\n",
            "benchmarks validate the effectiveness of MC-SMoE. For instance, our MC-SMoE\n",
            "achieves up to 80% memory and a 20% FLOPs reduction, with virtually no loss in\n",
            "performance.\n",
            "\n",
            "1002. Title: On Trajectory Augmentations for Off-Policy Evaluation\n",
            "   Abstract: The objective of domain generalization (DG) is to enhance the transferability\n",
            "of the model learned from a source domain to unobserved domains. To prevent\n",
            "overfitting to a specific domain, Sharpness-Aware Minimization (SAM) reduces\n",
            "source domain's loss sharpness. Although SAM variants have delivered\n",
            "significant improvements in DG, we highlight that there's still potential for\n",
            "improvement in generalizing to unknown domains through the exploration on data\n",
            "space. This paper introduces an objective rooted in both parameter and data\n",
            "perturbed regions for domain generalization, coined Unknown Domain\n",
            "Inconsistency Minimization (UDIM). UDIM reduces the loss landscape\n",
            "inconsistency between source domain and unknown domains. As unknown domains are\n",
            "inaccessible, these domains are empirically crafted by perturbing instances\n",
            "from the source domain dataset. In particular, by aligning the loss landscape\n",
            "acquired in the source domain to the loss landscape of perturbed domains, we\n",
            "expect to achieve generalization grounded on these flat minima for the unknown\n",
            "domains. Theoretically, we validate that merging SAM optimization with the UDIM\n",
            "objective establishes an upper bound for the true objective of the DG task. In\n",
            "an empirical aspect, UDIM consistently outperforms SAM variants across multiple\n",
            "DG benchmark datasets. Notably, UDIM shows statistically significant\n",
            "improvements in scenarios with more restrictive domain information,\n",
            "underscoring UDIM's generalization capability in unseen domains. Our code is\n",
            "available at \\url{https://github.com/SJShin-AI/UDIM}.\n",
            "\n",
            "1003. Title: Unknown Domain Inconsistency Minimization for Domain Generalization\n",
            "   Abstract: Lion (Evolved Sign Momentum), a new optimizer discovered through program\n",
            "search, has shown promising results in training large AI models. It performs\n",
            "comparably or favorably to AdamW but with greater memory efficiency. As we can\n",
            "expect from the results of a random search program, Lion incorporates elements\n",
            "from several existing algorithms, including signed momentum, decoupled weight\n",
            "decay, Polak, and Nesterov momentum, but does not fit into any existing\n",
            "category of theoretically grounded optimizers. Thus, even though Lion appears\n",
            "to perform well as a general-purpose optimizer for a wide range of tasks, its\n",
            "theoretical basis remains uncertain. This lack of theoretical clarity limits\n",
            "opportunities to further enhance and expand Lion's efficacy.\n",
            "  This work aims to demystify Lion. Based on both continuous-time and\n",
            "discrete-time analysis, we demonstrate that Lion is a theoretically novel and\n",
            "principled approach for minimizing a general loss function $f(x)$ while\n",
            "enforcing a bound constraint $\\|x\\|_\\infty \\leq 1/\\lambda$. Lion achieves this\n",
            "through the incorporation of decoupled weight decay, where $\\lambda$ represents\n",
            "the weight decay coefficient. Our analysis is made possible by the development\n",
            "of a new Lyapunov function for the Lion updates. It applies to a broader family\n",
            "of Lion-$\\kappa$ algorithms, where the $\\text{sign}(\\cdot)$ operator in Lion is\n",
            "replaced by the subgradient of a convex function $\\kappa$, leading to the\n",
            "solution of a general composite optimization problem of $\\min_x f(x) +\n",
            "\\kappa^*(x)$. Our findings provide valuable insights into the dynamics of Lion\n",
            "and pave the way for further improvements and extensions of Lion-related\n",
            "algorithms.\n",
            "\n",
            "1004. Title: Neural Fourier Transform: A General Approach to Equivariant Representation Learning\n",
            "   Abstract: In standard adversarial training, models are optimized to fit one-hot labels\n",
            "within allowable adversarial perturbation budgets. However, the ignorance of\n",
            "underlying distribution shifts brought by perturbations causes the problem of\n",
            "robust overfitting. To address this issue and enhance adversarial robustness,\n",
            "we analyze the characteristics of robust models and identify that robust models\n",
            "tend to produce smoother and well-calibrated outputs. Based on the observation,\n",
            "we propose a simple yet effective method, Annealing Self-Distillation\n",
            "Rectification (ADR), which generates soft labels as a better guidance mechanism\n",
            "that accurately reflects the distribution shift under attack during adversarial\n",
            "training. By utilizing ADR, we can obtain rectified distributions that\n",
            "significantly improve model robustness without the need for pre-trained models\n",
            "or extensive extra computation. Moreover, our method facilitates seamless\n",
            "plug-and-play integration with other adversarial training techniques by\n",
            "replacing the hard labels in their objectives. We demonstrate the efficacy of\n",
            "ADR through extensive experiments and strong performances across datasets.\n",
            "\n",
            "1005. Title: SaNN: Simple Yet Powerful Simplicial-aware Neural Networks\n",
            "   Abstract: We propose a new class of generative diffusion models, called functional\n",
            "diffusion. In contrast to previous work, functional diffusion works on samples\n",
            "that are represented by functions with a continuous domain. Functional\n",
            "diffusion can be seen as an extension of classical diffusion models to an\n",
            "infinite-dimensional domain. Functional diffusion is very versatile as images,\n",
            "videos, audio, 3D shapes, deformations, \\etc, can be handled by the same\n",
            "framework with minimal changes. In addition, functional diffusion is especially\n",
            "suited for irregular data or data defined in non-standard domains. In our work,\n",
            "we derive the necessary foundations for functional diffusion and propose a\n",
            "first implementation based on the transformer architecture. We show generative\n",
            "results on complicated signed distance functions and deformation functions\n",
            "defined on 3D surfaces.\n",
            "\n",
            "1006. Title: Generative Modeling of Regular and Irregular Time Series Data via Koopman VAEs\n",
            "   Abstract: Finding the name of an unknown symbol is often hard, but writing the symbol\n",
            "is easy. This bachelor's thesis presents multiple systems that use the pen\n",
            "trajectory to classify handwritten symbols. Five preprocessing steps, one data\n",
            "augmentation algorithm, five features and five variants for multilayer\n",
            "Perceptron training were evaluated using 166898 recordings which were collected\n",
            "with two crowdsourcing projects. The evaluation results of these 21 experiments\n",
            "were used to create an optimized recognizer which has a TOP1 error of less than\n",
            "17.5% and a TOP3 error of 4.0%. This is an improvement of 18.5% for the TOP1\n",
            "error and 29.7% for the TOP3 error.\n",
            "\n",
            "1007. Title: G$^2$N$^2$ : Weisfeiler and Lehman go grammatical\n",
            "   Abstract: Learning neural subset selection tasks, such as compound selection in\n",
            "AI-aided drug discovery, have become increasingly pivotal across diverse\n",
            "applications. The existing methodologies in the field primarily concentrate on\n",
            "constructing models that capture the relationship between utility function\n",
            "values and subsets within their respective supersets. However, these approaches\n",
            "tend to overlook the valuable information contained within the superset when\n",
            "utilizing neural networks to model set functions. In this work, we address this\n",
            "oversight by adopting a probabilistic perspective. Our theoretical findings\n",
            "demonstrate that when the target value is conditioned on both the input set and\n",
            "subset, it is essential to incorporate an \\textit{invariant sufficient\n",
            "statistic} of the superset into the subset of interest for effective learning.\n",
            "This ensures that the output value remains invariant to permutations of the\n",
            "subset and its corresponding superset, enabling identification of the specific\n",
            "superset from which the subset originated. Motivated by these insights, we\n",
            "propose a simple yet effective information aggregation module designed to merge\n",
            "the representations of subsets and supersets from a permutation invariance\n",
            "perspective. Comprehensive empirical evaluations across diverse tasks and\n",
            "datasets validate the enhanced efficacy of our approach over conventional\n",
            "methods, underscoring the practicality and potency of our proposed strategies\n",
            "in real-world contexts.\n",
            "\n",
            "1008. Title: Enhancing Neural Subset Selection: Integrating Background Information into Set Representations\n",
            "   Abstract: In this paper a study of $G$-minimality, i.e., minimality of four-manifolds\n",
            "equipped with an action of a finite group $G$, is initiated. We focus on cyclic\n",
            "actions on $CP^2\\# \\overline{CP^2}$, and our work shows that even in this\n",
            "simple setting, the comparison of $G$-minimality in the various categories,\n",
            "i.e., locally linear, smooth, and symplectic, is already delicate and\n",
            "interesting. For example, we show that if a symplectic $Z_n$-action on $CP^2\\#\n",
            "\\overline{CP^2}$ has an invariant locally linear topological $(-1)$-sphere,\n",
            "then it must admit an invariant symplectic $(-1)$-sphere, provided that $n=2$\n",
            "or $n$ is odd. For the case where $n>2$ and even, the same conclusion holds\n",
            "under a stronger assumption, i.e., the invariant $(-1)$-sphere is smoothly\n",
            "embedded. Along the way of these proofs we develop certain techniques for\n",
            "producing embedded invariant $J$-holomorphic two-spheres of self-intersection\n",
            "$-r$ under a weaker assumption of an invariant smooth $(-r)$-sphere for $r$\n",
            "relatively small compared with the group order $n$. We then apply the\n",
            "techniques to give a classification of $G$-Hirzebruch surfaces (i.e.,\n",
            "Hirzebruch surfaces equipped with a homologically trivial, holomorphic\n",
            "$G=Z_n$-action) up to orientation-preserving equivariant diffeomorphisms. The\n",
            "main issue of the classification is to distinguish non-diffeomorphic\n",
            "$G$-Hirzebruch surfaces which have the same fixed-point set structure. An\n",
            "interesting discovery is that these non-diffeomorphic $G$-Hirzebruch surfaces\n",
            "have distinct equivariant Gromov-Taubes invariant, giving the first examples of\n",
            "such kind. Going back to the original question of $G$-minimality, we show that\n",
            "for $G=Z_n$, a minimal rational $G$-surface is minimal as a symplectic\n",
            "$G$-manifold if and only if it is minimal as a smooth $G$-manifold.\n",
            "\n",
            "1009. Title: Multimodal Web Navigation with Instruction-Finetuned Foundation Models\n",
            "   Abstract: Large language models are powerful systems that excel at many tasks, ranging\n",
            "from translation to mathematical reasoning. Yet, at the same time, these models\n",
            "often show unhuman-like characteristics. In the present paper, we address this\n",
            "gap and ask whether large language models can be turned into cognitive models.\n",
            "We find that -- after finetuning them on data from psychological experiments --\n",
            "these models offer accurate representations of human behavior, even\n",
            "outperforming traditional cognitive models in two decision-making domains. In\n",
            "addition, we show that their representations contain the information necessary\n",
            "to model behavior on the level of individual subjects. Finally, we demonstrate\n",
            "that finetuning on multiple tasks enables large language models to predict\n",
            "human behavior in a previously unseen task. Taken together, these results\n",
            "suggest that large, pre-trained models can be adapted to become generalist\n",
            "cognitive models, thereby opening up new research directions that could\n",
            "transform cognitive psychology and the behavioral sciences as a whole.\n",
            "\n",
            "1010. Title: SEPT: Towards Efficient Scene Representation Learning for Motion Prediction\n",
            "   Abstract: The progress of autonomous web navigation has been hindered by the dependence\n",
            "on billions of exploratory interactions via online reinforcement learning, and\n",
            "domain-specific model designs that make it difficult to leverage generalization\n",
            "from rich out-of-domain data. In this work, we study data-driven offline\n",
            "training for web agents with vision-language foundation models. We propose an\n",
            "instruction-following multimodal agent, WebGUM, that observes both webpage\n",
            "screenshots and HTML pages and outputs web navigation actions, such as click\n",
            "and type. WebGUM is trained by jointly finetuning an instruction-finetuned\n",
            "language model and a vision encoder with temporal and local perception on a\n",
            "large corpus of demonstrations. We empirically demonstrate this recipe improves\n",
            "the agent's ability of grounded multimodal perception, HTML comprehension, and\n",
            "multi-step reasoning, outperforming prior works by a significant margin. On the\n",
            "MiniWoB, we improve over the previous best offline methods by more than 45.8%,\n",
            "even outperforming online-finetuned SoTA, humans, and GPT-4-based agent. On the\n",
            "WebShop benchmark, our 3-billion-parameter model achieves superior performance\n",
            "to the existing SoTA, PaLM-540B. Furthermore, WebGUM exhibits strong positive\n",
            "transfer to the real-world planning tasks on the Mind2Web. We also collect 347K\n",
            "high-quality demonstrations using our trained models, 38 times larger than\n",
            "prior work, and make them available to promote future research in this\n",
            "direction.\n",
            "\n",
            "1011. Title: Turning large language models into cognitive models\n",
            "   Abstract: Convolutional Neural Networks (CNN) have recently seen tremendous success in\n",
            "various computer vision tasks. However, their application to problems with high\n",
            "dimensional input and output, such as high-resolution image and video\n",
            "segmentation or 3D medical imaging, has been limited by various factors.\n",
            "Primarily, in the training stage, it is necessary to store network activations\n",
            "for back propagation. In these settings, the memory requirements associated\n",
            "with storing activations can exceed what is feasible with current hardware,\n",
            "especially for problems in 3D. Motivated by the propagation of signals over\n",
            "physical networks, that are governed by the hyperbolic Telegraph equation, in\n",
            "this work we introduce a fully conservative hyperbolic network for problems\n",
            "with high dimensional input and output. We introduce a coarsening operation\n",
            "that allows completely reversible CNNs by using a learnable Discrete Wavelet\n",
            "Transform and its inverse to both coarsen and interpolate the network state and\n",
            "change the number of channels. We show that fully reversible networks are able\n",
            "to achieve results comparable to the state of the art in 4D time-lapse hyper\n",
            "spectral image segmentation and full 3D video segmentation, with a much lower\n",
            "memory footprint that is a constant independent of the network depth. We also\n",
            "extend the use of such networks to Variational Auto Encoders with high\n",
            "resolution input and output.\n",
            "\n",
            "1012. Title: SAN: Inducing Metrizability of GAN with Discriminative Normalized Linear Layer\n",
            "   Abstract: Equivariant neural networks are neural networks with symmetry. Motivated by\n",
            "the theory of group representations, we decompose the layers of an equivariant\n",
            "neural network into simple representations. The nonlinear activation functions\n",
            "lead to interesting nonlinear equivariant maps between simple representations.\n",
            "For example, the rectified linear unit (ReLU) gives rise to piecewise linear\n",
            "maps. We show that these considerations lead to a filtration of equivariant\n",
            "neural networks, generalizing Fourier series. This observation might provide a\n",
            "useful tool for interpreting equivariant neural networks.\n",
            "\n",
            "1013. Title: Understanding In-Context Learning in Transformers and LLMs by Learning to Learn Discrete Functions\n",
            "   Abstract: The emergence of deep and large-scale spiking neural networks (SNNs)\n",
            "exhibiting high performance across diverse complex datasets has led to a need\n",
            "for compressing network models due to the presence of a significant number of\n",
            "redundant structural units, aiming to more effectively leverage their low-power\n",
            "consumption and biological interpretability advantages. Currently, most model\n",
            "compression techniques for SNNs are based on unstructured pruning of individual\n",
            "connections, which requires specific hardware support. Hence, we propose a\n",
            "structured pruning approach based on the activity levels of convolutional\n",
            "kernels named Spiking Channel Activity-based (SCA) network pruning framework.\n",
            "Inspired by synaptic plasticity mechanisms, our method dynamically adjusts the\n",
            "network's structure by pruning and regenerating convolutional kernels during\n",
            "training, enhancing the model's adaptation to the current target task. While\n",
            "maintaining model performance, this approach refines the network architecture,\n",
            "ultimately reducing computational load and accelerating the inference process.\n",
            "This indicates that structured dynamic sparse learning methods can better\n",
            "facilitate the application of deep SNNs in low-power and high-efficiency\n",
            "scenarios.\n",
            "\n",
            "1014. Title: Fully Hyperbolic Convolutional Neural Networks for Computer Vision\n",
            "   Abstract: Transformer based large-language models (LLMs) display extreme proficiency\n",
            "with language yet a precise understanding of how they work remains elusive. One\n",
            "way of demystifying transformer predictions would be to describe how they\n",
            "depend on their context in terms of simple template functions. This paper takes\n",
            "a first step in this direction by considering families of functions (i.e.\n",
            "rules) formed out of simple N-gram based statistics of the training data. By\n",
            "studying how well these rulesets approximate transformer predictions, we obtain\n",
            "a variety of novel discoveries: a simple method to detect overfitting during\n",
            "training without using a holdout set, a quantitative measure of how\n",
            "transformers progress from learning simple to more complex statistical rules\n",
            "over the course of training, a model-variance criterion governing when\n",
            "transformer predictions tend to be described by N-gram rules, and insights into\n",
            "how well transformers can be approximated by N-gram rulesets in the limit where\n",
            "these rulesets become increasingly complex. In this latter direction, we find\n",
            "that for 79% and 68% of LLM next-token distributions on TinyStories and\n",
            "Wikipedia, respectively, their top-1 predictions agree with those provided by\n",
            "our N-gram rulesets.\n",
            "\n",
            "1015. Title: Consciousness-Inspired Spatio-Temporal Abstractions for Better Generalization in Reinforcement Learning\n",
            "   Abstract: Generating realistic time series data is important for many engineering and\n",
            "scientific applications. Existing work tackles this problem using generative\n",
            "adversarial networks (GANs). However, GANs are unstable during training, and\n",
            "they can suffer from mode collapse. While variational autoencoders (VAEs) are\n",
            "known to be more robust to the these issues, they are (surprisingly) less\n",
            "considered for time series generation. In this work, we introduce Koopman VAE\n",
            "(KoVAE), a new generative framework that is based on a novel design for the\n",
            "model prior, and that can be optimized for either regular and irregular\n",
            "training data. Inspired by Koopman theory, we represent the latent conditional\n",
            "prior dynamics using a linear map. Our approach enhances generative modeling\n",
            "with two desired features: (i) incorporating domain knowledge can be achieved\n",
            "by leveraging spectral tools that prescribe constraints on the eigenvalues of\n",
            "the linear map; and (ii) studying the qualitative behavior and stability of the\n",
            "system can be performed using tools from dynamical systems theory. Our results\n",
            "show that KoVAE outperforms state-of-the-art GAN and VAE methods across several\n",
            "challenging synthetic and real-world time series generation benchmarks. Whether\n",
            "trained on regular or irregular data, KoVAE generates time series that improve\n",
            "both discriminative and predictive metrics. We also present visual evidence\n",
            "suggesting that KoVAE learns probability density functions that better\n",
            "approximate the empirical ground truth distribution.\n",
            "\n",
            "1016. Title: Maximum Likelihood Estimation is All You Need for Well-Specified Covariate Shift\n",
            "   Abstract: Diffusion models have made tremendous progress in text-driven image and video\n",
            "generation. Now text-to-image foundation models are widely applied to various\n",
            "downstream image synthesis tasks, such as controllable image generation and\n",
            "image editing, while downstream video synthesis tasks are less explored for\n",
            "several reasons. First, it requires huge memory and computation overhead to\n",
            "train a video generation foundation model. Even with video foundation models,\n",
            "additional costly training is still required for downstream video synthesis\n",
            "tasks. Second, although some works extend image diffusion models into videos in\n",
            "a training-free manner, temporal consistency cannot be well preserved. Finally,\n",
            "these adaption methods are specifically designed for one task and fail to\n",
            "generalize to different tasks. To mitigate these issues, we propose a\n",
            "training-free general-purpose video synthesis framework, coined as {\\bf\n",
            "BIVDiff}, via bridging specific image diffusion models and general\n",
            "text-to-video foundation diffusion models. Specifically, we first use a\n",
            "specific image diffusion model (e.g., ControlNet and Instruct Pix2Pix) for\n",
            "frame-wise video generation, then perform Mixed Inversion on the generated\n",
            "video, and finally input the inverted latents into the video diffusion models\n",
            "(e.g., VidRD and ZeroScope) for temporal smoothing. This decoupled framework\n",
            "enables flexible image model selection for different purposes with strong task\n",
            "generalization and high efficiency. To validate the effectiveness and general\n",
            "use of BIVDiff, we perform a wide range of video synthesis tasks, including\n",
            "controllable video generation, video editing, video inpainting, and\n",
            "outpainting.\n",
            "\n",
            "1017. Title: Sample-Efficiency in Multi-Batch Reinforcement Learning: The Need for Dimension-Dependent Adaptivity\n",
            "   Abstract: A key challenge of modern machine learning systems is to achieve\n",
            "Out-of-Distribution (OOD) generalization -- generalizing to target data whose\n",
            "distribution differs from that of source data. Despite its significant\n",
            "importance, the fundamental question of ``what are the most effective\n",
            "algorithms for OOD generalization'' remains open even under the standard\n",
            "setting of covariate shift. This paper addresses this fundamental question by\n",
            "proving that, surprisingly, classical Maximum Likelihood Estimation (MLE)\n",
            "purely using source data (without any modification) achieves the minimax\n",
            "optimality for covariate shift under the well-specified setting. That is, no\n",
            "algorithm performs better than MLE in this setting (up to a constant factor),\n",
            "justifying MLE is all you need. Our result holds for a very rich class of\n",
            "parametric models, and does not require any boundedness condition on the\n",
            "density ratio. We illustrate the wide applicability of our framework by\n",
            "instantiating it to three concrete examples -- linear regression, logistic\n",
            "regression, and phase retrieval. This paper further complement the study by\n",
            "proving that, under the misspecified setting, MLE is no longer the optimal\n",
            "choice, whereas Maximum Weighted Likelihood Estimator (MWLE) emerges as minimax\n",
            "optimal in certain scenarios.\n",
            "\n",
            "1018. Title: Convolution Meets LoRA: Parameter Efficient Finetuning for Segment Anything Model\n",
            "   Abstract: State abstraction enables sample-efficient learning and better task transfer\n",
            "in complex reinforcement learning environments. Recently, we proposed RePReL\n",
            "(Kokel et al. 2021), a hierarchical framework that leverages a relational\n",
            "planner to provide useful state abstractions for learning. We present a brief\n",
            "overview of this framework and the use of a dynamic probabilistic logic model\n",
            "to design these state abstractions. Our experiments show that RePReL not only\n",
            "achieves better performance and efficient learning on the task at hand but also\n",
            "demonstrates better generalization to unseen tasks.\n",
            "\n",
            "1019. Title: Cycle Consistency Driven Object Discovery\n",
            "   Abstract: Often the challenge associated with tasks like fraud and spam detection[1] is\n",
            "the lack of all likely patterns needed to train suitable supervised learning\n",
            "models. In order to overcome this limitation, such tasks are attempted as\n",
            "outlier or anomaly detection tasks. We also hypothesize that out- liers have\n",
            "behavioral patterns that change over time. Limited data and continuously\n",
            "changing patterns makes learning significantly difficult. In this work we are\n",
            "proposing an approach that detects outliers in large data sets by relying on\n",
            "data points that are consistent. The primary contribution of this work is that\n",
            "it will quickly help retrieve samples for both consistent and non-outlier data\n",
            "sets and is also mindful of new outlier patterns. No prior knowledge of each\n",
            "set is required to extract the samples. The method consists of two phases, in\n",
            "the first phase, consistent data points (non- outliers) are retrieved by an\n",
            "ensemble method of unsupervised clustering techniques and in the second phase a\n",
            "one class classifier trained on the consistent data point set is ap- plied on\n",
            "the remaining sample set to identify the outliers. The approach is tested on\n",
            "three publicly available data sets and the performance scores are competitive.\n",
            "\n",
            "1020. Title: LiDAR: Sensing Linear Probing Performance in Joint Embedding SSL Architectures\n",
            "   Abstract: The Segment Anything Model (SAM) stands as a foundational framework for image\n",
            "segmentation. While it exhibits remarkable zero-shot generalization in typical\n",
            "scenarios, its advantage diminishes when applied to specialized domains like\n",
            "medical imagery and remote sensing. To address this limitation, this paper\n",
            "introduces Conv-LoRA, a simple yet effective parameter-efficient fine-tuning\n",
            "approach. By integrating ultra-lightweight convolutional parameters into\n",
            "Low-Rank Adaptation (LoRA), Conv-LoRA can inject image-related inductive biases\n",
            "into the plain ViT encoder, further reinforcing SAM's local prior assumption.\n",
            "Notably, Conv-LoRA not only preserves SAM's extensive segmentation knowledge\n",
            "but also revives its capacity of learning high-level image semantics, which is\n",
            "constrained by SAM's foreground-background segmentation pretraining.\n",
            "Comprehensive experimentation across diverse benchmarks spanning multiple\n",
            "domains underscores Conv-LoRA's superiority in adapting SAM to real-world\n",
            "semantic segmentation tasks.\n",
            "\n",
            "1021. Title: UniAdapter: Unified Parameter-Efficient Transfer Learning for Cross-modal Modeling\n",
            "   Abstract: Developing deep learning models that effectively learn object-centric\n",
            "representations, akin to human cognition, remains a challenging task. Existing\n",
            "approaches facilitate object discovery by representing objects as fixed-size\n",
            "vectors, called ``slots'' or ``object files''. While these approaches have\n",
            "shown promise in certain scenarios, they still exhibit certain limitations.\n",
            "First, they rely on architectural priors which can be unreliable and usually\n",
            "require meticulous engineering to identify the correct objects. Second, there\n",
            "has been a notable gap in investigating the practical utility of these\n",
            "representations in downstream tasks. To address the first limitation, we\n",
            "introduce a method that explicitly optimizes the constraint that each object in\n",
            "a scene should be associated with a distinct slot. We formalize this constraint\n",
            "by introducing consistency objectives which are cyclic in nature. By\n",
            "integrating these consistency objectives into various existing slot-based\n",
            "object-centric methods, we showcase substantial improvements in\n",
            "object-discovery performance. These enhancements consistently hold true across\n",
            "both synthetic and real-world scenes, underscoring the effectiveness and\n",
            "adaptability of the proposed approach. To tackle the second limitation, we\n",
            "apply the learned object-centric representations from the proposed method to\n",
            "two downstream reinforcement learning tasks, demonstrating considerable\n",
            "performance enhancements compared to conventional slot-based and monolithic\n",
            "representation learning methods. Our results suggest that the proposed approach\n",
            "not only improves object discovery, but also provides richer features for\n",
            "downstream tasks.\n",
            "\n",
            "1022. Title: DIAGNOSIS: Detecting Unauthorized Data Usages in Text-to-image Diffusion Models\n",
            "   Abstract: Warehouse Management Systems have been evolving and improving thanks to new\n",
            "Data Intelligence techniques. However, many current optimizations have been\n",
            "applied to specific cases or are in great need of manual interaction. Here is\n",
            "where Reinforcement Learning techniques come into play, providing\n",
            "automatization and adaptability to current optimization policies. In this\n",
            "paper, we present Storehouse, a customizable environment that generalizes the\n",
            "definition of warehouse simulations for Reinforcement Learning. We also\n",
            "validate this environment against state-of-the-art reinforcement learning\n",
            "algorithms and compare these results to human and random policies.\n",
            "\n",
            "1023. Title: Pre-training LiDAR-based 3D Object Detectors through Colorization\n",
            "   Abstract: Most existing methods in vision language pre-training rely on object-centric\n",
            "features extracted through object detection and make fine-grained alignments\n",
            "between the extracted features and texts. It is challenging for these methods\n",
            "to learn relations among multiple objects. To this end, we propose a new method\n",
            "called X-VLM to perform `multi-grained vision language pre-training.' The key\n",
            "to learning multi-grained alignments is to locate visual concepts in the image\n",
            "given the associated texts, and in the meantime align the texts with the visual\n",
            "concepts, where the alignments are in multi-granularity. Experimental results\n",
            "show that X-VLM effectively leverages the learned multi-grained alignments to\n",
            "many downstream vision language tasks and consistently outperforms\n",
            "state-of-the-art methods.\n",
            "\n",
            "1024. Title: Near-Optimal Solutions of Constrained Learning Problems\n",
            "   Abstract: Hypernetworks, neural networks that predict the parameters of another neural\n",
            "network, are powerful models that have been successfully used in diverse\n",
            "applications from image generation to multi-task learning. Unfortunately,\n",
            "existing hypernetworks are often challenging to train. Training typically\n",
            "converges far more slowly than for non-hypernetwork models, and the rate of\n",
            "convergence can be very sensitive to hyperparameter choices. In this work, we\n",
            "identify a fundamental and previously unidentified problem that contributes to\n",
            "the challenge of training hypernetworks: a magnitude proportionality between\n",
            "the inputs and outputs of the hypernetwork. We demonstrate both analytically\n",
            "and empirically that this can lead to unstable optimization, thereby slowing\n",
            "down convergence, and sometimes even preventing any learning. We present a\n",
            "simple solution to this problem using a revised hypernetwork formulation that\n",
            "we call Magnitude Invariant Parametrizations (MIP). We demonstrate the proposed\n",
            "solution on several hypernetwork tasks, where it consistently stabilizes\n",
            "training and achieves faster convergence. Furthermore, we perform a\n",
            "comprehensive ablation study including choices of activation function,\n",
            "normalization strategies, input dimensionality, and hypernetwork architecture;\n",
            "and find that MIP improves training in all scenarios. We provide easy-to-use\n",
            "code that can turn existing networks into MIP-based hypernetworks.\n",
            "\n",
            "1025. Title: CoLiDE: Concomitant Linear DAG Estimation\n",
            "   Abstract: Dimension interpolation is a novel programme of research which attempts to\n",
            "unify the study of fractal dimension by considering various spectra which live\n",
            "in between well-studied notions of dimension such as Hausdorff, box, Assouad\n",
            "and Fourier dimension. These spectra often reveal novel features not witnessed\n",
            "by the individual notions and this information has applications in many\n",
            "directions. In this survey article, we discuss dimension interpolation broadly\n",
            "and then focus on applications to the dimension theory of orthogonal\n",
            "projections. We focus on three distinct applications coming from three\n",
            "different dimension spectra, namely, the Fourier spectrum, the intermediate\n",
            "dimensions, and the Assouad spectrum. The celebrated Marstrand--Mattila\n",
            "projection theorem gives the Hausdorff dimension of the orthogonal projection\n",
            "of a Borel set in Euclidean space for almost all orthogonal projections. This\n",
            "result has inspired much further research on the dimension theory of\n",
            "projections including the consideration of dimensions other than the Hausdorff\n",
            "dimension, and the study of the exceptional set in the Marstrand--Mattila\n",
            "theorem.\n",
            "\n",
            "1026. Title: Magnitude Invariant Parametrizations Improve Hypernetwork Learning\n",
            "   Abstract: As learning solutions reach critical applications in social, industrial, and\n",
            "medical domains, the need to curtail their behavior has become paramount. There\n",
            "is now ample evidence that without explicit tailoring, learning can lead to\n",
            "biased, unsafe, and prejudiced solutions. To tackle these problems, we develop\n",
            "a generalization theory of constrained learning based on the probably\n",
            "approximately correct (PAC) learning framework. In particular, we show that\n",
            "imposing requirements does not make a learning problem harder in the sense that\n",
            "any PAC learnable class is also PAC constrained learnable using a constrained\n",
            "counterpart of the empirical risk minimization (ERM) rule. For typical\n",
            "parametrized models, however, this learner involves solving a constrained\n",
            "non-convex optimization program for which even obtaining a feasible solution is\n",
            "challenging. To overcome this issue, we prove that under mild conditions the\n",
            "empirical dual problem of constrained learning is also a PAC constrained\n",
            "learner that now leads to a practical constrained learning algorithm based\n",
            "solely on solving unconstrained problems. We analyze the generalization\n",
            "properties of this solution and use it to illustrate how constrained learning\n",
            "can address problems in fair and robust classification.\n",
            "\n",
            "1027. Title: Convergence of Bayesian Bilevel Optimization\n",
            "   Abstract: Novelty Detection (ND) plays a crucial role in machine learning by\n",
            "identifying new or unseen data during model inference. This capability is\n",
            "especially important for the safe and reliable operation of automated systems.\n",
            "Despite advances in this field, existing techniques often fail to maintain\n",
            "their performance when subject to adversarial attacks. Our research addresses\n",
            "this gap by marrying the merits of nearest-neighbor algorithms with robust\n",
            "features obtained from models pretrained on ImageNet. We focus on enhancing the\n",
            "robustness and performance of ND algorithms. Experimental results demonstrate\n",
            "that our approach significantly outperforms current state-of-the-art methods\n",
            "across various benchmarks, particularly under adversarial conditions. By\n",
            "incorporating robust pretrained features into the k-NN algorithm, we establish\n",
            "a new standard for performance and robustness in the field of robust ND. This\n",
            "work opens up new avenues for research aimed at fortifying machine learning\n",
            "systems against adversarial vulnerabilities. Our implementation is publicly\n",
            "available at https://github.com/rohban-lab/ZARND.\n",
            "\n",
            "1028. Title: Unveiling the Pitfalls of Knowledge Editing for Large Language Models\n",
            "   Abstract: Hyperparameter optimization in machine learning is often achieved using naive\n",
            "techniques that only lead to an approximate set of hyperparameters. Although\n",
            "techniques such as Bayesian optimization perform an intelligent search on a\n",
            "given domain of hyperparameters, it does not guarantee an optimal solution. A\n",
            "major drawback of most of these approaches is an exponential increase of their\n",
            "search domain with number of hyperparameters, increasing the computational cost\n",
            "and making the approaches slow. The hyperparameter optimization problem is\n",
            "inherently a bilevel optimization task, and some studies have attempted bilevel\n",
            "solution methodologies for solving this problem. However, these studies assume\n",
            "a unique set of model weights that minimize the training loss, which is\n",
            "generally violated by deep learning architectures. This paper discusses a\n",
            "gradient-based bilevel method addressing these drawbacks for solving the\n",
            "hyperparameter optimization problem. The proposed method can handle continuous\n",
            "hyperparameters for which we have chosen the regularization hyperparameter in\n",
            "our experiments. The method guarantees convergence to the set of optimal\n",
            "hyperparameters that this study has theoretically proven. The idea is based on\n",
            "approximating the lower-level optimal value function using Gaussian process\n",
            "regression. As a result, the bilevel problem is reduced to a single level\n",
            "constrained optimization task that is solved using the augmented Lagrangian\n",
            "method. We have performed an extensive computational study on the MNIST and\n",
            "CIFAR-10 datasets on multi-layer perceptron and LeNet architectures that\n",
            "confirms the efficiency of the proposed method. A comparative study against\n",
            "grid search, random search, Bayesian optimization, and HyberBand method on\n",
            "various hyperparameter problems shows that the proposed algorithm converges\n",
            "with lower computation and leads to models that generalize better on the\n",
            "testing set.\n",
            "\n",
            "1029. Title: An interpretable error correction method for enhancing code-to-code translation\n",
            "   Abstract: As the cost associated with fine-tuning Large Language Models (LLMs)\n",
            "continues to rise, recent research efforts have pivoted towards developing\n",
            "methodologies to edit implicit knowledge embedded within LLMs. Yet, there's\n",
            "still a dark cloud lingering overhead -- will knowledge editing trigger\n",
            "butterfly effect? since it is still unclear whether knowledge editing might\n",
            "introduce side effects that pose potential risks or not. This paper pioneers\n",
            "the investigation into the potential pitfalls associated with knowledge editing\n",
            "for LLMs. To achieve this, we introduce new benchmark datasets and propose\n",
            "innovative evaluation metrics. Our results underline two pivotal concerns: (1)\n",
            "Knowledge Conflict: Editing groups of facts that logically clash can magnify\n",
            "the inherent inconsistencies in LLMs-a facet neglected by previous methods. (2)\n",
            "Knowledge Distortion: Altering parameters with the aim of editing factual\n",
            "knowledge can irrevocably warp the innate knowledge structure of LLMs.\n",
            "Experimental results vividly demonstrate that knowledge editing might\n",
            "inadvertently cast a shadow of unintended consequences on LLMs, which warrant\n",
            "attention and efforts for future works. Code and data are available at\n",
            "https://github.com/zjunlp/PitfallsKnowledgeEditing.\n",
            "\n",
            "1030. Title: Consistent Multi-Class Classification from Multiple Unlabeled Datasets\n",
            "   Abstract: Self-supervised learning has been widely used to obtain transferrable\n",
            "representations from unlabeled images. Especially, recent contrastive learning\n",
            "methods have shown impressive performances on downstream image classification\n",
            "tasks. While these contrastive methods mainly focus on generating invariant\n",
            "global representations at the image-level under semantic-preserving\n",
            "transformations, they are prone to overlook spatial consistency of local\n",
            "representations and therefore have a limitation in pretraining for localization\n",
            "tasks such as object detection and instance segmentation. Moreover,\n",
            "aggressively cropped views used in existing contrastive methods can minimize\n",
            "representation distances between the semantically different regions of a single\n",
            "image.\n",
            "  In this paper, we propose a spatially consistent representation learning\n",
            "algorithm (SCRL) for multi-object and location-specific tasks. In particular,\n",
            "we devise a novel self-supervised objective that tries to produce coherent\n",
            "spatial representations of a randomly cropped local region according to\n",
            "geometric translations and zooming operations. On various downstream\n",
            "localization tasks with benchmark datasets, the proposed SCRL shows significant\n",
            "performance improvements over the image-level supervised pretraining as well as\n",
            "the state-of-the-art self-supervised learning methods.\n",
            "  Code is available at https://github.com/kakaobrain/scrl\n",
            "\n",
            "1031. Title: A Paradigm Shift in Machine Translation: Boosting Translation Performance of Large Language Models\n",
            "   Abstract: Code translation converts code from one programming language to another while\n",
            "maintaining its original functionality, which is crucial for software\n",
            "migration, system refactoring, and cross-platform development. Traditional\n",
            "rule-based methods rely on manually-written rules, which can be time-consuming\n",
            "and often result in less readable code. To overcome this, learning-based\n",
            "methods have been developed, leveraging parallel data to train models for\n",
            "automated code translation. More recently, the advance of Large Language Models\n",
            "(LLMs) further boosts learning-based code translation. Although promising,\n",
            "LLM-translated program still suffers from diverse quality issues (e.g., syntax\n",
            "errors and semantic errors). In particular, it can be challenging for LLMs to\n",
            "self-debug these errors when simply provided with the corresponding error\n",
            "messages.\n",
            "  In this work, we propose a novel LLM-based multi-agent system TRANSAGENT,\n",
            "which enhances LLM-based code translation by fixing the syntax errors and\n",
            "semantic errors with the synergy between four LLM-based agents, including\n",
            "Initial Code Translator, Syntax Error Fixer, Code Aligner, and Semantic Error\n",
            "Fixer. The main insight of TRANSAGENT is to first localize the error code block\n",
            "in the target program based on the execution alignment between the target and\n",
            "source program, which can narrow down the fixing space and thus lower down the\n",
            "fixing difficulties. To evaluate TRANSAGENT, we first construct a new benchmark\n",
            "from recent programming tasks to mitigate the potential data leakage issue. On\n",
            "our benchmark, TRANSAGENT outperforms the latest LLM-based code translation\n",
            "technique UniTrans in both translation effectiveness and efficiency;\n",
            "additionally, our evaluation on different LLMs show the generalization of\n",
            "TRANSAGENT and our ablation study shows the contribution of each agent.\n",
            "\n",
            "1032. Title: AuG-KD: Anchor-Based Mixup Generation for Out-of-Domain Knowledge Distillation\n",
            "   Abstract: This paper introduces a Transformer-based integrative feature and cost\n",
            "aggregation network designed for dense matching tasks. In the context of dense\n",
            "matching, many works benefit from one of two forms of aggregation: feature\n",
            "aggregation, which pertains to the alignment of similar features, or cost\n",
            "aggregation, a procedure aimed at instilling coherence in the flow estimates\n",
            "across neighboring pixels. In this work, we first show that feature aggregation\n",
            "and cost aggregation exhibit distinct characteristics and reveal the potential\n",
            "for substantial benefits stemming from the judicious use of both aggregation\n",
            "processes. We then introduce a simple yet effective architecture that harnesses\n",
            "self- and cross-attention mechanisms to show that our approach unifies feature\n",
            "aggregation and cost aggregation and effectively harnesses the strengths of\n",
            "both techniques. Within the proposed attention layers, the features and cost\n",
            "volume both complement each other, and the attention layers are interleaved\n",
            "through a coarse-to-fine design to further promote accurate correspondence\n",
            "estimation. Finally at inference, our network produces multi-scale predictions,\n",
            "computes their confidence scores, and selects the most confident flow for final\n",
            "prediction. Our framework is evaluated on standard benchmarks for semantic\n",
            "matching, and also applied to geometric matching, where we show that our\n",
            "approach achieves significant improvements compared to existing methods.\n",
            "\n",
            "1033. Title: Federated Q-Learning: Linear Regret Speedup with Low Communication Cost\n",
            "   Abstract: In this paper, we consider federated reinforcement learning for tabular\n",
            "episodic Markov Decision Processes (MDP) where, under the coordination of a\n",
            "central server, multiple agents collaboratively explore the environment and\n",
            "learn an optimal policy without sharing their raw data. While linear speedup in\n",
            "the number of agents has been achieved for some metrics, such as convergence\n",
            "rate and sample complexity, in similar settings, it is unclear whether it is\n",
            "possible to design a model-free algorithm to achieve linear regret speedup with\n",
            "low communication cost. We propose two federated Q-Learning algorithms termed\n",
            "as FedQ-Hoeffding and FedQ-Bernstein, respectively, and show that the\n",
            "corresponding total regrets achieve a linear speedup compared with their\n",
            "single-agent counterparts when the time horizon is sufficiently large, while\n",
            "the communication cost scales logarithmically in the total number of time steps\n",
            "$T$. Those results rely on an event-triggered synchronization mechanism between\n",
            "the agents and the server, a novel step size selection when the server\n",
            "aggregates the local estimates of the state-action values to form the global\n",
            "estimates, and a set of new concentration inequalities to bound the sum of\n",
            "non-martingale differences. This is the first work showing that linear regret\n",
            "speedup and logarithmic communication cost can be achieved by model-free\n",
            "algorithms in federated reinforcement learning.\n",
            "\n",
            "1034. Title: From Posterior Sampling to Meaningful Diversity in Image Restoration\n",
            "   Abstract: Connectionist Temporal Classification (CTC) is a widely used criterion for\n",
            "training supervised sequence-to-sequence (seq2seq) models. It enables learning\n",
            "the relations between input and output sequences, termed alignments, by\n",
            "marginalizing over perfect alignments (that yield the ground truth), at the\n",
            "expense of imperfect alignments. This binary differentiation of perfect and\n",
            "imperfect alignments falls short of capturing other essential alignment\n",
            "properties that hold significance in other real-world applications. Here we\n",
            "propose $\\textit{Align With Purpose}$, a $\\textbf{general Plug-and-Play\n",
            "framework}$ for enhancing a desired property in models trained with the CTC\n",
            "criterion. We do that by complementing the CTC with an additional loss term\n",
            "that prioritizes alignments according to a desired property. Our method does\n",
            "not require any intervention in the CTC loss function, enables easy\n",
            "optimization of a variety of properties, and allows differentiation between\n",
            "both perfect and imperfect alignments. We apply our framework in the domain of\n",
            "Automatic Speech Recognition (ASR) and show its generality in terms of property\n",
            "selection, architectural choice, and scale of training dataset (up to 280,000\n",
            "hours). To demonstrate the effectiveness of our framework, we apply it to two\n",
            "unrelated properties: emission time and word error rate (WER). For the former,\n",
            "we report an improvement of up to 570ms in latency optimization with a minor\n",
            "reduction in WER, and for the latter, we report a relative improvement of 4.5%\n",
            "WER over the baseline models. To the best of our knowledge, these applications\n",
            "have never been demonstrated to work on a scale of data as large as ours.\n",
            "Notably, our method can be implemented using only a few lines of code, and can\n",
            "be extended to other alignment-free loss functions and to domains other than\n",
            "ASR.\n",
            "\n",
            "1035. Title: REFACTOR: Learning to Extract Theorems from Proofs\n",
            "   Abstract: As is well known, both sampling from the posterior and computing the mean of\n",
            "the posterior in Gaussian process regression reduces to solving a large linear\n",
            "system of equations. We study the use of stochastic gradient descent for\n",
            "solving this linear system, and show that when \\emph{done right} -- by which we\n",
            "mean using specific insights from the optimisation and kernel communities --\n",
            "stochastic gradient descent is highly effective. To that end, we introduce a\n",
            "particularly simple \\emph{stochastic dual descent} algorithm, explain its\n",
            "design in an intuitive manner and illustrate the design choices through a\n",
            "series of ablation studies. Further experiments demonstrate that our new method\n",
            "is highly competitive. In particular, our evaluations on the UCI regression\n",
            "tasks and on Bayesian optimisation set our approach apart from preconditioned\n",
            "conjugate gradients and variational Gaussian process approximations. Moreover,\n",
            "our method places Gaussian process regression on par with state-of-the-art\n",
            "graph neural networks for molecular binding affinity prediction.\n",
            "\n",
            "1036. Title: The Alignment Problem from a Deep Learning Perspective\n",
            "   Abstract: Deep learning has sparked a network of mutual interactions between different\n",
            "disciplines and AI. Naturally, each discipline focuses and interprets the\n",
            "workings of deep learning in different ways. This diversity of perspectives on\n",
            "deep learning, from neuroscience to statistical physics, is a rich source of\n",
            "inspiration that fuels novel developments in the theory and applications of\n",
            "machine learning. In this perspective, we collect and synthesize different\n",
            "intuitions scattered across several communities as for how deep learning works.\n",
            "In particular, we will briefly discuss the different perspectives that\n",
            "disciplines across mathematics, physics, computation, and neuroscience take on\n",
            "how deep learning does its tricks. Our discussion on each perspective is\n",
            "necessarily shallow due to the multiple views that had to be covered. The\n",
            "deepness in this case should come from putting all these faces of deep learning\n",
            "together in the reader's mind, so that one can look at the same problem from\n",
            "different angles.\n",
            "\n",
            "1037. Title: GAIA: a benchmark for General AI Assistants\n",
            "   Abstract: Image restoration problems are typically ill-posed in the sense that each\n",
            "degraded image can be restored in infinitely many valid ways. To accommodate\n",
            "this, many works generate a diverse set of outputs by attempting to randomly\n",
            "sample from the posterior distribution of natural images given the degraded\n",
            "input. Here we argue that this strategy is commonly of limited practical value\n",
            "because of the heavy tail of the posterior distribution. Consider for example\n",
            "inpainting a missing region of the sky in an image. Since there is a high\n",
            "probability that the missing region contains no object but clouds, any set of\n",
            "samples from the posterior would be entirely dominated by (practically\n",
            "identical) completions of sky. However, arguably, presenting users with only\n",
            "one clear sky completion, along with several alternative solutions such as\n",
            "airships, birds, and balloons, would better outline the set of possibilities.\n",
            "In this paper, we initiate the study of meaningfully diverse image restoration.\n",
            "We explore several post-processing approaches that can be combined with any\n",
            "diverse image restoration method to yield semantically meaningful diversity.\n",
            "Moreover, we propose a practical approach for allowing diffusion based image\n",
            "restoration methods to generate meaningfully diverse outputs, while incurring\n",
            "only negligent computational overhead. We conduct extensive user studies to\n",
            "analyze the proposed techniques, and find the strategy of reducing similarity\n",
            "between outputs to be significantly favorable over posterior sampling. Code and\n",
            "examples are available at https://noa-cohen.github.io/MeaningfulDiversityInIR.\n",
            "\n",
            "1038. Title: Stochastic Gradient Descent for Gaussian Processes Done Right\n",
            "   Abstract: We propose a new framework of variance-reduced Hamiltonian Monte Carlo (HMC)\n",
            "methods for sampling from an $L$-smooth and $m$-strongly log-concave\n",
            "distribution, based on a unified formulation of biased and unbiased variance\n",
            "reduction methods. We study the convergence properties for HMC with gradient\n",
            "estimators which satisfy the Mean-Squared-Error-Bias (MSEB) property. We show\n",
            "that the unbiased gradient estimators, including SAGA and SVRG, based HMC\n",
            "methods achieve highest gradient efficiency with small batch size under high\n",
            "precision regime, and require $\\tilde{O}(N + \\kappa^2 d^{\\frac{1}{2}}\n",
            "\\varepsilon^{-1} + N^{\\frac{2}{3}} \\kappa^{\\frac{4}{3}} d^{\\frac{1}{3}}\n",
            "\\varepsilon^{-\\frac{2}{3}} )$ gradient complexity to achieve\n",
            "$\\epsilon$-accuracy in 2-Wasserstein distance. Moreover, our HMC methods with\n",
            "biased gradient estimators, such as SARAH and SARGE, require\n",
            "$\\tilde{O}(N+\\sqrt{N} \\kappa^2 d^{\\frac{1}{2}} \\varepsilon^{-1})$ gradient\n",
            "complexity, which has the same dependency on condition number $\\kappa$ and\n",
            "dimension $d$ as full gradient method, but improves the dependency of sample\n",
            "size $N$ for a factor of $N^\\frac{1}{2}$. Experimental results on both\n",
            "synthetic and real-world benchmark data show that our new framework\n",
            "significantly outperforms the full gradient and stochastic gradient HMC\n",
            "approaches. The earliest version of this paper was submitted to ICML 2020 with\n",
            "three weak accept but was not finally accepted.\n",
            "\n",
            "1039. Title: New Insight of Variance reduce in Zero-Order Hard-Thresholding: Mitigating Gradient Error and Expansivity Contradictions\n",
            "   Abstract: Advanced electromagnetic potentials are indigenous to the classical Maxwell\n",
            "theory. Generally however they are deemed undesirable and are forcibly\n",
            "excluded, destroying the theory's inherent time-symmetry. We investigate the\n",
            "reason for this, pointing out that it is not necessary and in some cases is\n",
            "counter-productive. We then focus on the direct-action theory in which the\n",
            "advanced and retarded contributions are present symmetrically, with no\n",
            "opportunity to supplement the particular integral solution of the wave equation\n",
            "with an arbitrary complementary function. One then requires a plausible\n",
            "explanation for the observed broken symmetry that, commonly, is understood\n",
            "cannot be met by the Wheeler-Feynman mechanism because the necessary boundary\n",
            "condition cannot be satisfied in acceptable cosmologies. We take this\n",
            "opportunity to argue that the boundary condition is already met by all\n",
            "expanding cosmologies simply as a result of cosmological red-shift. A\n",
            "consequence is that the cosmological and thermodynamic arrows of time can be\n",
            "equated, the direct action version of EM is preferred, and that advanced\n",
            "potentials are ubiquitous.\n",
            "\n",
            "1040. Title: Efficient Dynamics Modeling in Interactive Environments with Koopman Theory\n",
            "   Abstract: While Large Language Models (LLMs) are increasingly being used in real-world\n",
            "applications, they remain vulnerable to prompt injection attacks: malicious\n",
            "third party prompts that subvert the intent of the system designer. To help\n",
            "researchers study this problem, we present a dataset of over 126,000 prompt\n",
            "injection attacks and 46,000 prompt-based \"defenses\" against prompt injection,\n",
            "all created by players of an online game called Tensor Trust. To the best of\n",
            "our knowledge, this is currently the largest dataset of human-generated\n",
            "adversarial examples for instruction-following LLMs. The attacks in our dataset\n",
            "have a lot of easily interpretable stucture, and shed light on the weaknesses\n",
            "of LLMs. We also use the dataset to create a benchmark for resistance to two\n",
            "types of prompt injection, which we refer to as prompt extraction and prompt\n",
            "hijacking. Our benchmark results show that many models are vulnerable to the\n",
            "attack strategies in the Tensor Trust dataset. Furthermore, we show that some\n",
            "attack strategies from the dataset generalize to deployed LLM-based\n",
            "applications, even though they have a very different set of constraints to the\n",
            "game. We release all data and source code at https://tensortrust.ai/paper\n",
            "\n",
            "1041. Title: Circuit Component Reuse Across Tasks in Transformer Language Models\n",
            "   Abstract: Robust Markov decision processes (MDPs) aim to handle changing or partially\n",
            "known system dynamics. To solve them, one typically resorts to robust\n",
            "optimization methods. However, this significantly increases computational\n",
            "complexity and limits scalability in both learning and planning. On the other\n",
            "hand, regularized MDPs show more stability in policy learning without impairing\n",
            "time complexity. Yet, they generally do not encompass uncertainty in the model\n",
            "dynamics. In this work, we aim to learn robust MDPs using regularization. We\n",
            "first show that regularized MDPs are a particular instance of robust MDPs with\n",
            "uncertain reward. We thus establish that policy iteration on reward-robust MDPs\n",
            "can have the same time complexity as on regularized MDPs. We further extend\n",
            "this relationship to MDPs with uncertain transitions: this leads to a\n",
            "regularization term with an additional dependence on the value function. We\n",
            "finally generalize regularized MDPs to twice regularized MDPs (R${}^2$ MDPs),\n",
            "i.e., MDPs with $\\textit{both}$ value and policy regularization. The\n",
            "corresponding Bellman operators enable developing policy iteration schemes with\n",
            "convergence and robustness guarantees. It also reduces planning and learning in\n",
            "robust MDPs to regularized MDPs.\n",
            "\n",
            "1042. Title: Sample-efficient Learning of Infinite-horizon Average-reward MDPs with General Function Approximation\n",
            "   Abstract: Image-Text Matching (ITM) task, a fundamental vision-language (VL) task,\n",
            "suffers from the inherent ambiguity arising from multiplicity and imperfect\n",
            "annotations. Deterministic functions are not sufficiently powerful to capture\n",
            "ambiguity, prompting the exploration of probabilistic embeddings to tackle the\n",
            "challenge. However, the existing probabilistic ITM approach encounters two key\n",
            "shortcomings; the burden of heavy computations due to the Monte Carlo\n",
            "approximation, and the loss saturation issue in the face of abundant false\n",
            "negatives. To overcome the issues, this paper presents an improved\n",
            "Probabilistic Cross-Modal Embeddings (named PCME++) by introducing a new\n",
            "probabilistic distance with a closed-form solution. In addition, two\n",
            "optimization techniques are proposed to enhance PCME++ further: first, the\n",
            "incorporation of pseudo-positives to prevent the negative effect under massive\n",
            "false negatives; second, mixed sample data augmentation for probabilistic\n",
            "matching. Experimental results on MS-COCO Caption and two extended benchmarks,\n",
            "CxC and ECCV Caption, demonstrate the effectiveness of PCME++ compared to\n",
            "state-of-the-art ITM methods. The robustness of PCME++ is also evaluated under\n",
            "noisy image-text correspondences. In addition, the potential applicability of\n",
            "PCME++ in automatic prompt-filtering for zero-shot classification is shown. The\n",
            "code is available at https://github.com/naver-ai/pcmepp\n",
            "\n",
            "1043. Title: Are Human-generated Demonstrations Necessary for In-context Learning?\n",
            "   Abstract: The accurate modeling of dynamics in interactive environments is critical for\n",
            "successful long-range prediction. Such a capability could advance Reinforcement\n",
            "Learning (RL) and Planning algorithms, but achieving it is challenging.\n",
            "Inaccuracies in model estimates can compound, resulting in increased errors\n",
            "over long horizons. We approach this problem from the lens of Koopman theory,\n",
            "where the nonlinear dynamics of the environment can be linearized in a\n",
            "high-dimensional latent space. This allows us to efficiently parallelize the\n",
            "sequential problem of long-range prediction using convolution while accounting\n",
            "for the agent's action at every time step. Our approach also enables stability\n",
            "analysis and better control over gradients through time. Taken together, these\n",
            "advantages result in significant improvement over the existing approaches, both\n",
            "in the efficiency and the accuracy of modeling dynamics over extended horizons.\n",
            "We also show that this model can be easily incorporated into dynamics modeling\n",
            "for model-based planning and model-free RL and report promising experimental\n",
            "results.\n",
            "\n",
            "1044. Title: Tensor Trust: Interpretable Prompt Injection Attacks from an Online Game\n",
            "   Abstract: In this work, we consider rather general and broad class of Markov chains,\n",
            "Ito chains, that look like Euler-Maryama discretization of some Stochastic\n",
            "Differential Equation. The chain we study is a unified framework for\n",
            "theoretical analysis. It comes with almost arbitrary isotropic and\n",
            "state-dependent noise instead of normal and state-independent one as in most\n",
            "related papers. Moreover, in our chain the drift and diffusion coefficient can\n",
            "be inexact in order to cover wide range of applications as Stochastic Gradient\n",
            "Langevin Dynamics, sampling, Stochastic Gradient Descent or Stochastic Gradient\n",
            "Boosting. We prove the bound in $W_{2}$-distance between the laws of our Ito\n",
            "chain and corresponding differential equation. These results improve or cover\n",
            "most of the known estimates. And for some particular cases, our analysis is the\n",
            "first.\n",
            "\n",
            "1045. Title: Can We Evaluate Domain Adaptation Models Without Target-Domain Labels?\n",
            "   Abstract: Recent work in mechanistic interpretability has shown that behaviors in\n",
            "language models can be successfully reverse-engineered through circuit\n",
            "analysis. A common criticism, however, is that each circuit is task-specific,\n",
            "and thus such analysis cannot contribute to understanding the models at a\n",
            "higher level. In this work, we present evidence that insights (both low-level\n",
            "findings about specific heads and higher-level findings about general\n",
            "algorithms) can indeed generalize across tasks. Specifically, we study the\n",
            "circuit discovered in Wang et al. (2022) for the Indirect Object Identification\n",
            "(IOI) task and 1.) show that it reproduces on a larger GPT2 model, and 2.) that\n",
            "it is mostly reused to solve a seemingly different task: Colored Objects\n",
            "(Ippolito & Callison-Burch, 2023). We provide evidence that the process\n",
            "underlying both tasks is functionally very similar, and contains about a 78%\n",
            "overlap in in-circuit attention heads. We further present a proof-of-concept\n",
            "intervention experiment, in which we adjust four attention heads in middle\n",
            "layers in order to 'repair' the Colored Objects circuit and make it behave like\n",
            "the IOI circuit. In doing so, we boost accuracy from 49.6% to 93.7% on the\n",
            "Colored Objects task and explain most sources of error. The intervention\n",
            "affects downstream attention heads in specific ways predicted by their\n",
            "interactions in the IOI circuit, indicating that this subcircuit behavior is\n",
            "invariant to the different task inputs. Overall, our results provide evidence\n",
            "that it may yet be possible to explain large language models' behavior in terms\n",
            "of a relatively small number of interpretable task-general algorithmic building\n",
            "blocks and computational components.\n",
            "\n",
            "1046. Title: Improved Probabilistic Image-Text Representations\n",
            "   Abstract: Recent studies on generalizing CLIP for monocular depth estimation reveal\n",
            "that CLIP pre-trained on web-crawled data is inefficient for deriving proper\n",
            "similarities between image patches and depth-related prompts. In this paper, we\n",
            "adapt CLIP for meaningful quality of monocular depth estimation with dense\n",
            "prediction, without fine-tuning its original vision-language alignment. By\n",
            "jointly training a compact deconvolutional decoder with a tiny learnable\n",
            "embedding matrix named mirror, as a static prompt for its text encoder, CLIP is\n",
            "enabled to understand depth. With this approach, our model exhibits impressive\n",
            "performance matching several previous state-of-the-art vision-only models on\n",
            "the NYU Depth v2 and KITTI datasets, outperforming every CLIP-based depth\n",
            "estimation model with a large margin. Experiments on temporal depth consistency\n",
            "and spatial continuity demonstrate that the prior knowledge of CLIP can be\n",
            "effectively refined by our proposed framework. Furthermore, an ablation study\n",
            "on mirror proves that the resulting model estimates depth utilizing knowledge\n",
            "not only from the image encoder but also text encoder despite not being given\n",
            "any prompt written in a human way. This research demonstrates that through\n",
            "minimal adjustments, the prior knowledge of vision-language foundation models,\n",
            "such as CLIP, can be generalized even to domains where learning during\n",
            "pretraining is challenging. We facilitate future works focused on methods to\n",
            "adjust suboptimal prior knowledge of vision-language models using non-human\n",
            "language prompts, achieving performance on par with task-specific\n",
            "state-of-the-art methodologies.\n",
            "\n",
            "1047. Title: Improving Generalization of Alignment with Human Preferences through Group Invariant Learning\n",
            "   Abstract: The field of machine translation (MT), the automatic translation of written\n",
            "text from one natural language into another, has experienced a major paradigm\n",
            "shift in recent years. Statistical MT, which mainly relies on various\n",
            "count-based models and which used to dominate MT research for decades, has\n",
            "largely been superseded by neural machine translation (NMT), which tackles\n",
            "translation with a single neural network. In this work we will trace back the\n",
            "origins of modern NMT architectures to word and sentence embeddings and earlier\n",
            "examples of the encoder-decoder network family. We will conclude with a survey\n",
            "of recent trends in the field.\n",
            "\n",
            "1048. Title: Crystalformer: Infinitely Connected Attention for Periodic Structure Encoding\n",
            "   Abstract: The success of AI assistants based on language models (LLMs) hinges crucially\n",
            "on Reinforcement Learning from Human Feedback (RLHF), which enables the\n",
            "generation of responses more aligned with human preferences. As universal AI\n",
            "assistants, there's a growing expectation for them to perform consistently\n",
            "across various domains. However, previous work shows that Reinforcement\n",
            "Learning (RL) often exploits shortcuts to attain high rewards and overlooks\n",
            "challenging samples. This focus on quick reward gains undermines both the\n",
            "stability in training and the model's ability to generalize to new, unseen\n",
            "data. In this work, we propose a novel approach that can learn a consistent\n",
            "policy via RL across various data groups or domains. Given the challenges\n",
            "associated with acquiring group annotations, our method automatically\n",
            "classifies data into different groups, deliberately maximizing performance\n",
            "variance. Then, we optimize the policy to perform well on challenging groups.\n",
            "Lastly, leveraging the established groups, our approach adaptively adjusts the\n",
            "exploration space, allocating more learning capacity to more challenging data\n",
            "and preventing the model from over-optimizing on simpler data. Experimental\n",
            "results indicate that our approach significantly enhances training stability\n",
            "and model generalization.\n",
            "\n",
            "1049. Title: Neural Active Learning Beyond Bandits\n",
            "   Abstract: We study both stream-based and pool-based active learning with neural network\n",
            "approximations. A recent line of works proposed bandit-based approaches that\n",
            "transformed active learning into a bandit problem, achieving both theoretical\n",
            "and empirical success. However, the performance and computational costs of\n",
            "these methods may be susceptible to the number of classes, denoted as $K$, due\n",
            "to this transformation. Therefore, this paper seeks to answer the question:\n",
            "\"How can we mitigate the adverse impacts of $K$ while retaining the advantages\n",
            "of principled exploration and provable performance guarantees in active\n",
            "learning?\" To tackle this challenge, we propose two algorithms based on the\n",
            "newly designed exploitation and exploration neural networks for stream-based\n",
            "and pool-based active learning. Subsequently, we provide theoretical\n",
            "performance guarantees for both algorithms in a non-parametric setting,\n",
            "demonstrating a slower error-growth rate concerning $K$ for the proposed\n",
            "approaches. We use extensive experiments to evaluate the proposed algorithms,\n",
            "which consistently outperform state-of-the-art baselines.\n",
            "\n",
            "1050. Title: A Progressive Training Framework for Spiking Neural Networks with Learnable Multi-hierarchical Model\n",
            "   Abstract: Federated learning is a paradigm of distributed machine learning in which\n",
            "multiple clients coordinate with a central server to learn a model, without\n",
            "sharing their own training data. Standard federated optimization methods such\n",
            "as Federated Averaging (FedAvg) ensure balance among the clients by using the\n",
            "same stepsize for local updates on all clients. However, this means that all\n",
            "clients need to respect the global geometry of the function which could yield\n",
            "slow convergence. In this work, we propose locally adaptive federated learning\n",
            "algorithms, that leverage the local geometric information for each client\n",
            "function. We show that such locally adaptive methods with uncoordinated\n",
            "stepsizes across all clients can be particularly efficient in interpolated\n",
            "(overparameterized) settings, and analyze their convergence in the presence of\n",
            "heterogeneous data for convex and strongly convex settings. We validate our\n",
            "theoretical claims by performing illustrative experiments for both i.i.d.\n",
            "non-i.i.d. cases. Our proposed algorithms match the optimization performance of\n",
            "tuned FedAvg in the convex setting, outperform FedAvg as well as\n",
            "state-of-the-art adaptive federated algorithms like FedAMS for non-convex\n",
            "experiments, and come with superior generalization performance.\n",
            "\n",
            "1051. Title: InfoCon: Concept Discovery with Generative and Discriminative Informativeness\n",
            "   Abstract: Non-convex optimal control problems occurring in, e.g., water or power\n",
            "systems, typically involve a large number of variables related through\n",
            "nonlinear equality constraints. The ideal goal is to find a globally optimal\n",
            "solution, and numerical experience indicates that algorithms aiming for\n",
            "Karush-Kuhn-Tucker points often find (near-)optimal solutions. In our paper, we\n",
            "provide a theoretical underpinning for this phenomenon, showing that on a broad\n",
            "class of problems the objective can be shown to be an invariantly convex\n",
            "function (invex function) of the control decision variables when state\n",
            "variables are eliminated using implicit function theory. In this way,\n",
            "near-global optimality can be demonstrated, where the exact nature of the\n",
            "global optimality guarantee depends on the position of the solution within the\n",
            "feasible set. In a numerical example, we show how high-quality solutions are\n",
            "obtained with local search for a river control problem where invexity holds.\n",
            "\n",
            "1052. Title: Flow Matching on General Geometries\n",
            "   Abstract: Latest insights from biology show that intelligence not only emerges from the\n",
            "connections between neurons but that individual neurons shoulder more\n",
            "computational responsibility than previously anticipated. This perspective\n",
            "should be critical in the context of constantly changing distinct reinforcement\n",
            "learning environments, yet current approaches still primarily employ static\n",
            "activation functions. In this work, we motivate why rationals are suitable for\n",
            "adaptable activation functions and why their inclusion into neural networks is\n",
            "crucial. Inspired by recurrence in residual networks, we derive a condition\n",
            "under which rational units are closed under residual connections and formulate\n",
            "a naturally regularised version: the recurrent-rational. We demonstrate that\n",
            "equipping popular algorithms with (recurrent-)rational activations leads to\n",
            "consistent improvements on Atari games, especially turning simple DQN into a\n",
            "solid approach, competitive to DDQN and Rainbow.\n",
            "\n",
            "1053. Title: On the Parameterization of Second-Order Optimization Effective towards the Infinite Width\n",
            "   Abstract: We attempt to determine whether the MACHO microlensing source stars are drawn\n",
            "from the average population of the LMC or from a population behind the LMC by\n",
            "examining the HST color-magnitude diagram (CMD) of microlensing source stars.\n",
            "We present WFPC2 HST photometry of eight MACHO microlensing source stars and\n",
            "the surrounding fields in the LMC. The microlensing source stars are identified\n",
            "by deriving accurate centroids in the ground-based MACHO images using\n",
            "difference image analysis (DIA) and then transforming the DIA coordinates to\n",
            "the HST frame. We consider in detail a model for the background population of\n",
            "source stars based on that presented by Zhao, Graff & Guhathakurta. In this\n",
            "model, the source stars have an additional reddening <E(B-V)> = 0.13 mag and a\n",
            "slightly larger distance modulus <Delta mu> ~ 0.3 mag than the average LMC\n",
            "population. We also investigate a series of source star models, varying the\n",
            "relative fraction of source stars drawn from the average and background\n",
            "populations and the displacement of the background population from the LMC. Due\n",
            "to the small number of analyzed events the distribution of probabilities of\n",
            "different models is rather flat. A shallow maximum occurs at a fraction s_LMC ~\n",
            "0.8 of the source stars in the LMC. This is consistent with the interpretation\n",
            "that a significant fraction of observed microlensing events are due to lenses\n",
            "in the Milky Way halo, but does not definitively exclude other models.\n",
            "\n",
            "1054. Title: Adaptive Rational Activations to Boost Deep Reinforcement Learning\n",
            "   Abstract: This study focuses on the evaluation of the Open Question Answering (Open-QA)\n",
            "task, which can directly estimate the factuality of large language models\n",
            "(LLMs). Current automatic evaluation methods have shown limitations, indicating\n",
            "that human evaluation still remains the most reliable approach. We introduce a\n",
            "new task, Evaluating QA Evaluation (QA-Eval) and the corresponding dataset\n",
            "EVOUNA, designed to assess the accuracy of AI-generated answers in relation to\n",
            "standard answers within Open-QA. Our evaluation of these methods utilizes\n",
            "human-annotated results to measure their performance. Specifically, the work\n",
            "investigates methods that show high correlation with human evaluations, deeming\n",
            "them more reliable. We also discuss the pitfalls of current methods and methods\n",
            "to improve LLM-based evaluators. We believe this new QA-Eval task and\n",
            "corresponding dataset EVOUNA will facilitate the development of more effective\n",
            "automatic evaluation tools and prove valuable for future research in this area.\n",
            "All resources are available at \\url{https://github.com/wangcunxiang/QA-Eval}\n",
            "and it is under the Apache-2.0 License.\n",
            "\n",
            "1055. Title: Evaluating the Zero-shot Robustness of Instruction-tuned Language Models\n",
            "   Abstract: Sampling-based algorithms, which eliminate ''unimportant'' computations\n",
            "during forward and/or back propagation (BP), offer potential solutions to\n",
            "accelerate neural network training. However, since sampling introduces\n",
            "approximations to training, such algorithms may not consistently maintain\n",
            "accuracy across various tasks. In this work, we introduce a variance-controlled\n",
            "adaptive sampling (VCAS) method designed to accelerate BP. VCAS computes an\n",
            "unbiased stochastic gradient with fine-grained layerwise importance sampling in\n",
            "data dimension for activation gradient calculation and leverage score sampling\n",
            "in token dimension for weight gradient calculation. To preserve accuracy, we\n",
            "control the additional variance by learning the sample ratio jointly with model\n",
            "parameters during training. We assessed VCAS on multiple fine-tuning and\n",
            "pre-training tasks in both vision and natural language domains. On all the\n",
            "tasks, VCAS can preserve the original training loss trajectory and validation\n",
            "accuracy with an up to 73.87% FLOPs reduction of BP and 49.58% FLOPs reduction\n",
            "of the whole training process. The implementation is available at\n",
            "https://github.com/thu-ml/VCAS .\n",
            "\n",
            "1056. Title: BESA: Pruning Large Language Models with Blockwise Parameter-Efficient Sparsity Allocation\n",
            "   Abstract: We propose Riemannian Flow Matching (RFM), a simple yet powerful framework\n",
            "for training continuous normalizing flows on manifolds. Existing methods for\n",
            "generative modeling on manifolds either require expensive simulation, are\n",
            "inherently unable to scale to high dimensions, or use approximations for\n",
            "limiting quantities that result in biased training objectives. Riemannian Flow\n",
            "Matching bypasses these limitations and offers several advantages over previous\n",
            "approaches: it is simulation-free on simple geometries, does not require\n",
            "divergence computation, and computes its target vector field in closed-form.\n",
            "The key ingredient behind RFM is the construction of a relatively simple\n",
            "premetric for defining target vector fields, which encompasses the existing\n",
            "Euclidean case. To extend to general geometries, we rely on the use of spectral\n",
            "decompositions to efficiently compute premetrics on the fly. Our method\n",
            "achieves state-of-the-art performance on many real-world non-Euclidean\n",
            "datasets, and we demonstrate tractable training on general geometries,\n",
            "including triangular meshes with highly non-trivial curvature and boundaries.\n",
            "\n",
            "1057. Title: Efficient Backpropagation with Variance Controlled Adaptive Sampling\n",
            "   Abstract: Second-order optimization has been developed to accelerate the training of\n",
            "deep neural networks and it is being applied to increasingly larger-scale\n",
            "models. In this study, towards training on further larger scales, we identify a\n",
            "specific parameterization for second-order optimization that promotes feature\n",
            "learning in a stable manner even if the network width increases significantly.\n",
            "Inspired by a maximal update parameterization, we consider a one-step update of\n",
            "the gradient and reveal the appropriate scales of hyperparameters including\n",
            "random initialization, learning rates, and damping terms. Our approach covers\n",
            "two major second-order optimization algorithms, K-FAC and Shampoo, and we\n",
            "demonstrate that our parameterization achieves higher generalization\n",
            "performance in feature learning. In particular, it enables us to transfer the\n",
            "hyperparameters across models with different widths.\n",
            "\n",
            "1058. Title: Gene Regulatory Network Inference in the Presence of Dropouts: a Causal View\n",
            "   Abstract: Stripe phases were predicted to arise in doped antiferromagnets through\n",
            "competition between magnetism and the kinetic energy of mobile carriers\n",
            "(typically holes). In copper-oxides the main experimental evidence for stripes\n",
            "is neutron scattering from La1.48Nd0.4Sr0.12CuO4 (LNSCO) and La1.875Ba0.125CuO4\n",
            "(LBCO) which reveals coexisting static spin and charge order whose wavelengths\n",
            "differ by a factor of two, reminiscent of charged rivers separating regions of\n",
            "oppositely-phased antiferromagnetism. A neutron is an electrically neutral\n",
            "object, however, so does not detect charge but rather its associated lattice\n",
            "distortion ; it is not known if the \"stripe\" phase in LNSCO and LBCO actually\n",
            "involves ordering of the doped holes. Here we present a study of the charge\n",
            "order in LBCO with resonant soft x-ray scattering (RSXS). We observe giant\n",
            "resonances at both the mobile carrier and upper-Hubbard band features in the OK\n",
            "edge. These demonstrate a substantial modulation in the doped hole density as\n",
            "well as the amount of spectral weight near the correlated gap, i.e. the degree\n",
            "of \"Mottness\". The peak-to-trough amplitude of the valence modulation is\n",
            "estimated to be 0.063 holes, which if interpreted with a model of the stripe\n",
            "form factor suggests an integrated area of 0.59 holes under a single stripe.\n",
            "While only an estimate, this number agrees with what is expected for\n",
            "half-filled stripes.\n",
            "\n",
            "1059. Title: LUT-GEMM: Quantized Matrix Multiplication based on LUTs for Efficient Inference in Large-Scale Generative Language Models\n",
            "   Abstract: We study the Neural Optimal Transport (NOT) algorithm which uses the general\n",
            "optimal transport formulation and learns stochastic transport plans. We show\n",
            "that NOT with the weak quadratic cost might learn fake plans which are not\n",
            "optimal. To resolve this issue, we introduce kernel weak quadratic costs. We\n",
            "show that they provide improved theoretical guarantees and practical\n",
            "performance. We test NOT with kernel costs on the unpaired image-to-image\n",
            "translation task.\n",
            "\n",
            "1060. Title: FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores\n",
            "   Abstract: Brain-inspired spiking neural networks (SNNs) have demonstrated great\n",
            "potential for temporal signal processing. However, their performance in speech\n",
            "processing remains limited due to the lack of an effective auditory front-end.\n",
            "To address this limitation, we introduce Spiking-LEAF, a learnable auditory\n",
            "front-end meticulously designed for SNN-based speech processing. Spiking-LEAF\n",
            "combines a learnable filter bank with a novel two-compartment spiking neuron\n",
            "model called IHC-LIF. The IHC-LIF neurons draw inspiration from the structure\n",
            "of inner hair cells (IHC) and they leverage segregated dendritic and somatic\n",
            "compartments to effectively capture multi-scale temporal dynamics of speech\n",
            "signals. Additionally, the IHC-LIF neurons incorporate the lateral feedback\n",
            "mechanism along with spike regularization loss to enhance spike encoding\n",
            "efficiency. On keyword spotting and speaker identification tasks, the proposed\n",
            "Spiking-LEAF outperforms both SOTA spiking auditory front-ends and conventional\n",
            "real-valued acoustic features in terms of classification accuracy, noise\n",
            "robustness, and encoding efficiency.\n",
            "\n",
            "1061. Title: Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language Models that Follow Instructions\n",
            "   Abstract: Recently, text watermarking algorithms for large language models (LLMs) have\n",
            "been proposed to mitigate the potential harms of text generated by LLMs,\n",
            "including fake news and copyright issues. However, current watermark detection\n",
            "algorithms require the secret key used in the watermark generation process,\n",
            "making them susceptible to security breaches and counterfeiting during public\n",
            "detection. To address this limitation, we propose an unforgeable publicly\n",
            "verifiable watermark algorithm named UPV that uses two different neural\n",
            "networks for watermark generation and detection, instead of using the same key\n",
            "at both stages. Meanwhile, the token embedding parameters are shared between\n",
            "the generation and detection networks, which makes the detection network\n",
            "achieve a high accuracy very efficiently. Experiments demonstrate that our\n",
            "algorithm attains high detection accuracy and computational efficiency through\n",
            "neural networks. Subsequent analysis confirms the high complexity involved in\n",
            "forging the watermark from the detection network. Our code is available at\n",
            "\\href{https://github.com/THU-BPM/unforgeable_watermark}{https://github.com/THU-BPM/unforgeable\\_watermark}.\n",
            "Additionally, our algorithm could also be accessed through MarkLLM\n",
            "\\citep{pan2024markllm} \\footnote{https://github.com/THU-BPM/MarkLLM}.\n",
            "\n",
            "1062. Title: Würstchen: An Efficient Architecture for Large-Scale Text-to-Image Diffusion Models\n",
            "   Abstract: Vision Transformers (ViT) have emerged as the de-facto choice for numerous\n",
            "industry grade vision solutions. But their inference cost can be prohibitive\n",
            "for many settings, as they compute self-attention in each layer which suffers\n",
            "from quadratic computational complexity in the number of tokens. On the other\n",
            "hand, spatial information in images and spatio-temporal information in videos\n",
            "is usually sparse and redundant. In this work, we introduce LookupViT, that\n",
            "aims to exploit this information sparsity to reduce ViT inference cost.\n",
            "LookupViT provides a novel general purpose vision transformer block that\n",
            "operates by compressing information from higher resolution tokens to a fixed\n",
            "number of tokens. These few compressed tokens undergo meticulous processing,\n",
            "while the higher-resolution tokens are passed through computationally cheaper\n",
            "layers. Information sharing between these two token sets is enabled through a\n",
            "bidirectional cross-attention mechanism. The approach offers multiple\n",
            "advantages - (a) easy to implement on standard ML accelerators (GPUs/TPUs) via\n",
            "standard high-level operators, (b) applicable to standard ViT and its variants,\n",
            "thus generalizes to various tasks, (c) can handle different tokenization and\n",
            "attention approaches. LookupViT also offers flexibility for the compressed\n",
            "tokens, enabling performance-computation trade-offs in a single trained model.\n",
            "We show LookupViT's effectiveness on multiple domains - (a) for\n",
            "image-classification (ImageNet-1K and ImageNet-21K), (b) video classification\n",
            "(Kinetics400 and Something-Something V2), (c) image captioning (COCO-Captions)\n",
            "with a frozen encoder. LookupViT provides $2\\times$ reduction in FLOPs while\n",
            "upholding or improving accuracy across these domains. In addition, LookupViT\n",
            "also demonstrates out-of-the-box robustness and generalization on image\n",
            "classification (ImageNet-C,R,A,O), improving by up to $4\\%$ over ViT.\n",
            "\n",
            "1063. Title: Logical Languages Accepted by Transformer Encoders with Hard Attention\n",
            "   Abstract: In this work, we present a quantum circuit model for inferring gene\n",
            "regulatory networks (GRNs). The model is based on the idea of using qubit-qubit\n",
            "entanglement to simulate interactions between genes. We provide preliminary\n",
            "results that suggest our quantum GRN modeling method is competitive and\n",
            "warrants further investigation. Specifically, we present the results derived\n",
            "from the single-cell transcriptomic data of human cell lines, focusing on genes\n",
            "in involving innate immunity regulation. We demonstrate that our quantum\n",
            "circuit model can be used to predict the presence or absence of regulatory\n",
            "interactions between genes and estimate the strength and direction of the\n",
            "interactions, setting the stage for further investigations on how quantum\n",
            "computing finds applications in data-driven life sciences and, more\n",
            "importantly, to invite exploration of quantum algorithm design that takes\n",
            "advantage of the single-cell data. The application of quantum computing on\n",
            "single-cell transcriptomic data likewise contributes to a novel understanding\n",
            "of GRNs, given that the relationship between fully interconnected genes can be\n",
            "approached more effectively by quantum modeling than by statistical\n",
            "correlations.\n",
            "\n",
            "1064. Title: Feature Collapse\n",
            "   Abstract: We contribute to the study of formal languages that can be recognized by\n",
            "transformer encoders. We focus on two self-attention mechanisms: (1) UHAT\n",
            "(Unique Hard Attention Transformers) and (2) AHAT (Average Hard Attention\n",
            "Transformers). UHAT encoders are known to recognize only languages inside the\n",
            "circuit complexity class ${\\sf AC}^0$, i.e., accepted by a family of poly-sized\n",
            "and depth-bounded boolean circuits with unbounded fan-ins. On the other hand,\n",
            "AHAT encoders can recognize languages outside ${\\sf AC}^0$), but their\n",
            "expressive power still lies within the bigger circuit complexity class ${\\sf\n",
            "TC}^0$, i.e., ${\\sf AC}^0$-circuits extended by majority gates. We first show a\n",
            "negative result that there is an ${\\sf AC}^0$-language that cannot be\n",
            "recognized by an UHAT encoder. On the positive side, we show that UHAT encoders\n",
            "can recognize a rich fragment of ${\\sf AC}^0$-languages, namely, all languages\n",
            "definable in first-order logic with arbitrary unary numerical predicates. This\n",
            "logic, includes, for example, all regular languages from ${\\sf AC}^0$. We then\n",
            "show that AHAT encoders can recognize all languages of our logic even when we\n",
            "enrich it with counting terms. We apply these results to derive new results on\n",
            "the expressive power of UHAT and AHAT up to permutation of letters (a.k.a.\n",
            "Parikh images).\n",
            "\n",
            "1065. Title: Attention Satisfies: A Constraint-Satisfaction Lens on Factual Errors of Language Models\n",
            "   Abstract: We formalize and study a phenomenon called feature collapse that makes\n",
            "precise the intuitive idea that entities playing a similar role in a learning\n",
            "task receive similar representations. As feature collapse requires a notion of\n",
            "task, we leverage a simple but prototypical NLP task to study it. We start by\n",
            "showing experimentally that feature collapse goes hand in hand with\n",
            "generalization. We then prove that, in the large sample limit, distinct words\n",
            "that play identical roles in this NLP task receive identical local feature\n",
            "representations in a neural network. This analysis reveals the crucial role\n",
            "that normalization mechanisms, such as LayerNorm, play in feature collapse and\n",
            "in generalization.\n",
            "\n",
            "1066. Title: PRES: Toward Scalable Memory-Based Dynamic Graph Neural Networks\n",
            "   Abstract: We investigate the internal behavior of Transformer-based Large Language\n",
            "Models (LLMs) when they generate factually incorrect text. We propose modeling\n",
            "factual queries as constraint satisfaction problems and use this framework to\n",
            "investigate how the LLM interacts internally with factual constraints. We find\n",
            "a strong positive relationship between the LLM's attention to constraint tokens\n",
            "and the factual accuracy of generations. We curate a suite of 10 datasets\n",
            "containing over 40,000 prompts to study the task of predicting factual errors\n",
            "with the Llama-2 family across all scales (7B, 13B, 70B). We propose SAT Probe,\n",
            "a method probing attention patterns, that can predict factual errors and\n",
            "fine-grained constraint satisfaction, and allow early error identification. The\n",
            "approach and findings take another step towards using the mechanistic\n",
            "understanding of LLMs to enhance their reliability.\n",
            "\n",
            "1067. Title: Confronting Reward Model Overoptimization with Constrained RLHF\n",
            "   Abstract: Training large language models to follow instructions makes them perform\n",
            "better on a wide range of tasks and generally become more helpful. However, a\n",
            "perfectly helpful model will follow even the most malicious instructions and\n",
            "readily generate harmful content. In this paper, we raise concerns over the\n",
            "safety of models that only emphasize helpfulness, not harmlessness, in their\n",
            "instruction-tuning. We show that several popular instruction-tuned models are\n",
            "highly unsafe. Moreover, we show that adding just 3% safety examples (a few\n",
            "hundred demonstrations) when fine-tuning a model like LLaMA can substantially\n",
            "improve its safety. Our safety-tuning does not make models significantly less\n",
            "capable or helpful as measured by standard benchmarks. However, we do find\n",
            "exaggerated safety behaviours, where too much safety-tuning makes models refuse\n",
            "perfectly safe prompts if they superficially resemble unsafe ones. As a whole,\n",
            "our results illustrate trade-offs in training LLMs to be helpful and training\n",
            "them to be safe.\n",
            "\n",
            "1068. Title: VertiBench: Advancing Feature Distribution Diversity in Vertical Federated Learning Benchmarks\n",
            "   Abstract: Empowering large language models to accurately express confidence in their\n",
            "answers is essential for trustworthy decision-making. Previous confidence\n",
            "elicitation methods, which primarily rely on white-box access to internal model\n",
            "information or model fine-tuning, have become less suitable for LLMs,\n",
            "especially closed-source commercial APIs. This leads to a growing need to\n",
            "explore the untapped area of black-box approaches for LLM uncertainty\n",
            "estimation. To better break down the problem, we define a systematic framework\n",
            "with three components: prompting strategies for eliciting verbalized\n",
            "confidence, sampling methods for generating multiple responses, and aggregation\n",
            "techniques for computing consistency. We then benchmark these methods on two\n",
            "key tasks-confidence calibration and failure prediction-across five types of\n",
            "datasets (e.g., commonsense and arithmetic reasoning) and five widely-used LLMs\n",
            "including GPT-4 and LLaMA 2 Chat. Our analysis uncovers several key insights:\n",
            "1) LLMs, when verbalizing their confidence, tend to be overconfident,\n",
            "potentially imitating human patterns of expressing confidence. 2) As model\n",
            "capability scales up, both calibration and failure prediction performance\n",
            "improve. 3) Employing our proposed strategies, such as human-inspired prompts,\n",
            "consistency among multiple responses, and better aggregation strategies can\n",
            "help mitigate this overconfidence from various perspectives. 4) Comparisons\n",
            "with white-box methods indicate that while white-box methods perform better,\n",
            "the gap is narrow, e.g., 0.522 to 0.605 in AUROC. Despite these advancements,\n",
            "none of these techniques consistently outperform others, and all investigated\n",
            "methods struggle in challenging tasks, such as those requiring professional\n",
            "knowledge, indicating significant scope for improvement. We believe this study\n",
            "can serve as a strong baseline and provide insights for eliciting confidence in\n",
            "black-box LLMs.\n",
            "\n",
            "1069. Title: SalUn: Empowering Machine Unlearning via Gradient-based Weight Saliency in Both Image Classification and Generation\n",
            "   Abstract: Subgraph GNNs are provably expressive neural architectures that learn graph\n",
            "representations from sets of subgraphs. Unfortunately, their applicability is\n",
            "hampered by the computational complexity associated with performing message\n",
            "passing on many subgraphs. In this paper, we consider the problem of learning\n",
            "to select a small subset of the large set of possible subgraphs in a\n",
            "data-driven fashion. We first motivate the problem by proving that there are\n",
            "families of WL-indistinguishable graphs for which there exist efficient\n",
            "subgraph selection policies: small subsets of subgraphs that can already\n",
            "identify all the graphs within the family. We then propose a new approach,\n",
            "called Policy-Learn, that learns how to select subgraphs in an iterative\n",
            "manner. We prove that, unlike popular random policies and prior work addressing\n",
            "the same problem, our architecture is able to learn the efficient policies\n",
            "mentioned above. Our experimental results demonstrate that Policy-Learn\n",
            "outperforms existing baselines across a wide range of datasets.\n",
            "\n",
            "1070. Title: Efficient Subgraph GNNs by Learning Effective Selection Policies\n",
            "   Abstract: The interactive use of large language models (LLMs) in AI assistants (at\n",
            "work, home, etc.) introduces a new set of inference-time privacy risks: LLMs\n",
            "are fed different types of information from multiple sources in their inputs\n",
            "and are expected to reason about what to share in their outputs, for what\n",
            "purpose and with whom, within a given context. In this work, we draw attention\n",
            "to the highly critical yet overlooked notion of contextual privacy by proposing\n",
            "ConfAIde, a benchmark designed to identify critical weaknesses in the privacy\n",
            "reasoning capabilities of instruction-tuned LLMs. Our experiments show that\n",
            "even the most capable models such as GPT-4 and ChatGPT reveal private\n",
            "information in contexts that humans would not, 39% and 57% of the time,\n",
            "respectively. This leakage persists even when we employ privacy-inducing\n",
            "prompts or chain-of-thought reasoning. Our work underscores the immediate need\n",
            "to explore novel inference-time privacy-preserving approaches, based on\n",
            "reasoning and theory of mind.\n",
            "\n",
            "1071. Title: Generative Judge for Evaluating Alignment\n",
            "   Abstract: Large language models are typically aligned with human preferences by\n",
            "optimizing $\\textit{reward models}$ (RMs) fitted to human feedback. However,\n",
            "human preferences are multi-faceted, and it is increasingly common to derive\n",
            "reward from a composition of simpler reward models which each capture a\n",
            "different aspect of language quality. This itself presents a challenge, as it\n",
            "is difficult to appropriately weight these component RMs when combining them.\n",
            "Compounding this difficulty, because any RM is only a proxy for human\n",
            "evaluation, this process is vulnerable to $\\textit{overoptimization}$, wherein\n",
            "past a certain point, accumulating higher reward is associated with worse human\n",
            "ratings. In this paper, we perform, to our knowledge, the first study on\n",
            "overoptimization in composite RMs, showing that correlation between component\n",
            "RMs has a significant effect on the locations of these points. We then\n",
            "introduce an approach to solve this issue using constrained reinforcement\n",
            "learning as a means of preventing the agent from exceeding each RM's threshold\n",
            "of usefulness. Our method addresses the problem of weighting component RMs by\n",
            "learning dynamic weights, naturally expressed by Lagrange multipliers. As a\n",
            "result, each RM stays within the range at which it is an effective proxy,\n",
            "improving evaluation performance. Finally, we introduce an adaptive method\n",
            "using gradient-free optimization to identify and optimize towards these points\n",
            "during a single run.\n",
            "\n",
            "1072. Title: Dual Associated Encoder for Face Restoration\n",
            "   Abstract: The rapid development of Large Language Models (LLMs) has substantially\n",
            "expanded the range of tasks they can address. In the field of Natural Language\n",
            "Processing (NLP), researchers have shifted their focus from conventional NLP\n",
            "tasks (e.g., sequence tagging and parsing) towards tasks that revolve around\n",
            "aligning with human needs (e.g., brainstorming and email writing). This shift\n",
            "in task distribution imposes new requirements on evaluating these aligned\n",
            "models regarding generality (i.e., assessing performance across diverse\n",
            "scenarios), flexibility (i.e., examining under different protocols), and\n",
            "interpretability (i.e., scrutinizing models with explanations). In this paper,\n",
            "we propose a generative judge with 13B parameters, Auto-J, designed to address\n",
            "these challenges. Our model is trained on user queries and LLM-generated\n",
            "responses under massive real-world scenarios and accommodates diverse\n",
            "evaluation protocols (e.g., pairwise response comparison and single-response\n",
            "evaluation) with well-structured natural language critiques. To demonstrate the\n",
            "efficacy of our approach, we construct a new testbed covering 58 different\n",
            "scenarios. Experimentally, Auto-J outperforms a series of strong competitors,\n",
            "including both open-source and closed-source models, by a large margin. We also\n",
            "provide detailed analysis and case studies to further reveal the potential of\n",
            "our method and make a variety of resources public at\n",
            "https://github.com/GAIR-NLP/auto-j.\n",
            "\n",
            "1073. Title: Learning Large DAGs is Harder than you Think: Many Losses are Minimal for the Wrong DAG\n",
            "   Abstract: Recent empirical evidence indicates that transformer based in-context\n",
            "learning performs better when using a prefix language model (prefixLM), in\n",
            "which in-context samples can all attend to each other, compared to causal\n",
            "language models (causalLM), which use auto-regressive attention that prohibits\n",
            "in-context samples to attend to future samples. While this result is intuitive,\n",
            "it is not understood from a theoretical perspective. In this paper we take a\n",
            "theoretical approach and analyze the convergence behavior of prefixLM and\n",
            "causalLM under a certain parameter construction. Our analysis shows that both\n",
            "LM types converge to their stationary points at a linear rate, but that while\n",
            "prefixLM converges to the optimal solution of linear regression, causalLM\n",
            "convergence dynamics follows that of an online gradient descent algorithm,\n",
            "which is not guaranteed to be optimal even as the number of samples grows\n",
            "infinitely. We supplement our theoretical claims with empirical experiments\n",
            "over synthetic and real tasks and using various types of transformers. Our\n",
            "experiments verify that causalLM consistently underperforms prefixLM in all\n",
            "settings.\n",
            "\n",
            "1074. Title: ZeRO++: Extremely Efficient Collective Communication for Large Model Training\n",
            "   Abstract: Restoring facial details from low-quality (LQ) images has remained a\n",
            "challenging problem due to its ill-posedness induced by various degradations in\n",
            "the wild. The existing codebook prior mitigates the ill-posedness by leveraging\n",
            "an autoencoder and learned codebook of high-quality (HQ) features, achieving\n",
            "remarkable quality. However, existing approaches in this paradigm frequently\n",
            "depend on a single encoder pre-trained on HQ data for restoring HQ images,\n",
            "disregarding the domain gap between LQ and HQ images. As a result, the encoding\n",
            "of LQ inputs may be insufficient, resulting in suboptimal performance. To\n",
            "tackle this problem, we propose a novel dual-branch framework named DAEFR. Our\n",
            "method introduces an auxiliary LQ branch that extracts crucial information from\n",
            "the LQ inputs. Additionally, we incorporate association training to promote\n",
            "effective synergy between the two branches, enhancing code prediction and\n",
            "output quality. We evaluate the effectiveness of DAEFR on both synthetic and\n",
            "real-world datasets, demonstrating its superior performance in restoring facial\n",
            "details. Project page: https://liagm.github.io/DAEFR/\n",
            "\n",
            "1075. Title: Automatic Functional Differentiation in JAX\n",
            "   Abstract: We extend JAX with the capability to automatically differentiate higher-order\n",
            "functions (functionals and operators). By representing functions as a\n",
            "generalization of arrays, we seamlessly use JAX's existing primitive system to\n",
            "implement higher-order functions. We present a set of primitive operators that\n",
            "serve as foundational building blocks for constructing several key types of\n",
            "functionals. For every introduced primitive operator, we derive and implement\n",
            "both linearization and transposition rules, aligning with JAX's internal\n",
            "protocols for forward and reverse mode automatic differentiation. This\n",
            "enhancement allows for functional differentiation in the same syntax\n",
            "traditionally use for functions. The resulting functional gradients are\n",
            "themselves functions ready to be invoked in python. We showcase this tool's\n",
            "efficacy and simplicity through applications where functional derivatives are\n",
            "indispensable. The source code of this work is released at\n",
            "https://github.com/sail-sg/autofd .\n",
            "\n",
            "1076. Title: Path Choice Matters for Clear Attributions in Path Methods\n",
            "   Abstract: Mesh deformation plays a pivotal role in many 3D vision tasks including\n",
            "dynamic simulations, rendering, and reconstruction. However, defining an\n",
            "efficient discrepancy between predicted and target meshes remains an open\n",
            "problem. A prevalent approach in current deep learning is the set-based\n",
            "approach which measures the discrepancy between two surfaces by comparing two\n",
            "randomly sampled point-clouds from the two meshes with Chamfer pseudo-distance.\n",
            "Nevertheless, the set-based approach still has limitations such as lacking a\n",
            "theoretical guarantee for choosing the number of points in sampled\n",
            "point-clouds, and the pseudo-metricity and the quadratic complexity of the\n",
            "Chamfer divergence. To address these issues, we propose a novel metric for\n",
            "learning mesh deformation. The metric is defined by sliced Wasserstein distance\n",
            "on meshes represented as probability measures that generalize the set-based\n",
            "approach. By leveraging probability measure space, we gain flexibility in\n",
            "encoding meshes using diverse forms of probability measures, such as\n",
            "continuous, empirical, and discrete measures via varifold representation. After\n",
            "having encoded probability measures, we can compare meshes by using the sliced\n",
            "Wasserstein distance which is an effective optimal transport distance with\n",
            "linear computational complexity and can provide a fast statistical rate for\n",
            "approximating the surface of meshes. To the end, we employ a neural ordinary\n",
            "differential equation (ODE) to deform the input surface into the target shape\n",
            "by modeling the trajectories of the points on the surface. Our experiments on\n",
            "cortical surface reconstruction demonstrate that our approach surpasses other\n",
            "competing methods in multiple datasets and metrics.\n",
            "\n",
            "1077. Title: Language Model Beats Diffusion - Tokenizer is key to visual generation\n",
            "   Abstract: As robustness verification methods are becoming more precise, training\n",
            "certifiably robust neural networks is becoming ever more relevant. To this end,\n",
            "certified training methods compute and then optimize an upper bound on the\n",
            "worst-case loss over a robustness specification. Curiously, training methods\n",
            "based on the imprecise interval bound propagation (IBP) consistently outperform\n",
            "those leveraging more precise bounding methods. Still, we lack an understanding\n",
            "of the mechanisms making IBP so successful.\n",
            "  In this work, we thoroughly investigate these mechanisms by leveraging a\n",
            "novel metric measuring the tightness of IBP bounds. We first show theoretically\n",
            "that, for deep linear models, tightness decreases with width and depth at\n",
            "initialization, but improves with IBP training, given sufficient network width.\n",
            "We, then, derive sufficient and necessary conditions on weight matrices for IBP\n",
            "bounds to become exact and demonstrate that these impose strong regularization,\n",
            "explaining the empirically observed trade-off between robustness and accuracy\n",
            "in certified training.\n",
            "  Our extensive experimental evaluation validates our theoretical predictions\n",
            "for ReLU networks, including that wider networks improve performance, yielding\n",
            "state-of-the-art results. Interestingly, we observe that while all IBP-based\n",
            "training methods lead to high tightness, this is neither sufficient nor\n",
            "necessary to achieve high certifiable robustness. This hints at the existence\n",
            "of new training methods that do not induce the strong regularization required\n",
            "for tight IBP bounds, leading to improved robustness and standard accuracy.\n",
            "\n",
            "1078. Title: Understanding Certified Training with Interval Bound Propagation\n",
            "   Abstract: Recently, a series of papers proposed deep learning-based approaches to\n",
            "sample from target distributions using controlled diffusion processes, being\n",
            "trained only on the unnormalized target densities without access to samples.\n",
            "Building on previous work, we identify these approaches as special cases of a\n",
            "generalized Schr\\\"odinger bridge problem, seeking a stochastic evolution\n",
            "between a given prior distribution and the specified target. We further\n",
            "generalize this framework by introducing a variational formulation based on\n",
            "divergences between path space measures of time-reversed diffusion processes.\n",
            "This abstract perspective leads to practical losses that can be optimized by\n",
            "gradient-based algorithms and includes previous objectives as special cases. At\n",
            "the same time, it allows us to consider divergences other than the reverse\n",
            "Kullback-Leibler divergence that is known to suffer from mode collapse. In\n",
            "particular, we propose the so-called log-variance loss, which exhibits\n",
            "favorable numerical properties and leads to significantly improved performance\n",
            "across all considered approaches.\n",
            "\n",
            "1079. Title: DFormer: Rethinking RGBD Representation Learning for Semantic Segmentation\n",
            "   Abstract: The field of imbalanced self-supervised learning, especially in the context\n",
            "of tabular data, has not been extensively studied. Existing research has\n",
            "predominantly focused on image datasets. This paper aims to fill this gap by\n",
            "examining the specific challenges posed by data imbalance in self-supervised\n",
            "learning in the domain of tabular data, with a primary focus on autoencoders.\n",
            "Autoencoders are widely employed for learning and constructing a new\n",
            "representation of a dataset, particularly for dimensionality reduction. They\n",
            "are also often used for generative model learning, as seen in variational\n",
            "autoencoders. When dealing with mixed tabular data, qualitative variables are\n",
            "often encoded using a one-hot encoder with a standard loss function (MSE or\n",
            "Cross Entropy). In this paper, we analyze the drawbacks of this approach,\n",
            "especially when categorical variables are imbalanced. We propose a novel metric\n",
            "to balance learning: a Multi-Supervised Balanced MSE. This approach reduces the\n",
            "reconstruction error by balancing the influence of variables. Finally, we\n",
            "empirically demonstrate that this new metric, compared to the standard MSE: i)\n",
            "outperforms when the dataset is imbalanced, especially when the learning\n",
            "process is insufficient, and ii) provides similar results in the opposite case.\n",
            "\n",
            "1080. Title: Improved sampling via learned diffusions\n",
            "   Abstract: Rigorousness and clarity are both essential for interpretations of DNNs to\n",
            "engender human trust. Path methods are commonly employed to generate rigorous\n",
            "attributions that satisfy three axioms. However, the meaning of attributions\n",
            "remains ambiguous due to distinct path choices. To address the ambiguity, we\n",
            "introduce \\textbf{Concentration Principle}, which centrally allocates high\n",
            "attributions to indispensable features, thereby endowing aesthetic and\n",
            "sparsity. We then present \\textbf{SAMP}, a model-agnostic interpreter, which\n",
            "efficiently searches the near-optimal path from a pre-defined set of\n",
            "manipulation paths. Moreover, we propose the infinitesimal constraint (IC) and\n",
            "momentum strategy (MS) to improve the rigorousness and optimality.\n",
            "Visualizations show that SAMP can precisely reveal DNNs by pinpointing salient\n",
            "image pixels. We also perform quantitative experiments and observe that our\n",
            "method significantly outperforms the counterparts. Code:\n",
            "https://github.com/zbr17/SAMP.\n",
            "\n",
            "1081. Title: VQGraph: Rethinking Graph Representation Space for Bridging GNNs and MLPs\n",
            "   Abstract: As virtual agents become increasingly prevalent in human-computer\n",
            "interaction, generating realistic and contextually appropriate gestures in\n",
            "real-time remains a significant challenge. While neural rendering techniques\n",
            "have made substantial progress with static scripts, their applicability to\n",
            "human-computer interactions remains limited. To address this, we introduce\n",
            "Large Body Language Models (LBLMs) and present LBLM-AVA, a novel LBLM\n",
            "architecture that combines a Transformer-XL large language model with a\n",
            "parallelized diffusion model to generate human-like gestures from multimodal\n",
            "inputs (text, audio, and video). LBLM-AVA incorporates several key components\n",
            "enhancing its gesture generation capabilities, such as multimodal-to-pose\n",
            "embeddings, enhanced sequence-to-sequence mapping with redefined attention\n",
            "mechanisms, a temporal smoothing module for gesture sequence coherence, and an\n",
            "attention-based refinement module for enhanced realism. The model is trained on\n",
            "our large-scale proprietary open-source dataset Allo-AVA. LBLM-AVA achieves\n",
            "state-of-the-art performance in generating lifelike and contextually\n",
            "appropriate gestures with a 30% reduction in Fr\\'echet Gesture Distance (FGD),\n",
            "and a 25% improvement in Fr\\'echet Inception Distance compared to existing\n",
            "approaches.\n",
            "\n",
            "1082. Title: Active Retrosynthetic Planning Aware of Route Quality\n",
            "   Abstract: Retrosynthetic planning aims to devise a complete multi-step synthetic route\n",
            "from starting materials to a target molecule. Current strategies use a\n",
            "decoupled approach of single-step retrosynthesis models and search algorithms,\n",
            "taking only the product as the input to predict the reactants for each planning\n",
            "step and ignoring valuable context information along the synthetic route. In\n",
            "this work, we propose a novel framework that utilizes context information for\n",
            "improved retrosynthetic planning. We view synthetic routes as reaction graphs\n",
            "and propose to incorporate context through three principled steps: encode\n",
            "molecules into embeddings, aggregate information over routes, and readout to\n",
            "predict reactants. Our approach is the first attempt to utilize in-context\n",
            "learning for retrosynthesis prediction in retrosynthetic planning. The entire\n",
            "framework can be efficiently optimized in an end-to-end fashion and produce\n",
            "more practical and accurate predictions. Comprehensive experiments demonstrate\n",
            "that by fusing in the context information over routes, our model significantly\n",
            "improves the performance of retrosynthetic planning over baselines that are not\n",
            "context-aware, especially for long synthetic routes. Code is available at\n",
            "https://github.com/SongtaoLiu0823/FusionRetro.\n",
            "\n",
            "1083. Title: Multi-Source Diffusion Models for Simultaneous Music Generation and Separation\n",
            "   Abstract: Probabilistic Circuits (PCs) are a promising avenue for probabilistic\n",
            "modeling. They combine advantages of probabilistic graphical models (PGMs) with\n",
            "those of neural networks (NNs). Crucially, however, they are tractable\n",
            "probabilistic models, supporting efficient and exact computation of many\n",
            "probabilistic inference queries, such as marginals and MAP. Further, since PCs\n",
            "are structured computation graphs, they can take advantage of\n",
            "deep-learning-style parameter updates, which greatly improves their\n",
            "scalability. However, this innovation also makes PCs prone to overfitting,\n",
            "which has been observed in many standard benchmarks. Despite the existence of\n",
            "abundant regularization techniques for both PGMs and NNs, they are not\n",
            "effective enough when applied to PCs. Instead, we re-think regularization for\n",
            "PCs and propose two intuitive techniques, data softening and entropy\n",
            "regularization, that both take advantage of PCs' tractability and still have an\n",
            "efficient implementation as a computation graph. Specifically, data softening\n",
            "provides a principled way to add uncertainty in datasets in closed form, which\n",
            "implicitly regularizes PC parameters. To learn parameters from a softened\n",
            "dataset, PCs only need linear time by virtue of their tractability. In entropy\n",
            "regularization, the exact entropy of the distribution encoded by a PC can be\n",
            "regularized directly, which is again infeasible for most other density\n",
            "estimation models. We show that both methods consistently improve the\n",
            "generalization performance of a wide variety of PCs. Moreover, when paired with\n",
            "a simple PC structure, we achieved state-of-the-art results on 10 out of 20\n",
            "standard discrete density estimation benchmarks.\n",
            "\n",
            "1084. Title: FedWon: Triumphing Multi-domain Federated Learning Without Normalization\n",
            "   Abstract: Diffusion models are gaining widespread use in cutting-edge image, video, and\n",
            "audio generation. Score-based diffusion models stand out among these methods,\n",
            "necessitating the estimation of score function of the input data distribution.\n",
            "In this study, we present a theoretical framework to analyze two-layer neural\n",
            "network-based diffusion models by reframing score matching and denoising score\n",
            "matching as convex optimization. We prove that training shallow neural networks\n",
            "for score prediction can be done by solving a single convex program. Although\n",
            "most analyses of diffusion models operate in the asymptotic setting or rely on\n",
            "approximations, we characterize the exact predicted score function and\n",
            "establish convergence results for neural network-based diffusion models with\n",
            "finite data. Our results provide a precise characterization of what neural\n",
            "network-based diffusion models learn in non-asymptotic settings.\n",
            "\n",
            "1085. Title: PhyloGFN: Phylogenetic inference with generative flow networks\n",
            "   Abstract: Diffusion models have recently shown strong potential in both music\n",
            "generation and music source separation tasks. Although in early stages, a trend\n",
            "is emerging towards integrating these tasks into a single framework, as both\n",
            "involve generating musically aligned parts and can be seen as facets of the\n",
            "same generative process. In this work, we introduce a latent diffusion-based\n",
            "multi-track generation model capable of both source separation and multi-track\n",
            "music synthesis by learning the joint probability distribution of tracks\n",
            "sharing a musical context. Our model also enables arrangement generation by\n",
            "creating any subset of tracks given the others. We trained our model on the\n",
            "Slakh2100 dataset, compared it with an existing simultaneous generation and\n",
            "separation model, and observed significant improvements across objective\n",
            "metrics for source separation, music, and arrangement generation tasks. Sound\n",
            "examples are available at https://msg-ld.github.io/.\n",
            "\n",
            "1086. Title: PolyVoice: Language Models for Speech to Speech Translation\n",
            "   Abstract: Feedforward computation, such as evaluating a neural network or sampling from\n",
            "an autoregressive model, is ubiquitous in machine learning. The sequential\n",
            "nature of feedforward computation, however, requires a strict order of\n",
            "execution and cannot be easily accelerated with parallel computing. To enable\n",
            "parallelization, we frame the task of feedforward computation as solving a\n",
            "system of nonlinear equations. We then propose to find the solution using a\n",
            "Jacobi or Gauss-Seidel fixed-point iteration method, as well as hybrid methods\n",
            "of both. Crucially, Jacobi updates operate independently on each equation and\n",
            "can be executed in parallel. Our method is guaranteed to give exactly the same\n",
            "values as the original feedforward computation with a reduced (or equal) number\n",
            "of parallelizable iterations, and hence reduced time given sufficient parallel\n",
            "computing power. Experimentally, we demonstrate the effectiveness of our\n",
            "approach in accelerating (i) backpropagation of RNNs, (ii) evaluation of\n",
            "DenseNets, and (iii) autoregressive sampling of MADE and PixelCNN++, with\n",
            "speedup factors between 2.1 and 26 under various settings.\n",
            "\n",
            "1087. Title: P$^2$OT: Progressive Partial Optimal Transport for Deep Imbalanced Clustering\n",
            "   Abstract: Recovering underlying Directed Acyclic Graph (DAG) structures from\n",
            "observational data is highly challenging due to the combinatorial nature of the\n",
            "DAG-constrained optimization problem. Recently, DAG learning has been cast as a\n",
            "continuous optimization problem by characterizing the DAG constraint as a\n",
            "smooth equality one, generally based on polynomials over adjacency matrices.\n",
            "Existing methods place very small coefficients on high-order polynomial terms\n",
            "for stabilization, since they argue that large coefficients on the higher-order\n",
            "terms are harmful due to numeric exploding. On the contrary, we discover that\n",
            "large coefficients on higher-order terms are beneficial for DAG learning, when\n",
            "the spectral radiuses of the adjacency matrices are small, and that larger\n",
            "coefficients for higher-order terms can approximate the DAG constraints much\n",
            "better than the small counterparts. Based on this, we propose a novel DAG\n",
            "learning method with efficient truncated matrix power iteration to approximate\n",
            "geometric series based DAG constraints. Empirically, our DAG learning method\n",
            "outperforms the previous state-of-the-arts in various settings, often by a\n",
            "factor of $3$ or more in terms of structural Hamming distance.\n",
            "\n",
            "1088. Title: True Knowledge Comes from Practice: Aligning Large Language Models with Embodied Environments via Reinforcement Learning\n",
            "   Abstract: Deep clustering, which learns representation and semantic clustering without\n",
            "labels information, poses a great challenge for deep learning-based approaches.\n",
            "Despite significant progress in recent years, most existing methods focus on\n",
            "uniformly distributed datasets, significantly limiting the practical\n",
            "applicability of their methods. In this paper, we first introduce a more\n",
            "practical problem setting named deep imbalanced clustering, where the\n",
            "underlying classes exhibit an imbalance distribution. To tackle this problem,\n",
            "we propose a novel pseudo-labeling-based learning framework. Our framework\n",
            "formulates pseudo-label generation as a progressive partial optimal transport\n",
            "problem, which progressively transports each sample to imbalanced clusters\n",
            "under prior distribution constraints, thus generating imbalance-aware\n",
            "pseudo-labels and learning from high-confident samples. In addition, we\n",
            "transform the initial formulation into an unbalanced optimal transport problem\n",
            "with augmented constraints, which can be solved efficiently by a fast matrix\n",
            "scaling algorithm. Experiments on various datasets, including a human-curated\n",
            "long-tailed CIFAR100, challenging ImageNet-R, and large-scale subsets of\n",
            "fine-grained iNaturalist2018 datasets, demonstrate the superiority of our\n",
            "method.\n",
            "\n",
            "1089. Title: Contrastive Learning is Spectral Clustering on Similarity Graph\n",
            "   Abstract: Local motion blur commonly occurs in real-world photography due to the mixing\n",
            "between moving objects and stationary backgrounds during exposure. Existing\n",
            "image deblurring methods predominantly focus on global deblurring,\n",
            "inadvertently affecting the sharpness of backgrounds in locally blurred images\n",
            "and wasting unnecessary computation on sharp pixels, especially for\n",
            "high-resolution images. This paper aims to adaptively and efficiently restore\n",
            "high-resolution locally blurred images. We propose a local motion deblurring\n",
            "vision Transformer (LMD-ViT) built on adaptive window pruning Transformer\n",
            "blocks (AdaWPT). To focus deblurring on local regions and reduce computation,\n",
            "AdaWPT prunes unnecessary windows, only allowing the active windows to be\n",
            "involved in the deblurring processes. The pruning operation relies on the\n",
            "blurriness confidence predicted by a confidence predictor that is trained\n",
            "end-to-end using a reconstruction loss with Gumbel-Softmax re-parameterization\n",
            "and a pruning loss guided by annotated blur masks. Our method removes local\n",
            "motion blur effectively without distorting sharp regions, demonstrated by its\n",
            "exceptional perceptual and quantitative improvements compared to\n",
            "state-of-the-art methods. In addition, our approach substantially reduces FLOPs\n",
            "by 66% and achieves more than a twofold increase in inference speed compared to\n",
            "Transformer-based deblurring methods. We will make our code and annotated blur\n",
            "masks publicly available.\n",
            "\n",
            "1090. Title: Lemur: Harmonizing Natural Language and Code for Language Agents\n",
            "   Abstract: Despite the impressive performance across numerous tasks, large language\n",
            "models (LLMs) often fail in solving simple decision-making tasks due to the\n",
            "misalignment of the knowledge in LLMs with environments. On the contrary,\n",
            "reinforcement learning (RL) agents learn policies from scratch, which makes\n",
            "them always align with environments but difficult to incorporate prior\n",
            "knowledge for efficient explorations. To narrow the gap, we propose TWOSOME, a\n",
            "novel general online framework that deploys LLMs as decision-making agents to\n",
            "efficiently interact and align with embodied environments via RL without\n",
            "requiring any prepared datasets or prior knowledge of the environments.\n",
            "Firstly, we query the joint probabilities of each valid action with LLMs to\n",
            "form behavior policies. Then, to enhance the stability and robustness of the\n",
            "policies, we propose two normalization methods and summarize four prompt design\n",
            "principles. Finally, we design a novel parameter-efficient training\n",
            "architecture where the actor and critic share one frozen LLM equipped with\n",
            "low-rank adapters (LoRA) updated by PPO. We conduct extensive experiments to\n",
            "evaluate TWOSOME. i) TWOSOME exhibits significantly better sample efficiency\n",
            "and performance compared to the conventional RL method, PPO, and prompt tuning\n",
            "method, SayCan, in both classical decision-making environment, Overcooked, and\n",
            "simulated household environment, VirtualHome. ii) Benefiting from LLMs'\n",
            "open-vocabulary feature, TWOSOME shows superior generalization ability to\n",
            "unseen tasks. iii) Under our framework, there is no significant loss of the\n",
            "LLMs' original ability during online PPO finetuning.\n",
            "\n",
            "1091. Title: Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!\n",
            "   Abstract: Recently, some contrastive learning methods have been proposed to\n",
            "simultaneously learn representations and clustering assignments, achieving\n",
            "significant improvements. However, these methods do not take the category\n",
            "information and clustering objective into consideration, thus the learned\n",
            "representations are not optimal for clustering and the performance might be\n",
            "limited. Towards this issue, we first propose a novel graph contrastive\n",
            "learning framework, which is then applied to the clustering task and we come up\n",
            "with the Graph Constrastive Clustering~(GCC) method. Different from basic\n",
            "contrastive clustering that only assumes an image and its augmentation should\n",
            "share similar representation and clustering assignments, we lift the\n",
            "instance-level consistency to the cluster-level consistency with the assumption\n",
            "that samples in one cluster and their augmentations should all be similar.\n",
            "Specifically, on the one hand, the graph Laplacian based contrastive loss is\n",
            "proposed to learn more discriminative and clustering-friendly features. On the\n",
            "other hand, a novel graph-based contrastive learning strategy is proposed to\n",
            "learn more compact clustering assignments. Both of them incorporate the latent\n",
            "category information to reduce the intra-cluster variance while increasing the\n",
            "inter-cluster variance. Experiments on six commonly used datasets demonstrate\n",
            "the superiority of our proposed approach over the state-of-the-art methods.\n",
            "\n",
            "1092. Title: Bongard-OpenWorld: Few-Shot Reasoning for Free-form Visual Concepts in the Real World\n",
            "   Abstract: We study the problem of sampling from a target probability density function\n",
            "in frameworks where parallel evaluations of the log-density gradient are\n",
            "feasible. Focusing on smooth and strongly log-concave densities, we revisit the\n",
            "parallelized randomized midpoint method and investigate its properties using\n",
            "recently developed techniques for analyzing its sequential version. Through\n",
            "these techniques, we derive upper bounds on the Wasserstein distance between\n",
            "sampling and target densities. These bounds quantify the substantial runtime\n",
            "improvements achieved through parallel processing.\n",
            "\n",
            "1093. Title: Kosmos-G: Generating Images in Context with Multimodal Large Language Models\n",
            "   Abstract: Existing works on \"black-box\" model interpretation use local-linear\n",
            "approximations to explain the predictions made for each data instance in terms\n",
            "of the importance assigned to the different features for arriving at the\n",
            "prediction. These works provide instancewise explanations and thus give a local\n",
            "view of the model. To be able to trust the model it is important to understand\n",
            "the global model behavior and there are relatively fewer works which do the\n",
            "same. Piecewise local-linear models provide a natural way to extend\n",
            "local-linear models to explain the global behavior of the model. In this work,\n",
            "we provide a dynamic programming based framework to obtain piecewise\n",
            "approximations of the black-box model. We also provide provable fidelity, i.e.,\n",
            "how well the explanations reflect the black-box model, guarantees. We carry out\n",
            "simulations on synthetic and real datasets to show the utility of the proposed\n",
            "approach. At the end, we show that the ideas developed for our framework can\n",
            "also be used to address the problem of clustering for one-dimensional data. We\n",
            "give a polynomial time algorithm and prove that it achieves optimal clustering.\n",
            "\n",
            "1094. Title: A path-norm toolkit for modern networks: consequences, promises and challenges\n",
            "   Abstract: Recently, neural networks have been extensively employed to solve partial\n",
            "differential equations (PDEs) in physical system modeling. While major studies\n",
            "focus on learning system evolution on predefined static mesh discretizations,\n",
            "some methods utilize reinforcement learning or supervised learning techniques\n",
            "to create adaptive and dynamic meshes, due to the dynamic nature of these\n",
            "systems. However, these approaches face two primary challenges: (1) the need\n",
            "for expensive optimal mesh data, and (2) the change of the solution space's\n",
            "degree of freedom and topology during mesh refinement. To address these\n",
            "challenges, this paper proposes a neural PDE solver with a neural mesh adapter.\n",
            "To begin with, we introduce a novel data-free neural mesh adaptor, called\n",
            "Data-free Mesh Mover (DMM), with two main innovations. Firstly, it is an\n",
            "operator that maps the solution to adaptive meshes and is trained using the\n",
            "Monge-Amp\\`ere equation without optimal mesh data. Secondly, it dynamically\n",
            "changes the mesh by moving existing nodes rather than adding or deleting nodes\n",
            "and edges. Theoretical analysis shows that meshes generated by DMM have the\n",
            "lowest interpolation error bound. Based on DMM, to efficiently and accurately\n",
            "model dynamic systems, we develop a moving mesh based neural PDE solver\n",
            "(MM-PDE) that embeds the moving mesh with a two-branch architecture and a\n",
            "learnable interpolation framework to preserve information within the data.\n",
            "Empirical experiments demonstrate that our method generates suitable meshes and\n",
            "considerably enhances accuracy when modeling widely considered PDE systems. The\n",
            "code can be found at: https://github.com/Peiyannn/MM-PDE.git.\n",
            "\n",
            "1095. Title: Better Neural PDE Solvers Through Data-Free Mesh Movers\n",
            "   Abstract: Cross-speaker style transfer in speech synthesis aims at transferring a style\n",
            "from source speaker to synthesised speech of a target speaker's timbre. Most\n",
            "previous approaches rely on data with style labels, but manually-annotated\n",
            "labels are expensive and not always reliable. In response to this problem, we\n",
            "propose Style-Label-Free, a cross-speaker style transfer method, which can\n",
            "realize the style transfer from source speaker to target speaker without style\n",
            "labels. Firstly, a reference encoder structure based on quantized variational\n",
            "autoencoder (Q-VAE) and style bottleneck is designed to extract discrete style\n",
            "representations. Secondly, a speaker-wise batch normalization layer is proposed\n",
            "to reduce the source speaker leakage. In order to improve the style extraction\n",
            "ability of the reference encoder, a style invariant and contrastive data\n",
            "augmentation method is proposed. Experimental results show that the method\n",
            "outperforms the baseline. We provide a website with audio samples.\n",
            "\n",
            "1096. Title: A Differentially Private Clustering Algorithm for Well-Clustered Graphs\n",
            "   Abstract: This work introduces the first toolkit around path-norms that fully\n",
            "encompasses general DAG ReLU networks with biases, skip connections and any\n",
            "operation based on the extraction of order statistics: max pooling, GroupSort\n",
            "etc. This toolkit notably allows us to establish generalization bounds for\n",
            "modern neural networks that are not only the most widely applicable path-norm\n",
            "based ones, but also recover or beat the sharpest known bounds of this type.\n",
            "These extended path-norms further enjoy the usual benefits of path-norms: ease\n",
            "of computation, invariance under the symmetries of the network, and improved\n",
            "sharpness on layered fully-connected networks compared to the product of\n",
            "operator norms, another complexity measure most commonly used.\n",
            "  The versatility of the toolkit and its ease of implementation allow us to\n",
            "challenge the concrete promises of path-norm-based generalization bounds, by\n",
            "numerically evaluating the sharpest known bounds for ResNets on ImageNet.\n",
            "\n",
            "1097. Title: Polynormer: Polynomial-Expressive Graph Transformer in Linear Time\n",
            "   Abstract: We study differentially private (DP) algorithms for recovering clusters in\n",
            "well-clustered graphs, which are graphs whose vertex set can be partitioned\n",
            "into a small number of sets, each inducing a subgraph of high inner conductance\n",
            "and small outer conductance. Such graphs have widespread application as a\n",
            "benchmark in the theoretical analysis of spectral clustering. We provide an\n",
            "efficient ($\\epsilon$,$\\delta$)-DP algorithm tailored specifically for such\n",
            "graphs. Our algorithm draws inspiration from the recent work of Chen et al.,\n",
            "who developed DP algorithms for recovery of stochastic block models in cases\n",
            "where the graph comprises exactly two nearly-balanced clusters. Our algorithm\n",
            "works for well-clustered graphs with $k$ nearly-balanced clusters, and the\n",
            "misclassification ratio almost matches the one of the best-known non-private\n",
            "algorithms. We conduct experimental evaluations on datasets with known ground\n",
            "truth clusters to substantiate the prowess of our algorithm. We also show that\n",
            "any (pure) $\\epsilon$-DP algorithm would result in substantial error.\n",
            "\n",
            "1098. Title: Finetuning Text-to-Image Diffusion Models for Fairness\n",
            "   Abstract: We study distributed optimization problems over multi-agent networks,\n",
            "including consensus and network flow problems. Existing distributed methods\n",
            "neglect the heterogeneity among agents' computational capabilities, limiting\n",
            "their effectiveness. To address this, we propose DISH, a distributed hybrid\n",
            "method that leverages system heterogeneity. DISH allows agents with higher\n",
            "computational capabilities or lower computational costs to perform local\n",
            "Newton-type updates while others adopt simpler gradient-type updates. Notably,\n",
            "DISH covers existing methods like EXTRA, DIGing, and ESOM-0 as special cases.\n",
            "To analyze DISH's performance with general update directions, we formulate\n",
            "distributed problems as minimax problems and introduce GRAND (gradient-related\n",
            "ascent and descent) and its alternating version, Alt-GRAND, for solving these\n",
            "problems. GRAND generalizes DISH to centralized minimax settings, accommodating\n",
            "various descent ascent update directions, including gradient-type, Newton-type,\n",
            "scaled gradient, and other general directions, within acute angles to the\n",
            "partial gradients. Theoretical analysis establishes global sublinear and linear\n",
            "convergence rates for GRAND and Alt-GRAND in strongly-convex-nonconcave and\n",
            "strongly-convex-PL settings, providing linear rates for DISH. In addition, we\n",
            "derive the local superlinear convergence of Newton-based variations of GRAND in\n",
            "centralized settings. Numerical experiments validate the effectiveness of our\n",
            "methods.\n",
            "\n",
            "1099. Title: Conformal Inductive Graph Neural Networks\n",
            "   Abstract: Conformal prediction (CP) transforms any model's output into prediction sets\n",
            "guaranteed to include (cover) the true label. CP requires exchangeability, a\n",
            "relaxation of the i.i.d. assumption, to obtain a valid distribution-free\n",
            "coverage guarantee. This makes it directly applicable to transductive\n",
            "node-classification. However, conventional CP cannot be applied in inductive\n",
            "settings due to the implicit shift in the (calibration) scores caused by\n",
            "message passing with the new nodes. We fix this issue for both cases of node\n",
            "and edge-exchangeable graphs, recovering the standard coverage guarantee\n",
            "without sacrificing statistical efficiency. We further prove that the guarantee\n",
            "holds independently of the prediction time, e.g. upon arrival of a new\n",
            "node/edge or at any subsequent moment.\n",
            "\n",
            "1100. Title: Causally Aligned Curriculum Learning\n",
            "   Abstract: We propose to address the task of causal structure learning from data in a\n",
            "supervised manner. Existing work on learning causal directions by supervised\n",
            "learning is restricted to learning pairwise relation, and not well suited for\n",
            "whole DAG discovery. We propose a novel approach of modeling the whole DAG\n",
            "structure discovery as a supervised learning. To fit the problem in hand, we\n",
            "propose to use permutation equivariant models that align well with the problem\n",
            "domain. We evaluate the proposed approach extensively on synthetic graphs of\n",
            "size 10,20,50,100 and real data, and show promising results compared with a\n",
            "variety of previous approaches.\n",
            "\n",
            "1101. Title: InstructDET: Diversifying Referring Object Detection with Generalized Instructions\n",
            "   Abstract: The rapid adoption of text-to-image diffusion models in society underscores\n",
            "an urgent need to address their biases. Without interventions, these biases\n",
            "could propagate a skewed worldview and restrict opportunities for minority\n",
            "groups. In this work, we frame fairness as a distributional alignment problem.\n",
            "Our solution consists of two main technical contributions: (1) a distributional\n",
            "alignment loss that steers specific characteristics of the generated images\n",
            "towards a user-defined target distribution, and (2) adjusted direct finetuning\n",
            "of diffusion model's sampling process (adjusted DFT), which leverages an\n",
            "adjusted gradient to directly optimize losses defined on the generated images.\n",
            "Empirically, our method markedly reduces gender, racial, and their\n",
            "intersectional biases for occupational prompts. Gender bias is significantly\n",
            "reduced even when finetuning just five soft tokens. Crucially, our method\n",
            "supports diverse perspectives of fairness beyond absolute equality, which is\n",
            "demonstrated by controlling age to a $75\\%$ young and $25\\%$ old distribution\n",
            "while simultaneously debiasing gender and race. Finally, our method is\n",
            "scalable: it can debias multiple concepts at once by simply including these\n",
            "prompts in the finetuning data. We share code and various fair diffusion model\n",
            "adaptors at https://sail-sg.github.io/finetune-fair-diffusion/.\n",
            "\n",
            "1102. Title: Graph Parsing Networks\n",
            "   Abstract: Stability guarantees are crucial when ensuring a fully autonomous robot does\n",
            "not take undesirable or potentially harmful actions. Unfortunately, global\n",
            "stability guarantees are hard to provide in dynamical systems learned from\n",
            "data, especially when the learned dynamics are governed by neural networks. We\n",
            "propose a novel methodology to learn neural contractive dynamical systems,\n",
            "where our neural architecture ensures contraction, and hence, global stability.\n",
            "To efficiently scale the method to high-dimensional dynamical systems, we\n",
            "develop a variant of the variational autoencoder that learns dynamics in a\n",
            "low-dimensional latent representation space while retaining contractive\n",
            "stability after decoding. We further extend our approach to learning\n",
            "contractive systems on the Lie group of rotations to account for full-pose\n",
            "end-effector dynamic motions. The result is the first highly flexible learning\n",
            "architecture that provides contractive stability guarantees with capability to\n",
            "perform obstacle avoidance. Empirically, we demonstrate that our approach\n",
            "encodes the desired dynamics more accurately than the current state-of-the-art,\n",
            "which provides less strong stability guarantees.\n",
            "\n",
            "1103. Title: Strategic Preys Make Acute Predators: Enhancing Camouflaged Object Detectors by Generating Camouflaged Objects\n",
            "   Abstract: Understanding the internal representations learned by neural networks is a\n",
            "cornerstone challenge in the science of machine learning. While there have been\n",
            "significant recent strides in some cases towards understanding how neural\n",
            "networks implement specific target functions, this paper explores a\n",
            "complementary question -- why do networks arrive at particular computational\n",
            "strategies? Our inquiry focuses on the algebraic learning tasks of modular\n",
            "addition, sparse parities, and finite group operations. Our primary theoretical\n",
            "findings analytically characterize the features learned by stylized neural\n",
            "networks for these algebraic tasks. Notably, our main technique demonstrates\n",
            "how the principle of margin maximization alone can be used to fully specify the\n",
            "features learned by the network. Specifically, we prove that the trained\n",
            "networks utilize Fourier features to perform modular addition and employ\n",
            "features corresponding to irreducible group-theoretic representations to\n",
            "perform compositions in general groups, aligning closely with the empirical\n",
            "observations of Nanda et al. and Chughtai et al. More generally, we hope our\n",
            "techniques can help to foster a deeper understanding of why neural networks\n",
            "adopt specific computational strategies.\n",
            "\n",
            "1104. Title: On Bias-Variance Alignment in Deep Models\n",
            "   Abstract: Graph pooling compresses graph information into a compact representation.\n",
            "State-of-the-art graph pooling methods follow a hierarchical approach, which\n",
            "reduces the graph size step-by-step. These methods must balance memory\n",
            "efficiency with preserving node information, depending on whether they use node\n",
            "dropping or node clustering. Additionally, fixed pooling ratios or numbers of\n",
            "pooling layers are predefined for all graphs, which prevents personalized\n",
            "pooling structures from being captured for each individual graph. In this work,\n",
            "inspired by bottom-up grammar induction, we propose an efficient graph parsing\n",
            "algorithm to infer the pooling structure, which then drives graph pooling. The\n",
            "resulting Graph Parsing Network (GPN) adaptively learns personalized pooling\n",
            "structure for each individual graph. GPN benefits from the discrete assignments\n",
            "generated by the graph parsing algorithm, allowing good memory efficiency while\n",
            "preserving node information intact. Experimental results on standard benchmarks\n",
            "demonstrate that GPN outperforms state-of-the-art graph pooling methods in\n",
            "graph classification tasks while being able to achieve competitive performance\n",
            "in node classification tasks. We also conduct a graph reconstruction task to\n",
            "show GPN's ability to preserve node information and measure both memory and\n",
            "time efficiency through relevant tests.\n",
            "\n",
            "1105. Title: EventRPG: Event Data Augmentation with Relevance Propagation Guidance\n",
            "   Abstract: The Information Bottleneck (IB) principle offers an information-theoretic\n",
            "framework for analyzing the training process of deep neural networks (DNNs).\n",
            "Its essence lies in tracking the dynamics of two mutual information (MI)\n",
            "values: between the hidden layer output and the DNN input/target. According to\n",
            "the hypothesis put forth by Shwartz-Ziv & Tishby (2017), the training process\n",
            "consists of two distinct phases: fitting and compression. The latter phase is\n",
            "believed to account for the good generalization performance exhibited by DNNs.\n",
            "Due to the challenging nature of estimating MI between high-dimensional random\n",
            "vectors, this hypothesis was only partially verified for NNs of tiny sizes or\n",
            "specific types, such as quantized NNs. In this paper, we introduce a framework\n",
            "for conducting IB analysis of general NNs. Our approach leverages the\n",
            "stochastic NN method proposed by Goldfeld et al. (2019) and incorporates a\n",
            "compression step to overcome the obstacles associated with high dimensionality.\n",
            "In other words, we estimate the MI between the compressed representations of\n",
            "high-dimensional random vectors. The proposed method is supported by both\n",
            "theoretical and practical justifications. Notably, we demonstrate the accuracy\n",
            "of our estimator through synthetic experiments featuring predefined MI values\n",
            "and comparison with MINE (Belghazi et al., 2018). Finally, we perform IB\n",
            "analysis on a close-to-real-scale convolutional DNN, which reveals new features\n",
            "of the MI dynamics.\n",
            "\n",
            "1106. Title: Scaling Laws for Sparsely-Connected Foundation Models\n",
            "   Abstract: Face alignment is a crucial step in preparing face images for feature\n",
            "extraction in facial analysis tasks. For applications such as face recognition,\n",
            "facial expression recognition, and facial attribute classification, alignment\n",
            "is widely utilized during both training and inference to standardize the\n",
            "positions of key landmarks in the face. It is well known that the application\n",
            "and method of face alignment significantly affect the performance of facial\n",
            "analysis models. However, the impact of alignment on face image quality has not\n",
            "been thoroughly investigated. Current FIQA studies often assume alignment as a\n",
            "prerequisite but do not explicitly evaluate how alignment affects quality\n",
            "metrics, especially with the advent of modern deep learning-based detectors\n",
            "that integrate detection and landmark localization. To address this need, our\n",
            "study examines the impact of face alignment on face image quality scores. We\n",
            "conducted experiments on the LFW, IJB-B, and SCFace datasets, employing MTCNN\n",
            "and RetinaFace models for face detection and alignment. To evaluate face image\n",
            "quality, we utilized several assessment methods, including SER-FIQ, FaceQAN,\n",
            "DifFIQA, and SDD-FIQA. Our analysis included examining quality score\n",
            "distributions for the LFW and IJB-B datasets and analyzing average quality\n",
            "scores at varying distances in the SCFace dataset. Our findings reveal that\n",
            "face image quality assessment methods are sensitive to alignment. Moreover,\n",
            "this sensitivity increases under challenging real-life conditions, highlighting\n",
            "the importance of evaluating alignment's role in quality assessment.\n",
            "\n",
            "1107. Title: Asymptotically Free Sketched Ridge Ensembles: Risks, Cross-Validation, and Tuning\n",
            "   Abstract: Camouflaged object detection (COD) is the challenging task of identifying\n",
            "camouflaged objects visually blended into surroundings. Albeit achieving\n",
            "remarkable success, existing COD detectors still struggle to obtain precise\n",
            "results in some challenging cases. To handle this problem, we draw inspiration\n",
            "from the prey-vs-predator game that leads preys to develop better camouflage\n",
            "and predators to acquire more acute vision systems and develop algorithms from\n",
            "both the prey side and the predator side. On the prey side, we propose an\n",
            "adversarial training framework, Camouflageator, which introduces an auxiliary\n",
            "generator to generate more camouflaged objects that are harder for a COD method\n",
            "to detect. Camouflageator trains the generator and detector in an adversarial\n",
            "way such that the enhanced auxiliary generator helps produce a stronger\n",
            "detector. On the predator side, we introduce a novel COD method, called\n",
            "Internal Coherence and Edge Guidance (ICEG), which introduces a camouflaged\n",
            "feature coherence module to excavate the internal coherence of camouflaged\n",
            "objects, striving to obtain more complete segmentation results. Additionally,\n",
            "ICEG proposes a novel edge-guided separated calibration module to remove false\n",
            "predictions to avoid obtaining ambiguous boundaries. Extensive experiments show\n",
            "that ICEG outperforms existing COD detectors and Camouflageator is flexible to\n",
            "improve various COD detectors, including ICEG, which brings state-of-the-art\n",
            "COD performance.\n",
            "\n",
            "1108. Title: Feature emergence via margin maximization: case studies in algebraic tasks\n",
            "   Abstract: Neural scaling laws have driven significant advancements in machine learning,\n",
            "particularly in domains like language modeling and computer vision. However,\n",
            "the exploration of neural scaling laws within robotics has remained relatively\n",
            "underexplored, despite the growing adoption of foundation models in this field.\n",
            "This paper represents the first comprehensive study to quantify neural scaling\n",
            "laws for Robot Foundation Models (RFMs) and Large Language Models (LLMs) in\n",
            "robotics tasks. Through a meta-analysis of 327 research papers, we investigate\n",
            "how data size, model size, and compute resources influence downstream\n",
            "performance across a diverse set of robotic tasks. Consistent with previous\n",
            "scaling law research, our results reveal that the performance of robotic models\n",
            "improves with increased resources, following a power-law relationship.\n",
            "Promisingly, the improvement in robotic task performance scales notably faster\n",
            "than language tasks. This suggests that, while performance on downstream\n",
            "robotic tasks today is often moderate-to-poor, increased data and compute are\n",
            "likely to signficantly improve performance in the future. Also consistent with\n",
            "previous scaling law research, we also observe the emergence of new robot\n",
            "capabilities as models scale.\n",
            "\n",
            "1109. Title: Negatively Correlated Ensemble Reinforcement Learning for Online Diverse Game Level Generation\n",
            "   Abstract: Acquiring an accurate world model online for model-based reinforcement\n",
            "learning (MBRL) is challenging due to data nonstationarity, which typically\n",
            "causes catastrophic forgetting for neural networks (NNs). From the online\n",
            "learning perspective, a Follow-The-Leader (FTL) world model is desirable, which\n",
            "optimally fits all previous experiences at each round. Unfortunately, NN-based\n",
            "models need re-training on all accumulated data at every interaction step to\n",
            "achieve FTL, which is computationally expensive for lifelong agents. In this\n",
            "paper, we revisit models that can achieve FTL with incremental updates.\n",
            "Specifically, our world model is a linear regression model supported by\n",
            "nonlinear random features. The linear part ensures efficient FTL update while\n",
            "the nonlinear random feature empowers the fitting of complex environments. To\n",
            "best trade off model capacity and computation efficiency, we introduce a\n",
            "locality sensitive sparse encoding, which allows us to conduct efficient sparse\n",
            "updates even with very high dimensional nonlinear features. We validate the\n",
            "representation power of our encoding and verify that it allows efficient online\n",
            "learning under data covariate shift. We also show, in the Dyna MBRL setting,\n",
            "that our world models learned online using a single pass of trajectory data\n",
            "either surpass or match the performance of deep world models trained with\n",
            "replay and other continual learning methods.\n",
            "\n",
            "1110. Title: Neural Contractive Dynamical Systems\n",
            "   Abstract: We employ random matrix theory to establish consistency of generalized cross\n",
            "validation (GCV) for estimating prediction risks of sketched ridge regression\n",
            "ensembles, enabling efficient and consistent tuning of regularization and\n",
            "sketching parameters. Our results hold for a broad class of asymptotically free\n",
            "sketches under very mild data assumptions. For squared prediction risk, we\n",
            "provide a decomposition into an unsketched equivalent implicit ridge bias and a\n",
            "sketching-based variance, and prove that the risk can be globally optimized by\n",
            "only tuning sketch size in infinite ensembles. For general subquadratic\n",
            "prediction risk functionals, we extend GCV to construct consistent risk\n",
            "estimators, and thereby obtain distributional convergence of the GCV-corrected\n",
            "predictions in Wasserstein-2 metric. This in particular allows construction of\n",
            "prediction intervals with asymptotically correct coverage conditional on the\n",
            "training data. We also propose an \"ensemble trick\" whereby the risk for\n",
            "unsketched ridge regression can be efficiently estimated via GCV using small\n",
            "sketched ridge ensembles. We empirically validate our theoretical results using\n",
            "both synthetic and real large-scale datasets with practical sketches including\n",
            "CountSketch and subsampled randomized discrete cosine transforms.\n",
            "\n",
            "1111. Title: Rethinking Backdoor Attacks on Dataset Distillation: A Kernel Method Perspective\n",
            "   Abstract: Ensemble algorithms offer state of the art performance in many machine\n",
            "learning applications. A common explanation for their excellent performance is\n",
            "due to the bias-variance decomposition of the mean squared error which shows\n",
            "that the algorithm's error can be decomposed into its bias and variance. Both\n",
            "quantities are often opposed to each other and ensembles offer an effective way\n",
            "to manage them as they reduce the variance through a diverse set of base\n",
            "learners while keeping the bias low at the same time. Even though there have\n",
            "been numerous works on decomposing other loss functions, the exact mathematical\n",
            "connection is rarely exploited explicitly for ensembling, but merely used as a\n",
            "guiding principle. In this paper, we formulate a generalized bias-variance\n",
            "decomposition for arbitrary twice differentiable loss functions and study it in\n",
            "the context of Deep Learning. We use this decomposition to derive a Generalized\n",
            "Negative Correlation Learning (GNCL) algorithm which offers explicit control\n",
            "over the ensemble's diversity and smoothly interpolates between the two\n",
            "extremes of independent training and the joint training of the ensemble. We\n",
            "show how GNCL encapsulates many previous works and discuss under which\n",
            "circumstances training of an ensemble of Neural Networks might fail and what\n",
            "ensembling method should be favored depending on the choice of the individual\n",
            "networks. We make our code publicly available under\n",
            "https://github.com/sbuschjaeger/gncl\n",
            "\n",
            "1112. Title: Poly-View Contrastive Learning\n",
            "   Abstract: In the era of big data and Artificial Intelligence, an emerging paradigm is\n",
            "to utilize contrastive self-supervised learning to model large-scale\n",
            "heterogeneous data. Many existing foundation models benefit from the\n",
            "generalization capability of contrastive self-supervised learning by learning\n",
            "compact and high-quality representations without relying on any label\n",
            "information. Amidst the explosive advancements in foundation models across\n",
            "multiple domains, including natural language processing and computer vision, a\n",
            "thorough survey on heterogeneous contrastive learning for the foundation model\n",
            "is urgently needed. In response, this survey critically evaluates the current\n",
            "landscape of heterogeneous contrastive learning for foundation models,\n",
            "highlighting the open challenges and future trends of contrastive learning. In\n",
            "particular, we first present how the recent advanced contrastive learning-based\n",
            "methods deal with view heterogeneity and how contrastive learning is applied to\n",
            "train and fine-tune the multi-view foundation models. Then, we move to\n",
            "contrastive learning methods for task heterogeneity, including pretraining\n",
            "tasks and downstream tasks, and show how different tasks are combined with\n",
            "contrastive learning loss for different purposes. Finally, we conclude this\n",
            "survey by discussing the open challenges and shedding light on the future\n",
            "directions of contrastive learning.\n",
            "\n",
            "1113. Title: Disentangling Time Series Representations via Contrastive Independence-of-Support on l-Variational Inference\n",
            "   Abstract: Dataset distillation offers a potential means to enhance data efficiency in\n",
            "deep learning. Recent studies have shown its ability to counteract backdoor\n",
            "risks present in original training samples. In this study, we delve into the\n",
            "theoretical aspects of backdoor attacks and dataset distillation based on\n",
            "kernel methods. We introduce two new theory-driven trigger pattern generation\n",
            "methods specialized for dataset distillation. Following a comprehensive set of\n",
            "analyses and experiments, we show that our optimization-based trigger design\n",
            "framework informs effective backdoor attacks on dataset distillation. Notably,\n",
            "datasets poisoned by our designed trigger prove resilient against conventional\n",
            "backdoor attack detection and mitigation methods. Our empirical results\n",
            "validate that the triggers developed using our approaches are proficient at\n",
            "executing resilient backdoor attacks.\n",
            "\n",
            "1114. Title: ECoFLaP: Efficient Coarse-to-Fine Layer-Wise Pruning for Vision-Language Models\n",
            "   Abstract: We study deceptive fairness attacks on graphs to answer the following\n",
            "question: How can we achieve poisoning attacks on a graph learning model to\n",
            "exacerbate the bias deceptively? We answer this question via a bi-level\n",
            "optimization problem and propose a meta learning-based framework named FATE.\n",
            "FATE is broadly applicable with respect to various fairness definitions and\n",
            "graph learning models, as well as arbitrary choices of manipulation operations.\n",
            "We further instantiate FATE to attack statistical parity and individual\n",
            "fairness on graph neural networks. We conduct extensive experimental\n",
            "evaluations on real-world datasets in the task of semi-supervised node\n",
            "classification. The experimental results demonstrate that FATE could amplify\n",
            "the bias of graph neural networks with or without fairness consideration while\n",
            "maintaining the utility on the downstream task. We hope this paper provides\n",
            "insights into the adversarial robustness of fair graph learning and can shed\n",
            "light on designing robust and fair graph learning in future studies.\n",
            "\n",
            "1115. Title: Deceptive Fairness Attacks on Graphs via Meta Learning\n",
            "   Abstract: This paper proposes an efficient approach to learning disentangled\n",
            "representations with causal mechanisms based on the difference of conditional\n",
            "probabilities in original and new distributions. We approximate the difference\n",
            "with models' generalization abilities so that it fits in the standard machine\n",
            "learning framework and can be efficiently computed. In contrast to the\n",
            "state-of-the-art approach, which relies on the learner's adaptation speed to\n",
            "new distribution, the proposed approach only requires evaluating the model's\n",
            "generalization ability. We provide a theoretical explanation for the advantage\n",
            "of the proposed method, and our experiments show that the proposed technique is\n",
            "1.9--11.0$\\times$ more sample efficient and 9.4--32.4 times quicker than the\n",
            "previous method on various tasks. The source code is available at\n",
            "\\url{https://github.com/yuanpeng16/EDCR}.\n",
            "\n",
            "1116. Title: Kernelised Normalising Flows\n",
            "   Abstract: Policy gradient methods hold great potential for solving complex continuous\n",
            "control tasks. Still, their training efficiency can be improved by exploiting\n",
            "structure within the optimization problem. Recent work indicates that\n",
            "supervised learning can be accelerated by leveraging the fact that gradients\n",
            "lie in a low-dimensional and slowly-changing subspace. In this paper, we\n",
            "conduct a thorough evaluation of this phenomenon for two popular deep policy\n",
            "gradient methods on various simulated benchmark tasks. Our results demonstrate\n",
            "the existence of such gradient subspaces despite the continuously changing data\n",
            "distribution inherent to reinforcement learning. These findings reveal\n",
            "promising directions for future work on more efficient reinforcement learning,\n",
            "e.g., through improving parameter-space exploration or enabling second-order\n",
            "optimization.\n",
            "\n",
            "1117. Title: Gradual Domain Adaptation via Gradient Flow\n",
            "   Abstract: Standard domain adaptation methods do not work well when a large gap exists\n",
            "between the source and target domains. Gradual domain adaptation is one of the\n",
            "approaches used to address the problem. It involves leveraging the intermediate\n",
            "domain, which gradually shifts from the source domain to the target domain. In\n",
            "previous work, it is assumed that the number of intermediate domains is large\n",
            "and the distance between adjacent domains is small; hence, the gradual domain\n",
            "adaptation algorithm, involving self-training with unlabeled datasets, is\n",
            "applicable. In practice, however, gradual self-training will fail because the\n",
            "number of intermediate domains is limited and the distance between adjacent\n",
            "domains is large. We propose the use of normalizing flows to deal with this\n",
            "problem while maintaining the framework of unsupervised domain adaptation. The\n",
            "proposed method learns a transformation from the distribution of the target\n",
            "domain to the Gaussian mixture distribution via the source domain. We evaluate\n",
            "our proposed method by experiments using real-world datasets and confirm that\n",
            "it mitigates the above-explained problem and improves the classification\n",
            "performance.\n",
            "\n",
            "1118. Title: Contrastive Preference Learning: Learning from Human Feedback without Reinforcement Learning\n",
            "   Abstract: Normalising Flows are non-parametric statistical models characterised by\n",
            "their dual capabilities of density estimation and generation. This duality\n",
            "requires an inherently invertible architecture. However, the requirement of\n",
            "invertibility imposes constraints on their expressiveness, necessitating a\n",
            "large number of parameters and innovative architectural designs to achieve good\n",
            "results. Whilst flow-based models predominantly rely on neural-network-based\n",
            "transformations for expressive designs, alternative transformation methods have\n",
            "received limited attention. In this work, we present Ferumal flow, a novel\n",
            "kernelised normalising flow paradigm that integrates kernels into the\n",
            "framework. Our results demonstrate that a kernelised flow can yield competitive\n",
            "or superior results compared to neural network-based flows whilst maintaining\n",
            "parameter efficiency. Kernelised flows excel especially in the low-data regime,\n",
            "enabling flexible non-parametric density estimation in applications with sparse\n",
            "data availability.\n",
            "\n",
            "1119. Title: CausalTime: Realistically Generated Time-series for Benchmarking of Causal Discovery\n",
            "   Abstract: Reinforcement Learning from Human Feedback (RLHF) has emerged as a popular\n",
            "paradigm for aligning models with human intent. Typically RLHF algorithms\n",
            "operate in two phases: first, use human preferences to learn a reward function\n",
            "and second, align the model by optimizing the learned reward via reinforcement\n",
            "learning (RL). This paradigm assumes that human preferences are distributed\n",
            "according to reward, but recent work suggests that they instead follow the\n",
            "regret under the user's optimal policy. Thus, learning a reward function from\n",
            "feedback is not only based on a flawed assumption of human preference, but also\n",
            "leads to unwieldy optimization challenges that stem from policy gradients or\n",
            "bootstrapping in the RL phase. Because of these optimization challenges,\n",
            "contemporary RLHF methods restrict themselves to contextual bandit settings\n",
            "(e.g., as in large language models) or limit observation dimensionality (e.g.,\n",
            "state-based robotics). We overcome these limitations by introducing a new\n",
            "family of algorithms for optimizing behavior from human feedback using the\n",
            "regret-based model of human preferences. Using the principle of maximum\n",
            "entropy, we derive Contrastive Preference Learning (CPL), an algorithm for\n",
            "learning optimal policies from preferences without learning reward functions,\n",
            "circumventing the need for RL. CPL is fully off-policy, uses only a simple\n",
            "contrastive objective, and can be applied to arbitrary MDPs. This enables CPL\n",
            "to elegantly scale to high-dimensional and sequential RLHF problems while being\n",
            "simpler than prior methods.\n",
            "\n",
            "1120. Title: Epitopological learning and Cannistraci-Hebb network shape intelligence brain-inspired theory for ultra-sparse advantage in deep learning\n",
            "   Abstract: Due to its empirical success in few-shot classification and reinforcement\n",
            "learning, meta-learning has recently received significant interest.\n",
            "Meta-learning methods leverage data from previous tasks to learn a new task in\n",
            "a sample-efficient manner. In particular, model-agnostic methods look for\n",
            "initialization points from which gradient descent quickly adapts to any new\n",
            "task. Although it has been empirically suggested that such methods perform well\n",
            "by learning shared representations during pretraining, there is limited\n",
            "theoretical evidence of such behavior. More importantly, it has not been shown\n",
            "that these methods still learn a shared structure, despite architectural\n",
            "misspecifications. In this direction, this work shows, in the limit of an\n",
            "infinite number of tasks, that first-order ANIL with a linear two-layer network\n",
            "architecture successfully learns linear shared representations. This result\n",
            "even holds with overparametrization; having a width larger than the dimension\n",
            "of the shared representations results in an asymptotically low-rank solution.\n",
            "The learned solution then yields a good adaptation performance on any new task\n",
            "after a single gradient step. Overall, this illustrates how well model-agnostic\n",
            "methods such as first-order ANIL can learn shared representations.\n",
            "\n",
            "1121. Title: Dissecting Sample Hardness: A Fine-Grained Analysis of Hardness Characterization Methods for Data-Centric AI\n",
            "   Abstract: Causal representation learning aims to unveil latent high-level causal\n",
            "representations from observed low-level data. One of its primary tasks is to\n",
            "provide reliable assurance of identifying these latent causal models, known as\n",
            "identifiability. A recent breakthrough explores identifiability by leveraging\n",
            "the change of causal influences among latent causal variables across multiple\n",
            "environments \\citep{liu2022identifying}. However, this progress rests on the\n",
            "assumption that the causal relationships among latent causal variables adhere\n",
            "strictly to linear Gaussian models. In this paper, we extend the scope of\n",
            "latent causal models to involve nonlinear causal relationships, represented by\n",
            "polynomial models, and general noise distributions conforming to the\n",
            "exponential family. Additionally, we investigate the necessity of imposing\n",
            "changes on all causal parameters and present partial identifiability results\n",
            "when part of them remains unchanged. Further, we propose a novel empirical\n",
            "estimation method, grounded in our theoretical finding, that enables learning\n",
            "consistent latent causal representations. Our experimental results, obtained\n",
            "from both synthetic and real-world data, validate our theoretical contributions\n",
            "concerning identifiability and consistency.\n",
            "\n",
            "1122. Title: Graph Metanetworks for Processing Diverse Neural Architectures\n",
            "   Abstract: In these notes the epitopological and pseudotopological fundamental group\n",
            "functors are introduced. These are functors from the category of pointed\n",
            "epitopological and pseudotopological spaces respectively, to the category of\n",
            "their respective group-objects. Their restrictions to the full subcategory of\n",
            "topological spaces are lifts of the topologized fundamental group functor\n",
            "introduced in [Daniel K. Biss, The topological fundamental group and\n",
            "generalized covering spaces, Topology and its Applications, 124(3):355-371,\n",
            "2002] and thus retain its information. At the same time, they show greater\n",
            "regularity inherited from the convenient properties of EpiTop and PsTop.\n",
            "Moreover, the use of such convenient categories permits, in principle, to apply\n",
            "general techniques from enriched homotopy theory. Our approach should be\n",
            "compared with the alternative improvement of Biss's functor developed in\n",
            "[Jeremy Brazas, The fundamental group as a topological group, Topology and its\n",
            "Applications, 160(1):170-188, 2013] within the topological setting. Several\n",
            "open problems, including those aimed at understanding the precise relation to\n",
            "Brazas's approach, are scattered throughout the text.\n",
            "\n",
            "1123. Title: FreeNoise: Tuning-Free Longer Video Diffusion via Noise Rescheduling\n",
            "   Abstract: We introduce the problem of learning distributed representations of edits. By\n",
            "combining a \"neural editor\" with an \"edit encoder\", our models learn to\n",
            "represent the salient information of an edit and can be used to apply edits to\n",
            "new inputs. We experiment on natural language and source code edit data. Our\n",
            "evaluation yields promising results that suggest that our neural network models\n",
            "learn to capture the structure and semantics of edits. We hope that this\n",
            "interesting task and data source will inspire other researchers to work further\n",
            "on this problem.\n",
            "\n",
            "1124. Title: A Neural Framework for Generalized Causal Sensitivity Analysis\n",
            "   Abstract: In this work, we present efficient modulation, a novel design for efficient\n",
            "vision networks. We revisit the modulation mechanism, which operates input\n",
            "through convolutional context modeling and feature projection layers, and fuses\n",
            "features via element-wise multiplication and an MLP block. We demonstrate that\n",
            "the modulation mechanism is particularly well suited for efficient networks and\n",
            "further tailor the modulation design by proposing the efficient modulation\n",
            "(EfficientMod) block, which is considered the essential building block for our\n",
            "networks. Benefiting from the prominent representational ability of modulation\n",
            "mechanism and the proposed efficient design, our network can accomplish better\n",
            "trade-offs between accuracy and efficiency and set new state-of-the-art\n",
            "performance in the zoo of efficient networks. When integrating EfficientMod\n",
            "with the vanilla self-attention block, we obtain the hybrid architecture which\n",
            "further improves the performance without loss of efficiency. We carry out\n",
            "comprehensive experiments to verify EfficientMod's performance. With fewer\n",
            "parameters, our EfficientMod-s performs 0.6 top-1 accuracy better than\n",
            "EfficientFormerV2-s2 and is 25% faster on GPU, and 2.9 better than\n",
            "MobileViTv2-1.0 at the same GPU latency. Additionally, our method presents a\n",
            "notable improvement in downstream tasks, outperforming EfficientFormerV2-s by\n",
            "3.6 mIoU on the ADE20K benchmark. Code and checkpoints are available at\n",
            "https://github.com/ma-xu/EfficientMod.\n",
            "\n",
            "1125. Title: Scaling Convex Neural Networks with Burer-Monteiro Factorization\n",
            "   Abstract: We analyze single-layer neural networks with the Xavier initialization in the\n",
            "asymptotic regime of large numbers of hidden units and large numbers of\n",
            "stochastic gradient descent training steps. The evolution of the neural network\n",
            "during training can be viewed as a stochastic system and, using techniques from\n",
            "stochastic analysis, we prove the neural network converges in distribution to a\n",
            "random ODE with a Gaussian distribution. The limit is completely different than\n",
            "in the typical mean-field results for neural networks due to the\n",
            "$\\frac{1}{\\sqrt{N}}$ normalization factor in the Xavier initialization (versus\n",
            "the $\\frac{1}{N}$ factor in the typical mean-field framework). Although the\n",
            "pre-limit problem of optimizing a neural network is non-convex (and therefore\n",
            "the neural network may converge to a local minimum), the limit equation\n",
            "minimizes a (quadratic) convex objective function and therefore converges to a\n",
            "global minimum. Furthermore, under reasonable assumptions, the matrix in the\n",
            "limiting quadratic objective function is positive definite and thus the neural\n",
            "network (in the limit) will converge to a global minimum with zero loss on the\n",
            "training set.\n",
            "\n",
            "1126. Title: Multi-View Representation is What You Need for Point-Cloud Pre-Training\n",
            "   Abstract: Unobserved confounding is common in many applications, making causal\n",
            "inference from observational data challenging. As a remedy, causal sensitivity\n",
            "analysis is an important tool to draw causal conclusions under unobserved\n",
            "confounding with mathematical guarantees. In this paper, we propose NeuralCSA,\n",
            "a neural framework for generalized causal sensitivity analysis. Unlike previous\n",
            "work, our framework is compatible with (i) a large class of sensitivity models,\n",
            "including the marginal sensitivity model, f-sensitivity models, and Rosenbaum's\n",
            "sensitivity model; (ii) different treatment types (i.e., binary and\n",
            "continuous); and (iii) different causal queries, including (conditional)\n",
            "average treatment effects and simultaneous effects on multiple outcomes. The\n",
            "generality of NeuralCSA is achieved by learning a latent distribution shift\n",
            "that corresponds to a treatment intervention using two conditional normalizing\n",
            "flows. We provide theoretical guarantees that NeuralCSA is able to infer valid\n",
            "bounds on the causal query of interest and also demonstrate this empirically\n",
            "using both simulated and real-world data.\n",
            "\n",
            "1127. Title: Efficient Modulation for Vision Networks\n",
            "   Abstract: Zeroth-order optimization (ZO) typically relies on two-point feedback to\n",
            "estimate the unknown gradient of the objective function. Nevertheless,\n",
            "two-point feedback can not be used for online optimization of time-varying\n",
            "objective functions, where only a single query of the function value is\n",
            "possible at each time step. In this work, we propose a new one-point feedback\n",
            "method for online optimization that estimates the objective function gradient\n",
            "using the residual between two feedback points at consecutive time instants.\n",
            "Moreover, we develop regret bounds for ZO with residual feedback for both\n",
            "convex and nonconvex online optimization problems. Specifically, for both\n",
            "deterministic and stochastic problems and for both Lipschitz and smooth\n",
            "objective functions, we show that using residual feedback can produce gradient\n",
            "estimates with much smaller variance compared to conventional one-point\n",
            "feedback methods. As a result, our regret bounds are much tighter compared to\n",
            "existing regret bounds for ZO with conventional one-point feedback, which\n",
            "suggests that ZO with residual feedback can better track the optimizer of\n",
            "online optimization problems. Additionally, our regret bounds rely on weaker\n",
            "assumptions than those used in conventional one-point feedback methods.\n",
            "Numerical experiments show that ZO with residual feedback significantly\n",
            "outperforms existing one-point feedback methods also in practice.\n",
            "\n",
            "1128. Title: DOS: Diverse Outlier Sampling for Out-of-Distribution Detection\n",
            "   Abstract: Neural networks efficiently encode learned information within their\n",
            "parameters. Consequently, many tasks can be unified by treating neural networks\n",
            "themselves as input data. When doing so, recent studies demonstrated the\n",
            "importance of accounting for the symmetries and geometry of parameter spaces.\n",
            "However, those works developed architectures tailored to specific networks such\n",
            "as MLPs and CNNs without normalization layers, and generalizing such\n",
            "architectures to other types of networks can be challenging. In this work, we\n",
            "overcome these challenges by building new metanetworks - neural networks that\n",
            "take weights from other neural networks as input. Put simply, we carefully\n",
            "build graphs representing the input neural networks and process the graphs\n",
            "using graph neural networks. Our approach, Graph Metanetworks (GMNs),\n",
            "generalizes to neural architectures where competing methods struggle, such as\n",
            "multi-head attention layers, normalization layers, convolutional layers, ResNet\n",
            "blocks, and group-equivariant linear layers. We prove that GMNs are expressive\n",
            "and equivariant to parameter permutation symmetries that leave the input neural\n",
            "network functions unchanged. We validate the effectiveness of our method on\n",
            "several metanetwork tasks over diverse neural network architectures.\n",
            "\n",
            "1129. Title: MAPE-PPI: Towards Effective and Efficient Protein-Protein Interaction Prediction via Microenvironment-Aware Protein Embedding\n",
            "   Abstract: Characterizing samples that are difficult to learn from is crucial to\n",
            "developing highly performant ML models. This has led to numerous Hardness\n",
            "Characterization Methods (HCMs) that aim to identify \"hard\" samples. However,\n",
            "there is a lack of consensus regarding the definition and evaluation of\n",
            "\"hardness\". Unfortunately, current HCMs have only been evaluated on specific\n",
            "types of hardness and often only qualitatively or with respect to downstream\n",
            "performance, overlooking the fundamental quantitative identification task. We\n",
            "address this gap by presenting a fine-grained taxonomy of hardness types.\n",
            "Additionally, we propose the Hardness Characterization Analysis Toolkit\n",
            "(H-CAT), which supports comprehensive and quantitative benchmarking of HCMs\n",
            "across the hardness taxonomy and can easily be extended to new HCMs, hardness\n",
            "types, and datasets. We use H-CAT to evaluate 13 different HCMs across 8\n",
            "hardness types. This comprehensive evaluation encompassing over 14K setups\n",
            "uncovers strengths and weaknesses of different HCMs, leading to practical tips\n",
            "to guide HCM selection and future development. Our findings highlight the need\n",
            "for more comprehensive HCM evaluation, while we hope our hardness taxonomy and\n",
            "toolkit will advance the principled evaluation and uptake of data-centric AI\n",
            "methods.\n",
            "\n",
            "1130. Title: Learning Performance-Improving Code Edits\n",
            "   Abstract: This paper is an attempt to explain all the matrix calculus you need in order\n",
            "to understand the training of deep neural networks. We assume no math knowledge\n",
            "beyond what you learned in calculus 1, and provide links to help you refresh\n",
            "the necessary math where needed. Note that you do not need to understand this\n",
            "material before you start learning to train and use deep learning in practice;\n",
            "rather, this material is for those who are already familiar with the basics of\n",
            "neural networks, and wish to deepen their understanding of the underlying math.\n",
            "Don't worry if you get stuck at some point along the way---just go back and\n",
            "reread the previous section, and try writing down and working through some\n",
            "examples. And if you're still stuck, we're happy to answer your questions in\n",
            "the Theory category at forums.fast.ai. Note: There is a reference section at\n",
            "the end of the paper summarizing all the key matrix calculus rules and\n",
            "terminology discussed here. See related articles at http://explained.ai\n",
            "\n",
            "1131. Title: The Effect of Intrinsic Dataset Properties on Generalization: Unraveling Learning Differences Between Natural and Medical Images\n",
            "   Abstract: While large language models based on the transformer architecture have\n",
            "demonstrated remarkable in-context learning (ICL) capabilities, understandings\n",
            "of such capabilities are still in an early stage, where existing theory and\n",
            "mechanistic understanding focus mostly on simple scenarios such as learning\n",
            "simple function classes. This paper takes initial steps on understanding ICL in\n",
            "more complex scenarios, by studying learning with representations. Concretely,\n",
            "we construct synthetic in-context learning problems with a compositional\n",
            "structure, where the label depends on the input through a possibly complex but\n",
            "fixed representation function, composed with a linear function that differs in\n",
            "each instance. By construction, the optimal ICL algorithm first transforms the\n",
            "inputs by the representation function, and then performs linear ICL on top of\n",
            "the transformed dataset. We show theoretically the existence of transformers\n",
            "that approximately implement such algorithms with mild depth and size.\n",
            "Empirically, we find trained transformers consistently achieve near-optimal ICL\n",
            "performance in this setting, and exhibit the desired dissection where lower\n",
            "layers transforms the dataset and upper layers perform linear ICL. Through\n",
            "extensive probing and a new pasting experiment, we further reveal several\n",
            "mechanisms within the trained transformers, such as concrete copying behaviors\n",
            "on both the inputs and the representations, linear ICL capability of the upper\n",
            "layers alone, and a post-ICL representation selection mechanism in a harder\n",
            "mixture setting. These observed mechanisms align well with our theory and may\n",
            "shed light on how transformers perform ICL in more realistic scenarios.\n",
            "\n",
            "1132. Title: Parameter-Efficient Multi-Task Model Fusion with Partial Linearization\n",
            "   Abstract: Mixture-of-experts (MoE) models that employ sparse activation have\n",
            "demonstrated effectiveness in significantly increasing the number of parameters\n",
            "while maintaining low computational requirements per token. However, recent\n",
            "studies have established that MoE models are inherently parameter-inefficient\n",
            "as the improvement in performance diminishes with an increasing number of\n",
            "experts. We hypothesize this parameter inefficiency is a result of all experts\n",
            "having equal capacity, which may not adequately meet the varying complexity\n",
            "requirements of different tokens or tasks. In light of this, we propose\n",
            "Stratified Mixture of Experts (SMoE) models, which feature a stratified\n",
            "structure and can assign dynamic capacity to different tokens. We demonstrate\n",
            "the effectiveness of SMoE on three multilingual machine translation benchmarks,\n",
            "containing 4, 15, and 94 language pairs, respectively. We show that SMoE\n",
            "outperforms multiple state-of-the-art MoE models with the same or fewer\n",
            "parameters.\n",
            "\n",
            "1133. Title: Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM\n",
            "   Abstract: We present Spectron, a novel approach to adapting pre-trained large language\n",
            "models (LLMs) to perform spoken question answering (QA) and speech\n",
            "continuation. By endowing the LLM with a pre-trained speech encoder, our model\n",
            "becomes able to take speech inputs and generate speech outputs. The entire\n",
            "system is trained end-to-end and operates directly on spectrograms, simplifying\n",
            "our architecture. Key to our approach is a training objective that jointly\n",
            "supervises speech recognition, text continuation, and speech synthesis using\n",
            "only paired speech-text pairs, enabling a `cross-modal' chain-of-thought within\n",
            "a single decoding pass. Our method surpasses existing spoken language models in\n",
            "speaker preservation and semantic coherence. Furthermore, the proposed model\n",
            "improves upon direct initialization in retaining the knowledge of the original\n",
            "LLM as demonstrated through spoken QA datasets. We release our audio samples\n",
            "(https://michelleramanovich.github.io/spectron/spectron) and spoken QA dataset\n",
            "(https://github.com/google-research-datasets/LLAMA1-Test-Set).\n",
            "\n",
            "1134. Title: Safe Offline Reinforcement Learning with Feasibility-Guided Diffusion Model\n",
            "   Abstract: Training-free Vision Transformer (ViT) architecture search is presented to\n",
            "search for a better ViT with zero-cost proxies. While ViTs achieve significant\n",
            "distillation gains from CNN teacher models on small datasets, the current\n",
            "zero-cost proxies in ViTs do not generalize well to the distillation training\n",
            "paradigm according to our experimental observations. In this paper, for the\n",
            "first time, we investigate how to search in a training-free manner with the\n",
            "help of teacher models and devise an effective Training-free ViT (TVT) search\n",
            "framework. Firstly, we observe that the similarity of attention maps between\n",
            "ViT and ConvNet teachers affects distill accuracy notably. Thus, we present a\n",
            "teacher-aware metric conditioned on the feature attention relations between\n",
            "teacher and student. Additionally, TVT employs the L2-Norm of the student's\n",
            "weights as the student-capability metric to improve ranking consistency.\n",
            "Finally, TVT searches for the best ViT for distilling with ConvNet teachers via\n",
            "our teacher-aware metric and student-capability metric, resulting in impressive\n",
            "gains in efficiency and effectiveness. Extensive experiments on various tiny\n",
            "datasets and search spaces show that our TVT outperforms state-of-the-art\n",
            "training-free search methods. The code will be released.\n",
            "\n",
            "1135. Title: Solving Inverse Problems with Latent Diffusion Models via Hard Data Consistency\n",
            "   Abstract: As a natural extension to the standard conformal prediction method, several\n",
            "conformal risk control methods have been recently developed and applied to\n",
            "various learning problems. In this work, we seek to control the conformal risk\n",
            "in expectation for ordinal classification tasks, which have broad applications\n",
            "to many real problems. For this purpose, we firstly formulated the ordinal\n",
            "classification task in the conformal risk control framework, and provided\n",
            "theoretic risk bounds of the risk control method. Then we proposed two types of\n",
            "loss functions specially designed for ordinal classification tasks, and\n",
            "developed corresponding algorithms to determine the prediction set for each\n",
            "case to control their risks at a desired level. We demonstrated the\n",
            "effectiveness of our proposed methods, and analyzed the difference between the\n",
            "two types of risks on three different datasets, including a simulated dataset,\n",
            "the UTKFace dataset and the diabetic retinopathy detection dataset.\n",
            "\n",
            "1136. Title: Language Models Represent Space and Time\n",
            "   Abstract: This paper investigates discrepancies in how neural networks learn from\n",
            "different imaging domains, which are commonly overlooked when adopting computer\n",
            "vision techniques from the domain of natural images to other specialized\n",
            "domains such as medical images. Recent works have found that the generalization\n",
            "error of a trained network typically increases with the intrinsic dimension\n",
            "($d_{data}$) of its training set. Yet, the steepness of this relationship\n",
            "varies significantly between medical (radiological) and natural imaging\n",
            "domains, with no existing theoretical explanation. We address this gap in\n",
            "knowledge by establishing and empirically validating a generalization scaling\n",
            "law with respect to $d_{data}$, and propose that the substantial scaling\n",
            "discrepancy between the two considered domains may be at least partially\n",
            "attributed to the higher intrinsic ``label sharpness'' ($K_\\mathcal{F}$) of\n",
            "medical imaging datasets, a metric which we propose. Next, we demonstrate an\n",
            "additional benefit of measuring the label sharpness of a training set: it is\n",
            "negatively correlated with the trained model's adversarial robustness, which\n",
            "notably leads to models for medical images having a substantially higher\n",
            "vulnerability to adversarial attack. Finally, we extend our $d_{data}$\n",
            "formalism to the related metric of learned representation intrinsic dimension\n",
            "($d_{repr}$), derive a generalization scaling law with respect to $d_{repr}$,\n",
            "and show that $d_{data}$ serves as an upper bound for $d_{repr}$. Our\n",
            "theoretical results are supported by thorough experiments with six models and\n",
            "eleven natural and medical imaging datasets over a range of training set sizes.\n",
            "Our findings offer insights into the influence of intrinsic dataset properties\n",
            "on generalization, representation learning, and robustness in deep neural\n",
            "networks. Code link: https://github.com/mazurowski-lab/intrinsic-properties\n",
            "\n",
            "1137. Title: Compressing Latent Space via Least Volume\n",
            "   Abstract: Constrained reinforcement learning (RL) seeks high-performance policies under\n",
            "safety constraints. We focus on an offline setting where the agent has only a\n",
            "fixed dataset -- common in realistic tasks to prevent unsafe exploration. To\n",
            "address this, we propose Diffusion-Regularized Constrained Offline\n",
            "Reinforcement Learning (DRCORL), which first uses a diffusion model to capture\n",
            "the behavioral policy from offline data and then extracts a simplified policy\n",
            "to enable efficient inference. We further apply gradient manipulation for\n",
            "safety adaptation, balancing the reward objective and constraint satisfaction.\n",
            "This approach leverages high-quality offline data while incorporating safety\n",
            "requirements. Empirical results show that DRCORL achieves reliable safety\n",
            "performance, fast inference, and strong reward outcomes across robot learning\n",
            "tasks. Compared to existing safe offline RL methods, it consistently meets cost\n",
            "limits and performs well with the same hyperparameters, indicating practical\n",
            "applicability in real-world scenarios.\n",
            "\n",
            "1138. Title: IRAD: Implicit Representation-driven Image Resampling against Adversarial Attacks\n",
            "   Abstract: Humans live in a 3D world and commonly use natural language to interact with\n",
            "a 3D scene. Modeling a 3D language field to support open-ended language queries\n",
            "in 3D has gained increasing attention recently. This paper introduces\n",
            "LangSplat, which constructs a 3D language field that enables precise and\n",
            "efficient open-vocabulary querying within 3D spaces. Unlike existing methods\n",
            "that ground CLIP language embeddings in a NeRF model, LangSplat advances the\n",
            "field by utilizing a collection of 3D Gaussians, each encoding language\n",
            "features distilled from CLIP, to represent the language field. By employing a\n",
            "tile-based splatting technique for rendering language features, we circumvent\n",
            "the costly rendering process inherent in NeRF. Instead of directly learning\n",
            "CLIP embeddings, LangSplat first trains a scene-wise language autoencoder and\n",
            "then learns language features on the scene-specific latent space, thereby\n",
            "alleviating substantial memory demands imposed by explicit modeling. Existing\n",
            "methods struggle with imprecise and vague 3D language fields, which fail to\n",
            "discern clear boundaries between objects. We delve into this issue and propose\n",
            "to learn hierarchical semantics using SAM, thereby eliminating the need for\n",
            "extensively querying the language field across various scales and the\n",
            "regularization of DINO features. Extensive experimental results show that\n",
            "LangSplat significantly outperforms the previous state-of-the-art method LERF\n",
            "by a large margin. Notably, LangSplat is extremely efficient, achieving a 199\n",
            "$\\times$ speedup compared to LERF at the resolution of 1440 $\\times$ 1080. We\n",
            "strongly recommend readers to check out our video results at\n",
            "https://langsplat.github.io/\n",
            "\n",
            "1139. Title: LitCab: Lightweight Language Model Calibration over Short- and Long-form Responses\n",
            "   Abstract: This paper introduces Least Volume-a simple yet effective regularization\n",
            "inspired by geometric intuition-that can reduce the necessary number of latent\n",
            "dimensions needed by an autoencoder without requiring any prior knowledge of\n",
            "the intrinsic dimensionality of the dataset. We show that the Lipschitz\n",
            "continuity of the decoder is the key to making it work, provide a proof that\n",
            "PCA is just a linear special case of it, and reveal that it has a similar\n",
            "PCA-like importance ordering effect when applied to nonlinear models. We\n",
            "demonstrate the intuition behind the regularization on some pedagogical toy\n",
            "problems, and its effectiveness on several benchmark problems, including MNIST,\n",
            "CIFAR-10 and CelebA.\n",
            "\n",
            "1140. Title: BioBridge: Bridging Biomedical Foundation Models via Knowledge Graphs\n",
            "   Abstract: In lifelong learning, tasks (or classes) to be learned arrive sequentially\n",
            "over time in arbitrary order. During training, knowledge from previous tasks\n",
            "can be captured and transferred to subsequent ones to improve sample\n",
            "efficiency. We consider the setting where all target tasks can be represented\n",
            "in the span of a small number of unknown linear or nonlinear features of the\n",
            "input data. We propose a lifelong learning algorithm that maintains and refines\n",
            "the internal feature representation. We prove that for any desired accuracy on\n",
            "all tasks, the dimension of the representation remains close to that of the\n",
            "underlying representation. The resulting sample complexity improves\n",
            "significantly on existing bounds. In the setting of linear features, our\n",
            "algorithm is provably efficient and the sample complexity for input dimension\n",
            "$d$, $m$ tasks with $k$ features up to error $\\epsilon$ is\n",
            "$\\tilde{O}(dk^{1.5}/\\epsilon+km/\\epsilon)$. We also prove a matching lower\n",
            "bound for any lifelong learning algorithm that uses a single task learner as a\n",
            "black box. We complement our analysis with an empirical study, including a\n",
            "heuristic lifelong learning algorithm for deep neural networks. Our method\n",
            "performs favorably on challenging realistic image datasets compared to\n",
            "state-of-the-art continual learning methods.\n",
            "\n",
            "1141. Title: Task Planning for Visual Room Rearrangement under Partial Observability\n",
            "   Abstract: Diffusion models have recently emerged as powerful generative priors for\n",
            "solving inverse problems. However, training diffusion models in the pixel space\n",
            "are both data-intensive and computationally demanding, which restricts their\n",
            "applicability as priors for high-dimensional real-world data such as medical\n",
            "images. Latent diffusion models, which operate in a much lower-dimensional\n",
            "space, offer a solution to these challenges. However, incorporating latent\n",
            "diffusion models to solve inverse problems remains a challenging problem due to\n",
            "the nonlinearity of the encoder and decoder. To address these issues, we\n",
            "propose \\textit{ReSample}, an algorithm that can solve general inverse problems\n",
            "with pre-trained latent diffusion models. Our algorithm incorporates data\n",
            "consistency by solving an optimization problem during the reverse sampling\n",
            "process, a concept that we term as hard data consistency. Upon solving this\n",
            "optimization problem, we propose a novel resampling scheme to map the\n",
            "measurement-consistent sample back onto the noisy data manifold and\n",
            "theoretically demonstrate its benefits. Lastly, we apply our algorithm to solve\n",
            "a wide range of linear and nonlinear inverse problems in both natural and\n",
            "medical images, demonstrating that our approach outperforms existing\n",
            "state-of-the-art approaches, including those based on pixel-space diffusion\n",
            "models.\n",
            "\n",
            "1142. Title: Less is More: Fewer Interpretable Region via Submodular Subset Selection\n",
            "   Abstract: Object rearrangement in a multi-room setup should produce a reasonable plan\n",
            "that reduces the agent's overall travel and the number of steps. Recent\n",
            "state-of-the-art methods fail to produce such plans because they rely on\n",
            "explicit exploration for discovering unseen objects due to partial\n",
            "observability and a heuristic planner to sequence the actions for\n",
            "rearrangement. This paper proposes a novel hierarchical task planner to\n",
            "efficiently plan a sequence of actions to discover unseen objects and rearrange\n",
            "misplaced objects within an untidy house to achieve a desired tidy state. The\n",
            "proposed method introduces several novel techniques, including (i) a method for\n",
            "discovering unseen objects using commonsense knowledge from large language\n",
            "models, (ii) a collision resolution and buffer prediction method based on\n",
            "Cross-Entropy Method to handle blocked goal and swap cases, (iii) a directed\n",
            "spatial graph-based state space for scalability, and (iv) deep reinforcement\n",
            "learning (RL) for producing an efficient planner. The planner interleaves the\n",
            "discovery of unseen objects and rearrangement to minimize the number of steps\n",
            "taken and overall traversal of the agent. The paper also presents new metrics\n",
            "and a benchmark dataset called MoPOR to evaluate the effectiveness of the\n",
            "rearrangement planning in a multi-room setting. The experimental results\n",
            "demonstrate that the proposed method effectively addresses the multi-room\n",
            "rearrangement problem.\n",
            "\n",
            "1143. Title: Rethinking Branching on Exact Combinatorial Optimization Solver: The First Deep Symbolic Discovery Framework\n",
            "   Abstract: Image attribution algorithms aim to identify important regions that are\n",
            "highly relevant to model decisions. Although existing attribution solutions can\n",
            "effectively assign importance to target elements, they still face the following\n",
            "challenges: 1) existing attribution methods generate inaccurate small regions\n",
            "thus misleading the direction of correct attribution, and 2) the model cannot\n",
            "produce good attribution results for samples with wrong predictions. To address\n",
            "the above challenges, this paper re-models the above image attribution problem\n",
            "as a submodular subset selection problem, aiming to enhance model\n",
            "interpretability using fewer regions. To address the lack of attention to local\n",
            "regions, we construct a novel submodular function to discover more accurate\n",
            "small interpretation regions. To enhance the attribution effect for all\n",
            "samples, we also impose four different constraints on the selection of\n",
            "sub-regions, i.e., confidence, effectiveness, consistency, and collaboration\n",
            "scores, to assess the importance of various subsets. Moreover, our theoretical\n",
            "analysis substantiates that the proposed function is in fact submodular.\n",
            "Extensive experiments show that the proposed method outperforms SOTA methods on\n",
            "two face datasets (Celeb-A and VGG-Face2) and one fine-grained dataset\n",
            "(CUB-200-2011). For correctly predicted samples, the proposed method improves\n",
            "the Deletion and Insertion scores with an average of 4.9% and 2.5% gain\n",
            "relative to HSIC-Attribution. For incorrectly predicted samples, our method\n",
            "achieves gains of 81.0% and 18.4% compared to the HSIC-Attribution algorithm in\n",
            "the average highest confidence and Insertion score respectively. The code is\n",
            "released at https://github.com/RuoyuChen10/SMDL-Attribution.\n",
            "\n",
            "1144. Title: Debiasing Attention Mechanism in Transformer without Demographics\n",
            "   Abstract: Representation learning has been evolving from traditional supervised\n",
            "training to Contrastive Learning (CL) and Masked Image Modeling (MIM). Previous\n",
            "works have demonstrated their pros and cons in specific scenarios, i.e., CL and\n",
            "supervised pre-training excel at capturing longer-range global patterns and\n",
            "enabling better feature discrimination, while MIM can introduce more local and\n",
            "diverse attention across all transformer layers. In this paper, we explore how\n",
            "to obtain a model that combines their strengths. We start by examining previous\n",
            "feature distillation and mask feature reconstruction methods and identify their\n",
            "limitations. We find that their increasing diversity mainly derives from the\n",
            "asymmetric designs, but these designs may in turn compromise the discrimination\n",
            "ability. In order to better obtain both discrimination and diversity, we\n",
            "propose a simple but effective Hybrid Distillation strategy, which utilizes\n",
            "both the supervised/CL teacher and the MIM teacher to jointly guide the student\n",
            "model. Hybrid Distill imitates the token relations of the MIM teacher to\n",
            "alleviate attention collapse, as well as distills the feature maps of the\n",
            "supervised/CL teacher to enable discrimination. Furthermore, a progressive\n",
            "redundant token masking strategy is also utilized to reduce the distilling\n",
            "costs and avoid falling into local optima. Experiment results prove that Hybrid\n",
            "Distill can achieve superior performance on different benchmarks.\n",
            "\n",
            "1145. Title: ASID: Active Exploration for System Identification in Robotic Manipulation\n",
            "   Abstract: This paper takes on the problem of automatically identifying\n",
            "clinically-relevant patterns in medical datasets without compromising patient\n",
            "privacy. To achieve this goal, we treat datasets as a black box for both\n",
            "internal and external users of data that lets us handle clinical data queries\n",
            "directly and far more efficiently. The novelty of the approach lies in avoiding\n",
            "the data de-identification process often used as a means of preserving patient\n",
            "privacy. The implemented toolkit combines software engineering technologies\n",
            "such as Java EE and RESTful web services, to allow exchanging medical data in\n",
            "an unidentifiable XML format as well as restricting users to the need-to-know\n",
            "principle. Our technique also inhibits retrospective processing of data, such\n",
            "as attacks by an adversary on a medical dataset using advanced computational\n",
            "methods to reveal Protected Health Information (PHI). The approach is validated\n",
            "on an endoscopic reporting application based on openEHR and MST standards. From\n",
            "the usability perspective, the approach can be used to query datasets by\n",
            "clinical researchers, governmental or non-governmental organizations in\n",
            "monitoring health care services to improve quality of care.\n",
            "\n",
            "1146. Title: Analyzing and Improving Optimal-Transport-based Adversarial Networks\n",
            "   Abstract: End-to-end training of neural network solvers for graph combinatorial\n",
            "optimization problems such as the Travelling Salesperson Problem (TSP) have\n",
            "seen a surge of interest recently, but remain intractable and inefficient\n",
            "beyond graphs with few hundreds of nodes. While state-of-the-art\n",
            "learning-driven approaches for TSP perform closely to classical solvers when\n",
            "trained on trivially small sizes, they are unable to generalize the learnt\n",
            "policy to larger instances at practical scales. This work presents an\n",
            "end-to-end neural combinatorial optimization pipeline that unifies several\n",
            "recent papers in order to identify the inductive biases, model architectures\n",
            "and learning algorithms that promote generalization to instances larger than\n",
            "those seen in training. Our controlled experiments provide the first principled\n",
            "investigation into such zero-shot generalization, revealing that extrapolating\n",
            "beyond training data requires rethinking the neural combinatorial optimization\n",
            "pipeline, from network layers and learning paradigms to evaluation protocols.\n",
            "Additionally, we analyze recent advances in deep learning for routing problems\n",
            "through the lens of our pipeline and provide new directions to stimulate future\n",
            "research.\n",
            "\n",
            "1147. Title: Optimal Sample Complexity for Average Reward Markov Decision Processes\n",
            "   Abstract: Mixup is a data augmentation strategy that employs convex combinations of\n",
            "training instances and their respective labels to augment the robustness and\n",
            "calibration of deep neural networks. Despite its widespread adoption, the\n",
            "nuanced mechanisms that underpin its success are not entirely understood. The\n",
            "observed phenomenon of Neural Collapse, where the last-layer activations and\n",
            "classifier of deep networks converge to a simplex equiangular tight frame\n",
            "(ETF), provides a compelling motivation to explore whether mixup induces\n",
            "alternative geometric configurations and whether those could explain its\n",
            "success. In this study, we delve into the last-layer activations of training\n",
            "data for deep networks subjected to mixup, aiming to uncover insights into its\n",
            "operational efficacy. Our investigation, spanning various architectures and\n",
            "dataset pairs, reveals that mixup's last-layer activations predominantly\n",
            "converge to a distinctive configuration different than one might expect. In\n",
            "this configuration, activations from mixed-up examples of identical classes\n",
            "align with the classifier, while those from different classes delineate\n",
            "channels along the decision boundary. Moreover, activations in earlier layers\n",
            "exhibit patterns, as if trained with manifold mixup. These findings are\n",
            "unexpected, as mixed-up features are not simple convex combinations of feature\n",
            "class means (as one might get, for example, by training mixup with the mean\n",
            "squared error loss). By analyzing this distinctive geometric configuration, we\n",
            "elucidate the mechanisms by which mixup enhances model calibration. To further\n",
            "validate our empirical observations, we conduct a theoretical analysis under\n",
            "the assumption of an unconstrained features model, utilizing the mixup loss.\n",
            "Through this, we characterize and derive the optimal last-layer features under\n",
            "the assumption that the classifier forms a simplex ETF.\n",
            "\n",
            "1148. Title: Pushing Boundaries: Mixup's Influence on Neural Collapse\n",
            "   Abstract: Foundation models in language and vision have the ability to run inference on\n",
            "any textual and visual inputs thanks to the transferable representations such\n",
            "as a vocabulary of tokens in language. Knowledge graphs (KGs) have different\n",
            "entity and relation vocabularies that generally do not overlap. The key\n",
            "challenge of designing foundation models on KGs is to learn such transferable\n",
            "representations that enable inference on any graph with arbitrary entity and\n",
            "relation vocabularies. In this work, we make a step towards such foundation\n",
            "models and present ULTRA, an approach for learning universal and transferable\n",
            "graph representations. ULTRA builds relational representations as a function\n",
            "conditioned on their interactions. Such a conditioning strategy allows a\n",
            "pre-trained ULTRA model to inductively generalize to any unseen KG with any\n",
            "relation vocabulary and to be fine-tuned on any graph. Conducting link\n",
            "prediction experiments on 57 different KGs, we find that the zero-shot\n",
            "inductive inference performance of a single pre-trained ULTRA model on unseen\n",
            "graphs of various sizes is often on par or better than strong baselines trained\n",
            "on specific graphs. Fine-tuning further boosts the performance.\n",
            "\n",
            "1149. Title: Towards Foundation Models for Knowledge Graph Reasoning\n",
            "   Abstract: Vision Transformer (ViT) has recently gained significant attention in solving\n",
            "computer vision (CV) problems due to its capability of extracting informative\n",
            "features and modeling long-range dependencies through the attention mechanism.\n",
            "Whereas recent works have explored the trustworthiness of ViT, including its\n",
            "robustness and explainability, the issue of fairness has not yet been\n",
            "adequately addressed. We establish that the existing fairness-aware algorithms\n",
            "designed for CNNs do not perform well on ViT, which highlights the need to\n",
            "develop our novel framework via Debiased Self-Attention (DSA). DSA is a\n",
            "fairness-through-blindness approach that enforces ViT to eliminate spurious\n",
            "features correlated with the sensitive label for bias mitigation and\n",
            "simultaneously retain real features for target prediction. Notably, DSA\n",
            "leverages adversarial examples to locate and mask the spurious features in the\n",
            "input image patches with an additional attention weights alignment regularizer\n",
            "in the training objective to encourage learning real features for target\n",
            "prediction. Importantly, our DSA framework leads to improved fairness\n",
            "guarantees over prior works on multiple prediction tasks without compromising\n",
            "target prediction performance. Code is available at\n",
            "\\href{https://github.com/qiangyao1988/DSA}{https://github.com/qiangyao1988/DSA}.\n",
            "\n",
            "1150. Title: Jointly-Learned Exit and Inference for a Dynamic Neural Network\n",
            "   Abstract: By leveraging the data sample diversity, the early-exit network recently\n",
            "emerges as a prominent neural network architecture to accelerate the deep\n",
            "learning inference process. However, intermediate classifiers of the early\n",
            "exits introduce additional computation overhead, which is unfavorable for\n",
            "resource-constrained edge artificial intelligence (AI). In this paper, we\n",
            "propose an early exit prediction mechanism to reduce the on-device computation\n",
            "overhead in a device-edge co-inference system supported by early-exit networks.\n",
            "Specifically, we design a low-complexity module, namely the Exit Predictor, to\n",
            "guide some distinctly \"hard\" samples to bypass the computation of the early\n",
            "exits. Besides, considering the varying communication bandwidth, we extend the\n",
            "early exit prediction mechanism for latency-aware edge inference, which adapts\n",
            "the prediction thresholds of the Exit Predictor and the confidence thresholds\n",
            "of the early-exit network via a few simple regression models. Extensive\n",
            "experiment results demonstrate the effectiveness of the Exit Predictor in\n",
            "achieving a better tradeoff between accuracy and on-device computation overhead\n",
            "for early-exit networks. Besides, compared with the baseline methods, the\n",
            "proposed method for latency-aware edge inference attains higher inference\n",
            "accuracy under different bandwidth conditions.\n",
            "\n",
            "1151. Title: Dynamics-Informed Protein Design with Structure Conditioning\n",
            "   Abstract: We resolve the open question regarding the sample complexity of policy\n",
            "learning for maximizing the long-run average reward associated with a uniformly\n",
            "ergodic Markov decision process (MDP), assuming a generative model. In this\n",
            "context, the existing literature provides a sample complexity upper bound of\n",
            "$\\widetilde O(|S||A|t_{\\text{mix}}^2 \\epsilon^{-2})$ and a lower bound of\n",
            "$\\Omega(|S||A|t_{\\text{mix}} \\epsilon^{-2})$. In these expressions, $|S|$ and\n",
            "$|A|$ denote the cardinalities of the state and action spaces respectively,\n",
            "$t_{\\text{mix}}$ serves as a uniform upper limit for the total variation mixing\n",
            "times, and $\\epsilon$ signifies the error tolerance. Therefore, a notable gap\n",
            "of $t_{\\text{mix}}$ still remains to be bridged. Our primary contribution is\n",
            "the development of an estimator for the optimal policy of average reward MDPs\n",
            "with a sample complexity of $\\widetilde O(|S||A|t_{\\text{mix}}\\epsilon^{-2})$.\n",
            "This marks the first algorithm and analysis to reach the literature's lower\n",
            "bound. Our new algorithm draws inspiration from ideas in Li et al. (2020), Jin\n",
            "and Sidford (2021), and Wang et al. (2023). Additionally, we conduct numerical\n",
            "experiments to validate our theoretical findings.\n",
            "\n",
            "1152. Title: Foundation Model-oriented Robustness: Robust Image Model Evaluation with Pretrained Models\n",
            "   Abstract: Foundational models with billions of parameters which have been trained on\n",
            "large corpora of data have demonstrated non-trivial skills in a variety of\n",
            "domains. However, due to their monolithic structure, it is challenging and\n",
            "expensive to augment them or impart new skills. On the other hand, due to their\n",
            "adaptation abilities, several new instances of these models are being trained\n",
            "towards new domains and tasks. In this work, we study the problem of efficient\n",
            "and practical composition of existing foundation models with more specific\n",
            "models to enable newer capabilities. To this end, we propose CALM --\n",
            "Composition to Augment Language Models -- which introduces cross-attention\n",
            "between models to compose their representations and enable new capabilities.\n",
            "Salient features of CALM are: (i) Scales up LLMs on new tasks by 're-using'\n",
            "existing LLMs along with a few additional parameters and data, (ii) Existing\n",
            "model weights are kept intact, and hence preserves existing capabilities, and\n",
            "(iii) Applies to diverse domains and settings. We illustrate that augmenting\n",
            "PaLM2-S with a smaller model trained on low-resource languages results in an\n",
            "absolute improvement of up to 13\\% on tasks like translation into English and\n",
            "arithmetic reasoning for low-resource languages. Similarly, when PaLM2-S is\n",
            "augmented with a code-specific model, we see a relative improvement of 40\\%\n",
            "over the base model for code generation and explanation tasks -- on-par with\n",
            "fully fine-tuned counterparts.\n",
            "\n",
            "1153. Title: MuSR: Testing the Limits of Chain-of-thought with Multistep Soft Reasoning\n",
            "   Abstract: The core challenge of de novo protein design lies in creating proteins with\n",
            "specific functions or properties, guided by certain conditions. Current models\n",
            "explore to generate protein using structural and evolutionary guidance, which\n",
            "only provide indirect conditions concerning functions and properties. However,\n",
            "textual annotations of proteins, especially the annotations for protein\n",
            "domains, which directly describe the protein's high-level functionalities,\n",
            "properties, and their correlation with target amino acid sequences, remain\n",
            "unexplored in the context of protein design tasks. In this paper, we propose\n",
            "Protein-Annotation Alignment Generation, PAAG, a multi-modality protein design\n",
            "framework that integrates the textual annotations extracted from protein\n",
            "database for controllable generation in sequence space. Specifically, within a\n",
            "multi-level alignment module, PAAG can explicitly generate proteins containing\n",
            "specific domains conditioned on the corresponding domain annotations, and can\n",
            "even design novel proteins with flexible combinations of different kinds of\n",
            "annotations. Our experimental results underscore the superiority of the aligned\n",
            "protein representations from PAAG over 7 prediction tasks. Furthermore, PAAG\n",
            "demonstrates a significant increase in generation success rate (24.7% vs 4.7%\n",
            "in zinc finger, and 54.3% vs 22.0% in the immunoglobulin domain) in comparison\n",
            "to the existing model. We anticipate that PAAG will broaden the horizons of\n",
            "protein design by leveraging the knowledge from between textual annotation and\n",
            "proteins.\n",
            "\n",
            "1154. Title: Look, Remember and Reason: Grounded Reasoning in Videos with Language Models\n",
            "   Abstract: Communication compression, a technique aiming to reduce the information\n",
            "volume to be transmitted over the air, has gained great interests in Federated\n",
            "Learning (FL) for the potential of alleviating its communication overhead.\n",
            "However, communication compression brings forth new challenges in FL due to the\n",
            "interplay of compression-incurred information distortion and inherent\n",
            "characteristics of FL such as partial participation and data heterogeneity.\n",
            "Despite the recent development, the performance of compressed FL approaches has\n",
            "not been fully exploited. The existing approaches either cannot accommodate\n",
            "arbitrary data heterogeneity or partial participation, or require stringent\n",
            "conditions on compression.\n",
            "  In this paper, we revisit the seminal stochastic controlled averaging method\n",
            "by proposing an equivalent but more efficient/simplified formulation with\n",
            "halved uplink communication costs. Building upon this implementation, we\n",
            "propose two compressed FL algorithms, SCALLION and SCAFCOM, to support unbiased\n",
            "and biased compression, respectively. Both the proposed methods outperform the\n",
            "existing compressed FL methods in terms of communication and computation\n",
            "complexities. Moreover, SCALLION and SCAFCOM accommodates arbitrary data\n",
            "heterogeneity and do not make any additional assumptions on compression errors.\n",
            "Experiments show that SCALLION and SCAFCOM can match the performance of\n",
            "corresponding full-precision FL approaches with substantially reduced uplink\n",
            "communication, and outperform recent compressed FL methods under the same\n",
            "communication budget.\n",
            "\n",
            "1155. Title: Knowledge Fusion of Large Language Models\n",
            "   Abstract: A big convergence of model architectures across language, vision, speech, and\n",
            "multimodal is emerging. However, under the same name \"Transformers\", the above\n",
            "areas use different implementations for better performance, e.g.,\n",
            "Post-LayerNorm for BERT, and Pre-LayerNorm for GPT and vision Transformers. We\n",
            "call for the development of Foundation Transformer for true general-purpose\n",
            "modeling, which serves as a go-to architecture for various tasks and modalities\n",
            "with guaranteed training stability. In this work, we introduce a Transformer\n",
            "variant, named Magneto, to fulfill the goal. Specifically, we propose\n",
            "Sub-LayerNorm for good expressivity, and the initialization strategy\n",
            "theoretically derived from DeepNet for stable scaling up. Extensive experiments\n",
            "demonstrate its superior performance and better stability than the de facto\n",
            "Transformer variants designed for various applications, including language\n",
            "modeling (i.e., BERT, and GPT), machine translation, vision pretraining (i.e.,\n",
            "BEiT), speech recognition, and multimodal pretraining (i.e., BEiT-3).\n",
            "\n",
            "1156. Title: Stochastic Controlled Averaging for Federated Learning with Communication Compression\n",
            "   Abstract: Using unlabeled data to regularize the machine learning models has\n",
            "demonstrated promise for improving safety and reliability in detecting\n",
            "out-of-distribution (OOD) data. Harnessing the power of unlabeled in-the-wild\n",
            "data is non-trivial due to the heterogeneity of both in-distribution (ID) and\n",
            "OOD data. This lack of a clean set of OOD samples poses significant challenges\n",
            "in learning an optimal OOD classifier. Currently, there is a lack of research\n",
            "on formally understanding how unlabeled data helps OOD detection. This paper\n",
            "bridges the gap by introducing a new learning framework SAL (Separate And\n",
            "Learn) that offers both strong theoretical guarantees and empirical\n",
            "effectiveness. The framework separates candidate outliers from the unlabeled\n",
            "data and then trains an OOD classifier using the candidate outliers and the\n",
            "labeled ID data. Theoretically, we provide rigorous error bounds from the lens\n",
            "of separability and learnability, formally justifying the two components in our\n",
            "algorithm. Our theory shows that SAL can separate the candidate outliers with\n",
            "small error rates, which leads to a generalization guarantee for the learned\n",
            "OOD classifier. Empirically, SAL achieves state-of-the-art performance on\n",
            "common benchmarks, reinforcing our theoretical insights. Code is publicly\n",
            "available at https://github.com/deeplearning-wisc/sal.\n",
            "\n",
            "1157. Title: LLM Augmented LLMs: Expanding Capabilities through Composition\n",
            "   Abstract: Multi-modal language models (LM) have recently shown promising performance in\n",
            "high-level reasoning tasks on videos. However, existing methods still fall\n",
            "short in tasks like causal or compositional spatiotemporal reasoning over\n",
            "actions, in which model predictions need to be grounded in fine-grained\n",
            "low-level details, such as object motions and object interactions. In this\n",
            "work, we propose training an LM end-to-end on low-level surrogate tasks,\n",
            "including object detection, re-identification, and tracking, to endow the model\n",
            "with the required low-level visual capabilities. We show that a two-stream\n",
            "video encoder with spatiotemporal attention is effective at capturing the\n",
            "required static and motion-based cues in the video. By leveraging the LM's\n",
            "ability to perform the low-level surrogate tasks, we can cast reasoning in\n",
            "videos as the three-step process of Look, Remember, Reason wherein visual\n",
            "information is extracted using low-level visual skills step-by-step and then\n",
            "integrated to arrive at a final answer. We demonstrate the effectiveness of our\n",
            "framework on diverse visual reasoning tasks from the ACRE, CATER,\n",
            "Something-Else and STAR datasets. Our approach is trainable end-to-end and\n",
            "surpasses state-of-the-art task-specific methods across these tasks by a large\n",
            "margin.\n",
            "\n",
            "1158. Title: Self-Supervised High Dynamic Range Imaging with Multi-Exposure Images in Dynamic Scenes\n",
            "   Abstract: In this paper we provide a comprehensive introduction to knowledge graphs,\n",
            "which have recently garnered significant attention from both industry and\n",
            "academia in scenarios that require exploiting diverse, dynamic, large-scale\n",
            "collections of data. After some opening remarks, we motivate and contrast\n",
            "various graph-based data models and query languages that are used for knowledge\n",
            "graphs. We discuss the roles of schema, identity, and context in knowledge\n",
            "graphs. We explain how knowledge can be represented and extracted using a\n",
            "combination of deductive and inductive techniques. We summarise methods for the\n",
            "creation, enrichment, quality assessment, refinement, and publication of\n",
            "knowledge graphs. We provide an overview of prominent open knowledge graphs and\n",
            "enterprise knowledge graphs, their applications, and how they use the\n",
            "aforementioned techniques. We conclude with high-level future research\n",
            "directions for knowledge graphs.\n",
            "\n",
            "1159. Title: COPlanner: Plan to Roll Out Conservatively but to Explore Optimistically for Model-Based RL\n",
            "   Abstract: Crystal structures and the Bloch theorem play a fundamental role in condensed\n",
            "matter physics. We extend the static crystal to the dynamic \"space-time\"\n",
            "crystal characterized by the general intertwined space-time periodicities in\n",
            "$D+1$ dimensions, which include both the static crystal and the Floquet crystal\n",
            "as special cases. A new group structure dubbed \"space-time\" group is\n",
            "constructed to describe the discrete symmetries of space-time crystal. Compared\n",
            "to space and magnetic groups, space-time group is augmented by \"time-screw\"\n",
            "rotations and \"time-glide\" reflections involving fractional translations along\n",
            "the time direction. A complete classification of the 13 space-time groups in\n",
            "1+1D is performed. The Kramers-type degeneracy can arise from the glide\n",
            "time-reversal symmetry without the half-integer spinor structure, which\n",
            "constrains the winding number patterns of spectral dispersions. In 2+1D,\n",
            "non-symmorphic space-time symmetries enforce spectral degeneracies, leading to\n",
            "protected Floquet semi-metal states. Our work provides a general framework for\n",
            "further studying topological properties of the $D+1$ dimensional space-time\n",
            "crystal.\n",
            "\n",
            "1160. Title: MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language Feedback\n",
            "   Abstract: The field of imbalanced self-supervised learning, especially in the context\n",
            "of tabular data, has not been extensively studied. Existing research has\n",
            "predominantly focused on image datasets. This paper aims to fill this gap by\n",
            "examining the specific challenges posed by data imbalance in self-supervised\n",
            "learning in the domain of tabular data, with a primary focus on autoencoders.\n",
            "Autoencoders are widely employed for learning and constructing a new\n",
            "representation of a dataset, particularly for dimensionality reduction. They\n",
            "are also often used for generative model learning, as seen in variational\n",
            "autoencoders. When dealing with mixed tabular data, qualitative variables are\n",
            "often encoded using a one-hot encoder with a standard loss function (MSE or\n",
            "Cross Entropy). In this paper, we analyze the drawbacks of this approach,\n",
            "especially when categorical variables are imbalanced. We propose a novel metric\n",
            "to balance learning: a Multi-Supervised Balanced MSE. This approach reduces the\n",
            "reconstruction error by balancing the influence of variables. Finally, we\n",
            "empirically demonstrate that this new metric, compared to the standard MSE: i)\n",
            "outperforms when the dataset is imbalanced, especially when the learning\n",
            "process is insufficient, and ii) provides similar results in the opposite case.\n",
            "\n",
            "1161. Title: Deep Reinforcement Learning Guided Improvement Heuristic for Job Shop Scheduling\n",
            "   Abstract: Seven years ago, researchers proposed a postprocessing method to equalize the\n",
            "error rates of a model across different demographic groups. The work launched\n",
            "hundreds of papers purporting to improve over the postprocessing baseline. We\n",
            "empirically evaluate these claims through thousands of model evaluations on\n",
            "several tabular datasets. We find that the fairness-accuracy Pareto frontier\n",
            "achieved by postprocessing contains all other methods we were feasibly able to\n",
            "evaluate. In doing so, we address two common methodological errors that have\n",
            "confounded previous observations. One relates to the comparison of methods with\n",
            "different unconstrained base models. The other concerns methods achieving\n",
            "different levels of constraint relaxation. At the heart of our study is a\n",
            "simple idea we call unprocessing that roughly corresponds to the inverse of\n",
            "postprocessing. Unprocessing allows for a direct comparison of methods using\n",
            "different underlying models and levels of relaxation.\n",
            "\n",
            "1162. Title: Language-Informed Visual Concept Learning\n",
            "   Abstract: Recent studies in using deep reinforcement learning (DRL) to solve Job-shop\n",
            "scheduling problems (JSSP) focus on construction heuristics. However, their\n",
            "performance is still far from optimality, mainly because the underlying graph\n",
            "representation scheme is unsuitable for modelling partial solutions at each\n",
            "construction step. This paper proposes a novel DRL-guided improvement heuristic\n",
            "for solving JSSP, where graph representation is employed to encode complete\n",
            "solutions. We design a Graph Neural-Network-based representation scheme,\n",
            "consisting of two modules to effectively capture the information of dynamic\n",
            "topology and different types of nodes in graphs encountered during the\n",
            "improvement process. To speed up solution evaluation during improvement, we\n",
            "present a novel message-passing mechanism that can evaluate multiple solutions\n",
            "simultaneously. We prove that the computational complexity of our method scales\n",
            "linearly with problem size. Experiments on classic benchmarks show that the\n",
            "improvement policy learned by our method outperforms state-of-the-art DRL-based\n",
            "methods by a large margin.\n",
            "\n",
            "1163. Title: Statistical Perspective of Top-K Sparse Softmax Gating Mixture of Experts\n",
            "   Abstract: Pruning encompasses a range of techniques aimed at increasing the sparsity of\n",
            "neural networks (NNs). These techniques can generally be framed as minimizing a\n",
            "loss function subject to an $L_0$-norm constraint. This paper introduces\n",
            "CoNNect, a novel differentiable regularizer for sparse NN training that ensures\n",
            "connectivity between input and output layers. CoNNect integrates with\n",
            "established pruning strategies and supports both structured and unstructured\n",
            "pruning. We proof that CoNNect approximates $L_0$-regularization, guaranteeing\n",
            "maximally connected network structures while avoiding issues like layer\n",
            "collapse. Numerical experiments demonstrate that CoNNect improves classical\n",
            "pruning strategies and enhances state-of-the-art one-shot pruners, such as\n",
            "DepGraph and LLM-pruner.\n",
            "\n",
            "1164. Title: Fair Classifiers that Abstain without Harm\n",
            "   Abstract: Do vision-language models (VLMs) pre-trained to caption an image of a\n",
            "\"durian\" learn visual concepts such as \"brown\" (color) and \"spiky\" (texture) at\n",
            "the same time? We aim to answer this question as visual concepts learned \"for\n",
            "free\" would enable wide applications such as neuro-symbolic reasoning or\n",
            "human-interpretable object classification. We assume that the visual concepts,\n",
            "if captured by pre-trained VLMs, can be extracted by their vision-language\n",
            "interface with text-based concept prompts. We observe that recent works\n",
            "prompting VLMs with concepts often differ in their strategies to define and\n",
            "evaluate the visual concepts, leading to conflicting conclusions. We propose a\n",
            "new concept definition strategy based on two observations: First, certain\n",
            "concept prompts include shortcuts that recognize correct concepts for wrong\n",
            "reasons; Second, multimodal information (e.g. visual discriminativeness, and\n",
            "textual knowledge) should be leveraged when selecting the concepts. Our\n",
            "proposed concept discovery and learning (CDL) framework is thus designed to\n",
            "identify a diverse list of generic visual concepts (e.g. \"spiky\" as opposed to\n",
            "\"spiky durian\"), which are ranked and selected based on visual and language\n",
            "mutual information. We carefully design quantitative and human evaluations of\n",
            "the discovered concepts on six diverse visual recognition datasets, which\n",
            "confirm that pre-trained VLMs do learn visual concepts that provide accurate\n",
            "and thorough descriptions for the recognized objects. All code and models are\n",
            "publicly released.\n",
            "\n",
            "1165. Title: From Sparse to Soft Mixtures of Experts\n",
            "   Abstract: In critical applications, it is vital for classifiers to defer\n",
            "decision-making to humans. We propose a post-hoc method that makes existing\n",
            "classifiers selectively abstain from predicting certain samples. Our abstaining\n",
            "classifier is incentivized to maintain the original accuracy for each\n",
            "sub-population (i.e. no harm) while achieving a set of group fairness\n",
            "definitions to a user specified degree. To this end, we design an Integer\n",
            "Programming (IP) procedure that assigns abstention decisions for each training\n",
            "sample to satisfy a set of constraints. To generalize the abstaining decisions\n",
            "to test samples, we then train a surrogate model to learn the abstaining\n",
            "decisions based on the IP solutions in an end-to-end manner. We analyze the\n",
            "feasibility of the IP procedure to determine the possible abstention rate for\n",
            "different levels of unfairness tolerance and accuracy constraint for achieving\n",
            "no harm. To the best of our knowledge, this work is the first to identify the\n",
            "theoretical relationships between the constraint parameters and the required\n",
            "abstention rate. Our theoretical results are important since a high abstention\n",
            "rate is often infeasible in practice due to a lack of human resources. Our\n",
            "framework outperforms existing methods in terms of fairness disparity without\n",
            "sacrificing accuracy at similar abstention rates.\n",
            "\n",
            "1166. Title: C-TPT: Calibrated Test-Time Prompt Tuning for Vision-Language Models via Text Feature Dispersion\n",
            "   Abstract: It has long been established that predictive models can be transformed into\n",
            "lossless compressors and vice versa. Incidentally, in recent years, the machine\n",
            "learning community has focused on training increasingly large and powerful\n",
            "self-supervised (language) models. Since these large language models exhibit\n",
            "impressive predictive capabilities, they are well-positioned to be strong\n",
            "compressors. In this work, we advocate for viewing the prediction problem\n",
            "through the lens of compression and evaluate the compression capabilities of\n",
            "large (foundation) models. We show that large language models are powerful\n",
            "general-purpose predictors and that the compression viewpoint provides novel\n",
            "insights into scaling laws, tokenization, and in-context learning. For example,\n",
            "Chinchilla 70B, while trained primarily on text, compresses ImageNet patches to\n",
            "43.4% and LibriSpeech samples to 16.4% of their raw size, beating\n",
            "domain-specific compressors like PNG (58.5%) or FLAC (30.3%), respectively.\n",
            "Finally, we show that the prediction-compression equivalence allows us to use\n",
            "any compressor (like gzip) to build a conditional generative model.\n",
            "\n",
            "1167. Title: Diagnosing Transformers: Illuminating Feature Spaces for Clinical Decision-Making\n",
            "   Abstract: Sparse mixture of expert architectures (MoEs) scale model capacity without\n",
            "significant increases in training or inference costs. Despite their success,\n",
            "MoEs suffer from a number of issues: training instability, token dropping,\n",
            "inability to scale the number of experts, or ineffective finetuning. In this\n",
            "work, we propose Soft MoE, a fully-differentiable sparse Transformer that\n",
            "addresses these challenges, while maintaining the benefits of MoEs. Soft MoE\n",
            "performs an implicit soft assignment by passing different weighted combinations\n",
            "of all input tokens to each expert. As in other MoEs, experts in Soft MoE only\n",
            "process a subset of the (combined) tokens, enabling larger model capacity (and\n",
            "performance) at lower inference cost. In the context of visual recognition,\n",
            "Soft MoE greatly outperforms dense Transformers (ViTs) and popular MoEs (Tokens\n",
            "Choice and Experts Choice). Furthermore, Soft MoE scales well: Soft MoE Huge/14\n",
            "with 128 experts in 16 MoE layers has over 40x more parameters than ViT\n",
            "Huge/14, with only 2% increased inference time, and substantially better\n",
            "quality.\n",
            "\n",
            "1168. Title: Deep Generative Clustering with Multimodal Diffusion Variational Autoencoders\n",
            "   Abstract: Top-K sparse softmax gating mixture of experts has been widely used for\n",
            "scaling up massive deep-learning architectures without increasing the\n",
            "computational cost. Despite its popularity in real-world applications, the\n",
            "theoretical understanding of that gating function has remained an open problem.\n",
            "The main challenge comes from the structure of the top-K sparse softmax gating\n",
            "function, which partitions the input space into multiple regions with distinct\n",
            "behaviors. By focusing on a Gaussian mixture of experts, we establish\n",
            "theoretical results on the effects of the top-K sparse softmax gating function\n",
            "on both density and parameter estimations. Our results hinge upon defining\n",
            "novel loss functions among parameters to capture different behaviors of the\n",
            "input regions. When the true number of experts $k_{\\ast}$ is known, we\n",
            "demonstrate that the convergence rates of density and parameter estimations are\n",
            "both parametric on the sample size. However, when $k_{\\ast}$ becomes unknown\n",
            "and the true model is over-specified by a Gaussian mixture of $k$ experts where\n",
            "$k > k_{\\ast}$, our findings suggest that the number of experts selected from\n",
            "the top-K sparse softmax gating function must exceed the total cardinality of a\n",
            "certain number of Voronoi cells associated with the true parameters to\n",
            "guarantee the convergence of the density estimation. Moreover, while the\n",
            "density estimation rate remains parametric under this setting, the parameter\n",
            "estimation rates become substantially slow due to an intrinsic interaction\n",
            "between the softmax gating and expert functions.\n",
            "\n",
            "1169. Title: BECLR: Batch Enhanced Contrastive Few-Shot Learning\n",
            "   Abstract: Pre-trained transformers are often fine-tuned to aid clinical decision-making\n",
            "using limited clinical notes. Model interpretability is crucial, especially in\n",
            "high-stakes domains like medicine, to establish trust and ensure safety, which\n",
            "requires human engagement. We introduce SUFO, a systematic framework that\n",
            "enhances interpretability of fine-tuned transformer feature spaces. SUFO\n",
            "utilizes a range of analytic and visualization techniques, including Supervised\n",
            "probing, Unsupervised similarity analysis, Feature dynamics, and Outlier\n",
            "analysis to address key questions about model trust and interpretability. We\n",
            "conduct a case study investigating the impact of pre-training data where we\n",
            "focus on real-world pathology classification tasks, and validate our findings\n",
            "on MedNLI. We evaluate five 110M-sized pre-trained transformer models,\n",
            "categorized into general-domain (BERT, TNLR), mixed-domain (BioBERT, Clinical\n",
            "BioBERT), and domain-specific (PubMedBERT) groups. Our SUFO analyses reveal\n",
            "that: (1) while PubMedBERT, the domain-specific model, contains valuable\n",
            "information for fine-tuning, it can overfit to minority classes when class\n",
            "imbalances exist. In contrast, mixed-domain models exhibit greater resistance\n",
            "to overfitting, suggesting potential improvements in domain-specific model\n",
            "robustness; (2) in-domain pre-training accelerates feature disambiguation\n",
            "during fine-tuning; and (3) feature spaces undergo significant sparsification\n",
            "during this process, enabling clinicians to identify common outlier modes among\n",
            "fine-tuned models as demonstrated in this paper. These findings showcase the\n",
            "utility of SUFO in enhancing trust and safety when using transformers in\n",
            "medicine, and we believe SUFO can aid practitioners in evaluating fine-tuned\n",
            "language models for other applications in medicine and in more critical\n",
            "domains.\n",
            "\n",
            "1170. Title: Task structure and nonlinearity jointly determine learned representational geometry\n",
            "   Abstract: Embracing the deep learning techniques for representation learning in\n",
            "clustering research has attracted broad attention in recent years, yielding a\n",
            "newly developed clustering paradigm, viz. the deep clustering (DC). Typically,\n",
            "the DC models capitalize on autoencoders to learn the intrinsic features which\n",
            "facilitate the clustering process in consequence. Nowadays, a generative model\n",
            "named variational autoencoder (VAE) has got wide acceptance in DC studies.\n",
            "Nevertheless, the plain VAE is insufficient to perceive the comprehensive\n",
            "latent features, leading to the deteriorative clustering performance. In this\n",
            "paper, a novel DC method is proposed to address this issue. Specifically, the\n",
            "generative adversarial network and VAE are coalesced into a new autoencoder\n",
            "called fusion autoencoder (FAE) for discerning more discriminative\n",
            "representation that benefits the downstream clustering task. Besides, the FAE\n",
            "is implemented with the deep residual network architecture which further\n",
            "enhances the representation learning ability. Finally, the latent space of the\n",
            "FAE is transformed to an embedding space shaped by a deep dense neural network\n",
            "for pulling away different clusters from each other and collapsing data points\n",
            "within individual clusters. Experiment conducted on several image datasets\n",
            "demonstrate the effectiveness of the proposed DC model against the baseline\n",
            "methods.\n",
            "\n",
            "1171. Title: FasterViT: Fast Vision Transformers with Hierarchical Attention\n",
            "   Abstract: The utility of a learned neural representation depends on how well its\n",
            "geometry supports performance in downstream tasks. This geometry depends on the\n",
            "structure of the inputs, the structure of the target outputs, and the\n",
            "architecture of the network. By studying the learning dynamics of networks with\n",
            "one hidden layer, we discovered that the network's activation function has an\n",
            "unexpectedly strong impact on the representational geometry: Tanh networks tend\n",
            "to learn representations that reflect the structure of the target outputs,\n",
            "while ReLU networks retain more information about the structure of the raw\n",
            "inputs. This difference is consistently observed across a broad class of\n",
            "parameterized tasks in which we modulated the degree of alignment between the\n",
            "geometry of the task inputs and that of the task labels. We analyzed the\n",
            "learning dynamics in weight space and show how the differences between the\n",
            "networks with Tanh and ReLU nonlinearities arise from the asymmetric asymptotic\n",
            "behavior of ReLU, which leads feature neurons to specialize for different\n",
            "regions of input space. By contrast, feature neurons in Tanh networks tend to\n",
            "inherit the task label structure. Consequently, when the target outputs are low\n",
            "dimensional, Tanh networks generate neural representations that are more\n",
            "disentangled than those obtained with a ReLU nonlinearity. Our findings shed\n",
            "light on the interplay between input-output geometry, nonlinearity, and learned\n",
            "representations in neural networks.\n",
            "\n",
            "1172. Title: Lifting Architectural Constraints of Injective Flows\n",
            "   Abstract: Large Language Models (LLMs) are often augmented with external information as\n",
            "contexts, but this external information can sometimes be inaccurate or even\n",
            "intentionally misleading. We argue that robust LLMs should demonstrate situated\n",
            "faithfulness, dynamically calibrating their trust in external information based\n",
            "on their confidence in the internal knowledge and the external context. To\n",
            "benchmark this capability, we evaluate LLMs across several QA datasets,\n",
            "including a newly created dataset called RedditQA featuring in-the-wild\n",
            "incorrect contexts sourced from Reddit posts. We show that when provided with\n",
            "both correct and incorrect contexts, both open-source and proprietary models\n",
            "tend to overly rely on external information, regardless of its factual\n",
            "accuracy. To enhance situated faithfulness, we propose two approaches:\n",
            "Self-Guided Confidence Reasoning (SCR) and Rule-Based Confidence Reasoning\n",
            "(RCR). SCR enables models to self-access the confidence of external information\n",
            "relative to their own internal knowledge to produce the most accurate answer.\n",
            "RCR, in contrast, extracts explicit confidence signals from the LLM and\n",
            "determines the final answer using predefined rules. Our results show that for\n",
            "LLMs with strong reasoning capabilities, such as GPT-4o and GPT-4o mini, SCR\n",
            "outperforms RCR, achieving improvements of up to 24.2% over a direct input\n",
            "augmentation baseline. Conversely, for a smaller model like Llama-3-8B, RCR\n",
            "outperforms SCR. Fine-tuning SCR with our proposed Confidence Reasoning Direct\n",
            "Preference Optimization (CR-DPO) method improves performance on both seen and\n",
            "unseen datasets, yielding an average improvement of 8.9% on Llama-3-8B. In\n",
            "addition to quantitative results, we offer insights into the relative strengths\n",
            "of SCR and RCR. Our findings highlight promising avenues for improving situated\n",
            "faithfulness in LLMs. The data and code are released.\n",
            "\n",
            "1173. Title: Faithful Rule Extraction for Differentiable Rule Learning Models\n",
            "   Abstract: Normalizing Flows explicitly maximize a full-dimensional likelihood on the\n",
            "training data. However, real data is typically only supported on a\n",
            "lower-dimensional manifold leading the model to expend significant compute on\n",
            "modeling noise. Injective Flows fix this by jointly learning a manifold and the\n",
            "distribution on it. So far, they have been limited by restrictive architectures\n",
            "and/or high computational cost. We lift both constraints by a new efficient\n",
            "estimator for the maximum likelihood loss, compatible with free-form bottleneck\n",
            "architectures. We further show that naively learning both the data manifold and\n",
            "the distribution on it can lead to divergent solutions, and use this insight to\n",
            "motivate a stable maximum likelihood training objective. We perform extensive\n",
            "experiments on toy, tabular and image data, demonstrating the competitive\n",
            "performance of the resulting model.\n",
            "\n",
            "1174. Title: Selective Visual Representations Improve Convergence and Generalization for Embodied AI\n",
            "   Abstract: Recent works have showcased the ability of LLMs to embody diverse personas in\n",
            "their responses, exemplified by prompts like 'You are Yoda. Explain the Theory\n",
            "of Relativity.' While this ability allows personalization of LLMs and enables\n",
            "human behavior simulation, its effect on LLMs' capabilities remains unclear. To\n",
            "fill this gap, we present the first extensive study of the unintended\n",
            "side-effects of persona assignment on the ability of LLMs to perform basic\n",
            "reasoning tasks. Our study covers 24 reasoning datasets, 4 LLMs, and 19 diverse\n",
            "personas (e.g. an Asian person) spanning 5 socio-demographic groups. Our\n",
            "experiments unveil that LLMs harbor deep rooted bias against various\n",
            "socio-demographics underneath a veneer of fairness. While they overtly reject\n",
            "stereotypes when explicitly asked ('Are Black people less skilled at\n",
            "mathematics?'), they manifest stereotypical and erroneous presumptions when\n",
            "asked to answer questions while adopting a persona. These can be observed as\n",
            "abstentions in responses, e.g., 'As a Black person, I can't answer this\n",
            "question as it requires math knowledge', and generally result in a substantial\n",
            "performance drop. Our experiments with ChatGPT-3.5 show that this bias is\n",
            "ubiquitous - 80% of our personas demonstrate bias; it is significant - some\n",
            "datasets show performance drops of 70%+; and can be especially harmful for\n",
            "certain groups - some personas suffer statistically significant drops on 80%+\n",
            "of the datasets. Overall, all 4 LLMs exhibit this bias to varying extents, with\n",
            "GPT-4-Turbo showing the least but still a problematic amount of bias (evident\n",
            "in 42% of the personas). Further analysis shows that these persona-induced\n",
            "errors can be hard-to-discern and hard-to-avoid. Our findings serve as a\n",
            "cautionary tale that the practice of assigning personas to LLMs - a trend on\n",
            "the rise - can surface their deep-rooted biases and have unforeseeable and\n",
            "detrimental side-effects.\n",
            "\n",
            "1175. Title: Bias Runs Deep: Implicit Reasoning Biases in Persona-Assigned LLMs\n",
            "   Abstract: Embodied AI models often employ off the shelf vision backbones like CLIP to\n",
            "encode their visual observations. Although such general purpose representations\n",
            "encode rich syntactic and semantic information about the scene, much of this\n",
            "information is often irrelevant to the specific task at hand. This introduces\n",
            "noise within the learning process and distracts the agent's focus from\n",
            "task-relevant visual cues. Inspired by selective attention in humans-the\n",
            "process through which people filter their perception based on their\n",
            "experiences, knowledge, and the task at hand-we introduce a parameter-efficient\n",
            "approach to filter visual stimuli for embodied AI. Our approach induces a\n",
            "task-conditioned bottleneck using a small learnable codebook module. This\n",
            "codebook is trained jointly to optimize task reward and acts as a\n",
            "task-conditioned selective filter over the visual observation. Our experiments\n",
            "showcase state-of-the-art performance for object goal navigation and object\n",
            "displacement across 5 benchmarks, ProcTHOR, ArchitecTHOR, RoboTHOR, AI2-iTHOR,\n",
            "and ManipulaTHOR. The filtered representations produced by the codebook are\n",
            "also able generalize better and converge faster when adapted to other\n",
            "simulation environments such as Habitat. Our qualitative analyses show that\n",
            "agents explore their environments more effectively and their representations\n",
            "retain task-relevant information like target object recognition while ignoring\n",
            "superfluous information about other objects. Code and pretrained models are\n",
            "available at our project website: https://embodied-codebook.github.io.\n",
            "\n",
            "1176. Title: On the Generalization and Approximation Capacities of Neural Controlled Differential Equations\n",
            "   Abstract: The conjoining of dynamical systems and deep learning has become a topic of\n",
            "great interest. In particular, neural differential equations (NDEs) demonstrate\n",
            "that neural networks and differential equation are two sides of the same coin.\n",
            "Traditional parameterised differential equations are a special case. Many\n",
            "popular neural network architectures, such as residual networks and recurrent\n",
            "networks, are discretisations.\n",
            "  NDEs are suitable for tackling generative problems, dynamical systems, and\n",
            "time series (particularly in physics, finance, ...) and are thus of interest to\n",
            "both modern machine learning and traditional mathematical modelling. NDEs offer\n",
            "high-capacity function approximation, strong priors on model space, the ability\n",
            "to handle irregular data, memory efficiency, and a wealth of available theory\n",
            "on both sides.\n",
            "  This doctoral thesis provides an in-depth survey of the field.\n",
            "  Topics include: neural ordinary differential equations (e.g. for hybrid\n",
            "neural/mechanistic modelling of physical systems); neural controlled\n",
            "differential equations (e.g. for learning functions of irregular time series);\n",
            "and neural stochastic differential equations (e.g. to produce generative models\n",
            "capable of representing complex stochastic dynamics, or sampling from complex\n",
            "high-dimensional distributions).\n",
            "  Further topics include: numerical methods for NDEs (e.g. reversible\n",
            "differential equations solvers, backpropagation through differential equations,\n",
            "Brownian reconstruction); symbolic regression for dynamical systems (e.g. via\n",
            "regularised evolution); and deep implicit models (e.g. deep equilibrium models,\n",
            "differentiable optimisation).\n",
            "  We anticipate this thesis will be of interest to anyone interested in the\n",
            "marriage of deep learning with dynamical systems, and hope it will provide a\n",
            "useful reference for the current state of the art.\n",
            "\n",
            "1177. Title: Reverse Diffusion Monte Carlo\n",
            "   Abstract: Recent efforts in fine-tuning language models often rely on automatic data\n",
            "selection, commonly using Nearest Neighbors retrieval from large datasets.\n",
            "However, we theoretically show that this approach tends to select redundant\n",
            "data, limiting its effectiveness or even hurting performance. To address this,\n",
            "we introduce SIFT, a data selection algorithm designed to reduce uncertainty\n",
            "about the model's response given a prompt, which unifies ideas from retrieval\n",
            "and active learning. Whereas Nearest Neighbor retrieval typically fails in the\n",
            "presence of information duplication, SIFT accounts for information duplication\n",
            "and optimizes the overall information gain of the selected examples. We focus\n",
            "our evaluations on fine-tuning at test-time for prompt-specific language\n",
            "modeling on the Pile dataset, and show that SIFT consistently outperforms\n",
            "Nearest Neighbor retrieval, with minimal computational overhead. Moreover, we\n",
            "show that our uncertainty estimates can predict the performance gain of\n",
            "test-time fine-tuning, and use this to develop an adaptive algorithm that\n",
            "invests test-time compute proportional to realized performance gains. We\n",
            "provide the $\\texttt{activeft}$ (Active Fine-Tuning) library which can be used\n",
            "as a drop-in replacement for Nearest Neighbor retrieval.\n",
            "\n",
            "1178. Title: Outliers with Opposing Signals Have an Outsized Effect on Neural Network Optimization\n",
            "   Abstract: We propose a Monte Carlo sampler from the reverse diffusion process. Unlike\n",
            "the practice of diffusion models, where the intermediary updates -- the score\n",
            "functions -- are learned with a neural network, we transform the score matching\n",
            "problem into a mean estimation one. By estimating the means of the regularized\n",
            "posterior distributions, we derive a novel Monte Carlo sampling algorithm\n",
            "called reverse diffusion Monte Carlo (rdMC), which is distinct from the Markov\n",
            "chain Monte Carlo (MCMC) methods. We determine the sample size from the error\n",
            "tolerance and the properties of the posterior distribution to yield an\n",
            "algorithm that can approximately sample the target distribution with any\n",
            "desired accuracy. Additionally, we demonstrate and prove under suitable\n",
            "conditions that sampling with rdMC can be significantly faster than that with\n",
            "MCMC. For multi-modal target distributions such as those in Gaussian mixture\n",
            "models, rdMC greatly improves over the Langevin-style MCMC sampling methods\n",
            "both theoretically and in practice. The proposed rdMC method offers a new\n",
            "perspective and solution beyond classical MCMC algorithms for the challenging\n",
            "complex distributions.\n",
            "\n",
            "1179. Title: Towards Aligned Layout Generation via Diffusion Model with Aesthetic Constraints\n",
            "   Abstract: We identify a new phenomenon in neural network optimization which arises from\n",
            "the interaction of depth and a particular heavy-tailed structure in natural\n",
            "data. Our result offers intuitive explanations for several previously reported\n",
            "observations about network training dynamics. In particular, it implies a\n",
            "conceptually new cause for progressive sharpening and the edge of stability; we\n",
            "also highlight connections to other concepts in optimization and generalization\n",
            "including grokking, simplicity bias, and Sharpness-Aware Minimization.\n",
            "  Experimentally, we demonstrate the significant influence of paired groups of\n",
            "outliers in the training data with strong opposing signals: consistent, large\n",
            "magnitude features which dominate the network output throughout training and\n",
            "provide gradients which point in opposite directions. Due to these outliers,\n",
            "early optimization enters a narrow valley which carefully balances the opposing\n",
            "groups; subsequent sharpening causes their loss to rise rapidly, oscillating\n",
            "between high on one group and then the other, until the overall loss spikes. We\n",
            "describe how to identify these groups, explore what sets them apart, and\n",
            "carefully study their effect on the network's optimization and behavior. We\n",
            "complement these experiments with a mechanistic explanation on a toy example of\n",
            "opposing signals and a theoretical analysis of a two-layer linear network on a\n",
            "simple model. Our finding enables new qualitative predictions of training\n",
            "behavior which we confirm experimentally. It also provides a new lens through\n",
            "which to study and improve modern training practices for stochastic\n",
            "optimization, which we highlight via a case study of Adam versus SGD.\n",
            "\n",
            "1180. Title: SE(3)-Stochastic Flow Matching for Protein Backbone Generation\n",
            "   Abstract: Controllable layout generation refers to the process of creating a plausible\n",
            "visual arrangement of elements within a graphic design (e.g., document and web\n",
            "designs) with constraints representing design intentions. Although recent\n",
            "diffusion-based models have achieved state-of-the-art FID scores, they tend to\n",
            "exhibit more pronounced misalignment compared to earlier transformer-based\n",
            "models. In this work, we propose the $\\textbf{LA}$yout $\\textbf{C}$onstraint\n",
            "diffusion mod$\\textbf{E}$l (LACE), a unified model to handle a broad range of\n",
            "layout generation tasks, such as arranging elements with specified attributes\n",
            "and refining or completing a coarse layout design. The model is based on\n",
            "continuous diffusion models. Compared with existing methods that use discrete\n",
            "diffusion models, continuous state-space design can enable the incorporation of\n",
            "differentiable aesthetic constraint functions in training. For conditional\n",
            "generation, we introduce conditions via masked input. Extensive experiment\n",
            "results show that LACE produces high-quality layouts and outperforms existing\n",
            "state-of-the-art baselines.\n",
            "\n",
            "1181. Title: ASMR: Activation-Sharing Multi-Resolution Coordinate Networks for Efficient Inference\n",
            "   Abstract: We present FrameFlow, a method for fast protein backbone generation using\n",
            "SE(3) flow matching. Specifically, we adapt FrameDiff, a state-of-the-art\n",
            "diffusion model, to the flow-matching generative modeling paradigm. We show how\n",
            "flow matching can be applied on SE(3) and propose modifications during training\n",
            "to effectively learn the vector field. Compared to FrameDiff, FrameFlow\n",
            "requires five times fewer sampling timesteps while achieving two fold better\n",
            "designability. The ability to generate high quality protein samples at a\n",
            "fraction of the cost of previous methods paves the way towards more efficient\n",
            "generative models in de novo protein design.\n",
            "\n",
            "1182. Title: What's in a Prior? Learned Proximal Networks for Inverse Problems\n",
            "   Abstract: Current vision-language generative models rely on expansive corpora of paired\n",
            "image-text data to attain optimal performance and generalization capabilities.\n",
            "However, automatically collecting such data (e.g. via large-scale web scraping)\n",
            "leads to low quality and poor image-text correlation, while human annotation is\n",
            "more accurate but requires significant manual effort and expense. We introduce\n",
            "$\\textbf{ITIT}$ ($\\textbf{I}$n$\\textbf{T}$egrating $\\textbf{I}$mage\n",
            "$\\textbf{T}$ext): an innovative training paradigm grounded in the concept of\n",
            "cycle consistency which allows vision-language training on unpaired image and\n",
            "text data. ITIT is comprised of a joint image-text encoder with disjoint image\n",
            "and text decoders that enable bidirectional image-to-text and text-to-image\n",
            "generation in a single framework. During training, ITIT leverages a small set\n",
            "of paired image-text data to ensure its output matches the input reasonably\n",
            "well in both directions. Simultaneously, the model is also trained on much\n",
            "larger datasets containing only images or texts. This is achieved by enforcing\n",
            "cycle consistency between the original unpaired samples and the cycle-generated\n",
            "counterparts. For instance, it generates a caption for a given input image and\n",
            "then uses the caption to create an output image, and enforces similarity\n",
            "between the input and output images. Our experiments show that ITIT with\n",
            "unpaired datasets exhibits similar scaling behavior as using high-quality\n",
            "paired data. We demonstrate image generation and captioning performance on par\n",
            "with state-of-the-art text-to-image and image-to-text models with orders of\n",
            "magnitude fewer (only 3M) paired image-text data.\n",
            "\n",
            "1183. Title: Leveraging Unpaired Data for Vision-Language Generative Models via Cycle Consistency\n",
            "   Abstract: We propose and theoretically analyze an approach for planning with an\n",
            "approximate model in reinforcement learning that can reduce the adverse impact\n",
            "of model error. If the model is accurate enough, it accelerates the convergence\n",
            "to the true value function too. One of its key components is the MaxEnt Model\n",
            "Correction (MoCo) procedure that corrects the model's next-state distributions\n",
            "based on a Maximum Entropy density estimation formulation. Based on MoCo, we\n",
            "introduce the Model Correcting Value Iteration (MoCoVI) algorithm, and its\n",
            "sampled-based variant MoCoDyna. We show that MoCoVI and MoCoDyna's convergence\n",
            "can be much faster than the conventional model-free algorithms. Unlike\n",
            "traditional model-based algorithms, MoCoVI and MoCoDyna effectively utilize an\n",
            "approximate model and still converge to the correct value function.\n",
            "\n",
            "1184. Title: Maximum Entropy Model Correction in Reinforcement Learning\n",
            "   Abstract: Dynamic Sparse Training (DST) methods achieve state-of-the-art results in\n",
            "sparse neural network training, matching the generalization of dense models\n",
            "while enabling sparse training and inference. Although the resulting models are\n",
            "highly sparse and theoretically less computationally expensive, achieving\n",
            "speedups with unstructured sparsity on real-world hardware is challenging. In\n",
            "this work, we propose a sparse-to-sparse DST method, Structured RigL (SRigL),\n",
            "to learn a variant of fine-grained structured N:M sparsity by imposing a\n",
            "constant fan-in constraint. Using our empirical analysis of existing DST\n",
            "methods at high sparsity, we additionally employ a neuron ablation method which\n",
            "enables SRigL to achieve state-of-the-art sparse-to-sparse structured DST\n",
            "performance on a variety of Neural Network (NN) architectures. Using a 90%\n",
            "sparse linear layer, we demonstrate a real-world acceleration of 3.4x/2.5x on\n",
            "CPU for online inference and 1.7x/13.0x on GPU for inference with a batch size\n",
            "of 256 when compared to equivalent dense/unstructured (CSR) sparse layers,\n",
            "respectively.\n",
            "\n",
            "1185. Title: Neuron-Enhanced AutoEncoder Matrix Completion and Collaborative Filtering: Theory and Practice\n",
            "   Abstract: Learning dynamics of collectively moving agents such as fish or humans is an\n",
            "active field in research. Due to natural phenomena such as occlusion and change\n",
            "of illumination, the multi-object methods tracking such dynamics might lose\n",
            "track of the agents where that might result fragmentation in the constructed\n",
            "trajectories. Here, we present an extended deep autoencoder (DA) that we train\n",
            "only on fully observed segments of the trajectories by defining its loss\n",
            "function as the Hadamard product of a binary indicator matrix with the absolute\n",
            "difference between the outputs and the labels. The trajectories of the agents\n",
            "practicing collective motion is low-rank due to mutual interactions and\n",
            "dependencies between the agents that we utilize as the underlying pattern that\n",
            "our Hadamard deep autoencoder (HDA) codes during its training. The performance\n",
            "of our HDA is compared with that of a low-rank matrix completion scheme in the\n",
            "context of fragmented trajectory reconstruction.\n",
            "\n",
            "1186. Title: SF(DA)$^2$: Source-free Domain Adaptation Through the Lens of Data Augmentation\n",
            "   Abstract: Self-supervised pre-trained speech models have strongly improved speech\n",
            "recognition, yet they are still sensitive to domain shifts and accented or\n",
            "atypical speech. Many of these models rely on quantisation or clustering to\n",
            "learn discrete acoustic units. We propose to correct the discovered discrete\n",
            "units for accented speech back to a standard pronunciation in an unsupervised\n",
            "manner. A masked language model is trained on discrete units from a standard\n",
            "accent and iteratively corrects an accented token sequence by masking\n",
            "unexpected cluster sequences and predicting their common variant. Small accent\n",
            "adapter blocks are inserted in the pre-trained model and fine-tuned by\n",
            "predicting the corrected clusters, which leads to an increased robustness of\n",
            "the pre-trained model towards a target accent, and this without supervision. We\n",
            "are able to improve a state-of-the-art HuBERT Large model on a downstream\n",
            "accented speech recognition task by altering the training regime with the\n",
            "proposed method.\n",
            "\n",
            "1187. Title: Multi-resolution HuBERT: Multi-resolution Speech Self-Supervised Learning with Masked Unit Prediction\n",
            "   Abstract: Proximal operators are ubiquitous in inverse problems, commonly appearing as\n",
            "part of algorithmic strategies to regularize problems that are otherwise\n",
            "ill-posed. Modern deep learning models have been brought to bear for these\n",
            "tasks too, as in the framework of plug-and-play or deep unrolling, where they\n",
            "loosely resemble proximal operators. Yet, something essential is lost in\n",
            "employing these purely data-driven approaches: there is no guarantee that a\n",
            "general deep network represents the proximal operator of any function, nor is\n",
            "there any characterization of the function for which the network might provide\n",
            "some approximate proximal. This not only makes guaranteeing convergence of\n",
            "iterative schemes challenging but, more fundamentally, complicates the analysis\n",
            "of what has been learned by these networks about their training data. Herein we\n",
            "provide a framework to develop learned proximal networks (LPN), prove that they\n",
            "provide exact proximal operators for a data-driven nonconvex regularizer, and\n",
            "show how a new training strategy, dubbed proximal matching, provably promotes\n",
            "the recovery of the log-prior of the true data distribution. Such LPN provide\n",
            "general, unsupervised, expressive proximal operators that can be used for\n",
            "general inverse problems with convergence guarantees. We illustrate our results\n",
            "in a series of cases of increasing complexity, demonstrating that these models\n",
            "not only result in state-of-the-art performance, but provide a window into the\n",
            "resulting priors learned from data.\n",
            "\n",
            "1188. Title: Improving equilibrium propagation without weight symmetry through Jacobian homeostasis\n",
            "   Abstract: In the face of the deep learning model's vulnerability to domain shift,\n",
            "source-free domain adaptation (SFDA) methods have been proposed to adapt models\n",
            "to new, unseen target domains without requiring access to source domain data.\n",
            "Although the potential benefits of applying data augmentation to SFDA are\n",
            "attractive, several challenges arise such as the dependence on prior knowledge\n",
            "of class-preserving transformations and the increase in memory and\n",
            "computational requirements. In this paper, we propose Source-free Domain\n",
            "Adaptation Through the Lens of Data Augmentation (SF(DA)$^2$), a novel approach\n",
            "that leverages the benefits of data augmentation without suffering from these\n",
            "challenges. We construct an augmentation graph in the feature space of the\n",
            "pretrained model using the neighbor relationships between target features and\n",
            "propose spectral neighborhood clustering to identify partitions in the\n",
            "prediction space. Furthermore, we propose implicit feature augmentation and\n",
            "feature disentanglement as regularization loss functions that effectively\n",
            "utilize class semantic information within the feature space. These regularizers\n",
            "simulate the inclusion of an unlimited number of augmented target features into\n",
            "the augmentation graph while minimizing computational and memory demands. Our\n",
            "method shows superior adaptation performance in SFDA scenarios, including 2D\n",
            "image and 3D point cloud datasets and a highly imbalanced dataset.\n",
            "\n",
            "1189. Title: Simple Hierarchical Planning with Diffusion\n",
            "   Abstract: Equilibrium propagation (EP) is a compelling alternative to the\n",
            "backpropagation of error algorithm (BP) for computing gradients of neural\n",
            "networks on biological or analog neuromorphic substrates. Still, the algorithm\n",
            "requires weight symmetry and infinitesimal equilibrium perturbations, i.e.,\n",
            "nudges, to estimate unbiased gradients efficiently. Both requirements are\n",
            "challenging to implement in physical systems. Yet, whether and how weight\n",
            "asymmetry affects its applicability is unknown because, in practice, it may be\n",
            "masked by biases introduced through the finite nudge. To address this question,\n",
            "we study generalized EP, which can be formulated without weight symmetry, and\n",
            "analytically isolate the two sources of bias. For complex-differentiable\n",
            "non-symmetric networks, we show that the finite nudge does not pose a problem,\n",
            "as exact derivatives can still be estimated via a Cauchy integral. In contrast,\n",
            "weight asymmetry introduces bias resulting in low task performance due to poor\n",
            "alignment of EP's neuronal error vectors compared to BP. To mitigate this\n",
            "issue, we present a new homeostatic objective that directly penalizes\n",
            "functional asymmetries of the Jacobian at the network's fixed point. This\n",
            "homeostatic objective dramatically improves the network's ability to solve\n",
            "complex tasks such as ImageNet 32x32. Our results lay the theoretical\n",
            "groundwork for studying and mitigating the adverse effects of imperfections of\n",
            "physical networks on learning algorithms that rely on the substrate's\n",
            "relaxation dynamics.\n",
            "\n",
            "1190. Title: Large Language Models as Automated Aligners for benchmarking Vision-Language Models\n",
            "   Abstract: Restricted Boltzmann machines (RBMs) and their extensions, called\n",
            "'deep-belief networks', are powerful neural networks that have found\n",
            "applications in the fields of machine learning and artificial intelligence. The\n",
            "standard way to training these models resorts to an iterative unsupervised\n",
            "procedure based on Gibbs sampling, called 'contrastive divergence' (CD), and\n",
            "additional supervised tuning via back-propagation. However, this procedure has\n",
            "been shown not to follow any gradient and can lead to suboptimal solutions. In\n",
            "this paper, we show an efficient alternative to CD by means of simulations of\n",
            "digital memcomputing machines (DMMs). We test our approach on pattern\n",
            "recognition using a modified version of the MNIST data set. DMMs sample\n",
            "effectively the vast phase space given by the model distribution of the RBM,\n",
            "and provide a very good approximation close to the optimum. This efficient\n",
            "search significantly reduces the number of pretraining iterations necessary to\n",
            "achieve a given level of accuracy, as well as a total performance gain over CD.\n",
            "In fact, the acceleration of pretraining achieved by simulating DMMs is\n",
            "comparable to, in number of iterations, the recently reported hardware\n",
            "application of the quantum annealing method on the same network and data set.\n",
            "Notably, however, DMMs perform far better than the reported quantum annealing\n",
            "results in terms of quality of the training. We also compare our method to\n",
            "advances in supervised training, like batch-normalization and rectifiers, that\n",
            "work to reduce the advantage of pretraining. We find that the memcomputing\n",
            "method still maintains a quality advantage ($>1\\%$ in accuracy, and a $20\\%$\n",
            "reduction in error rate) over these approaches. Furthermore, our method is\n",
            "agnostic about the connectivity of the network. Therefore, it can be extended\n",
            "to train full Boltzmann machines, and even deep networks at once.\n",
            "\n",
            "1191. Title: Towards Unified Multi-Modal Personalization: Large Vision-Language Models for Generative Recommendation and Beyond\n",
            "   Abstract: Automated alignment develops alignment systems with minimal human\n",
            "intervention. The key to automated alignment lies in providing learnable and\n",
            "accurate preference signals for preference learning without human annotation.\n",
            "In this paper, we introduce Self-Steering Optimization ($SSO$), an algorithm\n",
            "that autonomously generates high-quality preference signals based on predefined\n",
            "principles during iterative training, eliminating the need for manual\n",
            "annotation. $SSO$ maintains the accuracy of signals by ensuring a consistent\n",
            "gap between chosen and rejected responses while keeping them both on-policy to\n",
            "suit the current policy model's learning capacity. $SSO$ can benefit the online\n",
            "and offline training of the policy model, as well as enhance the training of\n",
            "reward models. We validate the effectiveness of $SSO$ with two foundation\n",
            "models, Qwen2 and Llama3.1, indicating that it provides accurate, on-policy\n",
            "preference signals throughout iterative training. Without any manual annotation\n",
            "or external models, $SSO$ leads to significant performance improvements across\n",
            "six subjective or objective benchmarks. Besides, the preference data generated\n",
            "by $SSO$ significantly enhanced the performance of the reward model on\n",
            "Rewardbench. Our work presents a scalable approach to preference optimization,\n",
            "paving the way for more efficient and effective automated alignment.\n",
            "\n",
            "1192. Title: FedDA: Faster Adaptive Gradient Methods for Federated Constrained Optimization\n",
            "   Abstract: Diffusion-based generative methods have proven effective in modeling\n",
            "trajectories with offline datasets. However, they often face computational\n",
            "challenges and can falter in generalization, especially in capturing temporal\n",
            "abstractions for long-horizon tasks. To overcome this, we introduce the\n",
            "Hierarchical Diffuser, a simple, fast, yet surprisingly effective planning\n",
            "method combining the advantages of hierarchical and diffusion-based planning.\n",
            "Our model adopts a \"jumpy\" planning strategy at the higher level, which allows\n",
            "it to have a larger receptive field but at a lower computational cost -- a\n",
            "crucial factor for diffusion-based planning methods, as we have empirically\n",
            "verified. Additionally, the jumpy sub-goals guide our low-level planner,\n",
            "facilitating a fine-tuning stage and further improving our approach's\n",
            "effectiveness. We conducted empirical evaluations on standard offline\n",
            "reinforcement learning benchmarks, demonstrating our method's superior\n",
            "performance and efficiency in terms of training and planning speed compared to\n",
            "the non-hierarchical Diffuser as well as other hierarchical planning methods.\n",
            "Moreover, we explore our model's generalization capability, particularly on how\n",
            "our method improves generalization capabilities on compositional\n",
            "out-of-distribution tasks.\n",
            "\n",
            "1193. Title: Beyond Memorization: Violating Privacy via Inference with Large Language Models\n",
            "   Abstract: Gradient-based optimization methods are the most popular choice for finding\n",
            "local optima for classical minimization and saddle point problems. Here, we\n",
            "highlight a systemic issue of gradient dynamics that arise for saddle point\n",
            "problems, namely the presence of undesired stable stationary points that are no\n",
            "local optima. We propose a novel optimization approach that exploits curvature\n",
            "information in order to escape from these undesired stationary points. We prove\n",
            "that different optimization methods, including gradient method and Adagrad,\n",
            "equipped with curvature exploitation can escape non-optimal stationary points.\n",
            "We also provide empirical results on common saddle point problems which confirm\n",
            "the advantage of using curvature exploitation.\n",
            "\n",
            "1194. Title: DynaVol: Unsupervised Learning for Dynamic Scenes through Object-Centric Voxelization\n",
            "   Abstract: Current privacy research on large language models (LLMs) primarily focuses on\n",
            "the issue of extracting memorized training data. At the same time, models'\n",
            "inference capabilities have increased drastically. This raises the key question\n",
            "of whether current LLMs could violate individuals' privacy by inferring\n",
            "personal attributes from text given at inference time. In this work, we present\n",
            "the first comprehensive study on the capabilities of pretrained LLMs to infer\n",
            "personal attributes from text. We construct a dataset consisting of real Reddit\n",
            "profiles, and show that current LLMs can infer a wide range of personal\n",
            "attributes (e.g., location, income, sex), achieving up to $85\\%$ top-1 and\n",
            "$95\\%$ top-3 accuracy at a fraction of the cost ($100\\times$) and time\n",
            "($240\\times$) required by humans. As people increasingly interact with\n",
            "LLM-powered chatbots across all aspects of life, we also explore the emerging\n",
            "threat of privacy-invasive chatbots trying to extract personal information\n",
            "through seemingly benign questions. Finally, we show that common mitigations,\n",
            "i.e., text anonymization and model alignment, are currently ineffective at\n",
            "protecting user privacy against LLM inference. Our findings highlight that\n",
            "current LLMs can infer personal data at a previously unattainable scale. In the\n",
            "absence of working defenses, we advocate for a broader discussion around LLM\n",
            "privacy implications beyond memorization, striving for a wider privacy\n",
            "protection.\n",
            "\n",
            "1195. Title: Hiding in Plain Sight: Disguising Data Stealing Attacks in Federated Learning\n",
            "   Abstract: Computer Vision (CV), Natural Language Processing (NLP), and Recommender\n",
            "Systems (RecSys) are three prominent AI applications that have traditionally\n",
            "developed independently, resulting in disparate modeling and engineering\n",
            "methodologies. This has impeded the ability for these fields to directly\n",
            "benefit from each other's advancements. With the recent development of\n",
            "foundation models, large language models have emerged as a potential\n",
            "general-purpose interface for unifying different modalities and problem\n",
            "formulations. In light of this, we propose the development of a multimodal\n",
            "foundation model (MFM) considering visual, textual, and personalization\n",
            "modalities under the P5 recommendation paradigm, thus named VIP5 (Visual P5),\n",
            "to unify various modalities and recommendation tasks. This will enable the\n",
            "processing of multiple modalities in a shared architecture for improved\n",
            "recommendations. To achieve this, we introduce multimodal personalized prompts\n",
            "to accommodate multiple modalities under a shared format. Additionally, we\n",
            "propose a parameter-efficient training method for foundation models, which\n",
            "involves freezing the P5 backbone and fine-tuning lightweight adapters,\n",
            "resulting in improved recommendation performance and increased efficiency in\n",
            "terms of training time and memory usage. Code and data of VIP5 are available at\n",
            "https://github.com/jeykigung/VIP5.\n",
            "\n",
            "1196. Title: Text-to-3D with Classifier Score Distillation\n",
            "   Abstract: Text-to-3D generation has made remarkable progress recently, particularly\n",
            "with methods based on Score Distillation Sampling (SDS) that leverages\n",
            "pre-trained 2D diffusion models. While the usage of classifier-free guidance is\n",
            "well acknowledged to be crucial for successful optimization, it is considered\n",
            "an auxiliary trick rather than the most essential component. In this paper, we\n",
            "re-evaluate the role of classifier-free guidance in score distillation and\n",
            "discover a surprising finding: the guidance alone is enough for effective\n",
            "text-to-3D generation tasks. We name this method Classifier Score Distillation\n",
            "(CSD), which can be interpreted as using an implicit classification model for\n",
            "generation. This new perspective reveals new insights for understanding\n",
            "existing techniques. We validate the effectiveness of CSD across a variety of\n",
            "text-to-3D tasks including shape generation, texture synthesis, and shape\n",
            "editing, achieving results superior to those of state-of-the-art methods. Our\n",
            "project page is https://xinyu-andy.github.io/Classifier-Score-Distillation\n",
            "\n",
            "1197. Title: On Accelerating Diffusion-Based Sampling Processes via Improved Integration Approximation\n",
            "   Abstract: Malicious server (MS) attacks have enabled the scaling of data stealing in\n",
            "federated learning to large batch sizes and secure aggregation, settings\n",
            "previously considered private. However, many concerns regarding the client-side\n",
            "detectability of MS attacks were raised, questioning their practicality. In\n",
            "this work, for the first time, we thoroughly study client-side detectability.\n",
            "We first demonstrate that all prior MS attacks are detectable by principled\n",
            "checks, and formulate a necessary set of requirements that a practical MS\n",
            "attack must satisfy. Next, we propose SEER, a novel attack framework that\n",
            "satisfies these requirements. The key insight of SEER is the use of a secret\n",
            "decoder, jointly trained with the shared model. We show that SEER can steal\n",
            "user data from gradients of realistic networks, even for large batch sizes of\n",
            "up to 512 and under secure aggregation. Our work is a promising step towards\n",
            "assessing the true vulnerability of federated learning in real-world settings.\n",
            "\n",
            "1198. Title: Towards image compression with perfect realism at ultra-low bitrates\n",
            "   Abstract: A popular approach to sample a diffusion-based generative model is to solve\n",
            "an ordinary differential equation (ODE). In existing samplers, the coefficients\n",
            "of the ODE solvers are pre-determined by the ODE formulation, the reverse\n",
            "discrete timesteps, and the employed ODE methods. In this paper, we consider\n",
            "accelerating several popular ODE-based sampling processes (including EDM, DDIM,\n",
            "and DPM-Solver) by optimizing certain coefficients via improved integration\n",
            "approximation (IIA). We propose to minimize, for each time step, a mean squared\n",
            "error (MSE) function with respect to the selected coefficients. The MSE is\n",
            "constructed by applying the original ODE solver for a set of fine-grained\n",
            "timesteps, which in principle provides a more accurate integration\n",
            "approximation in predicting the next diffusion state. The proposed IIA\n",
            "technique does not require any change of a pre-trained model, and only\n",
            "introduces a very small computational overhead for solving a number of\n",
            "quadratic optimization problems. Extensive experiments show that considerably\n",
            "better FID scores can be achieved by using IIA-EDM, IIA-DDIM, and\n",
            "IIA-DPM-Solver than the original counterparts when the neural function\n",
            "evaluation (NFE) is small (i.e., less than 25).\n",
            "\n",
            "1199. Title: Continuous Field Reconstruction from Sparse Observations with Implicit Neural Networks\n",
            "   Abstract: Image codecs are typically optimized to trade-off bitrate \\vs distortion\n",
            "metrics. At low bitrates, this leads to compression artefacts which are easily\n",
            "perceptible, even when training with perceptual or adversarial losses. To\n",
            "improve image quality and remove dependency on the bitrate, we propose to\n",
            "decode with iterative diffusion models. We condition the decoding process on a\n",
            "vector-quantized image representation, as well as a global image description to\n",
            "provide additional context. We dub our model PerCo for 'perceptual\n",
            "compression', and compare it to state-of-the-art codecs at rates from 0.1 down\n",
            "to 0.003 bits per pixel. The latter rate is more than an order of magnitude\n",
            "smaller than those considered in most prior work, compressing a 512x768 Kodak\n",
            "image with less than 153 bytes. Despite this ultra-low bitrate, our approach\n",
            "maintains the ability to reconstruct realistic images. We find that our model\n",
            "leads to reconstructions with state-of-the-art visual quality as measured by\n",
            "FID and KID. As predicted by rate-distortion-perception theory, visual quality\n",
            "is less dependent on the bitrate than previous methods.\n",
            "\n",
            "1200. Title: JointNet: Extending Text-to-Image Diffusion for Dense Distribution Modeling\n",
            "   Abstract: Reliably reconstructing physical fields from sparse sensor data is a\n",
            "challenge that frequently arises in many scientific domains. In practice, the\n",
            "process generating the data often is not understood to sufficient accuracy.\n",
            "Therefore, there is a growing interest in using the deep neural network route\n",
            "to address the problem. This work presents a novel approach that learns a\n",
            "continuous representation of the physical field using implicit neural\n",
            "representations (INRs). Specifically, after factorizing spatiotemporal\n",
            "variability into spatial and temporal components using the separation of\n",
            "variables technique, the method learns relevant basis functions from sparsely\n",
            "sampled irregular data points to develop a continuous representation of the\n",
            "data. In experimental evaluations, the proposed model outperforms recent INR\n",
            "methods, offering superior reconstruction quality on simulation data from a\n",
            "state-of-the-art climate model and a second dataset that comprises ultra-high\n",
            "resolution satellite-based sea surface temperature fields.\n",
            "\n",
            "1201. Title: Estimating Shape Distances on Neural Representations with Limited Samples\n",
            "   Abstract: Measuring geometric similarity between high-dimensional network\n",
            "representations is a topic of longstanding interest to neuroscience and deep\n",
            "learning. Although many methods have been proposed, only a few works have\n",
            "rigorously analyzed their statistical efficiency or quantified estimator\n",
            "uncertainty in data-limited regimes. Here, we derive upper and lower bounds on\n",
            "the worst-case convergence of standard estimators of shape\n",
            "distance$\\unicode{x2014}$a measure of representational dissimilarity proposed\n",
            "by Williams et al. (2021).These bounds reveal the challenging nature of the\n",
            "problem in high-dimensional feature spaces. To overcome these challenges, we\n",
            "introduce a new method-of-moments estimator with a tunable bias-variance\n",
            "tradeoff. We show that this estimator achieves substantially lower bias than\n",
            "standard estimators in simulation and on neural data, particularly in\n",
            "high-dimensional settings. Thus, we lay the foundation for a rigorous\n",
            "statistical theory for high-dimensional shape analysis, and we contribute a new\n",
            "estimation method that is well-suited to practical scientific settings.\n",
            "\n",
            "1202. Title: COLLIE: Systematic Construction of Constrained Text Generation Tasks\n",
            "   Abstract: In this work we present successor heads: attention heads that increment\n",
            "tokens with a natural ordering, such as numbers, months, and days. For example,\n",
            "successor heads increment 'Monday' into 'Tuesday'. We explain the successor\n",
            "head behavior with an approach rooted in mechanistic interpretability, the\n",
            "field that aims to explain how models complete tasks in human-understandable\n",
            "terms. Existing research in this area has found interpretable language model\n",
            "components in small toy models. However, results in toy models have not yet led\n",
            "to insights that explain the internals of frontier models and little is\n",
            "currently understood about the internal operations of large language models. In\n",
            "this paper, we analyze the behavior of successor heads in large language models\n",
            "(LLMs) and find that they implement abstract representations that are common to\n",
            "different architectures. They form in LLMs with as few as 31 million\n",
            "parameters, and at least as many as 12 billion parameters, such as GPT-2,\n",
            "Pythia, and Llama-2. We find a set of 'mod-10 features' that underlie how\n",
            "successor heads increment in LLMs across different architectures and sizes. We\n",
            "perform vector arithmetic with these features to edit head behavior and provide\n",
            "insights into numeric representations within LLMs. Additionally, we study the\n",
            "behavior of successor heads on natural language data, identifying interpretable\n",
            "polysemanticity in a Pythia successor head.\n",
            "\n",
            "1203. Title: Navigating the Design Space of Equivariant Diffusion-Based Generative Models for De Novo 3D Molecule Generation\n",
            "   Abstract: In the absence of prior knowledge, ordinal embedding methods obtain new\n",
            "representation for items in a low-dimensional Euclidean space via a set of\n",
            "quadruple-wise comparisons. These ordinal comparisons often come from human\n",
            "annotators, and sufficient comparisons induce the success of classical\n",
            "approaches. However, collecting a large number of labeled data is known as a\n",
            "hard task, and most of the existing work pay little attention to the\n",
            "generalization ability with insufficient samples. Meanwhile, recent progress in\n",
            "large margin theory discloses that rather than just maximizing the minimum\n",
            "margin, both the margin mean and variance, which characterize the margin\n",
            "distribution, are more crucial to the overall generalization performance. To\n",
            "address the issue of insufficient training samples, we propose a margin\n",
            "distribution learning paradigm for ordinal embedding, entitled Distributional\n",
            "Margin based Ordinal Embedding (\\textit{DMOE}). Precisely, we first define the\n",
            "margin for ordinal embedding problem. Secondly, we formulate a concise\n",
            "objective function which avoids maximizing margin mean and minimizing margin\n",
            "variance directly but exhibits the similar effect. Moreover, an Augmented\n",
            "Lagrange Multiplier based algorithm is customized to seek the optimal solution\n",
            "of \\textit{DMOE} effectively. Experimental studies on both simulated and\n",
            "real-world datasets are provided to show the effectiveness of the proposed\n",
            "algorithm.\n",
            "\n",
            "1204. Title: DV-3DLane: End-to-end Multi-modal 3D Lane Detection with Dual-view Representation\n",
            "   Abstract: Deep generative diffusion models are a promising avenue for 3D de novo\n",
            "molecular design in materials science and drug discovery. However, their\n",
            "utility is still limited by suboptimal performance on large molecular\n",
            "structures and limited training data. To address this gap, we explore the\n",
            "design space of E(3)-equivariant diffusion models, focusing on previously\n",
            "unexplored areas. Our extensive comparative analysis evaluates the interplay\n",
            "between continuous and discrete state spaces. From this investigation, we\n",
            "present the EQGAT-diff model, which consistently outperforms established models\n",
            "for the QM9 and GEOM-Drugs datasets. Significantly, EQGAT-diff takes continuous\n",
            "atom positions, while chemical elements and bond types are categorical and uses\n",
            "time-dependent loss weighting, substantially increasing training convergence,\n",
            "the quality of generated samples, and inference time. We also showcase that\n",
            "including chemically motivated additional features like hybridization states in\n",
            "the diffusion process enhances the validity of generated molecules. To further\n",
            "strengthen the applicability of diffusion models to limited training data, we\n",
            "investigate the transferability of EQGAT-diff trained on the large PubChem3D\n",
            "dataset with implicit hydrogen atoms to target different data distributions.\n",
            "Fine-tuning EQGAT-diff for just a few iterations shows an efficient\n",
            "distribution shift, further improving performance throughout data sets.\n",
            "Finally, we test our model on the Crossdocked data set for structure-based de\n",
            "novo ligand generation, underlining the importance of our findings showing\n",
            "state-of-the-art performance on Vina docking scores.\n",
            "\n",
            "1205. Title: A Poincaré Inequality and Consistency Results for Signal Sampling on Large Graphs\n",
            "   Abstract: Many potential applications of reinforcement learning (RL) require guarantees\n",
            "that the agent will perform well in the face of disturbances to the dynamics or\n",
            "reward function. In this paper, we prove theoretically that maximum entropy\n",
            "(MaxEnt) RL maximizes a lower bound on a robust RL objective, and thus can be\n",
            "used to learn policies that are robust to some disturbances in the dynamics and\n",
            "the reward function. While this capability of MaxEnt RL has been observed\n",
            "empirically in prior work, to the best of our knowledge our work provides the\n",
            "first rigorous proof and theoretical characterization of the MaxEnt RL robust\n",
            "set. While a number of prior robust RL algorithms have been designed to handle\n",
            "similar disturbances to the reward function or dynamics, these methods\n",
            "typically require additional moving parts and hyperparameters on top of a base\n",
            "RL algorithm. In contrast, our results suggest that MaxEnt RL by itself is\n",
            "robust to certain disturbances, without requiring any additional modifications.\n",
            "While this does not imply that MaxEnt RL is the best available robust RL\n",
            "method, MaxEnt RL is a simple robust RL method with appealing formal\n",
            "guarantees.\n",
            "\n",
            "1206. Title: Assessing Uncertainty in Similarity Scoring: Performance & Fairness in Face Recognition\n",
            "   Abstract: The scientific scale-up of large language models (LLMs) necessitates a\n",
            "comprehensive understanding of their scaling properties. However, the existing\n",
            "literature on the scaling properties only yields an incomplete answer:\n",
            "optimization loss decreases predictably as the model size increases, in line\n",
            "with established scaling law; yet no scaling law for task has been established\n",
            "and the task performances are far from predictable during scaling. Task\n",
            "performances typically show minor gains on small models until they improve\n",
            "dramatically once models exceed a size threshold, exemplifying the ``emergent\n",
            "abilities''. In this study, we discover that small models, although they\n",
            "exhibit minor performance, demonstrate critical and consistent task performance\n",
            "improvements that are not captured by conventional evaluation strategies due to\n",
            "insufficient measurement resolution. To measure such improvements, we introduce\n",
            "PassUntil, an evaluation strategy with theoretically infinite resolution,\n",
            "through massive sampling in the decoding phase. With PassUntil, we conduct a\n",
            "quantitative investigation into the scaling law of task performance. The\n",
            "investigation contains two parts. Firstly, a strict task scaling law that is\n",
            "not conventionally known to exist, is identified, enhancing the predictability\n",
            "of task performances. Remarkably, we are able to predict the performance of the\n",
            "2.4B model on code generation with merely 0.05\\% deviation before training\n",
            "starts, which is the first systematic attempt to verify predictable scaling\n",
            "proposed by GPT-4's report. Secondly, we are able to study emergent abilities\n",
            "quantitatively. We identify a kind of accelerated emergence whose scaling curve\n",
            "cannot be fitted by standard scaling law function and has a increasing speed.\n",
            "We then examine two hypothesis and imply that the ``multiple circuits\n",
            "hypothesis'' might be responsible for the accelerated emergence.\n",
            "\n",
            "1207. Title: Multi-Scale Representations by Varying Window Attention for Semantic Segmentation\n",
            "   Abstract: The Learning-to-match (LTM) framework proves to be an effective inverse\n",
            "optimal transport approach for learning the underlying ground metric between\n",
            "two sources of data, facilitating subsequent matching. However, the\n",
            "conventional LTM framework faces scalability challenges, necessitating the use\n",
            "of the entire dataset each time the parameters of the ground metric are\n",
            "updated. In adapting LTM to the deep learning context, we introduce the\n",
            "mini-batch Learning-to-match (m-LTM) framework for audio-text retrieval\n",
            "problems. This framework leverages mini-batch subsampling and\n",
            "Mahalanobis-enhanced family of ground metrics. Moreover, to cope with\n",
            "misaligned training data in practice, we propose a variant using partial\n",
            "optimal transport to mitigate the harm of misaligned data pairs in training\n",
            "data. We conduct extensive experiments on audio-text matching problems using\n",
            "three datasets: AudioCaps, Clotho, and ESC-50. Results demonstrate that our\n",
            "proposed method is capable of learning rich and expressive joint embedding\n",
            "space, which achieves SOTA performance. Beyond this, the proposed m-LTM\n",
            "framework is able to close the modality gap across audio and text embedding,\n",
            "which surpasses both triplet and contrastive loss in the zero-shot sound event\n",
            "detection task on the ESC-50 dataset. Notably, our strategy of employing\n",
            "partial optimal transport with m-LTM demonstrates greater noise tolerance than\n",
            "contrastive loss, especially under varying noise ratios in training data on the\n",
            "AudioCaps dataset. Our code is available at\n",
            "https://github.com/v-manhlt3/m-LTM-Audio-Text-Retrieval\n",
            "\n",
            "1208. Title: Predicting Emergent Abilities with Infinite Resolution Evaluation\n",
            "   Abstract: Large-scale graph machine learning is challenging as the complexity of\n",
            "learning models scales with the graph size. Subsampling the graph is a viable\n",
            "alternative, but sampling on graphs is nontrivial as graphs are non-Euclidean.\n",
            "Existing graph sampling techniques require not only computing the spectra of\n",
            "large matrices but also repeating these computations when the graph changes,\n",
            "e.g., grows. In this paper, we introduce a signal sampling theory for a type of\n",
            "graph limit -- the graphon. We prove a Poincar\\'e inequality for graphon\n",
            "signals and show that complements of node subsets satisfying this inequality\n",
            "are unique sampling sets for Paley-Wiener spaces of graphon signals. Exploiting\n",
            "connections with spectral clustering and Gaussian elimination, we prove that\n",
            "such sampling sets are consistent in the sense that unique sampling sets on a\n",
            "convergent graph sequence converge to unique sampling sets on the graphon. We\n",
            "then propose a related graphon signal sampling algorithm for large graphs, and\n",
            "demonstrate its good empirical performance on graph machine learning tasks.\n",
            "\n",
            "1209. Title: Demonstration-Regularized RL\n",
            "   Abstract: We derive a novel information-theoretic analysis of the generalization\n",
            "property of meta-learning algorithms. Concretely, our analysis proposes a\n",
            "generic understanding of both the conventional learning-to-learn framework and\n",
            "the modern model-agnostic meta-learning (MAML) algorithms. Moreover, we provide\n",
            "a data-dependent generalization bound for a stochastic variant of MAML, which\n",
            "is non-vacuous for deep few-shot learning. As compared to previous bounds that\n",
            "depend on the square norm of gradients, empirical validations on both simulated\n",
            "data and a well-known few-shot benchmark show that our bound is orders of\n",
            "magnitude tighter in most situations.\n",
            "\n",
            "1210. Title: Vision-Language Foundation Models as Effective Robot Imitators\n",
            "   Abstract: The ROC curve is the major tool for assessing not only the performance but\n",
            "also the fairness properties of a similarity scoring function. In order to draw\n",
            "reliable conclusions based on empirical ROC analysis, accurately evaluating the\n",
            "uncertainty level related to statistical versions of the ROC curves of interest\n",
            "is absolutely necessary, especially for applications with considerable societal\n",
            "impact such as Face Recognition. In this article, we prove asymptotic\n",
            "guarantees for empirical ROC curves of similarity functions as well as for\n",
            "by-product metrics useful to assess fairness. We also explain that, because the\n",
            "false acceptance/rejection rates are of the form of U-statistics in the case of\n",
            "similarity scoring, the naive bootstrap approach may jeopardize the assessment\n",
            "procedure. A dedicated recentering technique must be used instead. Beyond the\n",
            "theoretical analysis carried out, various experiments using real face image\n",
            "datasets provide strong empirical evidence of the practical relevance of the\n",
            "methods promoted here, when applied to several ROC-based measures such as\n",
            "popular fairness metrics.\n",
            "\n",
            "1211. Title: Can we get the best of both Binary Neural Networks and Spiking Neural Networks for Efficient Computer Vision?\n",
            "   Abstract: Multi-scale learning is central to semantic segmentation. We visualize the\n",
            "effective receptive field (ERF) of canonical multi-scale representations and\n",
            "point out two risks in learning them: scale inadequacy and field inactivation.\n",
            "A novel multi-scale learner, varying window attention (VWA), is presented to\n",
            "address these issues. VWA leverages the local window attention (LWA) and\n",
            "disentangles LWA into the query window and context window, allowing the\n",
            "context's scale to vary for the query to learn representations at multiple\n",
            "scales. However, varying the context to large-scale windows (enlarging ratio R)\n",
            "can significantly increase the memory footprint and computation cost (R^2 times\n",
            "larger than LWA). We propose a simple but professional re-scaling strategy to\n",
            "zero the extra induced cost without compromising performance. Consequently, VWA\n",
            "uses the same cost as LWA to overcome the receptive limitation of the local\n",
            "window. Furthermore, depending on VWA and employing various MLPs, we introduce\n",
            "a multi-scale decoder (MSD), VWFormer, to improve multi-scale representations\n",
            "for semantic segmentation. VWFormer achieves efficiency competitive with the\n",
            "most compute-friendly MSDs, like FPN and MLP decoder, but performs much better\n",
            "than any MSDs. For instance, using nearly half of UPerNet's computation,\n",
            "VWFormer outperforms it by 1.0%-2.5% mIoU on ADE20K. With little extra\n",
            "overhead, ~10G FLOPs, Mask2Former armed with VWFormer improves by 1.0%-1.3%.\n",
            "The code and models are available at https://github.com/yan-hao-tian/vw\n",
            "\n",
            "1212. Title: Efficient ConvBN Blocks for Transfer Learning and Beyond\n",
            "   Abstract: Real-world tasks such as garment manipulation and table rearrangement demand\n",
            "robots to perform generalizable, highly precise, and long-horizon actions.\n",
            "Although imitation learning has proven to be an effective approach for teaching\n",
            "robots new skills, large amounts of expert demonstration data are still\n",
            "indispensible for these complex tasks, resulting in high sample complexity and\n",
            "costly data collection. To address this, we propose Semantic Keypoint Imitation\n",
            "Learning (SKIL), a framework which automatically obtain semantic keypoints with\n",
            "help of vision foundation models, and forms the descriptor of semantic\n",
            "keypoints that enables effecient imitation learning of complex robotic tasks\n",
            "with significantly lower sample complexity. In real world experiments, SKIL\n",
            "doubles the performance of baseline methods in tasks such as picking a cup or\n",
            "mouse, while demonstrating exceptional robustness to variations in objects,\n",
            "environmental changes, and distractors. For long-horizon tasks like hanging a\n",
            "towel on a rack where previous methods fail completely, SKIL achieves a mean\n",
            "success rate of 70\\% with as few as 30 demonstrations. Furthermore, SKIL\n",
            "naturally supports cross-embodiment learning due to its semantic keypoints\n",
            "abstraction, our experiments demonstrate that even human videos bring\n",
            "considerable improvement to the learning performance. All these results\n",
            "demonstrate the great success of SKIL in achieving data-efficint generalizable\n",
            "robotic learning. Visualizations and code are available at:\n",
            "https://skil-robotics.github.io/SKIL-robotics/.\n",
            "\n",
            "1213. Title: Context-Aware Meta-Learning\n",
            "   Abstract: Spiking neural networks (SNNs), that operate via binary spikes distributed\n",
            "over time, have emerged as a promising energy efficient ML paradigm for\n",
            "resource-constrained devices. However, the current state-of-the-art (SOTA) SNNs\n",
            "require multiple time steps for acceptable inference accuracy, increasing\n",
            "spiking activity and, consequently, energy consumption. SOTA training\n",
            "strategies for SNNs involve conversion from a non-spiking deep neural network\n",
            "(DNN). In this paper, we determine that SOTA conversion strategies cannot yield\n",
            "ultra low latency because they incorrectly assume that the DNN and SNN\n",
            "pre-activation values are uniformly distributed. We propose a new training\n",
            "algorithm that accurately captures these distributions, minimizing the error\n",
            "between the DNN and converted SNN. The resulting SNNs have ultra low latency\n",
            "and high activation sparsity, yielding significant improvements in compute\n",
            "efficiency. In particular, we evaluate our framework on image recognition tasks\n",
            "from CIFAR-10 and CIFAR-100 datasets on several VGG and ResNet architectures.\n",
            "We obtain top-1 accuracy of 64.19% with only 2 time steps on the CIFAR-100\n",
            "dataset with ~159.2x lower compute energy compared to an iso-architecture\n",
            "standard DNN. Compared to other SOTA SNN models, our models perform inference\n",
            "2.5-8x faster (i.e., with fewer time steps).\n",
            "\n",
            "1214. Title: Pathformer: Multi-scale Transformers with Adaptive Pathways for Time Series Forecasting\n",
            "   Abstract: Convolution-BatchNorm (ConvBN) blocks are integral components in various\n",
            "computer vision tasks and other domains. A ConvBN block can operate in three\n",
            "modes: Train, Eval, and Deploy. While the Train mode is indispensable for\n",
            "training models from scratch, the Eval mode is suitable for transfer learning\n",
            "and beyond, and the Deploy mode is designed for the deployment of models. This\n",
            "paper focuses on the trade-off between stability and efficiency in ConvBN\n",
            "blocks: Deploy mode is efficient but suffers from training instability; Eval\n",
            "mode is widely used in transfer learning but lacks efficiency. To solve the\n",
            "dilemma, we theoretically reveal the reason behind the diminished training\n",
            "stability observed in the Deploy mode. Subsequently, we propose a novel Tune\n",
            "mode to bridge the gap between Eval mode and Deploy mode. The proposed Tune\n",
            "mode is as stable as Eval mode for transfer learning, and its computational\n",
            "efficiency closely matches that of the Deploy mode. Through extensive\n",
            "experiments in object detection, classification, and adversarial example\n",
            "generation across $5$ datasets and $12$ model architectures, we demonstrate\n",
            "that the proposed Tune mode retains the performance while significantly\n",
            "reducing GPU memory footprint and training time, thereby contributing efficient\n",
            "ConvBN blocks for transfer learning and beyond. Our method has been integrated\n",
            "into both PyTorch (general machine learning framework) and MMCV/MMEngine\n",
            "(computer vision framework). Practitioners just need one line of code to enjoy\n",
            "our efficient ConvBN blocks thanks to PyTorch's builtin machine learning\n",
            "compilers.\n",
            "\n",
            "1215. Title: MCM: Masked Cell Modeling for Anomaly Detection in Tabular Data\n",
            "   Abstract: In the era of big data and Artificial Intelligence, an emerging paradigm is\n",
            "to utilize contrastive self-supervised learning to model large-scale\n",
            "heterogeneous data. Many existing foundation models benefit from the\n",
            "generalization capability of contrastive self-supervised learning by learning\n",
            "compact and high-quality representations without relying on any label\n",
            "information. Amidst the explosive advancements in foundation models across\n",
            "multiple domains, including natural language processing and computer vision, a\n",
            "thorough survey on heterogeneous contrastive learning for the foundation model\n",
            "is urgently needed. In response, this survey critically evaluates the current\n",
            "landscape of heterogeneous contrastive learning for foundation models,\n",
            "highlighting the open challenges and future trends of contrastive learning. In\n",
            "particular, we first present how the recent advanced contrastive learning-based\n",
            "methods deal with view heterogeneity and how contrastive learning is applied to\n",
            "train and fine-tune the multi-view foundation models. Then, we move to\n",
            "contrastive learning methods for task heterogeneity, including pretraining\n",
            "tasks and downstream tasks, and show how different tasks are combined with\n",
            "contrastive learning loss for different purposes. Finally, we conclude this\n",
            "survey by discussing the open challenges and shedding light on the future\n",
            "directions of contrastive learning.\n",
            "\n",
            "1216. Title: Improved Efficiency Based on Learned Saccade and Continuous Scene Reconstruction From Foveated Visual Sampling\n",
            "   Abstract: This paper introduces VLAP, a novel approach that bridges pretrained vision\n",
            "models and large language models (LLMs) to make frozen LLMs understand the\n",
            "visual world. VLAP transforms the embedding space of pretrained vision models\n",
            "into the LLMs' word embedding space using a single linear layer for efficient\n",
            "and general-purpose visual and language understanding. Specifically, we harness\n",
            "well-established word embeddings to bridge two modality embedding spaces. The\n",
            "visual and text representations are simultaneously assigned to a set of word\n",
            "embeddings within pretrained LLMs by formulating the assigning procedure as an\n",
            "optimal transport problem. We predict the assignment of one modality from the\n",
            "representation of another modality data, enforcing consistent assignments for\n",
            "paired multimodal data. This allows vision and language representations to\n",
            "contain the same information, grounding the frozen LLMs' word embedding space\n",
            "in visual data. Moreover, a robust semantic taxonomy of LLMs can be preserved\n",
            "with visual data since the LLMs interpret and reason linguistic information\n",
            "from correlations between word embeddings. Experimental results show that VLAP\n",
            "achieves substantial improvements over the previous linear transformation-based\n",
            "approaches across a range of vision-language tasks, including image captioning,\n",
            "visual question answering, and cross-modal retrieval. We also demonstrate the\n",
            "learned visual representations hold a semantic taxonomy of LLMs, making visual\n",
            "semantic arithmetic possible.\n",
            "\n",
            "1217. Title: On Diffusion Modeling for Anomaly Detection\n",
            "   Abstract: How does scaling the number of parameters in large language models (LLMs)\n",
            "affect their core capabilities? We study two natural scaling techniques --\n",
            "weight pruning and simply training a smaller or larger model, which we refer to\n",
            "as dense scaling -- and their effects on two core capabilities of LLMs: (a)\n",
            "recalling facts presented during pre-training and (b) processing information\n",
            "presented in-context during inference. By curating a suite of tasks that help\n",
            "disentangle these two capabilities, we find a striking difference in how these\n",
            "two abilities evolve due to scaling. Reducing the model size by more than 30\\%\n",
            "(via either scaling approach) significantly decreases the ability to recall\n",
            "facts seen in pre-training. Yet, a 60--70\\% reduction largely preserves the\n",
            "various ways the model can process in-context information, ranging from\n",
            "retrieving answers from a long context to learning parameterized functions from\n",
            "in-context exemplars. The fact that both dense scaling and weight pruning\n",
            "exhibit this behavior suggests that scaling model size has an inherently\n",
            "disparate effect on fact recall and in-context learning.\n",
            "\n",
            "1218. Title: Node2ket: Efficient High-Dimensional Network Embedding in Quantum Hilbert Space\n",
            "   Abstract: Markov chain sampling methods that automatically adapt to characteristics of\n",
            "the distribution being sampled can be constructed by exploiting the principle\n",
            "that one can sample from a distribution by sampling uniformly from the region\n",
            "under the plot of its density function. A Markov chain that converges to this\n",
            "uniform distribution can be constructed by alternating uniform sampling in the\n",
            "vertical direction with uniform sampling from the horizontal `slice' defined by\n",
            "the current vertical position, or more generally, with some update that leaves\n",
            "the uniform distribution over this slice invariant. Variations on such `slice\n",
            "sampling' methods are easily implemented for univariate distributions, and can\n",
            "be used to sample from a multivariate distribution by updating each variable in\n",
            "turn. This approach is often easier to implement than Gibbs sampling, and more\n",
            "efficient than simple Metropolis updates, due to the ability of slice sampling\n",
            "to adaptively choose the magnitude of changes made. It is therefore attractive\n",
            "for routine and automated use. Slice sampling methods that update all variables\n",
            "simultaneously are also possible. These methods can adaptively choose the\n",
            "magnitudes of changes made to each variable, based on the local properties of\n",
            "the density function. More ambitiously, such methods could potentially allow\n",
            "the sampling to adapt to dependencies between variables by constructing local\n",
            "quadratic approximations. Another approach is to improve sampling efficiency by\n",
            "suppressing random walks. This can be done using `overrelaxed' versions of\n",
            "univariate slice sampling procedures, or by using `reflective' multivariate\n",
            "slice sampling methods, which bounce off the edges of the slice.\n",
            "\n",
            "1219. Title: Delphic Offline Reinforcement Learning under Nonidentifiable Hidden Confounding\n",
            "   Abstract: Self-supervised learning converts raw perceptual data such as images to a\n",
            "compact space where simple Euclidean distances measure meaningful variations in\n",
            "data. In this paper, we extend this formulation by adding additional geometric\n",
            "structure to the embedding space by enforcing transformations of input space to\n",
            "correspond to simple (i.e., linear) transformations of embedding space.\n",
            "Specifically, in the contrastive learning setting, we introduce an equivariance\n",
            "objective and theoretically prove that its minima forces augmentations on input\n",
            "space to correspond to rotations on the spherical embedding space. We show that\n",
            "merely combining our equivariant loss with a non-collapse term results in\n",
            "non-trivial representations, without requiring invariance to data\n",
            "augmentations. Optimal performance is achieved by also encouraging approximate\n",
            "invariance, where input augmentations correspond to small rotations. Our\n",
            "method, CARE: Contrastive Augmentation-induced Rotational Equivariance, leads\n",
            "to improved performance on downstream tasks, and ensures sensitivity in\n",
            "embedding space to important variations in data (e.g., color) that standard\n",
            "contrastive methods do not achieve. Code is available at\n",
            "https://github.com/Sharut/CARE.\n",
            "\n",
            "1220. Title: SRL: Scaling Distributed Reinforcement Learning to Over Ten Thousand Cores\n",
            "   Abstract: A prominent challenge of offline reinforcement learning (RL) is the issue of\n",
            "hidden confounding: unobserved variables may influence both the actions taken\n",
            "by the agent and the observed outcomes. Hidden confounding can compromise the\n",
            "validity of any causal conclusion drawn from data and presents a major obstacle\n",
            "to effective offline RL. In the present paper, we tackle the problem of hidden\n",
            "confounding in the nonidentifiable setting. We propose a definition of\n",
            "uncertainty due to hidden confounding bias, termed delphic uncertainty, which\n",
            "uses variation over world models compatible with the observations, and\n",
            "differentiate it from the well-known epistemic and aleatoric uncertainties. We\n",
            "derive a practical method for estimating the three types of uncertainties, and\n",
            "construct a pessimistic offline RL algorithm to account for them. Our method\n",
            "does not assume identifiability of the unobserved confounders, and attempts to\n",
            "reduce the amount of confounding bias. We demonstrate through extensive\n",
            "experiments and ablations the efficacy of our approach on a sepsis management\n",
            "benchmark, as well as on electronic health records. Our results suggest that\n",
            "nonidentifiable hidden confounding bias can be mitigated to improve offline RL\n",
            "solutions in practice.\n",
            "\n",
            "1221. Title: The Cost of Scaling Down Large Language Models: Reducing Model Size Affects Memory before In-context Learning\n",
            "   Abstract: Known for their impressive performance in generative modeling, diffusion\n",
            "models are attractive candidates for density-based anomaly detection. This\n",
            "paper investigates different variations of diffusion modeling for unsupervised\n",
            "and semi-supervised anomaly detection. In particular, we find that Denoising\n",
            "Diffusion Probability Models (DDPM) are performant on anomaly detection\n",
            "benchmarks yet computationally expensive. By simplifying DDPM in application to\n",
            "anomaly detection, we are naturally led to an alternative approach called\n",
            "Diffusion Time Estimation (DTE). DTE estimates the distribution over diffusion\n",
            "time for a given input and uses the mode or mean of this distribution as the\n",
            "anomaly score. We derive an analytical form for this density and leverage a\n",
            "deep neural network to improve inference efficiency. Through empirical\n",
            "evaluations on the ADBench benchmark, we demonstrate that all diffusion-based\n",
            "anomaly detection methods perform competitively for both semi-supervised and\n",
            "unsupervised settings. Notably, DTE achieves orders of magnitude faster\n",
            "inference time than DDPM, while outperforming it on this benchmark. These\n",
            "results establish diffusion-based anomaly detection as a scalable alternative\n",
            "to traditional methods and recent deep-learning techniques for standard\n",
            "unsupervised and semi-supervised anomaly detection settings.\n",
            "\n",
            "1222. Title: Sliced Denoising: A Physics-Informed Molecular Pre-Training Method\n",
            "   Abstract: The remarkable performance of large language models (LLMs) on complex\n",
            "linguistic tasks has sparked a lively debate on the nature of their\n",
            "capabilities. Unlike humans, these models learn language exclusively from\n",
            "textual data, without direct interaction with the real world. Nevertheless,\n",
            "they can generate seemingly meaningful text about a wide range of topics. This\n",
            "impressive accomplishment has rekindled interest in the classical 'Symbol\n",
            "Grounding Problem,' which questioned whether the internal representations and\n",
            "outputs of classical symbolic AI systems could possess intrinsic meaning.\n",
            "Unlike these systems, modern LLMs are artificial neural networks that compute\n",
            "over vectors rather than symbols. However, an analogous problem arises for such\n",
            "systems, which we dub the Vector Grounding Problem. This paper has two primary\n",
            "objectives. First, we differentiate various ways in which internal\n",
            "representations can be grounded in biological or artificial systems,\n",
            "identifying five distinct notions discussed in the literature: referential,\n",
            "sensorimotor, relational, communicative, and epistemic grounding.\n",
            "Unfortunately, these notions of grounding are often conflated. We clarify the\n",
            "differences between them, and argue that referential grounding is the one that\n",
            "lies at the heart of the Vector Grounding Problem. Second, drawing on theories\n",
            "of representational content in philosophy and cognitive science, we propose\n",
            "that certain LLMs, particularly those fine-tuned with Reinforcement Learning\n",
            "from Human Feedback (RLHF), possess the necessary features to overcome the\n",
            "Vector Grounding Problem, as they stand in the requisite causal-historical\n",
            "relations to the world that underpin intrinsic meaning. We also argue that,\n",
            "perhaps unexpectedly, multimodality and embodiment are neither necessary nor\n",
            "sufficient conditions for referential grounding in artificial systems.\n",
            "\n",
            "1223. Title: The Expressive Power of Low-Rank Adaptation\n",
            "   Abstract: Previous studies have proposed image-based clutter measures that correlate\n",
            "with human search times and/or eye movements. However, most models do not take\n",
            "into account the fact that the effects of clutter interact with the foveated\n",
            "nature of the human visual system: visual clutter further from the fovea has an\n",
            "increasing detrimental influence on perception. Here, we introduce a new\n",
            "foveated clutter model to predict the detrimental effects in target search\n",
            "utilizing a forced fixation search task. We use Feature Congestion (Rosenholtz\n",
            "et al.) as our non foveated clutter model, and we stack a peripheral\n",
            "architecture on top of Feature Congestion for our foveated model. We introduce\n",
            "the Peripheral Integration Feature Congestion (PIFC) coefficient, as a\n",
            "fundamental ingredient of our model that modulates clutter as a non-linear gain\n",
            "contingent on eccentricity. We finally show that Foveated Feature Congestion\n",
            "(FFC) clutter scores r(44) = -0.82 correlate better with target detection (hit\n",
            "rate) than regular Feature Congestion r(44) = -0.19 in forced fixation search.\n",
            "Thus, our model allows us to enrich clutter perception research by computing\n",
            "fixation specific clutter maps. A toolbox for creating peripheral\n",
            "architectures: Piranhas: Peripheral Architectures for Natural, Hybrid and\n",
            "Artificial Systems will be made available.\n",
            "\n",
            "1224. Title: Ring-A-Bell! How Reliable are Concept Removal Methods For Diffusion Models?\n",
            "   Abstract: Methods and algorithms that work with data on nonlinear manifolds are\n",
            "collectively summarized under the term `Riemannian computing'. In practice,\n",
            "curvature can be a key limiting factor for the performance of Riemannian\n",
            "computing methods. Yet, curvature can also be a powerful tool in the\n",
            "theoretical analysis of Riemannian algorithms. In this work, we investigate the\n",
            "sectional curvature of the Stiefel and Grassmann manifold. On the Grassmannian,\n",
            "tight curvature bounds are known since the late 1960ies. On the Stiefel\n",
            "manifold under the canonical metric, it was believed that the sectional\n",
            "curvature does not exceed 5/4. Under the Euclidean metric, the maximum was\n",
            "conjectured to be at 1. For both manifolds, the sectional curvature is given by\n",
            "the Frobenius norm of certain structured commutator brackets of skew-symmetric\n",
            "matrices. We provide refined inequalities for such terms and pay special\n",
            "attention to the maximizers of the curvature bounds. In this way, we prove for\n",
            "the Stiefel manifold that the global bounds of 5/4 (canonical metric) and 1\n",
            "(Euclidean metric) hold indeed. With this addition, a complete account of the\n",
            "curvature bounds in all admissible dimensions is obtained. We observe that\n",
            "`high curvature means low-rank', more precisely, for the Stiefel and Grassmann\n",
            "manifolds under the canonical metric, the global curvature maximum is attained\n",
            "at tangent plane sections that are spanned by rank-two matrices, while the\n",
            "extreme curvature cases of the Euclidean Stiefel manifold occur for rank-one\n",
            "matrices. Numerical examples are included for illustration purposes.\n",
            "\n",
            "1225. Title: Bounding Box Stability against Feature Dropout Reflects Detector Generalization across Environments\n",
            "   Abstract: Conventional wisdom suggests that neural network predictions tend to be\n",
            "unpredictable and overconfident when faced with out-of-distribution (OOD)\n",
            "inputs. Our work reassesses this assumption for neural networks with\n",
            "high-dimensional inputs. Rather than extrapolating in arbitrary ways, we\n",
            "observe that neural network predictions often tend towards a constant value as\n",
            "input data becomes increasingly OOD. Moreover, we find that this value often\n",
            "closely approximates the optimal constant solution (OCS), i.e., the prediction\n",
            "that minimizes the average loss over the training data without observing the\n",
            "input. We present results showing this phenomenon across 8 datasets with\n",
            "different distributional shifts (including CIFAR10-C and ImageNet-R, S),\n",
            "different loss functions (cross entropy, MSE, and Gaussian NLL), and different\n",
            "architectures (CNNs and transformers). Furthermore, we present an explanation\n",
            "for this behavior, which we first validate empirically and then study\n",
            "theoretically in a simplified setting involving deep homogeneous networks with\n",
            "ReLU activations. Finally, we show how one can leverage our insights in\n",
            "practice to enable risk-sensitive decision-making in the presence of OOD\n",
            "inputs.\n",
            "\n",
            "1226. Title: Submodular Reinforcement Learning\n",
            "   Abstract: Reliable tools to extract patterns from high-dimensionality spaces are\n",
            "becoming more necessary as astronomical datasets increase both in volume and\n",
            "complexity. Contrastive Learning is a self-supervised machine learning\n",
            "algorithm that extracts informative measurements from multi-dimensional\n",
            "datasets, which has become increasingly popular in the computer vision and\n",
            "Machine Learning communities in recent years. To do so, it maximizes the\n",
            "agreement between the information extracted from augmented versions of the same\n",
            "input data, making the final representation invariant to the applied\n",
            "transformations. Contrastive Learning is particularly useful in astronomy for\n",
            "removing known instrumental effects and for performing supervised\n",
            "classifications and regressions with a limited amount of available labels,\n",
            "showing a promising avenue towards \\emph{Foundation Models}. This short review\n",
            "paper briefly summarizes the main concepts behind contrastive learning and\n",
            "reviews the first promising applications to astronomy. We include some\n",
            "practical recommendations on which applications are particularly attractive for\n",
            "contrastive learning.\n",
            "\n",
            "1227. Title: A Dynamical View of the Question of Why\n",
            "   Abstract: In reinforcement learning (RL), rewards of states are typically considered\n",
            "additive, and following the Markov assumption, they are $\\textit{independent}$\n",
            "of states visited previously. In many important applications, such as coverage\n",
            "control, experiment design and informative path planning, rewards naturally\n",
            "have diminishing returns, i.e., their value decreases in light of similar\n",
            "states visited previously. To tackle this, we propose $\\textit{submodular RL}$\n",
            "(SubRL), a paradigm which seeks to optimize more general, non-additive (and\n",
            "history-dependent) rewards modelled via submodular set functions which capture\n",
            "diminishing returns. Unfortunately, in general, even in tabular settings, we\n",
            "show that the resulting optimization problem is hard to approximate. On the\n",
            "other hand, motivated by the success of greedy algorithms in classical\n",
            "submodular optimization, we propose SubPO, a simple policy gradient-based\n",
            "algorithm for SubRL that handles non-additive rewards by greedily maximizing\n",
            "marginal gains. Indeed, under some assumptions on the underlying Markov\n",
            "Decision Process (MDP), SubPO recovers optimal constant factor approximations\n",
            "of submodular bandits. Moreover, we derive a natural policy gradient approach\n",
            "for locally optimizing SubRL instances even in large state- and action- spaces.\n",
            "We showcase the versatility of our approach by applying SubPO to several\n",
            "applications, such as biodiversity monitoring, Bayesian experiment design,\n",
            "informative path planning, and coverage maximization. Our results demonstrate\n",
            "sample efficiency, as well as scalability to high-dimensional state-action\n",
            "spaces.\n",
            "\n",
            "1228. Title: EControl: Fast Distributed Optimization with Compression and Error Control\n",
            "   Abstract: Bounding boxes uniquely characterize object detection, where a good detector\n",
            "gives accurate bounding boxes of categories of interest. However, in the\n",
            "real-world where test ground truths are not provided, it is non-trivial to find\n",
            "out whether bounding boxes are accurate, thus preventing us from assessing the\n",
            "detector generalization ability. In this work, we find under feature map\n",
            "dropout, good detectors tend to output bounding boxes whose locations do not\n",
            "change much, while bounding boxes of poor detectors will undergo noticeable\n",
            "position changes. We compute the box stability score (BoS score) to reflect\n",
            "this stability. Specifically, given an image, we compute a normal set of\n",
            "bounding boxes and a second set after feature map dropout. To obtain BoS score,\n",
            "we use bipartite matching to find the corresponding boxes between the two sets\n",
            "and compute the average Intersection over Union (IoU) across the entire test\n",
            "set. We contribute to finding that BoS score has a strong, positive correlation\n",
            "with detection accuracy measured by mean average precision (mAP) under various\n",
            "test environments. This relationship allows us to predict the accuracy of\n",
            "detectors on various real-world test sets without accessing test ground truths,\n",
            "verified on canonical detection tasks such as vehicle detection and pedestrian\n",
            "detection. Code and data are available at https://github.com/YangYangGirl/BoS.\n",
            "\n",
            "1229. Title: RobustTSF: Towards Theory and Design of Robust Time Series Forecasting with Anomalies\n",
            "   Abstract: Using the Isospin Quantum Molecular Dynamics (IQMD) model we analyzed the\n",
            "production of pions and kaons in the energy range of 1-2 AGeV in order to study\n",
            "the question why thermal models could achieve a successful description. For\n",
            "this purpose we study the variation of pion and kaon yields using different\n",
            "elementary cross sections. We show that several ratios appear to be rather\n",
            "robust versus their variations.\n",
            "\n",
            "1230. Title: Faster Approximation of Probabilistic and Distributional Values via Least Squares\n",
            "   Abstract: We study a strategic variant of the multi-armed bandit problem, which we coin\n",
            "the strategic click-bandit. This model is motivated by applications in online\n",
            "recommendation where the choice of recommended items depends on both the\n",
            "click-through rates and the post-click rewards. Like in classical bandits,\n",
            "rewards follow a fixed unknown distribution. However, we assume that the\n",
            "click-rate of each arm is chosen strategically by the arm (e.g., a host on\n",
            "Airbnb) in order to maximize the number of times it gets clicked. The algorithm\n",
            "designer does not know the post-click rewards nor the arms' actions (i.e.,\n",
            "strategically chosen click-rates) in advance, and must learn both values over\n",
            "time. To solve this problem, we design an incentive-aware learning algorithm,\n",
            "UCB-S, which achieves two goals simultaneously: (a) incentivizing desirable arm\n",
            "behavior under uncertainty; (b) minimizing regret by learning unknown\n",
            "parameters. We characterize all approximate Nash equilibria among arms under\n",
            "UCB-S and show a $\\tilde{\\mathcal{O}} (\\sqrt{KT})$ regret bound uniformly in\n",
            "every equilibrium. We also show that incentive-unaware algorithms generally\n",
            "fail to achieve low regret in the strategic click-bandit. Finally, we support\n",
            "our theoretical results by simulations of strategic arm behavior which confirm\n",
            "the effectiveness and robustness of our proposed incentive design.\n",
            "\n",
            "1231. Title: To the Cutoff... and Beyond? A Longitudinal Perspective on LLM Data Contamination\n",
            "   Abstract: Least squares approximation is a technique to find an approximate solution to\n",
            "a system of linear equations that has no exact solution. In a typical setting,\n",
            "one lets $n$ be the number of constraints and $d$ be the number of variables,\n",
            "with $n \\gg d$. Then, existing exact methods find a solution vector in\n",
            "$O(nd^2)$ time. We present two randomized algorithms that provide very accurate\n",
            "relative-error approximations to the optimal value and the solution vector of a\n",
            "least squares approximation problem more rapidly than existing exact\n",
            "algorithms. Both of our algorithms preprocess the data with the Randomized\n",
            "Hadamard Transform. One then uniformly randomly samples constraints and solves\n",
            "the smaller problem on those constraints, and the other performs a sparse\n",
            "random projection and solves the smaller problem on those projected\n",
            "coordinates. In both cases, solving the smaller problem provides relative-error\n",
            "approximations, and, if $n$ is sufficiently larger than $d$, the approximate\n",
            "solution can be computed in $O(nd \\log d)$ time.\n",
            "\n",
            "1232. Title: DENEVIL: TOWARDS DECIPHERING AND NAVIGATING THE ETHICAL VALUES OF LARGE LANGUAGE MODELS VIA INSTRUCTION LEARNING\n",
            "   Abstract: Composed image retrieval (CIR) is the task of retrieving specific images by\n",
            "using a query that involves both a reference image and a relative caption. Most\n",
            "existing CIR models adopt the late-fusion strategy to combine visual and\n",
            "language features. Besides, several approaches have also been suggested to\n",
            "generate a pseudo-word token from the reference image, which is further\n",
            "integrated into the relative caption for CIR. However, these pseudo-word-based\n",
            "prompting methods have limitations when target image encompasses complex\n",
            "changes on reference image, e.g., object removal and attribute modification. In\n",
            "this work, we demonstrate that learning an appropriate sentence-level prompt\n",
            "for the relative caption (SPRC) is sufficient for achieving effective composed\n",
            "image retrieval. Instead of relying on pseudo-word-based prompts, we propose to\n",
            "leverage pretrained V-L models, e.g., BLIP-2, to generate sentence-level\n",
            "prompts. By concatenating the learned sentence-level prompt with the relative\n",
            "caption, one can readily use existing text-based image retrieval models to\n",
            "enhance CIR performance. Furthermore, we introduce both image-text contrastive\n",
            "loss and text prompt alignment loss to enforce the learning of suitable\n",
            "sentence-level prompts. Experiments show that our proposed method performs\n",
            "favorably against the state-of-the-art CIR methods on the Fashion-IQ and CIRR\n",
            "datasets. The source code and pretrained model are publicly available at\n",
            "https://github.com/chunmeifeng/SPRC\n",
            "\n",
            "1233. Title: Sentence-level Prompts Benefit Composed Image Retrieval\n",
            "   Abstract: Large Language Models (LLMs) have demonstrated remarkable performance in\n",
            "solving math problems, a hallmark of human intelligence. Despite high success\n",
            "rates on current benchmarks; however, these often feature simple problems with\n",
            "only one or two unknowns, which do not sufficiently challenge their reasoning\n",
            "capacities. This paper introduces a novel benchmark, BeyondX, designed to\n",
            "address these limitations by incorporating problems with multiple unknowns.\n",
            "Recognizing the challenges in proposing multi-unknown problems from scratch, we\n",
            "developed BeyondX using an innovative automated pipeline that progressively\n",
            "increases complexity by expanding the number of unknowns in simpler problems.\n",
            "Empirical study on BeyondX reveals that the performance of existing LLMs, even\n",
            "those fine-tuned specifically on math tasks, significantly decreases as the\n",
            "number of unknowns increases - with a performance drop of up to 70\\% observed\n",
            "in GPT-4. To tackle these challenges, we propose the Formulate-and-Solve\n",
            "strategy, a generalized prompting approach that effectively handles problems\n",
            "with an arbitrary number of unknowns. Our findings reveal that this strategy\n",
            "not only enhances LLM performance on the BeyondX benchmark but also provides\n",
            "deeper insights into the computational limits of LLMs when faced with more\n",
            "complex mathematical challenges.\n",
            "\n",
            "1234. Title: Policy Rehearsing: Training Generalizable Policies for Reinforcement Learning\n",
            "   Abstract: We introduce Diffusion Policy Policy Optimization, DPPO, an algorithmic\n",
            "framework including best practices for fine-tuning diffusion-based policies\n",
            "(e.g. Diffusion Policy) in continuous control and robot learning tasks using\n",
            "the policy gradient (PG) method from reinforcement learning (RL). PG methods\n",
            "are ubiquitous in training RL policies with other policy parameterizations;\n",
            "nevertheless, they had been conjectured to be less efficient for\n",
            "diffusion-based policies. Surprisingly, we show that DPPO achieves the\n",
            "strongest overall performance and efficiency for fine-tuning in common\n",
            "benchmarks compared to other RL methods for diffusion-based policies and also\n",
            "compared to PG fine-tuning of other policy parameterizations. Through\n",
            "experimental investigation, we find that DPPO takes advantage of unique\n",
            "synergies between RL fine-tuning and the diffusion parameterization, leading to\n",
            "structured and on-manifold exploration, stable training, and strong policy\n",
            "robustness. We further demonstrate the strengths of DPPO in a range of\n",
            "realistic settings, including simulated robotic tasks with pixel observations,\n",
            "and via zero-shot deployment of simulation-trained policies on robot hardware\n",
            "in a long-horizon, multi-stage manipulation task. Website with code:\n",
            "diffusion-ppo.github.io\n",
            "\n",
            "1235. Title: DRSM: De-Randomized Smoothing on Malware Classifier Providing Certified Robustness\n",
            "   Abstract: Causal discovery serves a pivotal role in mitigating model uncertainty\n",
            "through recovering the underlying causal mechanisms among variables. In many\n",
            "practical domains, such as healthcare, access to the data gathered by\n",
            "individual entities is limited, primarily for privacy and regulatory\n",
            "constraints. However, the majority of existing causal discovery methods require\n",
            "the data to be available in a centralized location. In response, researchers\n",
            "have introduced federated causal discovery. While previous federated methods\n",
            "consider distributed observational data, the integration of interventional data\n",
            "remains largely unexplored. We propose FedCDI, a federated framework for\n",
            "inferring causal structures from distributed data containing interventional\n",
            "samples. In line with the federated learning framework, FedCDI improves privacy\n",
            "by exchanging belief updates rather than raw samples. Additionally, it\n",
            "introduces a novel intervention-aware method for aggregating individual\n",
            "updates. We analyze scenarios with shared or disjoint intervened covariates,\n",
            "and mitigate the adverse effects of interventional data heterogeneity. The\n",
            "performance and scalability of FedCDI is rigorously tested across a variety of\n",
            "synthetic and real-world graphs.\n",
            "\n",
            "1236. Title: EquiformerV2: Improved Equivariant Transformer for Scaling to Higher-Degree Representations\n",
            "   Abstract: Vertical federated learning (VFL), where each participating client holds a\n",
            "subset of data features, has found numerous applications in finance,\n",
            "healthcare, and IoT systems. However, adversarial attacks, particularly through\n",
            "the injection of adversarial examples (AEs), pose serious challenges to the\n",
            "security of VFL models. In this paper, we investigate such vulnerabilities\n",
            "through developing a novel attack to disrupt the VFL inference process, under a\n",
            "practical scenario where the adversary is able to adaptively corrupt a subset\n",
            "of clients. We formulate the problem of finding optimal attack strategies as an\n",
            "online optimization problem, which is decomposed into an inner problem of\n",
            "adversarial example generation (AEG) and an outer problem of corruption pattern\n",
            "selection (CPS). Specifically, we establish the equivalence between the\n",
            "formulated CPS problem and a multi-armed bandit (MAB) problem, and propose the\n",
            "Thompson sampling with Empirical maximum reward (E-TS) algorithm for the\n",
            "adversary to efficiently identify the optimal subset of clients for corruption.\n",
            "The key idea of E-TS is to introduce an estimation of the expected maximum\n",
            "reward for each arm, which helps to specify a small set of competitive arms, on\n",
            "which the exploration for the optimal arm is performed. This significantly\n",
            "reduces the exploration space, which otherwise can quickly become prohibitively\n",
            "large as the number of clients increases. We analytically characterize the\n",
            "regret bound of E-TS, and empirically demonstrate its capability of efficiently\n",
            "revealing the optimal corruption pattern with the highest attack success rate,\n",
            "under various datasets of popular VFL tasks.\n",
            "\n",
            "1237. Title: An Analytical Solution to Gauss-Newton Loss for Direct Image Alignment\n",
            "   Abstract: Large Language Models (LLMs) inherently encode a wealth of knowledge within\n",
            "their parameters through pre-training on extensive corpora. While prior\n",
            "research has delved into operations on these parameters to manipulate the\n",
            "underlying implicit knowledge (encompassing detection, editing, and merging),\n",
            "there remains an ambiguous understanding regarding their transferability across\n",
            "models with varying scales. In this paper, we seek to empirically investigate\n",
            "knowledge transfer from larger to smaller models through a parametric\n",
            "perspective. To achieve this, we employ sensitivity-based techniques to extract\n",
            "and align knowledge-specific parameters between different LLMs. Moreover, the\n",
            "LoRA module is used as the intermediary mechanism for injecting the extracted\n",
            "knowledge into smaller models. Evaluations across four benchmarks validate the\n",
            "efficacy of our proposed method. Our findings highlight the critical factors\n",
            "contributing to the process of parametric knowledge transfer, underscoring the\n",
            "transferability of model parameters across LLMs of different scales. Project\n",
            "website: https://maszhongming.github.io/ParaKnowTransfer.\n",
            "\n",
            "1238. Title: Sum-Product-Set Networks: Deep Tractable Models for Tree-Structured Graphs\n",
            "   Abstract: Training deep networks requires various design decisions regarding for\n",
            "instance their architecture, data augmentation, or optimization. In this work,\n",
            "we find these training variations to result in networks learning unique feature\n",
            "sets from the data. Using public model libraries comprising thousands of models\n",
            "trained on canonical datasets like ImageNet, we observe that for arbitrary\n",
            "pairings of pretrained models, one model extracts significant data context\n",
            "unavailable in the other -- independent of overall performance. Given any\n",
            "arbitrary pairing of pretrained models and no external rankings (such as\n",
            "separate test sets, e.g. due to data privacy), we investigate if it is possible\n",
            "to transfer such \"complementary\" knowledge from one model to another without\n",
            "performance degradation -- a task made particularly difficult as additional\n",
            "knowledge can be contained in stronger, equiperformant or weaker models. Yet\n",
            "facilitating robust transfer in scenarios agnostic to pretrained model pairings\n",
            "would unlock auxiliary gains and knowledge fusion from any model repository\n",
            "without restrictions on model and problem specifics - including from weaker,\n",
            "lower-performance models. This work therefore provides an initial, in-depth\n",
            "exploration on the viability of such general-purpose knowledge transfer. Across\n",
            "large-scale experiments, we first reveal the shortcomings of standard knowledge\n",
            "distillation techniques, and then propose a much more general extension through\n",
            "data partitioning for successful transfer between nearly all pretrained models,\n",
            "which we show can also be done unsupervised. Finally, we assess both the\n",
            "scalability and impact of fundamental model properties on successful\n",
            "model-agnostic knowledge transfer.\n",
            "\n",
            "1239. Title: Rethinking the Benefits of Steerable Features in 3D Equivariant Graph Neural Networks\n",
            "   Abstract: Real-world deployment of machine learning models is challenging because data\n",
            "evolves over time. While no model can work when data evolves in an arbitrary\n",
            "fashion, if there is some pattern to these changes, we might be able to design\n",
            "methods to address it. This paper addresses situations when data evolves\n",
            "gradually. We introduce a time-varying propensity score that can detect gradual\n",
            "shifts in the distribution of data which allows us to selectively sample past\n",
            "data to update the model -- not just similar data from the past like that of a\n",
            "standard propensity score but also data that evolved in a similar fashion in\n",
            "the past. The time-varying propensity score is quite general: we demonstrate\n",
            "different ways of implementing it and evaluate it on a variety of problems\n",
            "ranging from supervised learning (e.g., image classification problems) where\n",
            "data undergoes a sequence of gradual shifts, to reinforcement learning tasks\n",
            "(e.g., robotic manipulation and continuous control) where data shifts as the\n",
            "policy or the task changes.\n",
            "\n",
            "1240. Title: Seeking Neural Nuggets: Knowledge Transfer in Large Language Models from a Parametric Perspective\n",
            "   Abstract: It is well-known that inference in graphical models is hard in the worst\n",
            "case, but tractable for models with bounded treewidth. We ask whether treewidth\n",
            "is the only structural criterion of the underlying graph that enables tractable\n",
            "inference. In other words, is there some class of structures with unbounded\n",
            "treewidth in which inference is tractable? Subject to a combinatorial\n",
            "hypothesis due to Robertson et al. (1994), we show that low treewidth is indeed\n",
            "the only structural restriction that can ensure tractability. Thus, even for\n",
            "the \"best case\" graph structure, there is no inference algorithm with\n",
            "complexity polynomial in the treewidth.\n",
            "\n",
            "1241. Title: Emu: Generative Pretraining in Multimodality\n",
            "   Abstract: Exploring the narratives conveyed by fine-art paintings is a challenge in\n",
            "image captioning, where the goal is to generate descriptions that not only\n",
            "precisely represent the visual content but also offer a in-depth interpretation\n",
            "of the artwork's meaning. The task is particularly complex for artwork images\n",
            "due to their diverse interpretations and varied aesthetic principles across\n",
            "different artistic schools and styles. In response to this, we present KALE\n",
            "Knowledge-Augmented vision-Language model for artwork Elaborations), a novel\n",
            "approach that enhances existing vision-language models by integrating artwork\n",
            "metadata as additional knowledge. KALE incorporates the metadata in two ways:\n",
            "firstly as direct textual input, and secondly through a multimodal\n",
            "heterogeneous knowledge graph. To optimize the learning of graph\n",
            "representations, we introduce a new cross-modal alignment loss that maximizes\n",
            "the similarity between the image and its corresponding metadata. Experimental\n",
            "results demonstrate that KALE achieves strong performance (when evaluated with\n",
            "CIDEr, in particular) over existing state-of-the-art work across several\n",
            "artwork datasets. Source code of the project is available at\n",
            "https://github.com/Yanbei-Jiang/Artwork-Interpretation.\n",
            "\n",
            "1242. Title: SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents\n",
            "   Abstract: Steerable CNN imposes the prior knowledge of transformation invariance or\n",
            "equivariance in the network architecture to enhance the the network robustness\n",
            "on geometry transformation of data and reduce overfitting. It has been an\n",
            "intuitive and widely used technique to construct a steerable filter by\n",
            "augmenting a filter with its transformed copies in the past decades, which is\n",
            "named as filter transform in this paper. Recently, the problem of steerable CNN\n",
            "has been studied from aspect of group representation theory, which reveals the\n",
            "function space structure of a steerable kernel function. However, it is not yet\n",
            "clear on how this theory is related to the filter transform technique. In this\n",
            "paper, we show that kernel constructed by filter transform can also be\n",
            "interpreted in the group representation theory. This interpretation help\n",
            "complete the puzzle of steerable CNN theory and provides a novel and simple\n",
            "approach to implement steerable convolution operators. Experiments are executed\n",
            "on multiple datasets to verify the feasibility of the proposed approach.\n",
            "\n",
            "1243. Title: A Hierarchical Bayesian Model for Few-Shot Meta Learning\n",
            "   Abstract: We propose a novel hierarchical Bayesian model for learning with a large\n",
            "(possibly infinite) number of tasks/episodes, which suits well the few-shot\n",
            "meta learning problem. We consider episode-wise random variables to model\n",
            "episode-specific target generative processes, where these local random\n",
            "variables are governed by a higher-level global random variate. The global\n",
            "variable helps memorize the important information from historic episodes while\n",
            "controlling how much the model needs to be adapted to new episodes in a\n",
            "principled Bayesian manner. Within our model framework, the prediction on a\n",
            "novel episode/task can be seen as a Bayesian inference problem. However, a main\n",
            "obstacle in learning with a large/infinite number of local random variables in\n",
            "online nature, is that one is not allowed to store the posterior distribution\n",
            "of the current local random variable for frequent future updates, typical in\n",
            "conventional variational inference. We need to be able to treat each local\n",
            "variable as a one-time iterate in the optimization. We propose a\n",
            "Normal-Inverse-Wishart model, for which we show that this one-time iterate\n",
            "optimization becomes feasible due to the approximate closed-form solutions for\n",
            "the local posterior distributions. The resulting algorithm is more attractive\n",
            "than the MAML in that it is not required to maintain computational graphs for\n",
            "the whole gradient optimization steps per episode. Our approach is also\n",
            "different from existing Bayesian meta learning methods in that unlike dealing\n",
            "with a single random variable for the whole episodes, our approach has a\n",
            "hierarchical structure that allows one-time episodic optimization, desirable\n",
            "for principled Bayesian learning with many/infinite tasks. The code is\n",
            "available at \\url{https://github.com/minyoungkim21/niwmeta}.\n",
            "\n",
            "1244. Title: On the Vulnerability of Adversarially Trained Models Against Two-faced Attacks\n",
            "   Abstract: The advantages of pre-trained large language models (LLMs) are apparent in a\n",
            "variety of language processing tasks. But can a language model's knowledge be\n",
            "further harnessed to effectively disambiguate objects and navigate\n",
            "decision-making challenges within the realm of robotics? Our study reveals the\n",
            "LLM's aptitude for solving complex decision making challenges that are often\n",
            "previously modeled by Partially Observable Markov Decision Processes (POMDPs).\n",
            "A pivotal focus of our research is the object disambiguation capability of\n",
            "LLMs. We detail the integration of an LLM into a tabletop environment\n",
            "disambiguation task, a decision making problem where the robot's task is to\n",
            "discern and retrieve a user's desired object from an arbitrarily large and\n",
            "complex cluster of objects. Despite multiple query attempts with zero-shot\n",
            "prompt engineering (details can be found in the Appendix), the LLM struggled to\n",
            "inquire about features not explicitly provided in the scene description. In\n",
            "response, we have developed a few-shot prompt engineering system to improve the\n",
            "LLM's ability to pose disambiguating queries. The result is a model capable of\n",
            "both using given features when they are available and inferring new relevant\n",
            "features when necessary, to successfully generate and navigate down a precise\n",
            "decision tree to the correct object--even when faced with identical options.\n",
            "\n",
            "1245. Title: Analyzing Feed-Forward Blocks in Transformers through the Lens of Attention Maps\n",
            "   Abstract: Natural language to code generation is an important application area of LLMs\n",
            "and has received wide attention from the community. The majority of relevant\n",
            "studies have exclusively concentrated on increasing the quantity and functional\n",
            "correctness of training sets while disregarding other stylistic elements of\n",
            "programs. More recently, data quality has garnered a lot of interest and\n",
            "multiple works have showcased its importance for improving performance. In this\n",
            "work, we investigate data quality for code and find that making the code more\n",
            "structured and readable leads to improved code generation performance of the\n",
            "system. We build a novel data-cleaning pipeline that uses these principles to\n",
            "transform existing programs by 1.) renaming variables, 2.) modularizing and\n",
            "decomposing complex code into smaller helper sub-functions, and 3.) inserting\n",
            "natural-language based plans via LLM based transformations. We evaluate our\n",
            "approach on two challenging algorithmic code generation benchmarks and find\n",
            "that fine-tuning CodeLLaMa-7B on our transformed modularized programs improves\n",
            "the performance by up to 30% compared to fine-tuning on the original dataset.\n",
            "Additionally, we demonstrate improved performance from using a smaller amount\n",
            "of higher-quality data, finding that a model fine-tuned on the entire original\n",
            "dataset is outperformed by a model trained on 15% of our cleaned dataset. Even\n",
            "in comparison to closed-source models, our models outperform the much larger\n",
            "AlphaCoder models.\n",
            "\n",
            "1246. Title: LLM-Assisted Code Cleaning For Training Accurate Code Generators\n",
            "   Abstract: Transformers are ubiquitous in wide tasks. Interpreting their internals is a\n",
            "pivotal goal. Nevertheless, their particular components, feed-forward (FF)\n",
            "blocks, have typically been less analyzed despite their substantial parameter\n",
            "amounts. We analyze the input contextualization effects of FF blocks by\n",
            "rendering them in the attention maps as a human-friendly visualization scheme.\n",
            "Our experiments with both masked- and causal-language models reveal that FF\n",
            "networks modify the input contextualization to emphasize specific types of\n",
            "linguistic compositions. In addition, FF and its surrounding components tend to\n",
            "cancel out each other's effects, suggesting potential redundancy in the\n",
            "processing of the Transformer layer.\n",
            "\n",
            "1247. Title: Enabling Efficient Equivariant Operations in the Fourier Basis via Gaunt Tensor Products\n",
            "   Abstract: This survey delves into the application of diffusion models in time-series\n",
            "forecasting. Diffusion models are demonstrating state-of-the-art results in\n",
            "various fields of generative AI. The paper includes comprehensive background\n",
            "information on diffusion models, detailing their conditioning methods and\n",
            "reviewing their use in time-series forecasting. The analysis covers 11 specific\n",
            "time-series implementations, the intuition and theory behind them, the\n",
            "effectiveness on different datasets, and a comparison among each other. Key\n",
            "contributions of this work are the thorough exploration of diffusion models'\n",
            "applications in time-series forecasting and a chronologically ordered overview\n",
            "of these models. Additionally, the paper offers an insightful discussion on the\n",
            "current state-of-the-art in this domain and outlines potential future research\n",
            "directions. This serves as a valuable resource for researchers in AI and\n",
            "time-series analysis, offering a clear view of the latest advancements and\n",
            "future potential of diffusion models.\n",
            "\n",
            "1248. Title: RECOMP: Improving Retrieval-Augmented LMs with Context Compression and Selective Augmentation\n",
            "   Abstract: Developing equivariant neural networks for the E(3) group plays an important\n",
            "role in modeling 3D data across real-world applications. Enforcing this\n",
            "equivariance primarily involves the tensor products of irreducible\n",
            "representations (irreps). However, the computational complexity of such\n",
            "operations increases significantly as higher-order tensors are used. In this\n",
            "work, we propose a systematic approach to substantially accelerate the\n",
            "computation of the tensor products of irreps. We mathematically connect the\n",
            "commonly used Clebsch-Gordan coefficients to the Gaunt coefficients, which are\n",
            "integrals of products of three spherical harmonics. Through Gaunt coefficients,\n",
            "the tensor product of irreps becomes equivalent to the multiplication between\n",
            "spherical functions represented by spherical harmonics. This perspective\n",
            "further allows us to change the basis for the equivariant operations from\n",
            "spherical harmonics to a 2D Fourier basis. Consequently, the multiplication\n",
            "between spherical functions represented by a 2D Fourier basis can be\n",
            "efficiently computed via the convolution theorem and Fast Fourier Transforms.\n",
            "This transformation reduces the complexity of full tensor products of irreps\n",
            "from $\\mathcal{O}(L^6)$ to $\\mathcal{O}(L^3)$, where $L$ is the max degree of\n",
            "irreps. Leveraging this approach, we introduce the Gaunt Tensor Product, which\n",
            "serves as a new method to construct efficient equivariant operations across\n",
            "different model architectures. Our experiments on the Open Catalyst Project and\n",
            "3BPA datasets demonstrate both the increased efficiency and improved\n",
            "performance of our approach.\n",
            "\n",
            "1249. Title: Class Probability Matching with Calibrated Networks for Label Shift Adaption\n",
            "   Abstract: It is often stated that the second law of thermodynamics follows from the\n",
            "condition that at some given time in the past the entropy was lower than it is\n",
            "now. Formally, this condition is the statement that $E[S(t)|S(t_0)]$, the\n",
            "expected entropy of the universe at the current time $t$ conditioned on its\n",
            "value $S(t_0)$ at a time $t_0$ in the past, is an increasing function of $t $.\n",
            "We point out that in general this is incorrect. The epistemic axioms underlying\n",
            "probability theory say that we should condition expectations on all that we\n",
            "know, and on nothing that we do not know. Arguably, we know the value of the\n",
            "universe's entropy at the present time $t$ at least as well as its value at a\n",
            "time in the past, $t_0$. However, as we show here, conditioning expected\n",
            "entropy on its value at two times rather than one radically changes its\n",
            "dynamics, resulting in a unexpected, very rich structure. For example, the\n",
            "expectation value conditioned on two times can have a maximum at an\n",
            "intermediate time between $t_0$ and $t$, i.e., in our past. Moreover, it can\n",
            "have a negative rather than positive time derivative at the present. In such\n",
            "\"Boltzmann bridge\" situations, the second law would not hold at the present\n",
            "time. We illustrate and investigate these phenomena for a random walk model and\n",
            "an idealized gas model, and briefly discuss the role of Boltzmann bridges in\n",
            "our universe.\n",
            "\n",
            "1250. Title: Open the Black Box: Step-based Policy Updates for Temporally-Correlated Episodic Reinforcement Learning\n",
            "   Abstract: Backdoor attacks have been considered a severe security threat to deep\n",
            "learning. Such attacks can make models perform abnormally on inputs with\n",
            "predefined triggers and still retain state-of-the-art performance on clean\n",
            "data. While backdoor attacks have been thoroughly investigated in the image\n",
            "domain from both attackers' and defenders' sides, an analysis in the frequency\n",
            "domain has been missing thus far.\n",
            "  This paper first revisits existing backdoor triggers from a frequency\n",
            "perspective and performs a comprehensive analysis. Our results show that many\n",
            "current backdoor attacks exhibit severe high-frequency artifacts, which persist\n",
            "across different datasets and resolutions. We further demonstrate these\n",
            "high-frequency artifacts enable a simple way to detect existing backdoor\n",
            "triggers at a detection rate of 98.50% without prior knowledge of the attack\n",
            "details and the target model. Acknowledging previous attacks' weaknesses, we\n",
            "propose a practical way to create smooth backdoor triggers without\n",
            "high-frequency artifacts and study their detectability. We show that existing\n",
            "defense works can benefit by incorporating these smooth triggers into their\n",
            "design consideration. Moreover, we show that the detector tuned over stronger\n",
            "smooth triggers can generalize well to unseen weak smooth triggers. In short,\n",
            "our work emphasizes the importance of considering frequency analysis when\n",
            "designing both backdoor attacks and defenses in deep learning.\n",
            "\n",
            "1251. Title: KW-Design: Pushing the Limit of Protein Design via Knowledge Refinement\n",
            "   Abstract: Following the success of Large Language Models (LLMs), Large Multimodal\n",
            "Models (LMMs), such as the Flamingo model and its subsequent competitors, have\n",
            "started to emerge as natural steps towards generalist agents. However,\n",
            "interacting with recent LMMs reveals major limitations that are hardly captured\n",
            "by the current evaluation benchmarks. Indeed, task performances (e.g., VQA\n",
            "accuracy) alone do not provide enough clues to understand their real\n",
            "capabilities, limitations, and to which extent such models are aligned to human\n",
            "expectations. To refine our understanding of those flaws, we deviate from the\n",
            "current evaluation paradigm, and (1) evaluate 10 recent open-source LMMs from\n",
            "3B up to 80B parameter scale, on 5 different axes; hallucinations, abstention,\n",
            "compositionality, explainability and instruction following. Our evaluation on\n",
            "these axes reveals major flaws in LMMs. While the current go-to solution to\n",
            "align these models is based on training, such as instruction tuning or RLHF, we\n",
            "rather (2) explore the training-free in-context learning (ICL) as a solution,\n",
            "and study how it affects these limitations. Based on our ICL study, (3) we push\n",
            "ICL further and propose new multimodal ICL variants such as; Multitask-ICL,\n",
            "Chain-of-Hindsight-ICL, and Self-Correcting-ICL. Our findings are as follows.\n",
            "(1) Despite their success, LMMs have flaws that remain unsolved with scaling\n",
            "alone. (2) The effect of ICL on LMMs flaws is nuanced; despite its\n",
            "effectiveness for improved explainability, answer abstention, ICL only slightly\n",
            "improves instruction following, does not improve compositional abilities, and\n",
            "actually even amplifies hallucinations. (3) The proposed ICL variants are\n",
            "promising as post-hoc approaches to efficiently tackle some of those flaws. The\n",
            "code is available here: https://github.com/mshukor/EvALign-ICL.\n",
            "\n",
            "1252. Title: Skeleton-of-Thought: Prompting LLMs for Efficient Parallel Generation\n",
            "   Abstract: Scaling text-to-speech to a large and wild dataset has been proven to be\n",
            "highly effective in achieving timbre and speech style generalization,\n",
            "particularly in zero-shot TTS. However, previous works usually encode speech\n",
            "into latent using audio codec and use autoregressive language models or\n",
            "diffusion models to generate it, which ignores the intrinsic nature of speech\n",
            "and may lead to inferior or uncontrollable results. We argue that speech can be\n",
            "decomposed into several attributes (e.g., content, timbre, prosody, and phase)\n",
            "and each of them should be modeled using a module with appropriate inductive\n",
            "biases. From this perspective, we carefully design a novel and large zero-shot\n",
            "TTS system called Mega-TTS, which is trained with large-scale wild data and\n",
            "models different attributes in different ways: 1) Instead of using latent\n",
            "encoded by audio codec as the intermediate feature, we still choose spectrogram\n",
            "as it separates the phase and other attributes very well. Phase can be\n",
            "appropriately constructed by the GAN-based vocoder and does not need to be\n",
            "modeled by the language model. 2) We model the timbre using global vectors\n",
            "since timbre is a global attribute that changes slowly over time. 3) We further\n",
            "use a VQGAN-based acoustic model to generate the spectrogram and a latent code\n",
            "language model to fit the distribution of prosody, since prosody changes\n",
            "quickly over time in a sentence, and language models can capture both local and\n",
            "long-range dependencies. We scale Mega-TTS to multi-domain datasets with 20K\n",
            "hours of speech and evaluate its performance on unseen speakers. Experimental\n",
            "results demonstrate that Mega-TTS surpasses state-of-the-art TTS systems on\n",
            "zero-shot TTS, speech editing, and cross-lingual TTS tasks, with superior\n",
            "naturalness, robustness, and speaker similarity due to the proper inductive\n",
            "bias of each module. Audio samples are available at\n",
            "https://mega-tts.github.io/demo-page.\n",
            "\n",
            "1253. Title: Bridging State and History Representations: Understanding Self-Predictive RL\n",
            "   Abstract: Deep learning technology has made great achievements in the field of image.\n",
            "In order to defend against malware attacks, researchers have proposed many\n",
            "Windows malware detection models based on deep learning. However, deep learning\n",
            "models are vulnerable to adversarial example attacks. Malware can generate\n",
            "adversarial malware with the same malicious function to attack the malware\n",
            "detection model and evade detection of the model. Currently, many adversarial\n",
            "defense studies have been proposed, but existing adversarial defense studies\n",
            "are based on image sample and cannot be directly applied to malware sample.\n",
            "Therefore, this paper proposes an adversarial malware defense method based on\n",
            "adversarial training. This method uses preprocessing to defend simple\n",
            "adversarial examples to reduce the difficulty of adversarial training.\n",
            "Moreover, this method improves the adversarial defense capability of the model\n",
            "through adversarial training. We experimented with three attack methods in two\n",
            "sets of datasets, and the results show that the method in this paper can\n",
            "improve the adversarial defense capability of the model without reducing the\n",
            "accuracy of the model.\n",
            "\n",
            "1254. Title: FedCompass: Efficient Cross-Silo Federated Learning on Heterogeneous Client Devices Using a Computing Power-Aware Scheduler\n",
            "   Abstract: In domain adaptation, covariate shift and label shift problems are two\n",
            "distinct and complementary tasks. In covariate shift adaptation where the\n",
            "differences in data distribution arise from variations in feature\n",
            "probabilities, existing approaches naturally address this problem based on\n",
            "\\textit{feature probability matching} (\\textit{FPM}). However, for label shift\n",
            "adaptation where the differences in data distribution stem solely from\n",
            "variations in class probability, current methods still use FPM on the\n",
            "$d$-dimensional feature space to estimate the class probability ratio on the\n",
            "one-dimensional label space. To address label shift adaptation more naturally\n",
            "and effectively, inspired by a new representation of the source domain's class\n",
            "probability, we propose a new framework called \\textit{class probability\n",
            "matching} (\\textit{CPM}) which matches two class probability functions on the\n",
            "one-dimensional label space to estimate the class probability ratio,\n",
            "fundamentally different from FPM operating on the $d$-dimensional feature\n",
            "space. Furthermore, by incorporating the kernel logistic regression into the\n",
            "CPM framework to estimate the conditional probability, we propose an algorithm\n",
            "called \\textit{class probability matching using kernel methods}\n",
            "(\\textit{CPMKM}) for label shift adaptation. From the theoretical perspective,\n",
            "we establish the optimal convergence rates of CPMKM with respect to the\n",
            "cross-entropy loss for multi-class label shift adaptation. From the\n",
            "experimental perspective, comparisons on real datasets demonstrate that CPMKM\n",
            "outperforms existing FPM-based and maximum-likelihood-based algorithms.\n",
            "\n",
            "1255. Title: Views Can Be Deceiving: Improved SSL Through Feature Space Augmentation\n",
            "   Abstract: Current advancements in reinforcement learning (RL) have predominantly\n",
            "focused on learning step-based policies that generate actions for each\n",
            "perceived state. While these methods efficiently leverage step information from\n",
            "environmental interaction, they often ignore the temporal correlation between\n",
            "actions, resulting in inefficient exploration and unsmooth trajectories that\n",
            "are challenging to implement on real hardware. Episodic RL (ERL) seeks to\n",
            "overcome these challenges by exploring in parameters space that capture the\n",
            "correlation of actions. However, these approaches typically compromise data\n",
            "efficiency, as they treat trajectories as opaque \\emph{black boxes}. In this\n",
            "work, we introduce a novel ERL algorithm, Temporally-Correlated Episodic RL\n",
            "(TCE), which effectively utilizes step information in episodic policy updates,\n",
            "opening the 'black box' in existing ERL methods while retaining the smooth and\n",
            "consistent exploration in parameter space. TCE synergistically combines the\n",
            "advantages of step-based and episodic RL, achieving comparable performance to\n",
            "recent ERL methods while maintaining data efficiency akin to state-of-the-art\n",
            "(SoTA) step-based RL.\n",
            "\n",
            "1256. Title: Mega-TTS 2: Boosting Prompting Mechanisms for Zero-Shot Speech Synthesis\n",
            "   Abstract: Text-guided diffusion models have become a popular tool in image synthesis,\n",
            "known for producing high-quality and diverse images. However, their application\n",
            "to editing real images often encounters hurdles primarily due to the text\n",
            "condition deteriorating the reconstruction quality and subsequently affecting\n",
            "editing fidelity. Null-text Inversion (NTI) has made strides in this area, but\n",
            "it fails to capture spatial context and requires computationally intensive\n",
            "per-timestep optimization. Addressing these challenges, we present Noise Map\n",
            "Guidance (NMG), an inversion method rich in a spatial context, tailored for\n",
            "real-image editing. Significantly, NMG achieves this without necessitating\n",
            "optimization, yet preserves the editing quality. Our empirical investigations\n",
            "highlight NMG's adaptability across various editing techniques and its\n",
            "robustness to variants of DDIM inversions.\n",
            "\n",
            "1257. Title: Scalable Language Model with Generalized Continual Learning\n",
            "   Abstract: Supervised learning methods have been found to exhibit inductive biases\n",
            "favoring simpler features. When such features are spuriously correlated with\n",
            "the label, this can result in suboptimal performance on minority subgroups.\n",
            "Despite the growing popularity of methods which learn from unlabeled data, the\n",
            "extent to which these representations rely on spurious features for prediction\n",
            "is unclear. In this work, we explore the impact of spurious features on\n",
            "Self-Supervised Learning (SSL) for visual representation learning. We first\n",
            "empirically show that commonly used augmentations in SSL can cause undesired\n",
            "invariances in the image space, and illustrate this with a simple example. We\n",
            "further show that classical approaches in combating spurious correlations, such\n",
            "as dataset re-sampling during SSL, do not consistently lead to invariant\n",
            "representations. Motivated by these findings, we propose LateTVG to remove\n",
            "spurious information from these representations during pre-training, by\n",
            "regularizing later layers of the encoder via pruning. We find that our method\n",
            "produces representations which outperform the baselines on several benchmarks,\n",
            "without the need for group or label information during SSL.\n",
            "\n",
            "1258. Title: Expressive Losses for Verified Robustness via Convex Combinations\n",
            "   Abstract: Continual learning has gained increasing importance as it facilitates the\n",
            "acquisition and refinement of scalable knowledge and skills in language models.\n",
            "However, existing methods typically encounter strict limitations and challenges\n",
            "in real-world scenarios, such as reliance on experience replay, optimization\n",
            "constraints, and inference task-ID. In this study, we introduce the Scalable\n",
            "Language Model (SLM) to overcome these limitations within a more challenging\n",
            "and generalized setting, representing a significant advancement toward\n",
            "practical applications for continual learning. Specifically, we propose the\n",
            "Joint Adaptive Re-Parameterization (JARe), integrated with Dynamic Task-related\n",
            "Knowledge Retrieval (DTKR), to enable adaptive adjustment of language models\n",
            "based on specific downstream tasks. This approach leverages the task\n",
            "distribution within the vector space, aiming to achieve a smooth and effortless\n",
            "continual learning process. Our method demonstrates state-of-the-art\n",
            "performance on diverse backbones and benchmarks, achieving effective continual\n",
            "learning in both full-set and few-shot scenarios with minimal forgetting.\n",
            "Moreover, while prior research primarily focused on a single task type such as\n",
            "classification, our study goes beyond, with the large language model, i.e.,\n",
            "LLaMA-2, to explore the effects across diverse domains and task types, such\n",
            "that a single language model can be decently scaled to broader applications.\n",
            "\n",
            "1259. Title: HAZARD Challenge: Embodied Decision Making in Dynamically Changing Environments\n",
            "   Abstract: In order to train networks for verified adversarial robustness, it is common\n",
            "to over-approximate the worst-case loss over perturbation regions, resulting in\n",
            "networks that attain verifiability at the expense of standard performance. As\n",
            "shown in recent work, better trade-offs between accuracy and robustness can be\n",
            "obtained by carefully coupling adversarial training with over-approximations.\n",
            "We hypothesize that the expressivity of a loss function, which we formalize as\n",
            "the ability to span a range of trade-offs between lower and upper bounds to the\n",
            "worst-case loss through a single parameter (the over-approximation\n",
            "coefficient), is key to attaining state-of-the-art performance. To support our\n",
            "hypothesis, we show that trivial expressive losses, obtained via convex\n",
            "combinations between adversarial attacks and IBP bounds, yield state-of-the-art\n",
            "results across a variety of settings in spite of their conceptual simplicity.\n",
            "We provide a detailed analysis of the relationship between the\n",
            "over-approximation coefficient and performance profiles across different\n",
            "expressive losses, showing that, while expressivity is essential, better\n",
            "approximations of the worst-case loss are not necessarily linked to superior\n",
            "robustness-accuracy trade-offs.\n",
            "\n",
            "1260. Title: Mayfly: a Neural Data Structure for Graph Stream Summarization\n",
            "   Abstract: In this paper, we introduce a novel task called language-guided joint\n",
            "audio-visual editing. Given an audio and image pair of a sounding event, this\n",
            "task aims at generating new audio-visual content by editing the given sounding\n",
            "event conditioned on the language guidance. For instance, we can alter the\n",
            "background environment of a sounding object while keeping its appearance\n",
            "unchanged, or we can add new sounds contextualized to the visual content. To\n",
            "address this task, we propose a new diffusion-based framework for joint\n",
            "audio-visual editing and introduce two key ideas. Firstly, we propose a\n",
            "one-shot adaptation approach to tailor generative diffusion models for\n",
            "audio-visual content editing. With as few as one audio-visual sample, we\n",
            "jointly transfer the audio and vision diffusion models to the target domain.\n",
            "After fine-tuning, our model enables consistent generation of this audio-visual\n",
            "sample. Secondly, we introduce a cross-modal semantic enhancement approach. We\n",
            "observe that when using language as content editing guidance, the vision branch\n",
            "may overlook editing requirements. This phenomenon, termed catastrophic\n",
            "neglect, hampers audio-visual alignment during content editing. We therefore\n",
            "enhance semantic consistency between language and vision to mitigate this\n",
            "issue. Extensive experiments validate the effectiveness of our method in\n",
            "language-based audio-visual editing and highlight its superiority over several\n",
            "baseline approaches. We recommend that readers visit our project page for more\n",
            "details: https://liangsusan-git.github.io/project/avedit/.\n",
            "\n",
            "1261. Title: The Consensus Game: Language Model Generation via Equilibrium Search\n",
            "   Abstract: This paper studies federated linear contextual bandits under the notion of\n",
            "user-level differential privacy (DP). We first introduce a unified federated\n",
            "bandits framework that can accommodate various definitions of DP in the\n",
            "sequential decision-making setting. We then formally introduce user-level\n",
            "central DP (CDP) and local DP (LDP) in the federated bandits framework, and\n",
            "investigate the fundamental trade-offs between the learning regrets and the\n",
            "corresponding DP guarantees in a federated linear contextual bandits model. For\n",
            "CDP, we propose a federated algorithm termed as $\\texttt{ROBIN}$ and show that\n",
            "it is near-optimal in terms of the number of clients $M$ and the privacy budget\n",
            "$\\varepsilon$ by deriving nearly-matching upper and lower regret bounds when\n",
            "user-level DP is satisfied. For LDP, we obtain several lower bounds, indicating\n",
            "that learning under user-level $(\\varepsilon,\\delta)$-LDP must suffer a regret\n",
            "blow-up factor at least $\\min\\{1/\\varepsilon,M\\}$ or\n",
            "$\\min\\{1/\\sqrt{\\varepsilon},\\sqrt{M}\\}$ under different conditions.\n",
            "\n",
            "1262. Title: Federated Orthogonal Training: Mitigating Global Catastrophic Forgetting in Continual Federated Learning\n",
            "   Abstract: Federated Learning (FL) has gained significant attraction due to its ability\n",
            "to enable privacy-preserving training over decentralized data. Current\n",
            "literature in FL mostly focuses on single-task learning. However, over time,\n",
            "new tasks may appear in the clients and the global model should learn these\n",
            "tasks without forgetting previous tasks. This real-world scenario is known as\n",
            "Continual Federated Learning (CFL). The main challenge of CFL is Global\n",
            "Catastrophic Forgetting, which corresponds to the fact that when the global\n",
            "model is trained on new tasks, its performance on old tasks decreases. There\n",
            "have been a few recent works on CFL to propose methods that aim to address the\n",
            "global catastrophic forgetting problem. However, these works either have\n",
            "unrealistic assumptions on the availability of past data samples or violate the\n",
            "privacy principles of FL. We propose a novel method, Federated Orthogonal\n",
            "Training (FOT), to overcome these drawbacks and address the global catastrophic\n",
            "forgetting in CFL. Our algorithm extracts the global input subspace of each\n",
            "layer for old tasks and modifies the aggregated updates of new tasks such that\n",
            "they are orthogonal to the global principal subspace of old tasks for each\n",
            "layer. This decreases the interference between tasks, which is the main cause\n",
            "for forgetting. We empirically show that FOT outperforms state-of-the-art\n",
            "continual learning methods in the CFL setting, achieving an average accuracy\n",
            "gain of up to 15% with 27% lower forgetting while only incurring a minimal\n",
            "computation and communication cost.\n",
            "\n",
            "1263. Title: Self-Supervised Contrastive Learning for Long-term Forecasting\n",
            "   Abstract: In real-world environments, autonomous agents rely on their egocentric\n",
            "observations. They must learn adaptive strategies to interact with others who\n",
            "possess mixed motivations, discernible only through visible cues. Several\n",
            "Multi-Agent Reinforcement Learning (MARL) methods adopt centralized approaches\n",
            "that involve either centralized training or reward-sharing, often violating the\n",
            "realistic ways in which living organisms, like animals or humans, process\n",
            "information and interact. MARL strategies deploying decentralized training with\n",
            "intrinsic motivation offer a self-supervised approach, enable agents to develop\n",
            "flexible social strategies through the interaction of autonomous agents.\n",
            "However, by contrasting the self-supervised and centralized methods, we reveal\n",
            "that populations trained with reward-sharing methods surpass those using\n",
            "self-supervised methods in a mixed-motive environment. We link this superiority\n",
            "to specialized role emergence and an agent's expertise in its role.\n",
            "Interestingly, this gap shrinks in pure-motive settings, emphasizing the need\n",
            "for evaluations in more complex, realistic environments (mixed-motive). Our\n",
            "preliminary results suggest a gap in population performance that can be closed\n",
            "by improving self-supervised methods and thereby pushing MARL closer to\n",
            "real-world readiness.\n",
            "\n",
            "1264. Title: Listen, Think, and Understand\n",
            "   Abstract: When applied to question answering and other text generation tasks, language\n",
            "models (LMs) may be queried generatively (by sampling answers from their output\n",
            "distribution) or discriminatively (by using them to score or rank a set of\n",
            "candidate outputs). These procedures sometimes yield very different\n",
            "predictions. How do we reconcile mutually incompatible scoring procedures to\n",
            "obtain coherent LM predictions? We introduce a new, a training-free,\n",
            "game-theoretic procedure for language model decoding. Our approach casts\n",
            "language model decoding as a regularized imperfect-information sequential\n",
            "signaling game - which we term the CONSENSUS GAME - in which a GENERATOR seeks\n",
            "to communicate an abstract correctness parameter using natural language\n",
            "sentences to a DISCRIMINATOR. We develop computational procedures for finding\n",
            "approximate equilibria of this game, resulting in a decoding algorithm we call\n",
            "EQUILIBRIUM-RANKING. Applied to a large number of tasks (including reading\n",
            "comprehension, commonsense reasoning, mathematical problem-solving, and\n",
            "dialog), EQUILIBRIUM-RANKING consistently, and sometimes substantially,\n",
            "improves performance over existing LM decoding procedures - on multiple\n",
            "benchmarks, we observe that applying EQUILIBRIUM-RANKING to LLaMA-7B\n",
            "outperforms the much larger LLaMA-65B and PaLM-540B models. These results\n",
            "highlight the promise of game-theoretic tools for addressing fundamental\n",
            "challenges of truthfulness and consistency in LMs.\n",
            "\n",
            "1265. Title: Communication-Efficient Federated Non-Linear Bandit Optimization\n",
            "   Abstract: Recent advances in high-fidelity virtual environments serve as one of the\n",
            "major driving forces for building intelligent embodied agents to perceive,\n",
            "reason and interact with the physical world. Typically, these environments\n",
            "remain unchanged unless agents interact with them. However, in real-world\n",
            "scenarios, agents might also face dynamically changing environments\n",
            "characterized by unexpected events and need to rapidly take action accordingly.\n",
            "To remedy this gap, we propose a new simulated embodied benchmark, called\n",
            "HAZARD, specifically designed to assess the decision-making abilities of\n",
            "embodied agents in dynamic situations. HAZARD consists of three unexpected\n",
            "disaster scenarios, including fire, flood, and wind, and specifically supports\n",
            "the utilization of large language models (LLMs) to assist common sense\n",
            "reasoning and decision-making. This benchmark enables us to evaluate autonomous\n",
            "agents' decision-making capabilities across various pipelines, including\n",
            "reinforcement learning (RL), rule-based, and search-based methods in\n",
            "dynamically changing environments. As a first step toward addressing this\n",
            "challenge using large language models, we further develop an LLM-based agent\n",
            "and perform an in-depth analysis of its promise and challenge of solving these\n",
            "challenging tasks. HAZARD is available at https://vis-www.cs.umass.edu/hazard/.\n",
            "\n",
            "1266. Title: Cross-Modal Contextualized Diffusion Models for Text-Guided Visual Generation and Editing\n",
            "   Abstract: The ability of artificial intelligence (AI) systems to perceive and\n",
            "comprehend audio signals is crucial for many applications. Although significant\n",
            "progress has been made in this area since the development of AudioSet, most\n",
            "existing models are designed to map audio inputs to pre-defined, discrete sound\n",
            "label sets. In contrast, humans possess the ability to not only classify sounds\n",
            "into general categories, but also to listen to the finer details of the sounds,\n",
            "explain the reason for the predictions, think about what the sound infers, and\n",
            "understand the scene and what action needs to be taken, if any. Such\n",
            "capabilities beyond perception are not yet present in existing audio models. On\n",
            "the other hand, modern large language models (LLMs) exhibit emerging reasoning\n",
            "ability but they lack audio perception capabilities. Therefore, we ask the\n",
            "question: can we build a model that has both audio perception and a reasoning\n",
            "ability?\n",
            "  In this paper, we propose a new audio foundation model, called LTU (Listen,\n",
            "Think, and Understand). To train LTU, we created a new OpenAQA-5M dataset\n",
            "consisting of 1.9 million closed-ended and 3.7 million open-ended, diverse\n",
            "(audio, question, answer) tuples, and have used an autoregressive training\n",
            "framework with a perception-to-understanding curriculum. LTU demonstrates\n",
            "strong performance and generalization ability on conventional audio tasks such\n",
            "as classification and captioning. More importantly, it exhibits emerging audio\n",
            "reasoning and comprehension abilities that are absent in existing audio models.\n",
            "To the best of our knowledge, LTU is one of the first multimodal large language\n",
            "models that focus on general audio (rather than just speech) understanding.\n",
            "\n",
            "1267. Title: Monte Carlo guided Denoising Diffusion models for Bayesian linear inverse problems.\n",
            "   Abstract: Ill-posed linear inverse problems arise frequently in various applications,\n",
            "from computational photography to medical imaging. A recent line of research\n",
            "exploits Bayesian inference with informative priors to handle the ill-posedness\n",
            "of such problems. Amongst such priors, score-based generative models (SGM) have\n",
            "recently been successfully applied to several different inverse problems. In\n",
            "this study, we exploit the particular structure of the prior defined by the SGM\n",
            "to define a sequence of intermediate linear inverse problems. As the noise\n",
            "level decreases, the posteriors of these inverse problems get closer to the\n",
            "target posterior of the original inverse problem. To sample from this sequence\n",
            "of posteriors, we propose the use of Sequential Monte Carlo (SMC) methods. The\n",
            "proposed algorithm, MCGDiff, is shown to be theoretically grounded and we\n",
            "provide numerical simulations showing that it outperforms competing baselines\n",
            "when dealing with ill-posed inverse problems in a Bayesian setting.\n",
            "\n",
            "1268. Title: Structural Estimation of Partially Observed Linear Non-Gaussian Acyclic Model: A Practical Approach with Identifiability\n",
            "   Abstract: Deep Ensembles (DEs) demonstrate improved accuracy, calibration and\n",
            "robustness to perturbations over single neural networks partly due to their\n",
            "functional diversity. Particle-based variational inference (ParVI) methods\n",
            "enhance diversity by formalizing a repulsion term based on a network similarity\n",
            "kernel. However, weight-space repulsion is inefficient due to\n",
            "over-parameterization, while direct function-space repulsion has been found to\n",
            "produce little improvement over DEs. To sidestep these difficulties, we propose\n",
            "First-order Repulsive Deep Ensemble (FoRDE), an ensemble learning method based\n",
            "on ParVI, which performs repulsion in the space of first-order input gradients.\n",
            "As input gradients uniquely characterize a function up to translation and are\n",
            "much smaller in dimension than the weights, this method guarantees that\n",
            "ensemble members are functionally different. Intuitively, diversifying the\n",
            "input gradients encourages each network to learn different features, which is\n",
            "expected to improve the robustness of an ensemble. Experiments on image\n",
            "classification datasets and transfer learning tasks show that FoRDE\n",
            "significantly outperforms the gold-standard DEs and other ensemble methods in\n",
            "accuracy and calibration under covariate shift due to input perturbations.\n",
            "\n",
            "1269. Title: FedInverse: Evaluating Privacy Leakage in Federated Learning\n",
            "   Abstract: Existing analyses of the expressive capacity of Transformer models have\n",
            "required excessively deep layers for data memorization, leading to a\n",
            "discrepancy with the Transformers actually used in practice. This is primarily\n",
            "due to the interpretation of the softmax function as an approximation of the\n",
            "hardmax function. By clarifying the connection between the softmax function and\n",
            "the Boltzmann operator, we prove that a single layer of self-attention with\n",
            "low-rank weight matrices possesses the capability to perfectly capture the\n",
            "context of an entire input sequence. As a consequence, we show that one-layer\n",
            "and single-head Transformers have a memorization capacity for finite samples,\n",
            "and that Transformers consisting of one self-attention layer with two\n",
            "feed-forward neural networks are universal approximators for continuous\n",
            "permutation equivariant functions on a compact domain.\n",
            "\n",
            "1270. Title: Rethinking the Power of Graph Canonization in Graph Representation Learning with Stability\n",
            "   Abstract: Recently, there has been a surge of interest in employing neural networks for\n",
            "graph generation, a fundamental statistical learning problem with critical\n",
            "applications like molecule design and community analysis. However, most\n",
            "approaches encounter significant limitations when generating large-scale\n",
            "graphs. This is due to their requirement to output the full adjacency matrices\n",
            "whose size grows quadratically with the number of nodes. In response to this\n",
            "challenge, we introduce a new, simple, and scalable graph representation named\n",
            "gap encoded edge list (GEEL) that has a small representation size that aligns\n",
            "with the number of edges. In addition, GEEL significantly reduces the\n",
            "vocabulary size by incorporating the gap encoding and bandwidth restriction\n",
            "schemes. GEEL can be autoregressively generated with the incorporation of node\n",
            "positional encoding, and we further extend GEEL to deal with attributed graphs\n",
            "by designing a new grammar. Our findings reveal that the adoption of this\n",
            "compact representation not only enhances scalability but also bolsters\n",
            "performance by simplifying the graph generation process. We conduct a\n",
            "comprehensive evaluation across ten non-attributed and two molecular graph\n",
            "generation tasks, demonstrating the effectiveness of GEEL.\n",
            "\n",
            "1271. Title: Tight Rates in Supervised Outlier Transfer Learning\n",
            "   Abstract: Estimating causal models from observational data is a crucial task in data\n",
            "analysis. For continuous-valued data, Shimizu et al. have proposed a linear\n",
            "acyclic non-Gaussian model to understand the data generating process, and have\n",
            "shown that their model is identifiable when the number of data is sufficiently\n",
            "large. However, situations in which continuous and discrete variables coexist\n",
            "in the same problem are common in practice. Most existing causal discovery\n",
            "methods either ignore the discrete data and apply a continuous-valued algorithm\n",
            "or discretize all the continuous data and then apply a discrete Bayesian\n",
            "network approach. These methods possibly loss important information when we\n",
            "ignore discrete data or introduce the approximation error due to\n",
            "discretization. In this paper, we define a novel hybrid causal model which\n",
            "consists of both continuous and discrete variables. The model assumes: (1) the\n",
            "value of a continuous variable is a linear function of its parent variables\n",
            "plus a non-Gaussian noise, and (2) each discrete variable is a logistic\n",
            "variable whose distribution parameters depend on the values of its parent\n",
            "variables. In addition, we derive the BIC scoring function for model selection.\n",
            "The new discovery algorithm can learn causal structures from mixed continuous\n",
            "and discrete data without discretization. We empirically demonstrate the power\n",
            "of our method through thorough simulations.\n",
            "\n",
            "1272. Title: MIntRec2.0: A Large-scale Benchmark Dataset for Multimodal Intent Recognition and Out-of-scope Detection in Conversations\n",
            "   Abstract: A critical barrier to learning an accurate decision rule for outlier\n",
            "detection is the scarcity of outlier data. As such, practitioners often turn to\n",
            "the use of similar but imperfect outlier data from which they might transfer\n",
            "information to the target outlier detection task. Despite the recent empirical\n",
            "success of transfer learning approaches in outlier detection, a fundamental\n",
            "understanding of when and how knowledge can be transferred from a source to a\n",
            "target outlier detection task remains elusive. In this work, we adopt the\n",
            "traditional framework of Neyman-Pearson classification -- which formalizes\n",
            "supervised outlier detection -- with the added assumption that one has access\n",
            "to some related but imperfect outlier data. Our main results are as follows:\n",
            "  We first determine the information-theoretic limits of the problem under a\n",
            "measure of discrepancy that extends some existing notions from traditional\n",
            "balanced classification; interestingly, unlike in balanced classification,\n",
            "seemingly very dissimilar sources can provide much information about a target,\n",
            "thus resulting in fast transfer.\n",
            "  We then show that, in principle, these information-theoretic limits are\n",
            "achievable by adaptive procedures, i.e., procedures with no a priori\n",
            "information on the discrepancy between source and target outlier distributions.\n",
            "\n",
            "1273. Title: Optimal transport based adversarial patch to leverage large scale attack transferability\n",
            "   Abstract: The expressivity of Graph Neural Networks (GNNs) has been studied broadly in\n",
            "recent years to reveal the design principles for more powerful GNNs. Graph\n",
            "canonization is known as a typical approach to distinguish non-isomorphic\n",
            "graphs, yet rarely adopted when developing expressive GNNs. This paper proposes\n",
            "to maximize the expressivity of GNNs by graph canonization, then the power of\n",
            "such GNNs is studies from the perspective of model stability. A stable GNN will\n",
            "map similar graphs to close graph representations in the vectorial space, and\n",
            "the stability of GNNs is critical to generalize their performance to unseen\n",
            "graphs. We theoretically reveal the trade-off of expressivity and stability in\n",
            "graph-canonization-enhanced GNNs. Then we introduce a notion of universal graph\n",
            "canonization as the general solution to address the trade-off and characterize\n",
            "a widely applicable sufficient condition to solve the universal graph\n",
            "canonization. A comprehensive set of experiments demonstrates the effectiveness\n",
            "of the proposed method. In many popular graph benchmark datasets, graph\n",
            "canonization successfully enhances GNNs and provides highly competitive\n",
            "performance, indicating the capability and great potential of proposed method\n",
            "in general graph representation learning. In graph datasets where the\n",
            "sufficient condition holds, GNNs enhanced by universal graph canonization\n",
            "consistently outperform GNN baselines and successfully improve the SOTA\n",
            "performance up to $31\\%$, providing the optimal solution to numerous\n",
            "challenging real-world graph analytical tasks like gene network representation\n",
            "learning in bioinformatics.\n",
            "\n",
            "1274. Title: AdaMerging: Adaptive Model Merging for Multi-Task Learning\n",
            "   Abstract: The electronic design automation (EDA) community has been actively exploring\n",
            "machine learning (ML) for very large-scale integrated computer-aided design\n",
            "(VLSI CAD). Many studies explored learning-based techniques for cross-stage\n",
            "prediction tasks in the design flow to achieve faster design convergence.\n",
            "Although building ML models usually requires a large amount of data, most\n",
            "studies can only generate small internal datasets for validation because of the\n",
            "lack of large public datasets. In this essay, we present the first open-source\n",
            "dataset called CircuitNet for ML tasks in VLSI CAD.\n",
            "\n",
            "1275. Title: Out-of-Distribution Detection with Negative Prompts\n",
            "   Abstract: Video denoising refers to the problem of removing \"noise\" from a video\n",
            "sequence. Here the term \"noise\" is used in a broad sense to refer to any\n",
            "corruption or outlier or interference that is not the quantity of interest. In\n",
            "this work, we develop a novel approach to video denoising that is based on the\n",
            "idea that many noisy or corrupted videos can be split into three parts - the\n",
            "\"low-rank layer\", the \"sparse layer\", and a small residual (which is small and\n",
            "bounded). We show, using extensive experiments, that our denoising approach\n",
            "outperforms the state-of-the-art denoising algorithms.\n",
            "\n",
            "1276. Title: FedCDA: Federated Learning with Cross-rounds Divergence-aware Aggregation\n",
            "   Abstract: The vision-language model has brought great improvement to few-shot\n",
            "industrial anomaly detection, which usually needs to design of hundreds of\n",
            "prompts through prompt engineering. For automated scenarios, we first use\n",
            "conventional prompt learning with many-class paradigm as the baseline to\n",
            "automatically learn prompts but found that it can not work well in one-class\n",
            "anomaly detection. To address the above problem, this paper proposes a\n",
            "one-class prompt learning method for few-shot anomaly detection, termed\n",
            "PromptAD. First, we propose semantic concatenation which can transpose normal\n",
            "prompts into anomaly prompts by concatenating normal prompts with anomaly\n",
            "suffixes, thus constructing a large number of negative samples used to guide\n",
            "prompt learning in one-class setting. Furthermore, to mitigate the training\n",
            "challenge caused by the absence of anomaly images, we introduce the concept of\n",
            "explicit anomaly margin, which is used to explicitly control the margin between\n",
            "normal prompt features and anomaly prompt features through a hyper-parameter.\n",
            "For image-level/pixel-level anomaly detection, PromptAD achieves first place in\n",
            "11/12 few-shot settings on MVTec and VisA.\n",
            "\n",
            "1277. Title: Escape Sky-high Cost: Early-stopping Self-Consistency for Multi-step Reasoning\n",
            "   Abstract: We present a scalable and effective exploration strategy based on Thompson\n",
            "sampling for reinforcement learning (RL). One of the key shortcomings of\n",
            "existing Thompson sampling algorithms is the need to perform a Gaussian\n",
            "approximation of the posterior distribution, which is not a good surrogate in\n",
            "most practical settings. We instead directly sample the Q function from its\n",
            "posterior distribution, by using Langevin Monte Carlo, an efficient type of\n",
            "Markov Chain Monte Carlo (MCMC) method. Our method only needs to perform noisy\n",
            "gradient descent updates to learn the exact posterior distribution of the Q\n",
            "function, which makes our approach easy to deploy in deep RL. We provide a\n",
            "rigorous theoretical analysis for the proposed method and demonstrate that, in\n",
            "the linear Markov decision process (linear MDP) setting, it has a regret bound\n",
            "of $\\tilde{O}(d^{3/2}H^{3/2}\\sqrt{T})$, where $d$ is the dimension of the\n",
            "feature mapping, $H$ is the planning horizon, and $T$ is the total number of\n",
            "steps. We apply this approach to deep RL, by using Adam optimizer to perform\n",
            "gradient updates. Our approach achieves better or similar results compared with\n",
            "state-of-the-art deep RL algorithms on several challenging exploration tasks\n",
            "from the Atari57 suite.\n",
            "\n",
            "1278. Title: Provable and Practical: Efficient Exploration in Reinforcement Learning via Langevin Monte Carlo\n",
            "   Abstract: Lottery Ticket Hypothesis (LTH) claims the existence of a winning ticket\n",
            "(i.e., a properly pruned sub-network together with original weight\n",
            "initialization) that can achieve competitive performance to the original dense\n",
            "network. A recent work, called UGS, extended LTH to prune graph neural networks\n",
            "(GNNs) for effectively accelerating GNN inference. UGS simultaneously prunes\n",
            "the graph adjacency matrix and the model weights using the same masking\n",
            "mechanism, but since the roles of the graph adjacency matrix and the weight\n",
            "matrices are very different, we find that their sparsifications lead to\n",
            "different performance characteristics. Specifically, we find that the\n",
            "performance of a sparsified GNN degrades significantly when the graph sparsity\n",
            "goes beyond a certain extent. Therefore, we propose two techniques to improve\n",
            "GNN performance when the graph sparsity is high. First, UGS prunes the\n",
            "adjacency matrix using a loss formulation which, however, does not properly\n",
            "involve all elements of the adjacency matrix; in contrast, we add a new\n",
            "auxiliary loss head to better guide the edge pruning by involving the entire\n",
            "adjacency matrix. Second, by regarding unfavorable graph sparsification as\n",
            "adversarial data perturbations, we formulate the pruning process as a min-max\n",
            "optimization problem to gain the robustness of lottery tickets when the graph\n",
            "sparsity is high. We further investigate the question: Can the \"retrainable\"\n",
            "winning ticket of a GNN be also effective for graph transferring learning? We\n",
            "call it the transferable graph lottery ticket (GLT) hypothesis. Extensive\n",
            "experiments were conducted which demonstrate the superiority of our proposed\n",
            "sparsification method over UGS, and which empirically verified our transferable\n",
            "GLT hypothesis.\n",
            "\n",
            "1279. Title: Video Decomposition Prior: Editing Videos Layer by Layer\n",
            "   Abstract: We announce a tool for mapping derivations of the E theorem prover to Mizar\n",
            "proofs. Our mapping complements earlier work that generates problems for\n",
            "automated theorem provers from Mizar inference checking problems. We describe\n",
            "the tool, explain the mapping, and show how we solved some of the difficulties\n",
            "that arise in mapping proofs between different logical formalisms, even when\n",
            "they are based on the same notion of logical consequence, as Mizar and E are\n",
            "(namely, first-order classical logic with identity).\n",
            "\n",
            "1280. Title: Graph Lottery Ticket Automated\n",
            "   Abstract: Vision-language pre-training (VLP) models demonstrate impressive abilities in\n",
            "processing both images and text. However, they are vulnerable to multi-modal\n",
            "adversarial examples (AEs). Investigating the generation of\n",
            "high-transferability adversarial examples is crucial for uncovering VLP models'\n",
            "vulnerabilities in practical scenarios. Recent works have indicated that\n",
            "leveraging data augmentation and image-text modal interactions can enhance the\n",
            "transferability of adversarial examples for VLP models significantly. However,\n",
            "they do not consider the optimal alignment problem between dataaugmented\n",
            "image-text pairs. This oversight leads to adversarial examples that are overly\n",
            "tailored to the source model, thus limiting improvements in transferability. In\n",
            "our research, we first explore the interplay between image sets produced\n",
            "through data augmentation and their corresponding text sets. We find that\n",
            "augmented image samples can align optimally with certain texts while exhibiting\n",
            "less relevance to others. Motivated by this, we propose an Optimal\n",
            "Transport-based Adversarial Attack, dubbed OT-Attack. The proposed method\n",
            "formulates the features of image and text sets as two distinct distributions\n",
            "and employs optimal transport theory to determine the most efficient mapping\n",
            "between them. This optimal mapping informs our generation of adversarial\n",
            "examples to effectively counteract the overfitting issues. Extensive\n",
            "experiments across various network architectures and datasets in image-text\n",
            "matching tasks reveal that our OT-Attack outperforms existing state-of-the-art\n",
            "methods in terms of adversarial transferability.\n",
            "\n",
            "1281. Title: Think-on-Graph: Deep and Responsible Reasoning of Large Language Model on Knowledge Graph\n",
            "   Abstract: In many settings, we have multiple data sets (also called views) that capture\n",
            "different and overlapping aspects of the same phenomenon. We are often\n",
            "interested in finding patterns that are unique to one or to a subset of the\n",
            "views. For example, we might have one set of molecular observations and one set\n",
            "of physiological observations on the same group of individuals, and we want to\n",
            "quantify molecular patterns that are uncorrelated with physiology. Despite\n",
            "being a common problem, this is highly challenging when the correlations come\n",
            "from complex distributions. In this paper, we develop the general framework of\n",
            "Rich Component Analysis (RCA) to model settings where the observations from\n",
            "different views are driven by different sets of latent components, and each\n",
            "component can be a complex, high-dimensional distribution. We introduce\n",
            "algorithms based on cumulant extraction that provably learn each of the\n",
            "components without having to model the other components. We show how to\n",
            "integrate RCA with stochastic gradient descent into a meta-algorithm for\n",
            "learning general models, and demonstrate substantial improvement in accuracy on\n",
            "several synthetic and real datasets in both supervised and unsupervised tasks.\n",
            "Our method makes it possible to learn latent variable models when we don't have\n",
            "samples from the true model but only samples after complex perturbations.\n",
            "\n",
            "1282. Title: Interpretable Meta-Learning of Physical Systems\n",
            "   Abstract: Foundational deep learning (DL) models are general models, trained on large,\n",
            "diverse, and unlabelled datasets, typically using self-supervised learning\n",
            "techniques have led to significant advancements especially in natural language\n",
            "processing. These pretrained models can be fine-tuned for related downstream\n",
            "tasks, offering faster development and reduced training costs, while often\n",
            "achieving improved performance. In this work, we introduce Masked Spectrogram\n",
            "Modeling, a novel self-supervised learning approach for pretraining\n",
            "foundational DL models on radio signals. Adopting a Convolutional LSTM\n",
            "architecture for efficient spatio-temporal processing, we pretrain the model\n",
            "with an unlabelled radio dataset collected from over-the-air measurements.\n",
            "Subsequently, the pretrained model is fine-tuned for two downstream tasks:\n",
            "spectrum forecasting and segmentation. Experimental results demonstrate that\n",
            "our methodology achieves competitive performance in both forecasting accuracy\n",
            "and segmentation, validating its effectiveness for developing foundational\n",
            "radio models.\n",
            "\n",
            "1283. Title: PF-LRM: Pose-Free Large Reconstruction Model for Joint Pose and Shape Prediction\n",
            "   Abstract: Binary classification involves predicting the label of an instance based on\n",
            "whether the model score for the positive class exceeds a threshold chosen based\n",
            "on the application requirements (e.g., maximizing recall for a precision\n",
            "bound). However, model scores are often not aligned with the true positivity\n",
            "rate. This is especially true when the training involves a differential\n",
            "sampling across classes or there is distributional drift between train and test\n",
            "settings. In this paper, we provide theoretical analysis and empirical evidence\n",
            "of the dependence of model score estimation bias on both uncertainty and score\n",
            "itself. Further, we formulate the decision boundary selection in terms of both\n",
            "model score and uncertainty, prove that it is NP-hard, and present algorithms\n",
            "based on dynamic programming and isotonic regression. Evaluation of the\n",
            "proposed algorithms on three real-world datasets yield 25%-40% gain in recall\n",
            "at high precision bounds over the traditional approach of using model score\n",
            "alone, highlighting the benefits of leveraging uncertainty.\n",
            "\n",
            "1284. Title: Searching for High-Value Molecules Using Reinforcement Learning and Transformers\n",
            "   Abstract: Different from traditional task-specific vision models, recent large VLMs can\n",
            "readily adapt to different vision tasks by simply using different textual\n",
            "instructions, i.e., prompts. However, a well-known concern about traditional\n",
            "task-specific vision models is that they can be misled by imperceptible\n",
            "adversarial perturbations. Furthermore, the concern is exacerbated by the\n",
            "phenomenon that the same adversarial perturbations can fool different\n",
            "task-specific models. Given that VLMs rely on prompts to adapt to different\n",
            "tasks, an intriguing question emerges: Can a single adversarial image mislead\n",
            "all predictions of VLMs when a thousand different prompts are given? This\n",
            "question essentially introduces a novel perspective on adversarial\n",
            "transferability: cross-prompt adversarial transferability. In this work, we\n",
            "propose the Cross-Prompt Attack (CroPA). This proposed method updates the\n",
            "visual adversarial perturbation with learnable prompts, which are designed to\n",
            "counteract the misleading effects of the adversarial image. By doing this,\n",
            "CroPA significantly improves the transferability of adversarial examples across\n",
            "prompts. Extensive experiments are conducted to verify the strong cross-prompt\n",
            "adversarial transferability of CroPA with prevalent VLMs including Flamingo,\n",
            "BLIP-2, and InstructBLIP in various different tasks. Our source code is\n",
            "available at \\url{https://github.com/Haochen-Luo/CroPA}.\n",
            "\n",
            "1285. Title: Flat Minima in Linear Estimation and an Extended Gauss Markov Theorem\n",
            "   Abstract: Difficulties and discomfort with the interpretation of quantum mechanics are\n",
            "due to differences in language between it and classical physics. Analogies to\n",
            "The Special Theory of Relativity, which also required changes in the basic\n",
            "worldview and language of non-relativistic classical mechanics, may help in\n",
            "absorbing the changes called for by quantum physics. There is no need to invoke\n",
            "extravagances such as the many worlds interpretation or specify a central role\n",
            "for consciousness or neural microstructures. The simple, but basic, acceptance\n",
            "that what is meant by the state of a physical system is different in quantum\n",
            "physics from what it is in classical physics goes a long way in explaining its\n",
            "seeming peculiarities.\n",
            "\n",
            "1286. Title: Manifold Preserving Guided Diffusion\n",
            "   Abstract: Despite the recent advancements, conditional image generation still faces\n",
            "challenges of cost, generalizability, and the need for task-specific training.\n",
            "In this paper, we propose Manifold Preserving Guided Diffusion (MPGD), a\n",
            "training-free conditional generation framework that leverages pretrained\n",
            "diffusion models and off-the-shelf neural networks with minimal additional\n",
            "inference cost for a broad range of tasks. Specifically, we leverage the\n",
            "manifold hypothesis to refine the guided diffusion steps and introduce a\n",
            "shortcut algorithm in the process. We then propose two methods for on-manifold\n",
            "training-free guidance using pre-trained autoencoders and demonstrate that our\n",
            "shortcut inherently preserves the manifolds when applied to latent diffusion\n",
            "models. Our experiments show that MPGD is efficient and effective for solving a\n",
            "variety of conditional generation applications in low-compute settings, and can\n",
            "consistently offer up to 3.8x speed-ups with the same number of diffusion steps\n",
            "while maintaining high sample quality compared to the baselines.\n",
            "\n",
            "1287. Title: Learning Robust Generalizable Radiance Field with Visibility and Feature Augmented Point Representation\n",
            "   Abstract: Reinforcement learning (RL) over text representations can be effective for\n",
            "finding high-value policies that can search over graphs. However, RL requires\n",
            "careful structuring of the search space and algorithm design to be effective in\n",
            "this challenge. Through extensive experiments, we explore how different design\n",
            "choices for text grammar and algorithmic choices for training can affect an RL\n",
            "policy's ability to generate molecules with desired properties. We arrive at a\n",
            "new RL-based molecular design algorithm (ChemRLformer) and perform a thorough\n",
            "analysis using 25 molecule design tasks, including computationally complex\n",
            "protein docking simulations. From this analysis, we discover unique insights in\n",
            "this problem space and show that ChemRLformer achieves state-of-the-art\n",
            "performance while being more straightforward than prior work by demystifying\n",
            "which design choices are actually helpful for text-based molecule design.\n",
            "\n",
            "1288. Title: $\\pi$2vec: Policy Representation with Successor Features\n",
            "   Abstract: We consider the problem of linear estimation, and establish an extension of\n",
            "the Gauss-Markov theorem, in which the bias operator is allowed to be non-zero\n",
            "but bounded with respect to a matrix norm of Schatten type. We derive simple\n",
            "and explicit formulas for the optimal estimator in the cases of Nuclear and\n",
            "Spectral norms (with the Frobenius case recovering ridge regression).\n",
            "Additionally, we analytically derive the generalization error in multiple\n",
            "random matrix ensembles, and compare with Ridge regression. Finally, we conduct\n",
            "an extensive simulation study, in which we show that the cross-validated\n",
            "Nuclear and Spectral regressors can outperform Ridge in several circumstances.\n",
            "\n",
            "1289. Title: Adversarial AutoMixup\n",
            "   Abstract: This paper introduces a novel paradigm for the generalizable neural radiance\n",
            "field (NeRF). Previous generic NeRF methods combine multiview stereo techniques\n",
            "with image-based neural rendering for generalization, yielding impressive\n",
            "results, while suffering from three issues. First, occlusions often result in\n",
            "inconsistent feature matching. Then, they deliver distortions and artifacts in\n",
            "geometric discontinuities and locally sharp shapes due to their individual\n",
            "process of sampled points and rough feature aggregation. Third, their\n",
            "image-based representations experience severe degradations when source views\n",
            "are not near enough to the target view. To address challenges, we propose the\n",
            "first paradigm that constructs the generalizable neural field based on\n",
            "point-based rather than image-based rendering, which we call the Generalizable\n",
            "neural Point Field (GPF). Our approach explicitly models visibilities by\n",
            "geometric priors and augments them with neural features. We propose a novel\n",
            "nonuniform log sampling strategy to improve both rendering speed and\n",
            "reconstruction quality. Moreover, we present a learnable kernel spatially\n",
            "augmented with features for feature aggregations, mitigating distortions at\n",
            "places with drastically varying geometries. Besides, our representation can be\n",
            "easily manipulated. Experiments show that our model can deliver better\n",
            "geometries, view consistencies, and rendering quality than all counterparts and\n",
            "benchmarks on three datasets in both generalization and finetuning settings,\n",
            "preliminarily proving the potential of the new paradigm for generalizable NeRF.\n",
            "\n",
            "1290. Title: Sampling Multimodal Distributions with the Vanilla Score: Benefits of Data-Based Initialization\n",
            "   Abstract: Due to accelerating urbanization, the importance of solving the signal\n",
            "control problem increases. This paper analyzes various existing methods and\n",
            "suggests options for increasing the number of agents to reduce the average\n",
            "travel time. Experiments were carried out with 2 datasets. The results show\n",
            "that in some cases, the implementation of multiple agents can improve existing\n",
            "methods. For a fine-tuned large language model approach there is small\n",
            "enhancement on all metrics.\n",
            "\n",
            "1291. Title: Entropy-MCMC: Sampling from Flat Basins with Ease\n",
            "   Abstract: Data mixing augmentation has been widely applied to improve the\n",
            "generalization ability of deep neural networks. Recently, offline data mixing\n",
            "augmentation, e.g. handcrafted and saliency information-based mixup, has been\n",
            "gradually replaced by automatic mixing approaches. Through minimizing two\n",
            "sub-tasks, namely, mixed sample generation and mixup classification in an\n",
            "end-to-end way, AutoMix significantly improves accuracy on image classification\n",
            "tasks. However, as the optimization objective is consistent for the two\n",
            "sub-tasks, this approach is prone to generating consistent instead of diverse\n",
            "mixed samples, which results in overfitting for target task training. In this\n",
            "paper, we propose AdAutomixup, an adversarial automatic mixup augmentation\n",
            "approach that generates challenging samples to train a robust classifier for\n",
            "image classification, by alternatively optimizing the classifier and the mixup\n",
            "sample generator. AdAutomixup comprises two modules, a mixed example generator,\n",
            "and a target classifier. The mixed sample generator aims to produce hard mixed\n",
            "examples to challenge the target classifier, while the target classifier's aim\n",
            "is to learn robust features from hard mixed examples to improve generalization.\n",
            "To prevent the collapse of the inherent meanings of images, we further\n",
            "introduce an exponential moving average (EMA) teacher and cosine similarity to\n",
            "train AdAutomixup in an end-to-end way. Extensive experiments on seven image\n",
            "benchmarks consistently prove that our approach outperforms the state of the\n",
            "art in various classification scenarios. The source code is available at\n",
            "https://github.com/JinXins/Adversarial-AutoMixup.\n",
            "\n",
            "1292. Title: WebArena: A Realistic Web Environment for Building Autonomous Agents\n",
            "   Abstract: Like many chronic diseases, human immunodeficiency virus (HIV) is managed\n",
            "over time at regular clinic visits. At each visit, patient features are\n",
            "assessed, treatments are prescribed, and a subsequent visit is scheduled. There\n",
            "is a need for data-driven methods for both predicting retention and\n",
            "recommending scheduling decisions that optimize retention. Prediction models\n",
            "can be useful for estimating retention rates across a range of scheduling\n",
            "options. However, training such models with electronic health records (EHR)\n",
            "involves several complexities. First, formal causal inference methods are\n",
            "needed to adjust for observed confounding when estimating retention rates under\n",
            "counterfactual scheduling decisions. Second, competing events such as death\n",
            "preclude retention, while censoring events render retention missing. Third,\n",
            "inconsistent monitoring of features such as viral load and CD4 count lead to\n",
            "covariate missingness. This paper presents an all-in-one approach for both\n",
            "predicting HIV retention and optimizing scheduling while accounting for these\n",
            "complexities. We formulate and identify causal retention estimands in terms of\n",
            "potential return-time under a hypothetical scheduling decision. Flexible\n",
            "Bayesian approaches are used to model the observed return-time distribution\n",
            "while accounting for competing and censoring events and form posterior point\n",
            "and uncertainty estimates for these estimands. We address the urgent need for\n",
            "data-driven decision support in HIV care by applying our method to EHR from the\n",
            "Academic Model Providing Access to Healthcare (AMPATH) - a consortium of\n",
            "clinics that treat HIV in Western Kenya.\n",
            "\n",
            "1293. Title: DNABERT-2: Efficient Foundation Model and Benchmark For Multi-Species Genomes\n",
            "   Abstract: There is a long history, as well as a recent explosion of interest, in\n",
            "statistical and generative modeling approaches based on score functions --\n",
            "derivatives of the log-likelihood of a distribution. In seminal works,\n",
            "Hyv\\\"arinen proposed vanilla score matching as a way to learn distributions\n",
            "from data by computing an estimate of the score function of the underlying\n",
            "ground truth, and established connections between this method and established\n",
            "techniques like Contrastive Divergence and Pseudolikelihood estimation. It is\n",
            "by now well-known that vanilla score matching has significant difficulties\n",
            "learning multimodal distributions. Although there are various ways to overcome\n",
            "this difficulty, the following question has remained unanswered -- is there a\n",
            "natural way to sample multimodal distributions using just the vanilla score?\n",
            "Inspired by a long line of related experimental works, we prove that the\n",
            "Langevin diffusion with early stopping, initialized at the empirical\n",
            "distribution, and run on a score function estimated from data successfully\n",
            "generates natural multimodal distributions (mixtures of log-concave\n",
            "distributions).\n",
            "\n",
            "1294. Title: A Unified Framework for Bayesian Optimization under Contextual Uncertainty\n",
            "   Abstract: Self-supervised learning has recently gained growing interest in molecular\n",
            "modeling for scientific tasks such as AI-assisted drug discovery. Current\n",
            "studies consider leveraging both 2D and 3D molecular structures for\n",
            "representation learning. However, relying on straightforward alignment\n",
            "strategies that treat each modality separately, these methods fail to exploit\n",
            "the intrinsic correlation between 2D and 3D representations that reflect the\n",
            "underlying structural characteristics of molecules, and only perform\n",
            "coarse-grained molecule-level alignment. To derive fine-grained alignment and\n",
            "promote structural molecule understanding, we introduce an atomic-relation\n",
            "level \"blend-then-predict\" self-supervised learning approach, MoleBLEND, which\n",
            "first blends atom relations represented by different modalities into one\n",
            "unified relation matrix for joint encoding, then recovers modality-specific\n",
            "information for 2D and 3D structures individually. By treating atom\n",
            "relationships as anchors, MoleBLEND organically aligns and integrates visually\n",
            "dissimilar 2D and 3D modalities of the same molecule at fine-grained atomic\n",
            "level, painting a more comprehensive depiction of each molecule. Extensive\n",
            "experiments show that MoleBLEND achieves state-of-the-art performance across\n",
            "major 2D/3D molecular benchmarks. We further provide theoretical insights from\n",
            "the perspective of mutual-information maximization, demonstrating that our\n",
            "method unifies contrastive, generative (cross-modality prediction) and\n",
            "mask-then-predict (single-modality prediction) objectives into one single\n",
            "cohesive framework.\n",
            "\n",
            "1295. Title: Graph Neural Networks for Learning Equivariant Representations of Neural Networks\n",
            "   Abstract: In observational studies, balancing covariates in different treatment groups\n",
            "is essential to estimate treatment effects. One of the most commonly used\n",
            "methods for such purposes is weighting. The performance of this class of\n",
            "methods usually depends on strong regularity conditions for the underlying\n",
            "model, which might not hold in practice. In this paper, we investigate\n",
            "weighting methods from a functional estimation perspective and argue that the\n",
            "weights needed for covariate balancing could differ from those needed for\n",
            "treatment effects estimation under low regularity conditions. Motivated by this\n",
            "observation, we introduce a new framework of weighting that directly targets\n",
            "the treatment effects estimation. Unlike existing methods, the resulting\n",
            "estimator for a treatment effect under this new framework is a simple\n",
            "kernel-based $U$-statistic after applying a data-driven transformation to the\n",
            "observed covariates. We characterize the theoretical properties of the new\n",
            "estimators of treatment effects under a nonparametric setting and show that\n",
            "they are able to work robustly under low regularity conditions. The new\n",
            "framework is also applied to several numerical examples to demonstrate its\n",
            "practical merits.\n",
            "\n",
            "1296. Title: Treatment Effects Estimation By Uniform Transformer\n",
            "   Abstract: Catastrophe (CAT) bond markets are incomplete and hence carry uncertainty in\n",
            "instrument pricing. As such various pricing approaches have been proposed, but\n",
            "none treat the uncertainty in catastrophe occurrences and interest rates in a\n",
            "sufficiently flexible and statistically reliable way within a unifying asset\n",
            "pricing framework. Consequently, little is known empirically about the expected\n",
            "risk-premia of CAT bonds. The primary contribution of this paper is to present\n",
            "a unified Bayesian CAT bond pricing framework based on uncertainty\n",
            "quantification of catastrophes and interest rates. Our framework allows for\n",
            "complex beliefs about catastrophe risks to capture the distinct and common\n",
            "patterns in catastrophe occurrences, and when combined with stochastic interest\n",
            "rates, yields a unified asset pricing approach with informative expected risk\n",
            "premia. Specifically, using a modified collective risk model -- Dirichlet\n",
            "Prior-Hierarchical Bayesian Collective Risk Model (DP-HBCRM) framework -- we\n",
            "model catastrophe risk via a model-based clustering approach. Interest rate\n",
            "risk is modeled as a CIR process under the Bayesian approach. As a consequence\n",
            "of casting CAT pricing models into our framework, we evaluate the price and\n",
            "expected risk premia of various CAT bond contracts corresponding to clustering\n",
            "of catastrophe risk profiles. Numerical experiments show how these clusters\n",
            "reveal how CAT bond prices and expected risk premia relate to claim frequency\n",
            "and loss severity.\n",
            "\n",
            "1297. Title: Online Information Acquisition: Hiring Multiple Agents\n",
            "   Abstract: We investigate the mechanism design problem faced by a principal who hires\n",
            "\\emph{multiple} agents to gather and report costly information. Then, the\n",
            "principal exploits the information to make an informed decision. We model this\n",
            "problem as a game, where the principal announces a mechanism consisting in\n",
            "action recommendations and a payment function, a.k.a. scoring rule. Then, each\n",
            "agent chooses an effort level and receives partial information about an\n",
            "underlying state of nature based on the effort. Finally, the agents report the\n",
            "information (possibly non-truthfully), the principal takes a decision based on\n",
            "this information, and the agents are paid according to the scoring rule. While\n",
            "previous work focuses on single-agent problems, we consider multi-agents\n",
            "settings. This poses the challenge of coordinating the agents' efforts and\n",
            "aggregating correlated information. Indeed, we show that optimal mechanisms\n",
            "must correlate agents' efforts, which introduces externalities among the\n",
            "agents, and hence complex incentive compatibility constraints and equilibrium\n",
            "selection problems. First, we design a polynomial-time algorithm to find an\n",
            "optimal incentive compatible mechanism. Then, we study an online problem, where\n",
            "the principal repeatedly interacts with a group of unknown agents. We design a\n",
            "no-regret algorithm that provides $\\widetilde{\\mathcal{O}}(T^{2/3})$ regret\n",
            "with respect to an optimal mechanism, matching the state-of-the-art bound for\n",
            "single-agent settings.\n",
            "\n",
            "1298. Title: Retrieval is Accurate Generation\n",
            "   Abstract: Standard language models generate text by selecting tokens from a fixed,\n",
            "finite, and standalone vocabulary. We introduce a novel method that selects\n",
            "context-aware phrases from a collection of supporting documents. One of the\n",
            "most significant challenges for this paradigm shift is determining the training\n",
            "oracles, because a string of text can be segmented in various ways and each\n",
            "segment can be retrieved from numerous possible documents. To address this, we\n",
            "propose to initialize the training oracles using linguistic heuristics and,\n",
            "more importantly, bootstrap the oracles through iterative self-reinforcement.\n",
            "Extensive experiments show that our model not only outperforms standard\n",
            "language models on a variety of knowledge-intensive tasks but also demonstrates\n",
            "improved generation quality in open-ended text generation. For instance,\n",
            "compared to the standard language model counterpart, our model raises the\n",
            "accuracy from 23.47% to 36.27% on OpenbookQA, and improves the MAUVE score from\n",
            "42.61% to 81.58% in open-ended text generation. Remarkably, our model also\n",
            "achieves the best performance and the lowest latency among several\n",
            "retrieval-augmented baselines. In conclusion, we assert that retrieval is more\n",
            "accurate generation and hope that our work will encourage further research on\n",
            "this new paradigm shift.\n",
            "\n",
            "1299. Title: Magnushammer: A Transformer-Based Approach to Premise Selection\n",
            "   Abstract: 3D Euclidean symmetry equivariant neural networks have demonstrated notable\n",
            "success in modeling complex physical systems. We introduce a framework for\n",
            "relaxed $E(3)$ graph equivariant neural networks that can learn and represent\n",
            "symmetry breaking within continuous groups. Building on the existing e3nn\n",
            "framework, we propose the use of relaxed weights to allow for controlled\n",
            "symmetry breaking. We show empirically that these relaxed weights learn the\n",
            "correct amount of symmetry breaking.\n",
            "\n",
            "1300. Title: Privacy-Preserving In-Context Learning with Differentially Private Few-Shot Generation\n",
            "   Abstract: Large vision-language models (LVLMs) have shown remarkable abilities in\n",
            "understanding visual information with human languages. However, LVLMs still\n",
            "suffer from object hallucination, which is the problem of generating\n",
            "descriptions that include objects that do not actually exist in the images.\n",
            "This can negatively impact many vision-language tasks, such as visual\n",
            "summarization and reasoning. To address this issue, we propose a simple yet\n",
            "powerful algorithm, LVLM Hallucination Revisor (LURE), to post-hoc rectify\n",
            "object hallucination in LVLMs by reconstructing less hallucinatory\n",
            "descriptions. LURE is grounded in a rigorous statistical analysis of the key\n",
            "factors underlying object hallucination, including co-occurrence (the frequent\n",
            "appearance of certain objects alongside others in images), uncertainty (objects\n",
            "with higher uncertainty during LVLM decoding), and object position\n",
            "(hallucination often appears in the later part of the generated text). LURE can\n",
            "also be seamlessly integrated with any LVLMs. We evaluate LURE on six\n",
            "open-source LVLMs, achieving a 23% improvement in general object hallucination\n",
            "evaluation metrics over the previous best approach. In both GPT and human\n",
            "evaluations, LURE consistently ranks at the top. Our data and code are\n",
            "available at https://github.com/YiyangZhou/LURE.\n",
            "\n",
            "1301. Title: LDReg: Local Dimensionality Regularized Self-Supervised Learning\n",
            "   Abstract: Distribution shift is a major obstacle in offline reinforcement learning,\n",
            "which necessitates minimizing the discrepancy between the learned policy and\n",
            "the behavior policy to avoid overestimating rare or unseen actions. Previous\n",
            "conservative offline RL algorithms struggle to generalize to unseen actions,\n",
            "despite their success in learning good in-distribution policy. In contrast, we\n",
            "propose to use the gradient fields of the dataset density generated from a\n",
            "pre-trained offline RL algorithm to adjust the original actions. We decouple\n",
            "the conservatism constraints from the policy, thus can benefit wide offline RL\n",
            "algorithms. As a consequence, we propose the Conservative Denoising Score-based\n",
            "Algorithm (CDSA) which utilizes the denoising score-based model to model the\n",
            "gradient of the dataset density, rather than the dataset density itself, and\n",
            "facilitates a more accurate and efficient method to adjust the action generated\n",
            "by the pre-trained policy in a deterministic and continuous MDP environment. In\n",
            "experiments, we show that our approach significantly improves the performance\n",
            "of baseline algorithms in D4RL datasets, and demonstrate the generalizability\n",
            "and plug-and-play capability of our model across different pre-trained offline\n",
            "RL policy in different tasks. We also validate that the agent exhibits greater\n",
            "risk aversion after employing our method while showcasing its ability to\n",
            "generalize effectively across diverse tasks.\n",
            "\n",
            "1302. Title: Reconciling Spatial and Temporal Abstractions for Goal Representation\n",
            "   Abstract: We study the difficulties in learning that arise from robust and\n",
            "differentially private optimization. We first study convergence of gradient\n",
            "descent based adversarial training with differential privacy, taking a simple\n",
            "binary classification task on linearly separable data as an illustrative\n",
            "example. We compare the gap between adversarial and nominal risk in both\n",
            "private and non-private settings, showing that the data dimensionality\n",
            "dependent term introduced by private optimization compounds the difficulties of\n",
            "learning a robust model. After this, we discuss what parts of adversarial\n",
            "training and differential privacy hurt optimization, identifying that the size\n",
            "of adversarial perturbation and clipping norm in differential privacy both\n",
            "increase the curvature of the loss landscape, implying poorer generalization\n",
            "performance.\n",
            "\n",
            "1303. Title: AffineQuant: Affine Transformation Quantization for Large Language Models\n",
            "   Abstract: Quality-Diversity optimisation (QD) has proven to yield promising results\n",
            "across a broad set of applications. However, QD approaches struggle in the\n",
            "presence of uncertainty in the environment, as it impacts their ability to\n",
            "quantify the true performance and novelty of solutions. This problem has been\n",
            "highlighted multiple times independently in previous literature. In this work,\n",
            "we propose to uniformise the view on this problem through four main\n",
            "contributions. First, we formalise a common framework for uncertain domains:\n",
            "the Uncertain QD setting, a special case of QD in which fitness and descriptors\n",
            "for each solution are no longer fixed values but distribution over possible\n",
            "values. Second, we propose a new methodology to evaluate Uncertain QD\n",
            "approaches, relying on a new per-generation sampling budget and a set of\n",
            "existing and new metrics specifically designed for Uncertain QD. Third, we\n",
            "propose three new Uncertain QD algorithms: Archive-sampling,\n",
            "Parallel-Adaptive-sampling and Deep-Grid-sampling. We propose these approaches\n",
            "taking into account recent advances in the QD community toward the use of\n",
            "hardware acceleration that enable large numbers of parallel evaluations and\n",
            "make sampling an affordable approach to uncertainty. Our final and fourth\n",
            "contribution is to use this new framework and the associated comparison methods\n",
            "to benchmark existing and novel approaches. We demonstrate once again the\n",
            "limitation of MAP-Elites in uncertain domains and highlight the performance of\n",
            "the existing Deep-Grid approach, and of our new algorithms. The goal of this\n",
            "framework and methods is to become an instrumental benchmark for future works\n",
            "considering Uncertain QD.\n",
            "\n",
            "1304. Title: CLaM-TTS: Improving Neural Codec Language Model for Zero-Shot Text-to-Speech\n",
            "   Abstract: Manifold-valued measurements exist in numerous applications within computer\n",
            "vision and machine learning. Recent studies have extended Deep Neural Networks\n",
            "(DNNs) to manifolds, and concomitantly, normalization techniques have also been\n",
            "adapted to several manifolds, referred to as Riemannian normalization.\n",
            "Nonetheless, most of the existing Riemannian normalization methods have been\n",
            "derived in an ad hoc manner and only apply to specific manifolds. This paper\n",
            "establishes a unified framework for Riemannian Batch Normalization (RBN)\n",
            "techniques on Lie groups. Our framework offers the theoretical guarantee of\n",
            "controlling both the Riemannian mean and variance. Empirically, we focus on\n",
            "Symmetric Positive Definite (SPD) manifolds, which possess three distinct types\n",
            "of Lie group structures. Using the deformation concept, we generalize the\n",
            "existing Lie groups on SPD manifolds into three families of parameterized Lie\n",
            "groups. Specific normalization layers induced by these Lie groups are then\n",
            "proposed for SPD neural networks. We demonstrate the effectiveness of our\n",
            "approach through three sets of experiments: radar recognition, human action\n",
            "recognition, and electroencephalography (EEG) classification. The code is\n",
            "available at https://github.com/GitZH-Chen/LieBN.git.\n",
            "\n",
            "1305. Title: Copula Conformal prediction for multi-step time series prediction\n",
            "   Abstract: Implicit copulas are the most common copula choice for modeling dependence in\n",
            "high dimensions. This broad class of copulas is introduced and surveyed,\n",
            "including elliptical copulas, skew $t$ copulas, factor copulas, time series\n",
            "copulas and regression copulas. The common auxiliary representation of implicit\n",
            "copulas is outlined, and how this makes them both scalable and tractable for\n",
            "statistical modeling. Issues such as parameter identification, extended\n",
            "likelihoods for discrete or mixed data, parsimony in high dimensions, and\n",
            "simulation from the copula model are considered. Bayesian approaches to\n",
            "estimate the copula parameters, and predict from an implicit copula model, are\n",
            "outlined. Particular attention is given to implicit copula processes\n",
            "constructed from time series and regression models, which is at the forefront\n",
            "of current research. Two econometric applications -- one from macroeconomic\n",
            "time series and the other from financial asset pricing -- illustrate the\n",
            "advantages of implicit copula models.\n",
            "\n",
            "1306. Title: A Lie Group Approach to Riemannian Batch Normalization\n",
            "   Abstract: One intriguing property of deep neural networks (DNNs) is their inherent\n",
            "vulnerability to backdoor attacks -- a trojan model responds to\n",
            "trigger-embedded inputs in a highly predictable manner while functioning\n",
            "normally otherwise. Despite the plethora of prior work on DNNs for continuous\n",
            "data (e.g., images), the vulnerability of graph neural networks (GNNs) for\n",
            "discrete-structured data (e.g., graphs) is largely unexplored, which is highly\n",
            "concerning given their increasing use in security-sensitive domains. To bridge\n",
            "this gap, we present GTA, the first backdoor attack on GNNs. Compared with\n",
            "prior work, GTA departs in significant ways: graph-oriented -- it defines\n",
            "triggers as specific subgraphs, including both topological structures and\n",
            "descriptive features, entailing a large design spectrum for the adversary;\n",
            "input-tailored -- it dynamically adapts triggers to individual graphs, thereby\n",
            "optimizing both attack effectiveness and evasiveness; downstream model-agnostic\n",
            "-- it can be readily launched without knowledge regarding downstream models or\n",
            "fine-tuning strategies; and attack-extensible -- it can be instantiated for\n",
            "both transductive (e.g., node classification) and inductive (e.g., graph\n",
            "classification) tasks, constituting severe threats for a range of\n",
            "security-critical applications. Through extensive evaluation using benchmark\n",
            "datasets and state-of-the-art models, we demonstrate the effectiveness of GTA.\n",
            "We further provide analytical justification for its effectiveness and discuss\n",
            "potential countermeasures, pointing to several promising research directions.\n",
            "\n",
            "1307. Title: ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models\n",
            "   Abstract: Multimodal learning significantly benefits cancer survival prediction,\n",
            "especially the integration of pathological images and genomic data. Despite\n",
            "advantages of multimodal learning for cancer survival prediction, massive\n",
            "redundancy in multimodal data prevents it from extracting discriminative and\n",
            "compact information: (1) An extensive amount of intra-modal task-unrelated\n",
            "information blurs discriminability, especially for gigapixel whole slide images\n",
            "(WSIs) with many patches in pathology and thousands of pathways in genomic\n",
            "data, leading to an ``intra-modal redundancy\" issue. (2) Duplicated information\n",
            "among modalities dominates the representation of multimodal data, which makes\n",
            "modality-specific information prone to being ignored, resulting in an\n",
            "``inter-modal redundancy\" issue. To address these, we propose a new framework,\n",
            "Prototypical Information Bottlenecking and Disentangling (PIBD), consisting of\n",
            "Prototypical Information Bottleneck (PIB) module for intra-modal redundancy and\n",
            "Prototypical Information Disentanglement (PID) module for inter-modal\n",
            "redundancy. Specifically, a variant of information bottleneck, PIB, is proposed\n",
            "to model prototypes approximating a bunch of instances for different risk\n",
            "levels, which can be used for selection of discriminative instances within\n",
            "modality. PID module decouples entangled multimodal data into compact distinct\n",
            "components: modality-common and modality-specific knowledge, under the guidance\n",
            "of the joint prototypical distribution. Extensive experiments on five cancer\n",
            "benchmark datasets demonstrated our superiority over other methods.\n",
            "\n",
            "1308. Title: Prototypical Information Bottlenecking and Disentangling for Multimodal Cancer Survival Prediction\n",
            "   Abstract: Medical applications of machine learning (ML) have experienced a surge in\n",
            "popularity in recent years. The intensive care unit (ICU) is a natural habitat\n",
            "for ML given the abundance of available data from electronic health records.\n",
            "Models have been proposed to address numerous ICU prediction tasks like the\n",
            "early detection of complications. While authors frequently report\n",
            "state-of-the-art performance, it is challenging to verify claims of\n",
            "superiority. Datasets and code are not always published, and cohort\n",
            "definitions, preprocessing pipelines, and training setups are difficult to\n",
            "reproduce. This work introduces Yet Another ICU Benchmark (YAIB), a modular\n",
            "framework that allows researchers to define reproducible and comparable\n",
            "clinical ML experiments; we offer an end-to-end solution from cohort definition\n",
            "to model evaluation. The framework natively supports most open-access ICU\n",
            "datasets (MIMIC III/IV, eICU, HiRID, AUMCdb) and is easily adaptable to future\n",
            "ICU datasets. Combined with a transparent preprocessing pipeline and extensible\n",
            "training code for multiple ML and deep learning models, YAIB enables unified\n",
            "model development. Our benchmark comes with five predefined established\n",
            "prediction tasks (mortality, acute kidney injury, sepsis, kidney function, and\n",
            "length of stay) developed in collaboration with clinicians. Adding further\n",
            "tasks is straightforward by design. Using YAIB, we demonstrate that the choice\n",
            "of dataset, cohort definition, and preprocessing have a major impact on the\n",
            "prediction performance - often more so than model class - indicating an urgent\n",
            "need for YAIB as a holistic benchmarking tool. We provide our work to the\n",
            "clinical ML community to accelerate method development and enable real-world\n",
            "clinical implementations. Software Repository:\n",
            "https://github.com/rvandewater/YAIB.\n",
            "\n",
            "1309. Title: Advancing the Lower Bounds: an Accelerated, Stochastic, Second-order Method with Optimal Adaptation to Inexactness\n",
            "   Abstract: The majority of the research on the quantization of Deep Neural Networks\n",
            "(DNNs) is focused on reducing the precision of tensors visible by high-level\n",
            "frameworks (e.g., weights, activations, and gradients). However, current\n",
            "hardware still relies on high-accuracy core operations. Most significant is the\n",
            "operation of accumulating products. This high-precision accumulation operation\n",
            "is gradually becoming the main computational bottleneck. This is because, so\n",
            "far, the usage of low-precision accumulators led to a significant degradation\n",
            "in performance. In this work, we present a simple method to train and fine-tune\n",
            "high-end DNNs, to allow, for the first time, utilization of cheaper, $12$-bits\n",
            "accumulators, with no significant degradation in accuracy. Lastly, we show that\n",
            "as we decrease the accumulation precision further, using fine-grained gradient\n",
            "approximations can improve the DNN accuracy.\n",
            "\n",
            "1310. Title: Quality-Diversity through AI Feedback\n",
            "   Abstract: Transformer architectures have exhibited remarkable performance in image\n",
            "super-resolution (SR). Since the quadratic computational complexity of the\n",
            "self-attention (SA) in Transformer, existing methods tend to adopt SA in a\n",
            "local region to reduce overheads. However, the local design restricts the\n",
            "global context exploitation, which is crucial for accurate image\n",
            "reconstruction. In this work, we propose the Recursive Generalization\n",
            "Transformer (RGT) for image SR, which can capture global spatial information\n",
            "and is suitable for high-resolution images. Specifically, we propose the\n",
            "recursive-generalization self-attention (RG-SA). It recursively aggregates\n",
            "input features into representative feature maps, and then utilizes\n",
            "cross-attention to extract global information. Meanwhile, the channel\n",
            "dimensions of attention matrices (query, key, and value) are further scaled to\n",
            "mitigate the redundancy in the channel domain. Furthermore, we combine the\n",
            "RG-SA with local self-attention to enhance the exploitation of the global\n",
            "context, and propose the hybrid adaptive integration (HAI) for module\n",
            "integration. The HAI allows the direct and effective fusion between features at\n",
            "different levels (local or global). Extensive experiments demonstrate that our\n",
            "RGT outperforms recent state-of-the-art methods quantitatively and\n",
            "qualitatively. Code and pre-trained models are available at\n",
            "https://github.com/zhengchen1999/RGT.\n",
            "\n",
            "1311. Title: Recursive Generalization Transformer for Image Super-Resolution\n",
            "   Abstract: Addressing the limitations of text as a source of accurate layout\n",
            "representation in text-conditional diffusion models, many works incorporate\n",
            "additional signals to condition certain attributes within a generated image.\n",
            "Although successful, previous works do not account for the specific\n",
            "localization of said attributes extended into the three dimensional plane. In\n",
            "this context, we present a conditional diffusion model that integrates control\n",
            "over three-dimensional object placement with disentangled representations of\n",
            "global stylistic semantics from multiple exemplar images. Specifically, we\n",
            "first introduce \\textit{depth disentanglement training} to leverage the\n",
            "relative depth of objects as an estimator, allowing the model to identify the\n",
            "absolute positions of unseen objects through the use of synthetic image\n",
            "triplets. We also introduce \\textit{soft guidance}, a method for imposing\n",
            "global semantics onto targeted regions without the use of any additional\n",
            "localization cues. Our integrated framework, \\textsc{Compose and Conquer\n",
            "(CnC)}, unifies these techniques to localize multiple conditions in a\n",
            "disentangled manner. We demonstrate that our approach allows perception of\n",
            "objects at varying depths while offering a versatile framework for composing\n",
            "localized objects with different global semantics. Code:\n",
            "https://github.com/tomtom1103/compose-and-conquer/\n",
            "\n",
            "1312. Title: Yet Another ICU Benchmark: A Flexible Multi-Center Framework for Clinical ML\n",
            "   Abstract: Invariance and equivariance to geometrical transformations have proven to be\n",
            "very useful inductive biases when training (convolutional) neural network\n",
            "models, especially in the low-data regime. Much work has focused on the case\n",
            "where the symmetry group employed is compact or abelian, or both. Recent work\n",
            "has explored enlarging the class of transformations used to the case of Lie\n",
            "groups, principally through the use of their Lie algebra, as well as the group\n",
            "exponential and logarithm maps. The applicability of such methods is limited by\n",
            "the fact that depending on the group of interest $G$, the exponential map may\n",
            "not be surjective. Further limitations are encountered when $G$ is neither\n",
            "compact nor abelian. Using the structure and geometry of Lie groups and their\n",
            "homogeneous spaces, we present a framework by which it is possible to work with\n",
            "such groups primarily focusing on the groups $G = \\text{GL}^{+}(n, \\mathbb{R})$\n",
            "and $G = \\text{SL}(n, \\mathbb{R})$, as well as their representation as affine\n",
            "transformations $\\mathbb{R}^{n} \\rtimes G$. Invariant integration as well as a\n",
            "global parametrization is realized by a decomposition into subgroups and\n",
            "submanifolds which can be handled individually. Under this framework, we show\n",
            "how convolution kernels can be parametrized to build models equivariant with\n",
            "respect to affine transformations. We evaluate the robustness and\n",
            "out-of-distribution generalisation capability of our model on the benchmark\n",
            "affine-invariant classification task, outperforming previous proposals.\n",
            "\n",
            "1313. Title: Backdoor Contrastive Learning via Bi-level Trigger Optimization\n",
            "   Abstract: Goal representation affects the performance of Hierarchical Reinforcement\n",
            "Learning (HRL) algorithms by decomposing the complex learning problem into\n",
            "easier subtasks. Recent studies show that representations that preserve\n",
            "temporally abstract environment dynamics are successful in solving difficult\n",
            "problems and provide theoretical guarantees for optimality. These methods\n",
            "however cannot scale to tasks where environment dynamics increase in complexity\n",
            "i.e. the temporally abstract transition relations depend on larger number of\n",
            "variables. On the other hand, other efforts have tried to use spatial\n",
            "abstraction to mitigate the previous issues. Their limitations include\n",
            "scalability to high dimensional environments and dependency on prior knowledge.\n",
            "  In this paper, we propose a novel three-layer HRL algorithm that introduces,\n",
            "at different levels of the hierarchy, both a spatial and a temporal goal\n",
            "abstraction. We provide a theoretical study of the regret bounds of the learned\n",
            "policies. We evaluate the approach on complex continuous control tasks,\n",
            "demonstrating the effectiveness of spatial and temporal abstractions learned by\n",
            "this approach. Find open-source code at https://github.com/cosynus-lix/STAR.\n",
            "\n",
            "1314. Title: The Truth is in There: Improving Reasoning in Language Models with Layer-Selective Rank Reduction\n",
            "   Abstract: Large Language Models (LLMs) with billions of parameters have drastically\n",
            "transformed AI applications. However, their demanding computation during\n",
            "inference has raised significant challenges for deployment on\n",
            "resource-constrained devices. Despite recent trends favoring alternative\n",
            "activation functions such as GELU or SiLU, known for increased computation,\n",
            "this study strongly advocates for reinstating ReLU activation in LLMs. We\n",
            "demonstrate that using the ReLU activation function has a negligible impact on\n",
            "convergence and performance while significantly reducing computation and weight\n",
            "transfer. This reduction is particularly valuable during the memory-bound\n",
            "inference step, where efficiency is paramount. Exploring sparsity patterns in\n",
            "ReLU-based LLMs, we unveil the reutilization of activated neurons for\n",
            "generating new tokens and leveraging these insights, we propose practical\n",
            "strategies to substantially reduce LLM inference computation up to three times,\n",
            "using ReLU activations with minimal performance trade-offs.\n",
            "\n",
            "1315. Title: Lie Group Decompositions for Equivariant Neural Networks\n",
            "   Abstract: Traveling waves of neural activity have been observed throughout the brain at\n",
            "a diversity of regions and scales; however, their precise computational role is\n",
            "still debated. One physically inspired hypothesis suggests that the cortical\n",
            "sheet may act like a wave-propagating system capable of invertibly storing a\n",
            "short-term memory of sequential stimuli through induced waves traveling across\n",
            "the cortical surface, and indeed many experimental results from neuroscience\n",
            "correlate wave activity with memory tasks. To date, however, the computational\n",
            "implications of this idea have remained hypothetical due to the lack of a\n",
            "simple recurrent neural network architecture capable of exhibiting such waves.\n",
            "In this work, we introduce a model to fill this gap, which we denote the\n",
            "Wave-RNN (wRNN), and demonstrate how such an architecture indeed efficiently\n",
            "encodes the recent past through a suite of synthetic memory tasks where wRNNs\n",
            "learn faster and reach significantly lower error than wave-free counterparts.\n",
            "We further explore the implications of this memory storage system on more\n",
            "complex sequence modeling tasks such as sequential image classification and\n",
            "find that wave-based models not only again outperform comparable wave-free RNNs\n",
            "while using significantly fewer parameters, but additionally perform comparably\n",
            "to more complex gated architectures such as LSTMs and GRUs.\n",
            "\n",
            "1316. Title: Traveling Waves Encode The Recent Past and Enhance Sequence Learning\n",
            "   Abstract: Transformer-based Large Language Models (LLMs) have become a fixture in\n",
            "modern machine learning. Correspondingly, significant resources are allocated\n",
            "towards research that aims to further advance this technology, typically\n",
            "resulting in models of increasing size that are trained on increasing amounts\n",
            "of data. This work, however, demonstrates the surprising result that it is\n",
            "often possible to significantly improve the performance of LLMs by selectively\n",
            "removing higher-order components of their weight matrices. This simple\n",
            "intervention, which we call LAyer-SElective Rank reduction (LASER), can be done\n",
            "on a model after training has completed, and requires no additional parameters\n",
            "or data. We show extensive experiments demonstrating the generality of this\n",
            "finding across language models and datasets, and provide in-depth analyses\n",
            "offering insights into both when LASER is effective and the mechanism by which\n",
            "it operates.\n",
            "\n",
            "1317. Title: Compose and Conquer: Diffusion-Based 3D Depth Aware Composable Image Synthesis\n",
            "   Abstract: We study episodic reinforcement learning under unknown adversarial\n",
            "corruptions in both the rewards and the transition probabilities of the\n",
            "underlying system. We propose new algorithms which, compared to the existing\n",
            "results in (Lykouris et al., 2020), achieve strictly better regret bounds in\n",
            "terms of total corruptions for the tabular setting. To be specific, firstly,\n",
            "our regret bounds depend on more precise numerical values of total rewards\n",
            "corruptions and transition corruptions, instead of only on the total number of\n",
            "corrupted episodes. Secondly, our regret bounds are the first of their kind in\n",
            "the reinforcement learning setting to have the number of corruptions show up\n",
            "additively with respect to $\\min\\{\\sqrt{T}, \\text{PolicyGapComplexity}\\}$\n",
            "rather than multiplicatively. Our results follow from a general algorithmic\n",
            "framework that combines corruption-robust policy elimination meta-algorithms,\n",
            "and plug-in reward-free exploration sub-algorithms. Replacing the\n",
            "meta-algorithm or sub-algorithm may extend the framework to address other\n",
            "corrupted settings with potentially more structure.\n",
            "\n",
            "1318. Title: Only Pay for What Is Uncertain: Variance-Adaptive Thompson Sampling\n",
            "   Abstract: Transportation distances have been used for more than a decade now in machine\n",
            "learning to compare histograms of features. They have one parameter: the ground\n",
            "metric, which can be any metric between the features themselves. As is the case\n",
            "for all parameterized distances, transportation distances can only prove useful\n",
            "in practice when this parameter is carefully chosen. To date, the only option\n",
            "available to practitioners to set the ground metric parameter was to rely on a\n",
            "priori knowledge of the features, which limited considerably the scope of\n",
            "application of transportation distances. We propose to lift this limitation and\n",
            "consider instead algorithms that can learn the ground metric using only a\n",
            "training set of labeled histograms. We call this approach ground metric\n",
            "learning. We formulate the problem of learning the ground metric as the\n",
            "minimization of the difference of two polyhedral convex functions over a convex\n",
            "set of distance matrices. We follow the presentation of our algorithms with\n",
            "promising experimental results on binary classification tasks using GIST\n",
            "descriptors of images taken in the Caltech-256 set.\n",
            "\n",
            "1319. Title: Improved Regret Bounds for Non-Convex Online-Within-Online Meta Learning\n",
            "   Abstract: This paper investigates efficient deep neural networks (DNNs) to replace\n",
            "dense unstructured weight matrices with structured ones that possess desired\n",
            "properties. The challenge arises because the optimal weight matrix structure in\n",
            "popular neural network models is obscure in most cases and may vary from layer\n",
            "to layer even in the same network. Prior structured matrices proposed for\n",
            "efficient DNNs were mostly hand-crafted without a generalized framework to\n",
            "systematically learn them. To address this issue, we propose a generalized and\n",
            "differentiable framework to learn efficient structures of weight matrices by\n",
            "gradient descent. We first define a new class of structured matrices that\n",
            "covers a wide range of structured matrices in the literature by adjusting the\n",
            "structural parameters. Then, the frequency-domain differentiable\n",
            "parameterization scheme based on the Gaussian-Dirichlet kernel is adopted to\n",
            "learn the structural parameters by proximal gradient descent. On the image and\n",
            "language tasks, our method learns efficient DNNs with structured matrices,\n",
            "achieving lower complexity and/or higher performance than prior approaches that\n",
            "employ low-rank, block-sparse, or block-low-rank matrices.\n",
            "\n",
            "1320. Title: Differentiable Learning of Generalized Structured Matrices for Efficient Deep Neural Networks\n",
            "   Abstract: Contrastive learning has shown to be effective to learn representations from\n",
            "time series in a self-supervised way. However, contrasting similar time series\n",
            "instances or values from adjacent timestamps within a time series leads to\n",
            "ignore their inherent correlations, which results in deteriorating the quality\n",
            "of learned representations. To address this issue, we propose SoftCLT, a simple\n",
            "yet effective soft contrastive learning strategy for time series. This is\n",
            "achieved by introducing instance-wise and temporal contrastive loss with soft\n",
            "assignments ranging from zero to one. Specifically, we define soft assignments\n",
            "for 1) instance-wise contrastive loss by the distance between time series on\n",
            "the data space, and 2) temporal contrastive loss by the difference of\n",
            "timestamps. SoftCLT is a plug-and-play method for time series contrastive\n",
            "learning that improves the quality of learned representations without bells and\n",
            "whistles. In experiments, we demonstrate that SoftCLT consistently improves the\n",
            "performance in various downstream tasks including classification,\n",
            "semi-supervised learning, transfer learning, and anomaly detection, showing\n",
            "state-of-the-art performance. Code is available at this repository:\n",
            "https://github.com/seunghan96/softclt.\n",
            "\n",
            "1321. Title: Causal Modelling Agents: Causal Graph Discovery through Synergising Metadata- and Data-driven Reasoning\n",
            "   Abstract: Most existing works focus on direct perturbations to the victim's\n",
            "state/action or the underlying transition dynamics to demonstrate the\n",
            "vulnerability of reinforcement learning agents to adversarial attacks. However,\n",
            "such direct manipulations may not be always realizable. In this paper, we\n",
            "consider a multi-agent setting where a well-trained victim agent $\\nu$ is\n",
            "exploited by an attacker controlling another agent $\\alpha$ with an\n",
            "\\textit{adversarial policy}. Previous models do not account for the possibility\n",
            "that the attacker may only have partial control over $\\alpha$ or that the\n",
            "attack may produce easily detectable \"abnormal\" behaviors. Furthermore, there\n",
            "is a lack of provably efficient defenses against these adversarial policies. To\n",
            "address these limitations, we introduce a generalized attack framework that has\n",
            "the flexibility to model to what extent the adversary is able to control the\n",
            "agent, and allows the attacker to regulate the state distribution shift and\n",
            "produce stealthier adversarial policies. Moreover, we offer a provably\n",
            "efficient defense with polynomial convergence to the most robust victim policy\n",
            "through adversarial training with timescale separation. This stands in sharp\n",
            "contrast to supervised learning, where adversarial training typically provides\n",
            "only \\textit{empirical} defenses. Using the Robosumo competition experiments,\n",
            "we show that our generalized attack formulation results in much stealthier\n",
            "adversarial policies when maintaining the same winning rate as baselines.\n",
            "Additionally, our adversarial training approach yields stable learning dynamics\n",
            "and less exploitable victim policies.\n",
            "\n",
            "1322. Title: Soft Contrastive Learning for Time Series\n",
            "   Abstract: Causal discovery and causal reasoning are classically treated as separate and\n",
            "consecutive tasks: one first infers the causal graph, and then uses it to\n",
            "estimate causal effects of interventions. However, such a two-stage approach is\n",
            "uneconomical, especially in terms of actively collected interventional data,\n",
            "since the causal query of interest may not require a fully-specified causal\n",
            "model. From a Bayesian perspective, it is also unnatural, since a causal query\n",
            "(e.g., the causal graph or some causal effect) can be viewed as a latent\n",
            "quantity subject to posterior inference -- other unobserved quantities that are\n",
            "not of direct interest (e.g., the full causal model) ought to be marginalized\n",
            "out in this process and contribute to our epistemic uncertainty. In this work,\n",
            "we propose Active Bayesian Causal Inference (ABCI), a fully-Bayesian active\n",
            "learning framework for integrated causal discovery and reasoning, which jointly\n",
            "infers a posterior over causal models and queries of interest. In our approach\n",
            "to ABCI, we focus on the class of causally-sufficient, nonlinear additive noise\n",
            "models, which we model using Gaussian processes. We sequentially design\n",
            "experiments that are maximally informative about our target causal query,\n",
            "collect the corresponding interventional data, and update our beliefs to choose\n",
            "the next experiment. Through simulations, we demonstrate that our approach is\n",
            "more data-efficient than several baselines that only focus on learning the full\n",
            "causal graph. This allows us to accurately learn downstream causal queries from\n",
            "fewer samples while providing well-calibrated uncertainty estimates for the\n",
            "quantities of interest.\n",
            "\n",
            "1323. Title: Near-Optimal Quantum Algorithm for Minimizing the Maximal Loss\n",
            "   Abstract: The ability to fool deep learning classifiers with tiny perturbations of the\n",
            "input has lead to the development of adversarial training in which the loss\n",
            "with respect to adversarial examples is minimized in addition to the training\n",
            "examples. While adversarial training improves the robustness of the learned\n",
            "classifiers, the procedure is computationally expensive, sensitive to\n",
            "hyperparameters and may still leave the classifier vulnerable to other types of\n",
            "small perturbations. In this paper we compare the performance of adversarial\n",
            "training to that of the simple 1 Nearest Neighbor (1NN) classifier. We prove\n",
            "that under reasonable assumptions, the 1NN classifier will be robust to {\\em\n",
            "any} small image perturbation of the training images. In experiments with 135\n",
            "different binary image classification problems taken from CIFAR10, MNIST and\n",
            "Fashion-MNIST we find that 1NN outperforms TRADES (a powerful adversarial\n",
            "training algorithm) in terms of average adversarial accuracy. In additional\n",
            "experiments with 69 robust models taken from the current adversarial robustness\n",
            "leaderboard, we find that 1NN outperforms almost all of them in terms of\n",
            "robustness to perturbations that are only slightly different from those used\n",
            "during training. Taken together, our results suggest that modern adversarial\n",
            "training methods still fall short of the robustness of the simple 1NN\n",
            "classifier. our code can be found at\n",
            "\\url{https://github.com/amirhagai/On-Adversarial-Training-And-The-1-Nearest-Neighbor-Classifier}\n",
            "\\keywords{Adversarial training}\n",
            "\n",
            "1324. Title: Large-scale Training of Foundation Models for Wearable Biosignals\n",
            "   Abstract: We present a new accelerated stochastic second-order method that is robust to\n",
            "both gradient and Hessian inexactness, which occurs typically in machine\n",
            "learning. We establish theoretical lower bounds and prove that our algorithm\n",
            "achieves optimal convergence in both gradient and Hessian inexactness in this\n",
            "key setting. We further introduce a tensor generalization for stochastic\n",
            "higher-order derivatives. When the oracles are non-stochastic, the proposed\n",
            "tensor algorithm matches the global convergence of Nesterov Accelerated Tensor\n",
            "method. Both algorithms allow for approximate solutions of their auxiliary\n",
            "subproblems with verifiable conditions on the accuracy of the solution.\n",
            "\n",
            "1325. Title: Two-stage LLM Fine-tuning with Less Specialization and More Generalization\n",
            "   Abstract: Most bandit algorithms assume that the reward variances or their upper bounds\n",
            "are known, and that they are the same for all arms. This naturally leads to\n",
            "suboptimal performance and higher regret due to variance overestimation. On the\n",
            "other hand, underestimated reward variances may lead to linear regret due to\n",
            "committing early to a suboptimal arm. This motivated prior works on\n",
            "variance-adaptive frequentist algorithms, which have strong instance-dependent\n",
            "regret bounds but cannot incorporate prior knowledge on reward variances. We\n",
            "lay foundations for the Bayesian setting, which incorporates prior knowledge.\n",
            "This results in lower regret in practice, due to using the prior in the\n",
            "algorithm design, and also improved regret guarantees. Specifically, we study\n",
            "Gaussian bandits with {unknown heterogeneous reward variances}, and develop a\n",
            "Thompson sampling algorithm with prior-dependent Bayes regret bounds. We\n",
            "achieve lower regret with lower reward variances and more informative priors on\n",
            "them, which is precisely why we pay only for what is uncertain. This is the\n",
            "first result of its kind. Finally, we corroborate our theory with extensive\n",
            "experiments, which show the superiority of our variance-adaptive Bayesian\n",
            "algorithm over prior frequentist approaches. We also show that our approach is\n",
            "robust to model misspecification and can be applied with estimated priors.\n",
            "\n",
            "1326. Title: Rethinking Adversarial Policies: A Generalized Attack Formulation and Provable Defense in RL\n",
            "   Abstract: We propose a scalable Forward-Forward (FF) algorithm that eliminates the need\n",
            "for backpropagation by training each layer separately. Unlike backpropagation,\n",
            "FF avoids backward gradients and can be more modular and memory efficient,\n",
            "making it appealing for large networks. We extend FF to modern convolutional\n",
            "architectures, such as MobileNetV3 and ResNet18, by introducing a new way to\n",
            "compute losses for convolutional layers. Experiments show that our method\n",
            "achieves performance comparable to standard backpropagation. Furthermore, when\n",
            "we divide the network into blocks, such as the residual blocks in ResNet, and\n",
            "apply backpropagation only within each block, but not across blocks, our hybrid\n",
            "design tends to outperform backpropagation baselines while maintaining a\n",
            "similar training speed. Finally, we present experiments on small datasets and\n",
            "transfer learning that confirm the adaptability of our method.\n",
            "\n",
            "1327. Title: RAIN: Your Language Models Can Align Themselves without Finetuning\n",
            "   Abstract: Robustness against adversarial attacks and distribution shifts is a\n",
            "long-standing goal of Reinforcement Learning (RL). To this end, Robust\n",
            "Adversarial Reinforcement Learning (RARL) trains a protagonist against\n",
            "destabilizing forces exercised by an adversary in a competitive zero-sum Markov\n",
            "game, whose optimal solution, i.e., rational strategy, corresponds to a Nash\n",
            "equilibrium. However, finding Nash equilibria requires facing complex saddle\n",
            "point optimization problems, which can be prohibitive to solve, especially for\n",
            "high-dimensional control. In this paper, we propose a novel approach for\n",
            "adversarial RL based on entropy regularization to ease the complexity of the\n",
            "saddle point optimization problem. We show that the solution of this\n",
            "entropy-regularized problem corresponds to a Quantal Response Equilibrium\n",
            "(QRE), a generalization of Nash equilibria that accounts for bounded\n",
            "rationality, i.e., agents sometimes play random actions instead of optimal\n",
            "ones. Crucially, the connection between the entropy-regularized objective and\n",
            "QRE enables free modulation of the rationality of the agents by simply tuning\n",
            "the temperature coefficient. We leverage this insight to propose our novel\n",
            "algorithm, Quantal Adversarial RL (QARL), which gradually increases the\n",
            "rationality of the adversary in a curriculum fashion until it is fully\n",
            "rational, easing the complexity of the optimization problem while retaining\n",
            "robustness. We provide extensive evidence of QARL outperforming RARL and recent\n",
            "baselines across several MuJoCo locomotion and navigation problems in overall\n",
            "performance and robustness.\n",
            "\n",
            "1328. Title: Robust agents learn causal world models\n",
            "   Abstract: Large Language Models (LLMs) have become increasingly important in natural\n",
            "language processing, enabling advanced data analytics through natural language\n",
            "queries. However, these models often generate \"hallucinations\"-inaccurate or\n",
            "fabricated information-that can undermine their reliability in critical\n",
            "data-driven decision-making. Addressing the challenge of hallucinations is\n",
            "essential to improve the accuracy and trustworthiness of LLMs in processing\n",
            "natural language queries. This research focuses on mitigating hallucinations in\n",
            "LLMs, specifically within the context of data analytics. We introduce and\n",
            "evaluate four targeted strategies: Structured Output Generation, Strict Rules\n",
            "Enforcement, System Prompt Enhancements, and Semantic Layer Integration. Our\n",
            "findings show that these methods are more effective than traditional\n",
            "fine-tuning approaches in reducing hallucinations, offering a more reliable\n",
            "framework for deploying LLMs in natural language queries for data analytics.\n",
            "This research demonstrates the potential of these strategies to enhance the\n",
            "accuracy of LLM-driven data queries, ensuring more dependable results in\n",
            "data-driven environments.\n",
            "\n",
            "1329. Title: Imitation Learning from Observation with Automatic Discount Scheduling\n",
            "   Abstract: It has long been hypothesised that causal reasoning plays a fundamental role\n",
            "in robust and general intelligence. However, it is not known if agents must\n",
            "learn causal models in order to generalise to new domains, or if other\n",
            "inductive biases are sufficient. We answer this question, showing that any\n",
            "agent capable of satisfying a regret bound under a large set of distributional\n",
            "shifts must have learned an approximate causal model of the data generating\n",
            "process, which converges to the true causal model for optimal agents. We\n",
            "discuss the implications of this result for several research areas including\n",
            "transfer learning and causal inference.\n",
            "\n",
            "1330. Title: PBADet: A One-Stage Anchor-Free Approach for Part-Body Association\n",
            "   Abstract: Mobile app development involves a unique set of challenges including device\n",
            "fragmentation and rapidly evolving platforms, making testing a difficult task.\n",
            "The design space for a comprehensive mobile testing strategy includes features,\n",
            "inputs, potential contextual app states, and large combinations of devices and\n",
            "underlying platforms. Therefore, automated testing is an essential activity of\n",
            "the development process. However, current state of the art of automated testing\n",
            "tools for mobile apps poses limitations that has driven a preference for manual\n",
            "testing in practice. As of today, there is no comprehensive automated solution\n",
            "for mobile testing that overcomes fundamental issues such as automated oracles,\n",
            "history awareness in test cases, or automated evolution of test cases.\n",
            "  In this perspective paper we survey the current state of the art in terms of\n",
            "the frameworks, tools, and services available to developers to aid in mobile\n",
            "testing, highlighting present shortcomings. Next, we provide commentary on\n",
            "current key challenges that restrict the possibility of a comprehensive,\n",
            "effective, and practical automated testing solution. Finally, we offer our\n",
            "vision of a comprehensive mobile app testing framework, complete with research\n",
            "agenda, that is succinctly summarized along three principles: Continuous,\n",
            "Evolutionary and Large-scale (CEL).\n",
            "\n",
            "1331. Title: RepoBench: Benchmarking Repository-Level Code Auto-Completion Systems\n",
            "   Abstract: Typical neural network trainings have substantial variance in test-set\n",
            "performance between repeated runs, impeding hyperparameter comparison and\n",
            "training reproducibility. In this work we present the following results towards\n",
            "understanding this variation. (1) Despite having significant variance on their\n",
            "test-sets, we demonstrate that standard CIFAR-10 and ImageNet trainings have\n",
            "little variance in performance on the underlying test-distributions from which\n",
            "their test-sets are sampled. (2) We show that these trainings make\n",
            "approximately independent errors on their test-sets. That is, the event that a\n",
            "trained network makes an error on one particular example does not affect its\n",
            "chances of making errors on other examples, relative to their average rates\n",
            "over repeated runs of training with the same hyperparameters. (3) We prove that\n",
            "the variance of neural network trainings on their test-sets is a downstream\n",
            "consequence of the class-calibration property discovered by Jiang et al.\n",
            "(2021). Our analysis yields a simple formula which accurately predicts variance\n",
            "for the binary classification case. (4) We conduct preliminary studies of data\n",
            "augmentation, learning rate, finetuning instability and distribution-shift\n",
            "through the lens of variance between runs.\n",
            "\n",
            "1332. Title: SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning\n",
            "   Abstract: The problem of minimizing the maximum of $N$ convex, Lipschitz functions\n",
            "plays significant roles in optimization and machine learning. It has a series\n",
            "of results, with the most recent one requiring $O(N\\epsilon^{-2/3} +\n",
            "\\epsilon^{-8/3})$ queries to a first-order oracle to compute an\n",
            "$\\epsilon$-suboptimal point. On the other hand, quantum algorithms for\n",
            "optimization are rapidly advancing with speedups shown on many important\n",
            "optimization problems. In this paper, we conduct a systematic study for quantum\n",
            "algorithms and lower bounds for minimizing the maximum of $N$ convex, Lipschitz\n",
            "functions. On one hand, we develop quantum algorithms with an improved\n",
            "complexity bound of $\\tilde{O}(\\sqrt{N}\\epsilon^{-5/3} + \\epsilon^{-8/3})$. On\n",
            "the other hand, we prove that quantum algorithms must take\n",
            "$\\tilde{\\Omega}(\\sqrt{N}\\epsilon^{-2/3})$ queries to a first order quantum\n",
            "oracle, showing that our dependence on $N$ is optimal up to poly-logarithmic\n",
            "factors.\n",
            "\n",
            "1333. Title: GNeRP: Gaussian-guided Neural Reconstruction of Reflective Objects with Noisy Polarization Priors\n",
            "   Abstract: Humans often acquire new skills through observation and imitation. For\n",
            "robotic agents, learning from the plethora of unlabeled video demonstration\n",
            "data available on the Internet necessitates imitating the expert without access\n",
            "to its action, presenting a challenge known as Imitation Learning from\n",
            "Observations (ILfO). A common approach to tackle ILfO problems is to convert\n",
            "them into inverse reinforcement learning problems, utilizing a proxy reward\n",
            "computed from the agent's and the expert's observations. Nonetheless, we\n",
            "identify that tasks characterized by a progress dependency property pose\n",
            "significant challenges for such approaches; in these tasks, the agent needs to\n",
            "initially learn the expert's preceding behaviors before mastering the\n",
            "subsequent ones. Our investigation reveals that the main cause is that the\n",
            "reward signals assigned to later steps hinder the learning of initial\n",
            "behaviors. To address this challenge, we present a novel ILfO framework that\n",
            "enables the agent to master earlier behaviors before advancing to later ones.\n",
            "We introduce an Automatic Discount Scheduling (ADS) mechanism that adaptively\n",
            "alters the discount factor in reinforcement learning during the training phase,\n",
            "prioritizing earlier rewards initially and gradually engaging later rewards\n",
            "only when the earlier behaviors have been mastered. Our experiments, conducted\n",
            "on nine Meta-World tasks, demonstrate that our method significantly outperforms\n",
            "state-of-the-art methods across all tasks, including those that are unsolvable\n",
            "by them.\n",
            "\n",
            "1334. Title: Harnessing Joint Rain-/Detail-aware Representations to Eliminate Intricate Rains\n",
            "   Abstract: Language models generate responses by producing a series of tokens in\n",
            "immediate succession: the $(K+1)^{th}$ token is an outcome of manipulating $K$\n",
            "hidden vectors per layer, one vector per preceding token. What if instead we\n",
            "were to let the model manipulate say, $K+10$ hidden vectors, before it outputs\n",
            "the $(K+1)^{th}$ token? We operationalize this idea by performing training and\n",
            "inference on language models with a (learnable) $\\textit{pause}$ token, a\n",
            "sequence of which is appended to the input prefix. We then delay extracting the\n",
            "model's outputs until the last pause token is seen, thereby allowing the model\n",
            "to process extra computation before committing to an answer. We empirically\n",
            "evaluate $\\textit{pause-training}$ on decoder-only models of 1B and 130M\n",
            "parameters with causal pretraining on C4, and on downstream tasks covering\n",
            "reasoning, question-answering, general understanding and fact recall. Our main\n",
            "finding is that inference-time delays show gains when the model is both\n",
            "pre-trained and finetuned with delays. For the 1B model, we witness gains on 8\n",
            "of 9 tasks, most prominently, a gain of $18\\%$ EM score on the QA task of\n",
            "SQuAD, $8\\%$ on CommonSenseQA and $1\\%$ accuracy on the reasoning task of\n",
            "GSM8k. Our work raises a range of conceptual and practical future research\n",
            "questions on making delayed next-token prediction a widely applicable new\n",
            "paradigm.\n",
            "\n",
            "1335. Title: Kernel Metric Learning for In-Sample Off-Policy Evaluation of Deterministic RL Policies\n",
            "   Abstract: Recent advances in image deraining have focused on training powerful models\n",
            "on mixed multiple datasets comprising diverse rain types and backgrounds.\n",
            "However, this approach tends to overlook the inherent differences among rainy\n",
            "images, leading to suboptimal results. To overcome this limitation, we focus on\n",
            "addressing various rainy images by delving into meaningful representations that\n",
            "encapsulate both the rain and background components. Leveraging these\n",
            "representations as instructive guidance, we put forth a Context-based\n",
            "Instance-level Modulation (CoI-M) mechanism adept at efficiently modulating\n",
            "CNN- or Transformer-based models. Furthermore, we devise a rain-/detail-aware\n",
            "contrastive learning strategy to help extract joint rain-/detail-aware\n",
            "representations. By integrating CoI-M with the rain-/detail-aware Contrastive\n",
            "learning, we develop CoIC, an innovative and potent algorithm tailored for\n",
            "training models on mixed datasets. Moreover, CoIC offers insight into modeling\n",
            "relationships of datasets, quantitatively assessing the impact of rain and\n",
            "details on restoration, and unveiling distinct behaviors of models given\n",
            "diverse inputs. Extensive experiments validate the efficacy of CoIC in boosting\n",
            "the deraining ability of CNN and Transformer models. CoIC also enhances the\n",
            "deraining prowess remarkably when real-world dataset is included.\n",
            "\n",
            "1336. Title: Jailbreak in pieces: Compositional Adversarial Attacks on Multi-Modal Language Models\n",
            "   Abstract: Transformers have recently emerged as powerful neural networks for graph\n",
            "learning, showcasing state-of-the-art performance on several graph property\n",
            "prediction tasks. However, these results have been limited to small-scale\n",
            "graphs, where the computational feasibility of the global attention mechanism\n",
            "is possible. The next goal is to scale up these architectures to handle very\n",
            "large graphs on the scale of millions or even billions of nodes. With\n",
            "large-scale graphs, global attention learning is proven impractical due to its\n",
            "quadratic complexity w.r.t. the number of nodes. On the other hand,\n",
            "neighborhood sampling techniques become essential to manage large graph sizes,\n",
            "yet finding the optimal trade-off between speed and accuracy with sampling\n",
            "techniques remains challenging. This work advances representation learning on\n",
            "single large-scale graphs with a focus on identifying model characteristics and\n",
            "critical design constraints for developing scalable graph transformer (GT)\n",
            "architectures. We argue such GT requires layers that can adeptly learn both\n",
            "local and global graph representations while swiftly sampling the graph\n",
            "topology. As such, a key innovation of this work lies in the creation of a fast\n",
            "neighborhood sampling technique coupled with a local attention mechanism that\n",
            "encompasses a 4-hop reception field, but achieved through just 2-hop\n",
            "operations. This local node embedding is then integrated with a global node\n",
            "embedding, acquired via another self-attention layer with an approximate global\n",
            "codebook, before finally sent through a downstream layer for node predictions.\n",
            "The proposed GT framework, named LargeGT, overcomes previous computational\n",
            "bottlenecks and is validated on three large-scale node classification\n",
            "benchmarks. We report a 3x speedup and 16.8% performance gain on ogbn-products\n",
            "and snap-patents, while we also scale LargeGT on ogbn-papers100M with a 5.9%\n",
            "performance improvement.\n",
            "\n",
            "1337. Title: AmortizedPeriod: Attention-based Amortized Inference for Periodicity Identification\n",
            "   Abstract: Refusal training on Large Language Models (LLMs) prevents harmful outputs,\n",
            "yet this defense remains vulnerable to both automated and human-crafted\n",
            "jailbreaks. We present a novel LLM-as-red-teamer approach in which a human\n",
            "jailbreaks a refusal-trained LLM to make it willing to jailbreak itself or\n",
            "other LLMs. We refer to the jailbroken LLMs as $J_2$ attackers, which can\n",
            "systematically evaluate target models using various red teaming strategies and\n",
            "improve its performance via in-context learning from the previous failures. Our\n",
            "experiments demonstrate that Sonnet 3.5 and Gemini 1.5 pro outperform other\n",
            "LLMs as $J_2$, achieving 93.0% and 91.0% attack success rates (ASRs)\n",
            "respectively against GPT-4o (and similar results across other capable LLMs) on\n",
            "Harmbench. Our work not only introduces a scalable approach to strategic red\n",
            "teaming, drawing inspiration from human red teamers, but also highlights\n",
            "jailbreaking-to-jailbreak as an overlooked failure mode of the safeguard.\n",
            "Specifically, an LLM can bypass its own safeguards by employing a jailbroken\n",
            "version of itself that is willing to assist in further jailbreaking. To prevent\n",
            "any direct misuse with $J_2$, while advancing research in AI safety, we\n",
            "publicly share our methodology while keeping specific prompting details\n",
            "private.\n",
            "\n",
            "1338. Title: #InsTag: Instruction Tagging for Analyzing Supervised Fine-tuning of Large Language Models\n",
            "   Abstract: Posterior sampling, i.e., exponential mechanism to sample from the posterior\n",
            "distribution, provides $\\varepsilon$-pure differential privacy (DP) guarantees\n",
            "and does not suffer from potentially unbounded privacy breach introduced by\n",
            "$(\\varepsilon,\\delta)$-approximate DP. In practice, however, one needs to apply\n",
            "approximate sampling methods such as Markov chain Monte Carlo (MCMC), thus\n",
            "re-introducing the unappealing $\\delta$-approximation error into the privacy\n",
            "guarantees. To bridge this gap, we propose the Approximate SAample Perturbation\n",
            "(abbr. ASAP) algorithm which perturbs an MCMC sample with noise proportional to\n",
            "its Wasserstein-infinity ($W_\\infty$) distance from a reference distribution\n",
            "that satisfies pure DP or pure Gaussian DP (i.e., $\\delta=0$). We then leverage\n",
            "a Metropolis-Hastings algorithm to generate the sample and prove that the\n",
            "algorithm converges in $W_\\infty$ distance. We show that by combining our new\n",
            "techniques with a localization step, we obtain the first nearly linear-time\n",
            "algorithm that achieves the optimal rates in the DP-ERM problem with strongly\n",
            "convex and smooth losses.\n",
            "\n",
            "1339. Title: Image Clustering via the Principle of Rate Reduction in the Age of Pretrained Models\n",
            "   Abstract: Inferring unbiased treatment effects has received widespread attention in the\n",
            "machine learning community. In recent years, our community has proposed\n",
            "numerous solutions in standard settings, high-dimensional treatment settings,\n",
            "and even longitudinal settings. While very diverse, the solution has mostly\n",
            "relied on neural networks for inference and simultaneous correction of\n",
            "assignment bias. New approaches typically build on top of previous approaches\n",
            "by proposing new (or refined) architectures and learning algorithms. However,\n",
            "the end result -- a neural-network-based inference machine -- remains\n",
            "unchallenged. In this paper, we introduce a different type of solution in the\n",
            "longitudinal setting: a closed-form ordinary differential equation (ODE). While\n",
            "we still rely on continuous optimization to learn an ODE, the resulting\n",
            "inference machine is no longer a neural network. Doing so yields several\n",
            "advantages such as interpretability, irregular sampling, and a different set of\n",
            "identification assumptions. Above all, we consider the introduction of a\n",
            "completely new type of solution to be our most important contribution as it may\n",
            "spark entirely new innovations in treatment effects in general. We facilitate\n",
            "this by formulating our contribution as a framework that can transform any ODE\n",
            "discovery method into a treatment effects method.\n",
            "\n",
            "1340. Title: $\\alpha$TC-VAE: On the relationship between Disentanglement and Diversity\n",
            "   Abstract: Probabilistic Spacetime is a simple generalization of the classical model of\n",
            "spacetime in General Relativity, such that it allows to consider multiple\n",
            "metric field realizations endowed with probabilities. The motivation for such a\n",
            "generalization is a possible application in the context of some quantum gravity\n",
            "approaches, particularly those using the path integral. It is argued that this\n",
            "model might be used to describe simplified geometry, resulting e.g. from\n",
            "discretization, while keeping the continuous manifold; or it may be used as an\n",
            "effective description of a probabilistic geometry resulting from a full-fledged\n",
            "quantum gravity computation.\n",
            "\n",
            "1341. Title: CIFAR-10-Warehouse: Broad and More Realistic Testbeds in Model Generalization Analysis\n",
            "   Abstract: We consider off-policy evaluation (OPE) of deterministic target policies for\n",
            "reinforcement learning (RL) in environments with continuous action spaces.\n",
            "While it is common to use importance sampling for OPE, it suffers from high\n",
            "variance when the behavior policy deviates significantly from the target\n",
            "policy. In order to address this issue, some recent works on OPE proposed\n",
            "in-sample learning with importance resampling. Yet, these approaches are not\n",
            "applicable to deterministic target policies for continuous action spaces. To\n",
            "address this limitation, we propose to relax the deterministic target policy\n",
            "using a kernel and learn the kernel metrics that minimize the overall mean\n",
            "squared error of the estimated temporal difference update vector of an action\n",
            "value function, where the action value function is used for policy evaluation.\n",
            "We derive the bias and variance of the estimation error due to this relaxation\n",
            "and provide analytic solutions for the optimal kernel metric. In empirical\n",
            "studies using various test domains, we show that the OPE with in-sample\n",
            "learning using the kernel with optimized metric achieves significantly improved\n",
            "accuracy than other baselines.\n",
            "\n",
            "1342. Title: InterpGNN: Understand and Improve Generalization Ability of Transdutive GNNs through the Lens of Interplay between Train and Test Nodes\n",
            "   Abstract: Federated learning has emerged as a promising distributed learning paradigm\n",
            "that facilitates collaborative learning among multiple parties without\n",
            "transferring raw data. However, most existing federated learning studies focus\n",
            "on either horizontal or vertical data settings, where the data of different\n",
            "parties are assumed to be from the same feature or sample space. In practice, a\n",
            "common scenario is the hybrid data setting, where data from different parties\n",
            "may differ both in the features and samples. To address this, we propose\n",
            "HybridTree, a novel federated learning approach that enables federated tree\n",
            "learning on hybrid data. We observe the existence of consistent split rules in\n",
            "trees. With the help of these split rules, we theoretically show that the\n",
            "knowledge of parties can be incorporated into the lower layers of a tree. Based\n",
            "on our theoretical analysis, we propose a layer-level solution that does not\n",
            "need frequent communication traffic to train a tree. Our experiments\n",
            "demonstrate that HybridTree can achieve comparable accuracy to the centralized\n",
            "setting with low computational and communication overhead. HybridTree can\n",
            "achieve up to 8 times speedup compared with the other baselines.\n",
            "\n",
            "1343. Title: ODE Discovery for Longitudinal Heterogeneous Treatment Effects Inference\n",
            "   Abstract: Language models generate text based on successively sampling the next word. A\n",
            "decoding procedure based on nucleus (top-$p$) sampling chooses from the\n",
            "smallest possible set of words whose cumulative probability exceeds the\n",
            "probability $p$. In this work, we assess whether a top-$p$ set is indeed\n",
            "aligned with its probabilistic meaning in various linguistic contexts. We\n",
            "employ conformal prediction, a calibration procedure that focuses on the\n",
            "construction of minimal prediction sets according to a desired confidence\n",
            "level, to calibrate the parameter $p$ as a function of the entropy of the next\n",
            "word distribution. We find that OPT models are overconfident, and that\n",
            "calibration shows a moderate inverse scaling with model size.\n",
            "\n",
            "1344. Title: Effective and Efficient Federated Tree Learning on Hybrid Data\n",
            "   Abstract: In many robotic applications, it is crucial to maintain a belief about the\n",
            "state of a system, which serves as input for planning and decision making and\n",
            "provides feedback during task execution. Bayesian Filtering algorithms address\n",
            "this state estimation problem, but they require models of process dynamics and\n",
            "sensory observations and the respective noise characteristics of these models.\n",
            "Recently, multiple works have demonstrated that these models can be learned by\n",
            "end-to-end training through differentiable versions of recursive filtering\n",
            "algorithms. In this work, we investigate the advantages of differentiable\n",
            "filters (DFs) over both unstructured learning approaches and manually-tuned\n",
            "filtering algorithms, and provide practical guidance to researchers interested\n",
            "in applying such differentiable filters. For this, we implement DFs with four\n",
            "different underlying filtering algorithms and compare them in extensive\n",
            "experiments. Specifically, we (i) evaluate different implementation choices and\n",
            "training approaches, (ii) investigate how well complex models of uncertainty\n",
            "can be learned in DFs, (iii) evaluate the effect of end-to-end training through\n",
            "DFs and (iv) compare the DFs among each other and to unstructured LSTM models.\n",
            "\n",
            "1345. Title: Quantifying the Sensitivity of Inverse Reinforcement Learning to Misspecification\n",
            "   Abstract: The advent of large pre-trained models has brought about a paradigm shift in\n",
            "both visual representation learning and natural language processing. However,\n",
            "clustering unlabeled images, as a fundamental and classic machine learning\n",
            "problem, still lacks an effective solution, particularly for large-scale\n",
            "datasets. In this paper, we propose a novel image clustering pipeline that\n",
            "leverages the powerful feature representation of large pre-trained models such\n",
            "as CLIP and cluster images effectively and efficiently at scale. We first\n",
            "developed a novel algorithm to estimate the number of clusters in a given\n",
            "dataset. We then show that the pre-trained features are significantly more\n",
            "structured by further optimizing the rate reduction objective. The resulting\n",
            "features may significantly improve the clustering accuracy, e.g., from 57\\% to\n",
            "66\\% on ImageNet-1k. Furthermore, by leveraging CLIP's multimodality bridge\n",
            "between image and text, we develop a simple yet effective self-labeling\n",
            "algorithm that produces meaningful captions for the clusters. Through extensive\n",
            "experiments, we show that our pipeline works well on standard datasets such as\n",
            "CIFAR-10, CIFAR-100, and ImageNet-1k. It also extends to datasets that are not\n",
            "curated for clustering, such as LAION-Aesthetics and WikiArts. We released the\n",
            "code in https://github.com/LeslieTrue/CPP.\n",
            "\n",
            "1346. Title: How I Warped Your Noise: a Temporally-Correlated Noise Prior for Diffusion Models\n",
            "   Abstract: Inverse reinforcement learning (IRL) aims to infer an agent's preferences\n",
            "(represented as a reward function $R$) from their behaviour (represented as a\n",
            "policy $\\pi$). To do this, we need a behavioural model of how $\\pi$ relates to\n",
            "$R$. In the current literature, the most common behavioural models are\n",
            "optimality, Boltzmann-rationality, and causal entropy maximisation. However,\n",
            "the true relationship between a human's preferences and their behaviour is much\n",
            "more complex than any of these behavioural models. This means that the\n",
            "behavioural models are misspecified, which raises the concern that they may\n",
            "lead to systematic errors if applied to real data. In this paper, we analyse\n",
            "how sensitive the IRL problem is to misspecification of the behavioural model.\n",
            "Specifically, we provide necessary and sufficient conditions that completely\n",
            "characterise how the observed data may differ from the assumed behavioural\n",
            "model without incurring an error above a given threshold. In addition to this,\n",
            "we also characterise the conditions under which a behavioural model is robust\n",
            "to small perturbations of the observed policy, and we analyse how robust many\n",
            "behavioural models are to misspecification of their parameter values (such as\n",
            "e.g.\\ the discount rate). Our analysis suggests that the IRL problem is highly\n",
            "sensitive to misspecification, in the sense that very mild misspecification can\n",
            "lead to very large errors in the inferred reward function.\n",
            "\n",
            "1347. Title: Adversarial Attacks on Fairness of Graph Neural Networks\n",
            "   Abstract: Fairness-aware graph neural networks (GNNs) have gained a surge of attention\n",
            "as they can reduce the bias of predictions on any demographic group (e.g.,\n",
            "female) in graph-based applications. Although these methods greatly improve the\n",
            "algorithmic fairness of GNNs, the fairness can be easily corrupted by carefully\n",
            "designed adversarial attacks. In this paper, we investigate the problem of\n",
            "adversarial attacks on fairness of GNNs and propose G-FairAttack, a general\n",
            "framework for attacking various types of fairness-aware GNNs in terms of\n",
            "fairness with an unnoticeable effect on prediction utility. In addition, we\n",
            "propose a fast computation technique to reduce the time complexity of\n",
            "G-FairAttack. The experimental study demonstrates that G-FairAttack\n",
            "successfully corrupts the fairness of different types of GNNs while keeping the\n",
            "attack unnoticeable. Our study on fairness attacks sheds light on potential\n",
            "vulnerabilities in fairness-aware GNNs and guides further research on the\n",
            "robustness of GNNs in terms of fairness.\n",
            "\n",
            "1348. Title: An Investigation of Representation and Allocation Harms in Contrastive Learning\n",
            "   Abstract: Typical diffusion models are trained to accept a particular form of\n",
            "conditioning, most commonly text, and cannot be conditioned on other modalities\n",
            "without retraining. In this work, we propose a universal guidance algorithm\n",
            "that enables diffusion models to be controlled by arbitrary guidance modalities\n",
            "without the need to retrain any use-specific components. We show that our\n",
            "algorithm successfully generates quality images with guidance functions\n",
            "including segmentation, face recognition, object detection, and classifier\n",
            "signals. Code is available at\n",
            "https://github.com/arpitbansal297/Universal-Guided-Diffusion.\n",
            "\n",
            "1349. Title: Large-Vocabulary 3D Diffusion Model with Transformer\n",
            "   Abstract: Machine learning based solvers have garnered much attention in physical\n",
            "simulation and scientific computing, with a prominent example, physics-informed\n",
            "neural networks (PINNs). However, PINNs often struggle to solve high-frequency\n",
            "and multi-scale PDEs, which can be due to spectral bias during neural network\n",
            "training. To address this problem, we resort to the Gaussian process (GP)\n",
            "framework. To flexibly capture the dominant frequencies, we model the power\n",
            "spectrum of the PDE solution with a student $t$ mixture or Gaussian mixture. We\n",
            "apply the inverse Fourier transform to obtain the covariance function (by\n",
            "Wiener-Khinchin theorem). The covariance derived from the Gaussian mixture\n",
            "spectrum corresponds to the known spectral mixture kernel. Next, we estimate\n",
            "the mixture weights in the log domain, which we show is equivalent to placing a\n",
            "Jeffreys prior. It automatically induces sparsity, prunes excessive\n",
            "frequencies, and adjusts the remaining toward the ground truth. Third, to\n",
            "enable efficient and scalable computation on massive collocation points, which\n",
            "are critical to capture high frequencies, we place the collocation points on a\n",
            "grid, and multiply our covariance function at each input dimension. We use the\n",
            "GP conditional mean to predict the solution and its derivatives so as to fit\n",
            "the boundary condition and the equation itself. As a result, we can derive a\n",
            "Kronecker product structure in the covariance matrix. We use Kronecker product\n",
            "properties and multilinear algebra to promote computational efficiency and\n",
            "scalability, without low-rank approximations. We show the advantage of our\n",
            "method in systematic experiments. The code is released at\n",
            "\\url{https://github.com/xuangu-fang/Gaussian-Process-Slover-for-High-Freq-PDE}.\n",
            "\n",
            "1350. Title: Leveraging Hyperbolic Embeddings for Coarse-to-Fine Robot Design\n",
            "   Abstract: Multi-cellular robot design aims to create robots comprised of numerous cells\n",
            "that can be efficiently controlled to perform diverse tasks. Previous research\n",
            "has demonstrated the ability to generate robots for various tasks, but these\n",
            "approaches often optimize robots directly in the vast design space, resulting\n",
            "in robots with complicated morphologies that are hard to control. In response,\n",
            "this paper presents a novel coarse-to-fine method for designing multi-cellular\n",
            "robots. Initially, this strategy seeks optimal coarse-grained robots and\n",
            "progressively refines them. To mitigate the challenge of determining the\n",
            "precise refinement juncture during the coarse-to-fine transition, we introduce\n",
            "the Hyperbolic Embeddings for Robot Design (HERD) framework. HERD unifies\n",
            "robots of various granularity within a shared hyperbolic space and leverages a\n",
            "refined Cross-Entropy Method for optimization. This framework enables our\n",
            "method to autonomously identify areas of exploration in hyperbolic space and\n",
            "concentrate on regions demonstrating promise. Finally, the extensive empirical\n",
            "studies on various challenging tasks sourced from EvoGym show our approach's\n",
            "superior efficiency and generalization capability.\n",
            "\n",
            "1351. Title: Efficient Integrators for Diffusion Generative Models\n",
            "   Abstract: Diffusion Handles is a novel approach to enabling 3D object edits on\n",
            "diffusion images. We accomplish these edits using existing pre-trained\n",
            "diffusion models, and 2D image depth estimation, without any fine-tuning or 3D\n",
            "object retrieval. The edited results remain plausible, photo-real, and preserve\n",
            "object identity. Diffusion Handles address a critically missing facet of\n",
            "generative image based creative design, and significantly advance the\n",
            "state-of-the-art in generative image editing. Our key insight is to lift\n",
            "diffusion activations for an object to 3D using a proxy depth, 3D-transform the\n",
            "depth and associated activations, and project them back to image space. The\n",
            "diffusion process applied to the manipulated activations with identity control,\n",
            "produces plausible edited images showing complex 3D occlusion and lighting\n",
            "effects. We evaluate Diffusion Handles: quantitatively, on a large synthetic\n",
            "data benchmark; and qualitatively by a user study, showing our output to be\n",
            "more plausible, and better than prior art at both, 3D editing and identity\n",
            "control. Project Webpage: https://diffusionhandles.github.io/\n",
            "\n",
            "1352. Title: Boosting of Thoughts: Trial-and-Error Problem Solving with Large Language Models\n",
            "   Abstract: Diffusion models suffer from slow sample generation at inference time.\n",
            "Therefore, developing a principled framework for fast deterministic/stochastic\n",
            "sampling for a broader class of diffusion models is a promising direction. We\n",
            "propose two complementary frameworks for accelerating sample generation in\n",
            "pre-trained models: Conjugate Integrators and Splitting Integrators. Conjugate\n",
            "integrators generalize DDIM, mapping the reverse diffusion dynamics to a more\n",
            "amenable space for sampling. In contrast, splitting-based integrators, commonly\n",
            "used in molecular dynamics, reduce the numerical simulation error by cleverly\n",
            "alternating between numerical updates involving the data and auxiliary\n",
            "variables. After extensively studying these methods empirically and\n",
            "theoretically, we present a hybrid method that leads to the best-reported\n",
            "performance for diffusion models in augmented spaces. Applied to Phase Space\n",
            "Langevin Diffusion [Pandey & Mandt, 2023] on CIFAR-10, our deterministic and\n",
            "stochastic samplers achieve FID scores of 2.11 and 2.36 in only 100 network\n",
            "function evaluations (NFE) as compared to 2.57 and 2.63 for the best-performing\n",
            "baselines, respectively. Our code and model checkpoints will be made publicly\n",
            "available at \\url{https://github.com/mandt-lab/PSLD}.\n",
            "\n",
            "1353. Title: Explaining Time Series via Contrastive and Locally Sparse Perturbations\n",
            "   Abstract: Anomaly detection (AD) is a critical machine learning task with diverse\n",
            "applications in web systems, including fraud detection, content moderation, and\n",
            "user behavior analysis. Despite its significance, AD in natural language\n",
            "processing (NLP) remains underexplored, limiting advancements in detecting\n",
            "anomalies in text data such as harmful content, phishing attempts, or spam\n",
            "reviews. In this paper, we introduce NLP-ADBench, the most comprehensive\n",
            "benchmark for NLP anomaly detection (NLP-AD), comprising eight curated datasets\n",
            "and evaluations of nineteen state-of-the-art algorithms. These include three\n",
            "end-to-end methods and sixteen two-step algorithms that apply traditional\n",
            "anomaly detection techniques to language embeddings generated by\n",
            "bert-base-uncased and OpenAI's text-embedding-3-large models.\n",
            "  Our results reveal critical insights and future directions for NLP-AD.\n",
            "Notably, no single model excels across all datasets, highlighting the need for\n",
            "automated model selection. Moreover, two-step methods leveraging\n",
            "transformer-based embeddings consistently outperform specialized end-to-end\n",
            "approaches, with OpenAI embeddings demonstrating superior performance over BERT\n",
            "embeddings. By releasing NLP-ADBench at\n",
            "https://github.com/USC-FORTIS/NLP-ADBench, we provide a standardized framework\n",
            "for evaluating NLP-AD methods, fostering the development of innovative\n",
            "approaches. This work fills a crucial gap in the field and establishes a\n",
            "foundation for advancing NLP anomaly detection, particularly in the context of\n",
            "improving the safety and reliability of web-based systems.\n",
            "\n",
            "1354. Title: Conditional Instrumental Variable Regression with Representation Learning for Causal Inference\n",
            "   Abstract: The effect of underrepresentation on the performance of minority groups is\n",
            "known to be a serious problem in supervised learning settings; however, it has\n",
            "been underexplored so far in the context of self-supervised learning (SSL). In\n",
            "this paper, we demonstrate that contrastive learning (CL), a popular variant of\n",
            "SSL, tends to collapse representations of minority groups with certain majority\n",
            "groups. We refer to this phenomenon as representation harm and demonstrate it\n",
            "on image and text datasets using the corresponding popular CL methods.\n",
            "Furthermore, our causal mediation analysis of allocation harm on a downstream\n",
            "classification task reveals that representation harm is partly responsible for\n",
            "it, thus emphasizing the importance of studying and mitigating representation\n",
            "harm. Finally, we provide a theoretical explanation for representation harm\n",
            "using a stochastic block model that leads to a representational neural collapse\n",
            "in a contrastive learning setting.\n",
            "\n",
            "1355. Title: Protein-Ligand Interaction Prior for Binding-aware 3D Molecule Diffusion Models\n",
            "   Abstract: Gradient Boosting (GB) is a popular methodology used to solve prediction\n",
            "problems by minimizing a differentiable loss function, $L$. GB performs very\n",
            "well on tabular machine learning (ML) problems; however, as a pure ML solver it\n",
            "lacks the ability to fit models with probabilistic but correlated\n",
            "multi-dimensional outputs, for example, multiple correlated Bernoulli outputs.\n",
            "GB also does not form intermediate abstract data embeddings, one property of\n",
            "Deep Learning that gives greater flexibility and performance on other types of\n",
            "problems. This paper presents a simple adjustment to GB motivated in part by\n",
            "artificial neural networks. Specifically, our adjustment inserts a matrix\n",
            "multiplication between the output of a GB model and the loss, $L$. This allows\n",
            "the output of a GB model to have increased dimension prior to being fed into\n",
            "the loss and is thus ``wider'' than standard GB implementations. We call our\n",
            "method Wide Boosting (WB) and show that WB outperforms GB on mult-dimesional\n",
            "output tasks and that the embeddings generated by WB contain are more useful in\n",
            "downstream prediction tasks than GB output predictions alone.\n",
            "\n",
            "1356. Title: Seer: Language Instructed Video Prediction with Latent Diffusion Models\n",
            "   Abstract: 3D molecule generation is crucial for drug discovery and material design.\n",
            "While prior efforts focus on 3D diffusion models for their benefits in modeling\n",
            "continuous 3D conformers, they overlook the advantages of 1D SELFIES-based\n",
            "Language Models (LMs), which can generate 100% valid molecules and leverage the\n",
            "billion-scale 1D molecule datasets. To combine these advantages for 3D molecule\n",
            "generation, we propose a foundation model -- NExT-Mol: 3D Diffusion Meets 1D\n",
            "Language Modeling for 3D Molecule Generation. NExT-Mol uses an extensively\n",
            "pretrained molecule LM for 1D molecule generation, and subsequently predicts\n",
            "the generated molecule's 3D conformers with a 3D diffusion model. We enhance\n",
            "NExT-Mol's performance by scaling up the LM's model size, refining the\n",
            "diffusion neural architecture, and applying 1D to 3D transfer learning.\n",
            "Notably, our 1D molecule LM significantly outperforms baselines in\n",
            "distributional similarity while ensuring validity, and our 3D diffusion model\n",
            "achieves leading performances in conformer prediction. Given these improvements\n",
            "in 1D and 3D modeling, NExT-Mol achieves a 26% relative improvement in 3D FCD\n",
            "for de novo 3D generation on GEOM-DRUGS, and a 13% average relative gain for\n",
            "conditional 3D generation on QM9-2014. Our codes and pretrained checkpoints are\n",
            "available at https://github.com/acharkq/NExT-Mol.\n",
            "\n",
            "1357. Title: Learning Grounded Action Abstractions from Language\n",
            "   Abstract: This paper presents a framework for learning state and action abstractions in\n",
            "sequential decision-making domains. Our framework, planning abstraction from\n",
            "language (PARL), utilizes language-annotated demonstrations to automatically\n",
            "discover a symbolic and abstract action space and induce a latent state\n",
            "abstraction based on it. PARL consists of three stages: 1) recovering\n",
            "object-level and action concepts, 2) learning state abstractions, abstract\n",
            "action feasibility, and transition models, and 3) applying low-level policies\n",
            "for abstract actions. During inference, given the task description, PARL first\n",
            "makes abstract action plans using the latent transition and feasibility\n",
            "functions, then refines the high-level plan using low-level policies. PARL\n",
            "generalizes across scenarios involving novel object instances and environments,\n",
            "unseen concept compositions, and tasks that require longer planning horizons\n",
            "than settings it is trained on.\n",
            "\n",
            "1358. Title: SAFLEX: Self-Adaptive Augmentation via Feature Label Extrapolation\n",
            "   Abstract: Explaining multivariate time series is a compound challenge, as it requires\n",
            "identifying important locations in the time series and matching complex\n",
            "temporal patterns. Although previous saliency-based methods addressed the\n",
            "challenges, their perturbation may not alleviate the distribution shift issue,\n",
            "which is inevitable especially in heterogeneous samples. We present ContraLSP,\n",
            "a locally sparse model that introduces counterfactual samples to build\n",
            "uninformative perturbations but keeps distribution using contrastive learning.\n",
            "Furthermore, we incorporate sample-specific sparse gates to generate more\n",
            "binary-skewed and smooth masks, which easily integrate temporal trends and\n",
            "select the salient features parsimoniously. Empirical studies on both synthetic\n",
            "and real-world datasets show that ContraLSP outperforms state-of-the-art\n",
            "models, demonstrating a substantial improvement in explanation quality for time\n",
            "series data. The source code is available at\n",
            "\\url{https://github.com/zichuan-liu/ContraLSP}.\n",
            "\n",
            "1359. Title: Zero and Few-shot Semantic Parsing with Ambiguous Inputs\n",
            "   Abstract: Despite the frequent challenges posed by ambiguity when representing meaning\n",
            "via natural language, it is often ignored or deliberately removed in tasks\n",
            "mapping language to formally-designed representations, which generally assume a\n",
            "one-to-one mapping between linguistic and formal representations. We attempt to\n",
            "address this shortcoming by introducing AmP, a framework, dataset, and\n",
            "challenge for translating ambiguous natural language to formal representations\n",
            "like logic and code. We define templates and generate data for five\n",
            "well-documented linguistic ambiguities. Using AmP, we investigate how several\n",
            "few-shot text-to-code systems handle ambiguity, introducing three new metrics.\n",
            "We find that large pre-trained models perform poorly at capturing the\n",
            "distribution of possible meanings without deliberate instruction. However,\n",
            "models are able to capture the distribution well when ambiguity is attested in\n",
            "their inputs. These results motivate a call for including ambiguity explicitly\n",
            "in datasets and promote considering the distribution of possible outputs when\n",
            "evaluating systems. Data and code: https://github.com/esteng/ambiguous_parsing\n",
            "\n",
            "1360. Title: FairSeg: A Large-Scale Medical Image Segmentation Dataset for Fairness Learning Using Segment Anything Model with Fair Error-Bound Scaling\n",
            "   Abstract: Learning Rate Rewinding (LRR) has been established as a strong variant of\n",
            "Iterative Magnitude Pruning (IMP) to find lottery tickets in deep\n",
            "overparameterized neural networks. While both iterative pruning schemes couple\n",
            "structure and parameter learning, understanding how LRR excels in both aspects\n",
            "can bring us closer to the design of more flexible deep learning algorithms\n",
            "that can optimize diverse sets of sparse architectures. To this end, we conduct\n",
            "experiments that disentangle the effect of mask learning and parameter\n",
            "optimization and how both benefit from overparameterization. The ability of LRR\n",
            "to flip parameter signs early and stay robust to sign perturbations seems to\n",
            "make it not only more effective in mask identification but also in optimizing\n",
            "diverse sets of masks, including random ones. In support of this hypothesis, we\n",
            "prove in a simplified single hidden neuron setting that LRR succeeds in more\n",
            "cases than IMP, as it can escape initially problematic sign configurations.\n",
            "\n",
            "1361. Title: Masks, Signs, And Learning Rate Rewinding\n",
            "   Abstract: This paper studies the challenging problem of estimating causal effects from\n",
            "observational data, in the presence of unobserved confounders. The two-stage\n",
            "least square (TSLS) method and its variants with a standard instrumental\n",
            "variable (IV) are commonly used to eliminate confounding bias, including the\n",
            "bias caused by unobserved confounders, but they rely on the linearity\n",
            "assumption. Besides, the strict condition of unconfounded instruments posed on\n",
            "a standard IV is too strong to be practical. To address these challenging and\n",
            "practical problems of the standard IV method (linearity assumption and the\n",
            "strict condition), in this paper, we use a conditional IV (CIV) to relax the\n",
            "unconfounded instrument condition of standard IV and propose a non-linear CIV\n",
            "regression with Confounding Balancing Representation Learning, CBRL.CIV, for\n",
            "jointly eliminating the confounding bias from unobserved confounders and\n",
            "balancing the observed confounders, without the linearity assumption. We\n",
            "theoretically demonstrate the soundness of CBRL.CIV. Extensive experiments on\n",
            "synthetic and two real-world datasets show the competitive performance of\n",
            "CBRL.CIV against state-of-the-art IV-based estimators and superiority in\n",
            "dealing with the non-linear situation.\n",
            "\n",
            "1362. Title: Guess & Sketch: Language Model Guided Transpilation\n",
            "   Abstract: Maintaining legacy software requires many software and systems engineering\n",
            "hours. Assembly code programs, which demand low-level control over the computer\n",
            "machine state and have no variable names, are particularly difficult for humans\n",
            "to analyze. Existing conventional program translators guarantee correctness,\n",
            "but are hand-engineered for the source and target programming languages in\n",
            "question. Learned transpilation, i.e. automatic translation of code, offers an\n",
            "alternative to manual re-writing and engineering efforts. Automated symbolic\n",
            "program translation approaches guarantee correctness but struggle to scale to\n",
            "longer programs due to the exponentially large search space. Their rigid\n",
            "rule-based systems also limit their expressivity, so they can only reason about\n",
            "a reduced space of programs. Probabilistic neural language models (LMs) produce\n",
            "plausible outputs for every input, but do so at the cost of guaranteed\n",
            "correctness. In this work, we leverage the strengths of LMs and symbolic\n",
            "solvers in a neurosymbolic approach to learned transpilation for assembly code.\n",
            "Assembly code is an appropriate setting for a neurosymbolic approach, since\n",
            "assembly code can be divided into shorter non-branching basic blocks amenable\n",
            "to the use of symbolic methods. Guess & Sketch extracts alignment and\n",
            "confidence information from features of the LM then passes it to a symbolic\n",
            "solver to resolve semantic equivalence of the transpilation input and output.\n",
            "We test Guess & Sketch on three different test sets of assembly transpilation\n",
            "tasks, varying in difficulty, and show that it successfully transpiles 57.6%\n",
            "more examples than GPT-4 and 39.6% more examples than an engineered transpiler.\n",
            "We also share a training and evaluation dataset for this task.\n",
            "\n",
            "1363. Title: Robustifying and Boosting Training-Free Neural Architecture Search\n",
            "   Abstract: Neural architecture search (NAS) has become a key component of AutoML and a\n",
            "standard tool to automate the design of deep neural networks. Recently,\n",
            "training-free NAS as an emerging paradigm has successfully reduced the search\n",
            "costs of standard training-based NAS by estimating the true architecture\n",
            "performance with only training-free metrics. Nevertheless, the estimation\n",
            "ability of these metrics typically varies across different tasks, making it\n",
            "challenging to achieve robust and consistently good search performance on\n",
            "diverse tasks with only a single training-free metric. Meanwhile, the\n",
            "estimation gap between training-free metrics and the true architecture\n",
            "performances limits training-free NAS to achieve superior performance. To\n",
            "address these challenges, we propose the robustifying and boosting\n",
            "training-free NAS (RoBoT) algorithm which (a) employs the optimized combination\n",
            "of existing training-free metrics explored from Bayesian optimization to\n",
            "develop a robust and consistently better-performing metric on diverse tasks,\n",
            "and (b) applies greedy search, i.e., the exploitation, on the newly developed\n",
            "metric to bridge the aforementioned gap and consequently to boost the search\n",
            "performance of standard training-free NAS further. Remarkably, the expected\n",
            "performance of our RoBoT can be theoretically guaranteed, which improves over\n",
            "the existing training-free NAS under mild conditions with additional\n",
            "interesting insights. Our extensive experiments on various NAS benchmark tasks\n",
            "yield substantial empirical evidence to support our theoretical results.\n",
            "\n",
            "1364. Title: Relay Diffusion: Unifying diffusion process across resolutions for image synthesis\n",
            "   Abstract: Diffusion models achieved great success in image synthesis, but still face\n",
            "challenges in high-resolution generation. Through the lens of discrete cosine\n",
            "transformation, we find the main reason is that \\emph{the same noise level on a\n",
            "higher resolution results in a higher Signal-to-Noise Ratio in the frequency\n",
            "domain}. In this work, we present Relay Diffusion Model (RDM), which transfers\n",
            "a low-resolution image or noise into an equivalent high-resolution one for\n",
            "diffusion model via blurring diffusion and block noise. Therefore, the\n",
            "diffusion process can continue seamlessly in any new resolution or model\n",
            "without restarting from pure noise or low-resolution conditioning. RDM achieves\n",
            "state-of-the-art FID on CelebA-HQ and sFID on ImageNet 256$\\times$256,\n",
            "surpassing previous works such as ADM, LDM and DiT by a large margin. All the\n",
            "codes and checkpoints are open-sourced at\n",
            "\\url{https://github.com/THUDM/RelayDiffusion}.\n",
            "\n",
            "1365. Title: Large Language Models as Tool Makers\n",
            "   Abstract: Recent research has highlighted the potential of large language models (LLMs)\n",
            "to improve their problem-solving capabilities with the aid of suitable external\n",
            "tools. In our work, we further advance this concept by introducing a\n",
            "closed-loop framework, referred to as LLMs A s Tool Makers (LATM), where LLMs\n",
            "create their own reusable tools for problem-solving. Our approach consists of\n",
            "two phases: 1) tool making: an LLM acts as the tool maker that crafts tools for\n",
            "a set of tasks. 2) tool using: another LLM acts as the tool user, which applies\n",
            "the tool built by the tool maker for problem-solving. On the problem-solving\n",
            "server side, tool-making enables continual tool generation and caching as new\n",
            "requests emerge. This framework enables subsequent requests to access cached\n",
            "tools via their corresponding APIs, enhancing the efficiency of task\n",
            "resolution. Recognizing that tool-making requires more sophisticated\n",
            "capabilities, we assign this task to a powerful, albeit resource-intensive,\n",
            "model. Conversely, the simpler tool-using phase is delegated to a lightweight\n",
            "model. This strategic division of labor allows the once-off cost of tool-making\n",
            "to be spread over multiple instances of tool-using, significantly reducing\n",
            "average costs while maintaining strong performance. Furthermore, our method\n",
            "offers a functional cache through the caching and reuse of tools, which stores\n",
            "the functionality of a class of requests instead of the natural language\n",
            "responses from LLMs, thus extending the applicability of the conventional cache\n",
            "mechanism. We evaluate our approach across various complex reasoning tasks,\n",
            "including Big-Bench tasks. With GPT-4 as the tool maker and GPT-3.5 as the tool\n",
            "user, LATM demonstrates performance equivalent to using GPT-4 for both roles,\n",
            "but with a significantly reduced inference cost.\n",
            "\n",
            "1366. Title: Counting Graph Substructures with Graph Neural Networks\n",
            "   Abstract: In this paper, we investigate a problem of actively learning threshold in\n",
            "latent space, where the unknown reward $g(\\gamma, v)$ depends on the proposed\n",
            "threshold $\\gamma$ and latent value $v$ and it can be $only$ achieved if the\n",
            "threshold is lower than or equal to the unknown latent value. This problem has\n",
            "broad applications in practical scenarios, e.g., reserve price optimization in\n",
            "online auctions, online task assignments in crowdsourcing, setting recruiting\n",
            "bars in hiring, etc. We first characterize the query complexity of learning a\n",
            "threshold with the expected reward at most $\\epsilon$ smaller than the optimum\n",
            "and prove that the number of queries needed can be infinitely large even when\n",
            "$g(\\gamma, v)$ is monotone with respect to both $\\gamma$ and $v$. On the\n",
            "positive side, we provide a tight query complexity\n",
            "$\\tilde{\\Theta}(1/\\epsilon^3)$ when $g$ is monotone and the CDF of value\n",
            "distribution is Lipschitz. Moreover, we show a tight\n",
            "$\\tilde{\\Theta}(1/\\epsilon^3)$ query complexity can be achieved as long as $g$\n",
            "satisfies one-sided Lipschitzness, which provides a complete characterization\n",
            "for this problem. Finally, we extend this model to an online learning setting\n",
            "and demonstrate a tight $\\Theta(T^{2/3})$ regret bound using continuous-arm\n",
            "bandit techniques and the aforementioned query complexity results.\n",
            "\n",
            "1367. Title: Learning Thresholds with Latent Values and Censored Feedback\n",
            "   Abstract: Sequential models like recurrent neural networks and transformers have become\n",
            "standard for probabilistic multivariate time series forecasting across various\n",
            "domains. Despite their strengths, they struggle with capturing high-dimensional\n",
            "distributions and cross-feature dependencies. Recent work explores generative\n",
            "approaches using diffusion or flow-based models, extending to time series\n",
            "imputation and forecasting. However, scalability remains a challenge. This work\n",
            "proposes a novel method combining recurrent neural networks' efficiency with\n",
            "diffusion models' probabilistic modeling, based on stochastic interpolants and\n",
            "conditional generation with control features, offering insights for future\n",
            "developments in this dynamic field.\n",
            "\n",
            "1368. Title: Transformer-Modulated Diffusion Models for Probabilistic Multivariate Time Series Forecasting\n",
            "   Abstract: We present an extension to the model-free anomaly detection algorithm,\n",
            "Isolation Forest. This extension, named Extended Isolation Forest (EIF),\n",
            "resolves issues with assignment of anomaly score to given data points. We\n",
            "motivate the problem using heat maps for anomaly scores. These maps suffer from\n",
            "artifacts generated by the criteria for branching operation of the binary tree.\n",
            "We explain this problem in detail and demonstrate the mechanism by which it\n",
            "occurs visually. We then propose two different approaches for improving the\n",
            "situation. First we propose transforming the data randomly before creation of\n",
            "each tree, which results in averaging out the bias. Second, which is the\n",
            "preferred way, is to allow the slicing of the data to use hyperplanes with\n",
            "random slopes. This approach results in remedying the artifact seen in the\n",
            "anomaly score heat maps. We show that the robustness of the algorithm is much\n",
            "improved using this method by looking at the variance of scores of data points\n",
            "distributed along constant level sets. We report AUROC and AUPRC for our\n",
            "synthetic datasets, along with real-world benchmark datasets. We find no\n",
            "appreciable difference in the rate of convergence nor in computation time\n",
            "between the standard Isolation Forest and EIF.\n",
            "\n",
            "1369. Title: Diverse Projection Ensembles for Distributional Reinforcement Learning\n",
            "   Abstract: The ability to detect and count certain substructures in graphs is important\n",
            "for solving many tasks on graph-structured data, especially in the contexts of\n",
            "computational chemistry and biology as well as social network analysis.\n",
            "Inspired by this, we propose to study the expressive power of graph neural\n",
            "networks (GNNs) via their ability to count attributed graph substructures,\n",
            "extending recent works that examine their power in graph isomorphism testing\n",
            "and function approximation. We distinguish between two types of substructure\n",
            "counting: induced-subgraph-count and subgraph-count, and establish both\n",
            "positive and negative answers for popular GNN architectures. Specifically, we\n",
            "prove that Message Passing Neural Networks (MPNNs), 2-Weisfeiler-Lehman (2-WL)\n",
            "and 2-Invariant Graph Networks (2-IGNs) cannot perform induced-subgraph-count\n",
            "of substructures consisting of 3 or more nodes, while they can perform\n",
            "subgraph-count of star-shaped substructures. As an intermediary step, we prove\n",
            "that 2-WL and 2-IGNs are equivalent in distinguishing non-isomorphic graphs,\n",
            "partly answering an open problem raised in Maron et al. (2019). We also prove\n",
            "positive results for k-WL and k-IGNs as well as negative results for k-WL with\n",
            "a finite number of iterations. We then conduct experiments that support the\n",
            "theoretical results for MPNNs and 2-IGNs. Moreover, motivated by substructure\n",
            "counting and inspired by Murphy et al. (2019), we propose the Local Relational\n",
            "Pooling model and demonstrate that it is not only effective for substructure\n",
            "counting but also able to achieve competitive performance on molecular\n",
            "prediction tasks.\n",
            "\n",
            "1370. Title: EBMDock: Neural Probabilistic Protein-Protein Docking via a Differentiable Energy Model\n",
            "   Abstract: In contrast to classical reinforcement learning, distributional reinforcement\n",
            "learning algorithms aim to learn the distribution of returns rather than their\n",
            "expected value. Since the nature of the return distribution is generally\n",
            "unknown a priori or arbitrarily complex, a common approach finds approximations\n",
            "within a set of representable, parametric distributions. Typically, this\n",
            "involves a projection of the unconstrained distribution onto the set of\n",
            "simplified distributions. We argue that this projection step entails a strong\n",
            "inductive bias when coupled with neural networks and gradient descent, thereby\n",
            "profoundly impacting the generalization behavior of learned models. In order to\n",
            "facilitate reliable uncertainty estimation through diversity, this work studies\n",
            "the combination of several different projections and representations in a\n",
            "distributional ensemble. We establish theoretical properties of such projection\n",
            "ensembles and derive an algorithm that uses ensemble disagreement, measured by\n",
            "the average $1$-Wasserstein distance, as a bonus for deep exploration. We\n",
            "evaluate our algorithm on the behavior suite benchmark and find that diverse\n",
            "projection ensembles lead to significant performance improvements over existing\n",
            "methods on a wide variety of tasks with the most pronounced gains in directed\n",
            "exploration problems.\n",
            "\n",
            "1371. Title: Closing the Gap between TD Learning and Supervised Learning - A Generalisation Point of View.\n",
            "   Abstract: Learning agents with reinforcement learning is difficult when dealing with\n",
            "long trajectories that involve a large number of states. To address these\n",
            "learning problems effectively, the number of states can be reduced by abstract\n",
            "representations that cluster states. In principle, deep reinforcement learning\n",
            "can find abstract states, but end-to-end learning is unstable. We propose\n",
            "contrastive abstraction learning to find abstract states, where we assume that\n",
            "successive states in a trajectory belong to the same abstract state. Such\n",
            "abstract states may be basic locations, achieved subgoals, inventory, or health\n",
            "conditions. Contrastive abstraction learning first constructs clusters of state\n",
            "representations by contrastive learning and then applies modern Hopfield\n",
            "networks to determine the abstract states. The first phase of contrastive\n",
            "abstraction learning is self-supervised learning, where contrastive learning\n",
            "forces states with sequential proximity to have similar representations. The\n",
            "second phase uses modern Hopfield networks to map similar state representations\n",
            "to the same fixed point, i.e.\\ to an abstract state. The level of abstraction\n",
            "can be adjusted by determining the number of fixed points of the modern\n",
            "Hopfield network. Furthermore, \\textit{contrastive abstraction learning} does\n",
            "not require rewards and facilitates efficient reinforcement learning for a wide\n",
            "range of downstream tasks. Our experiments demonstrate the effectiveness of\n",
            "contrastive abstraction learning for reinforcement learning.\n",
            "\n",
            "1372. Title: Fast Equilibrium of SGD in Generic Situations\n",
            "   Abstract: In certain media, light has been observed with group velocities faster than\n",
            "the speed of light. The recent OPERA report of superluminal 17 GeV neutrinos\n",
            "may describe a similar phenomenon.\n",
            "\n",
            "1373. Title: A Newborn Embodied Turing Test for Comparing Object Segmentation Across Animals and Machines\n",
            "   Abstract: Recent progress in artificial intelligence has renewed interest in building\n",
            "machines that learn like animals. Almost all of the work comparing learning\n",
            "across biological and artificial systems comes from studies where animals and\n",
            "machines received different training data, obscuring whether differences\n",
            "between animals and machines emerged from differences in learning mechanisms\n",
            "versus training data. We present an experimental approach-a \"newborn embodied\n",
            "Turing Test\"-that allows newborn animals and machines to be raised in the same\n",
            "environments and tested with the same tasks, permitting direct comparison of\n",
            "their learning abilities. To make this platform, we first collected\n",
            "controlled-rearing data from newborn chicks, then performed \"digital twin\"\n",
            "experiments in which machines were raised in virtual environments that mimicked\n",
            "the rearing conditions of the chicks. We found that (1) machines (deep\n",
            "reinforcement learning agents with intrinsic motivation) can spontaneously\n",
            "develop visually guided preference behavior, akin to imprinting in newborn\n",
            "chicks, and (2) machines are still far from newborn-level performance on object\n",
            "recognition tasks. Almost all of the chicks developed view-invariant object\n",
            "recognition, whereas the machines tended to develop view-dependent recognition.\n",
            "The learning outcomes were also far more constrained in the chicks versus\n",
            "machines. Ultimately, we anticipate that this approach will help researchers\n",
            "develop embodied AI systems that learn like newborn animals.\n",
            "\n",
            "1374. Title: Learning with Language-Guided State Abstractions\n",
            "   Abstract: It is desirable for an agent to be able to solve a rich variety of problems\n",
            "that can be specified through language in the same environment. A popular\n",
            "approach towards obtaining such agents is to reuse skills learned in prior\n",
            "tasks to generalise compositionally to new ones. However, this is a challenging\n",
            "problem due to the curse of dimensionality induced by the combinatorially large\n",
            "number of ways high-level goals can be combined both logically and temporally\n",
            "in language. To address this problem, we propose a framework where an agent\n",
            "first learns a sufficient set of skill primitives to achieve all high-level\n",
            "goals in its environment. The agent can then flexibly compose them both\n",
            "logically and temporally to provably achieve temporal logic specifications in\n",
            "any regular language, such as regular fragments of linear temporal logic. This\n",
            "provides the agent with the ability to map from complex temporal logic task\n",
            "specifications to near-optimal behaviours zero-shot. We demonstrate this\n",
            "experimentally in a tabular setting, as well as in a high-dimensional video\n",
            "game and continuous control environment. Finally, we also demonstrate that the\n",
            "performance of skill machines can be improved with regular off-policy\n",
            "reinforcement learning algorithms when optimal behaviours are desired.\n",
            "\n",
            "1375. Title: Skill Machines: Temporal Logic Skill Composition in Reinforcement Learning\n",
            "   Abstract: The goal of imitation learning is to mimic expert behavior from\n",
            "demonstrations, without access to an explicit reward signal. A popular class of\n",
            "approach infers the (unknown) reward function via inverse reinforcement\n",
            "learning (IRL) followed by maximizing this reward function via reinforcement\n",
            "learning (RL). The policies learned via these approaches are however very\n",
            "brittle in practice and deteriorate quickly even with small test-time\n",
            "perturbations due to compounding errors. We propose Imitation with Planning at\n",
            "Test-time (IMPLANT), a new meta-algorithm for imitation learning that utilizes\n",
            "decision-time planning to correct for compounding errors of any base imitation\n",
            "policy. In contrast to existing approaches, we retain both the imitation policy\n",
            "and the rewards model at decision-time, thereby benefiting from the learning\n",
            "signal of the two components. Empirically, we demonstrate that IMPLANT\n",
            "significantly outperforms benchmark imitation learning approaches on standard\n",
            "control environments and excels at zero-shot generalization when subject to\n",
            "challenging perturbations in test-time dynamics.\n",
            "\n",
            "1376. Title: Learning Stackable and Skippable LEGO Bricks for Efficient, Reconfigurable, and Variable-Resolution Diffusion Modeling\n",
            "   Abstract: Some highlights are given of the IAU Symposium 334, Rediscovering our Galaxy,\n",
            "held in Potsdam, in July 2017: from the first stars fossil records found in the\n",
            "halo, the carbon-enhanced metal poor CEMP-no, to the cosmological simulations\n",
            "presenting possible scenarios for the Milky Way formation, passing through the\n",
            "chemo-dynamical models of the various components, thin and thick disks,\n",
            "box/peanut bulge, halo, etc. The domain is experiencing (or will be in the near\n",
            "future) huge improvements with precise and accurate stellar ages, provided by\n",
            "astero-seismology, precise stellar distances and kinematics (parallaxes and\n",
            "proper motions from GAIA), and the big data resulting from large surveys are\n",
            "treated with deep learning algorithms.\n",
            "\n",
            "1377. Title: Fast Imitation via Behavior Foundation Models\n",
            "   Abstract: Grounding the common-sense reasoning of Large Language Models (LLMs) in\n",
            "physical domains remains a pivotal yet unsolved problem for embodied AI.\n",
            "Whereas prior works have focused on leveraging LLMs directly for planning in\n",
            "symbolic spaces, this work uses LLMs to guide the search of task structures and\n",
            "constraints implicit in multi-step demonstrations. Specifically, we borrow from\n",
            "manipulation planning literature the concept of mode families, which group\n",
            "robot configurations by specific motion constraints, to serve as an abstraction\n",
            "layer between the high-level language representations of an LLM and the\n",
            "low-level physical trajectories of a robot. By replaying a few human\n",
            "demonstrations with synthetic perturbations, we generate coverage over the\n",
            "demonstrations' state space with additional successful executions as well as\n",
            "counterfactuals that fail the task. Our explanation-based learning framework\n",
            "trains an end-to-end differentiable neural network to predict successful\n",
            "trajectories from failures and as a by-product learns classifiers that ground\n",
            "low-level states and images in mode families without dense labeling. The\n",
            "learned grounding classifiers can further be used to translate language plans\n",
            "into reactive policies in the physical domain in an interpretable manner. We\n",
            "show our approach improves the interpretability and reactivity of imitation\n",
            "learning through 2D navigation and simulated and real robot manipulation tasks.\n",
            "Website: https://yanweiw.github.io/glide\n",
            "\n",
            "1378. Title: Teach LLMs to Phish: Stealing Private Information from Language Models\n",
            "   Abstract: When large language models are trained on private data, it can be a\n",
            "significant privacy risk for them to memorize and regurgitate sensitive\n",
            "information. In this work, we propose a new practical data extraction attack\n",
            "that we call \"neural phishing\". This attack enables an adversary to target and\n",
            "extract sensitive or personally identifiable information (PII), e.g., credit\n",
            "card numbers, from a model trained on user data with upwards of 10% attack\n",
            "success rates, at times, as high as 50%. Our attack assumes only that an\n",
            "adversary can insert as few as 10s of benign-appearing sentences into the\n",
            "training dataset using only vague priors on the structure of the user data.\n",
            "\n",
            "1379. Title: Grounding Language Plans in Demonstrations Through Counterfactual Perturbations\n",
            "   Abstract: Dataset distillation aims to condense large datasets into a small number of\n",
            "synthetic examples that can be used as drop-in replacements when training new\n",
            "models. It has applications to interpretability, neural architecture search,\n",
            "privacy, and continual learning. Despite strong successes in supervised\n",
            "domains, such methods have not yet been extended to reinforcement learning,\n",
            "where the lack of a fixed dataset renders most distillation methods unusable.\n",
            "Filling the gap, we formalize behaviour distillation, a setting that aims to\n",
            "discover and then condense the information required for training an expert\n",
            "policy into a synthetic dataset of state-action pairs, without access to expert\n",
            "data. We then introduce Hallucinating Datasets with Evolution Strategies\n",
            "(HaDES), a method for behaviour distillation that can discover datasets of just\n",
            "four state-action pairs which, under supervised learning, train agents to\n",
            "competitive performance levels in continuous control tasks. We show that these\n",
            "datasets generalize out of distribution to training policies with a wide range\n",
            "of architectures and hyperparameters. We also demonstrate application to a\n",
            "downstream task, namely training multi-task agents in a zero-shot fashion.\n",
            "Beyond behaviour distillation, HaDES provides significant improvements in\n",
            "neuroevolution for RL over previous approaches and achieves SoTA results on one\n",
            "standard supervised dataset distillation task. Finally, we show that\n",
            "visualizing the synthetic datasets can provide human-interpretable task\n",
            "insights.\n",
            "\n",
            "1380. Title: Demystifying Embedding Spaces using Large Language Models\n",
            "   Abstract: Embeddings have become a pivotal means to represent complex, multi-faceted\n",
            "information about entities, concepts, and relationships in a condensed and\n",
            "useful format. Nevertheless, they often preclude direct interpretation. While\n",
            "downstream tasks make use of these compressed representations, meaningful\n",
            "interpretation usually requires visualization using dimensionality reduction or\n",
            "specialized machine learning interpretability methods. This paper addresses the\n",
            "challenge of making such embeddings more interpretable and broadly useful, by\n",
            "employing Large Language Models (LLMs) to directly interact with embeddings --\n",
            "transforming abstract vectors into understandable narratives. By injecting\n",
            "embeddings into LLMs, we enable querying and exploration of complex embedding\n",
            "data. We demonstrate our approach on a variety of diverse tasks, including:\n",
            "enhancing concept activation vectors (CAVs), communicating novel embedded\n",
            "entities, and decoding user preferences in recommender systems. Our work\n",
            "couples the immense information potential of embeddings with the interpretative\n",
            "power of LLMs.\n",
            "\n",
            "1381. Title: On the Expressivity of Objective-Specification Formalisms in Reinforcement Learning\n",
            "   Abstract: In Self-Supervised Learning (SSL), models are typically pretrained,\n",
            "fine-tuned, and evaluated on the same domains. However, they tend to perform\n",
            "poorly when evaluated on unseen domains, a challenge that Unsupervised Domain\n",
            "Generalization (UDG) seeks to address. Current UDG methods rely on domain\n",
            "labels, which are often challenging to collect, and domain-specific\n",
            "architectures that lack scalability when confronted with numerous domains,\n",
            "making the current methodology impractical and rigid. Inspired by\n",
            "contrastive-based UDG methods that mitigate spurious correlations by\n",
            "restricting comparisons to examples from the same domain, we hypothesize that\n",
            "eliminating style variability within a batch could provide a more convenient\n",
            "and flexible way to reduce spurious correlations without requiring domain\n",
            "labels. To verify this hypothesis, we introduce Batch Styles Standardization\n",
            "(BSS), a relatively simple yet powerful Fourier-based method to standardize the\n",
            "style of images in a batch specifically designed for integration with SSL\n",
            "methods to tackle UDG. Combining BSS with existing SSL methods offers serious\n",
            "advantages over prior UDG methods: (1) It eliminates the need for domain labels\n",
            "or domain-specific network components to enhance domain-invariance in SSL\n",
            "representations, and (2) offers flexibility as BSS can be seamlessly integrated\n",
            "with diverse contrastive-based but also non-contrastive-based SSL methods.\n",
            "Experiments on several UDG datasets demonstrate that it significantly improves\n",
            "downstream task performances on unseen domains, often outperforming or rivaling\n",
            "with UDG methods. Finally, this work clarifies the underlying mechanisms\n",
            "contributing to BSS's effectiveness in improving domain-invariance in SSL\n",
            "representations and performances on unseen domain.\n",
            "\n",
            "1382. Title: Behaviour Distillation\n",
            "   Abstract: Developing modern machine learning (ML) applications is data-centric, of\n",
            "which one fundamental challenge is to understand the influence of data quality\n",
            "to ML training -- \"Which training examples are 'guilty' in making the trained\n",
            "ML model predictions inaccurate or unfair?\" Modeling data influence for ML\n",
            "training has attracted intensive interest over the last decade, and one popular\n",
            "framework is to compute the Shapley value of each training example with respect\n",
            "to utilities such as validation accuracy and fairness of the trained ML model.\n",
            "Unfortunately, despite recent intensive interest and research, existing methods\n",
            "only consider a single ML model \"in isolation\" and do not consider an\n",
            "end-to-end ML pipeline that consists of data transformations, feature\n",
            "extractors, and ML training.\n",
            "  We present DataScope (ease.ml/datascope), the first system that efficiently\n",
            "computes Shapley values of training examples over an end-to-end ML pipeline,\n",
            "and illustrate its applications in data debugging for ML training. To this end,\n",
            "we first develop a novel algorithmic framework that computes Shapley value over\n",
            "a specific family of ML pipelines that we call canonical pipelines: a positive\n",
            "relational algebra query followed by a K-nearest-neighbor (KNN) classifier. We\n",
            "show that, for many subfamilies of canonical pipelines, computing Shapley value\n",
            "is in PTIME, contrasting the exponential complexity of computing Shapley value\n",
            "in general. We then put this to practice -- given an sklearn pipeline, we\n",
            "approximate it with a canonical pipeline to use as a proxy. We conduct\n",
            "extensive experiments illustrating different use cases and utilities. Our\n",
            "results show that DataScope is up to four orders of magnitude faster over\n",
            "state-of-the-art Monte Carlo-based methods, while being comparably, and often\n",
            "even more, effective in data debugging.\n",
            "\n",
            "1383. Title: From Graphs to Hypergraphs: Hypergraph Projection and its Reconstruction\n",
            "   Abstract: Diffusion models excel at generating photo-realistic images but come with\n",
            "significant computational costs in both training and sampling. While various\n",
            "techniques address these computational challenges, a less-explored issue is\n",
            "designing an efficient and adaptable network backbone for iterative refinement.\n",
            "Current options like U-Net and Vision Transformer often rely on\n",
            "resource-intensive deep networks and lack the flexibility needed for generating\n",
            "images at variable resolutions or with a smaller network than used in training.\n",
            "This study introduces LEGO bricks, which seamlessly integrate Local-feature\n",
            "Enrichment and Global-content Orchestration. These bricks can be stacked to\n",
            "create a test-time reconfigurable diffusion backbone, allowing selective\n",
            "skipping of bricks to reduce sampling costs and generate higher-resolution\n",
            "images than the training data. LEGO bricks enrich local regions with an MLP and\n",
            "transform them using a Transformer block while maintaining a consistent\n",
            "full-resolution image across all bricks. Experimental results demonstrate that\n",
            "LEGO bricks enhance training efficiency, expedite convergence, and facilitate\n",
            "variable-resolution image generation while maintaining strong generative\n",
            "performance. Moreover, LEGO significantly reduces sampling time compared to\n",
            "other methods, establishing it as a valuable enhancement for diffusion models.\n",
            "Our code and project page are available at\n",
            "https://jegzheng.github.io/LEGODiffusion.\n",
            "\n",
            "1384. Title: Data Debugging with Shapley Importance over Machine Learning Pipelines\n",
            "   Abstract: Most algorithms in reinforcement learning (RL) require that the objective is\n",
            "formalised with a Markovian reward function. However, it is well-known that\n",
            "certain tasks cannot be expressed by means of an objective in the Markov\n",
            "rewards formalism, motivating the study of alternative objective-specification\n",
            "formalisms in RL such as Linear Temporal Logic and Multi-Objective\n",
            "Reinforcement Learning. To date, there has not yet been any thorough analysis\n",
            "of how these formalisms relate to each other in terms of their expressivity. We\n",
            "fill this gap in the existing literature by providing a comprehensive\n",
            "comparison of 17 salient objective-specification formalisms. We place these\n",
            "formalisms in a preorder based on their expressive power, and present this\n",
            "preorder as a Hasse diagram. We find a variety of limitations for the different\n",
            "formalisms, and argue that no formalism is both dominantly expressive and\n",
            "straightforward to optimise with current techniques. For example, we prove that\n",
            "each of Regularised RL, (Outer) Nonlinear Markov Rewards, Reward Machines,\n",
            "Linear Temporal Logic, and Limit Average Rewards can express a task that the\n",
            "others cannot. The significance of our results is twofold. First, we identify\n",
            "important expressivity limitations to consider when specifying objectives for\n",
            "policy optimization. Second, our results highlight the need for future research\n",
            "which adapts reward learning to work with a greater variety of formalisms,\n",
            "since many existing reward learning methods assume that the desired objective\n",
            "takes a Markovian form. Our work contributes towards a more cohesive\n",
            "understanding of the costs and benefits of different RL objective-specification\n",
            "formalisms.\n",
            "\n",
            "1385. Title: Robust Training of Federated Models with Extremely Label Deficiency\n",
            "   Abstract: The problem of graph reconstruction has been studied in its various forms\n",
            "over the years. In particular, the Reconstruction Conjecture, proposed by Ulam\n",
            "and Kelly in 1942, has attracted much research attention and yet remains one of\n",
            "the foremost unsolved problems in graph theory. Recently, Bastide, Cook,\n",
            "Erickson, Groenland, Kreveld, Mannens, and Vermeulen proposed a new model of\n",
            "partial information, where we are given the set of connected triples T_3, which\n",
            "is the set of 3-subsets of the vertex set that induce connected subgraphs. They\n",
            "proved that reconstruction is unique within the class of triangle-free graphs,\n",
            "2-connected outerplanar graphs, and maximal planar graphs. They also showed\n",
            "that almost every graph can be uniquely reconstructed from their connected\n",
            "triples. However, little is known about other classes of non-triangle-free\n",
            "graphs within which reconstruction can occur uniquely, nor do we understand\n",
            "what kind of graphs can be uniquely reconstructed from their connected triples\n",
            "without assuming anything about the classes of graphs to which they belong.\n",
            "  The main result of this paper is a complete characterization of all graphs\n",
            "that can be uniquely reconstructed from their connected triples T_3. We also\n",
            "show that reconstruction from T_3 is unique within the class of regular planar\n",
            "graphs, 5-connected planar graphs, certain strongly regular graphs, and\n",
            "complete multi-partite graphs, whereas it is not unique for the class of\n",
            "k-connected planar graphs with k less or equal to 4, Eulerian graphs, or\n",
            "Hamiltonian graphs.\n",
            "\n",
            "1386. Title: Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation\n",
            "   Abstract: Causal representation learning has showed a variety of settings in which we\n",
            "can disentangle latent variables with identifiability guarantees (up to some\n",
            "reasonable equivalence class). Common to all of these approaches is the\n",
            "assumption that (1) the latent variables are represented as $d$-dimensional\n",
            "vectors, and (2) that the observations are the output of some injective\n",
            "generative function of these latent variables. While these assumptions appear\n",
            "benign, we show that when the observations are of multiple objects, the\n",
            "generative function is no longer injective and disentanglement fails in\n",
            "practice. We can address this failure by combining recent developments in\n",
            "object-centric learning and causal representation learning. By modifying the\n",
            "Slot Attention architecture arXiv:2006.15055, we develop an object-centric\n",
            "architecture that leverages weak supervision from sparse perturbations to\n",
            "disentangle each object's properties. This approach is more data-efficient in\n",
            "the sense that it requires significantly fewer perturbations than a comparable\n",
            "approach that encodes to a Euclidean space and we show that this approach\n",
            "successfully disentangles the properties of a set of objects in a series of\n",
            "simple image-based disentanglement experiments.\n",
            "\n",
            "1387. Title: Nearly $d$-Linear Convergence Bounds for Diffusion Models via Stochastic Localization\n",
            "   Abstract: Federated semi-supervised learning (FSSL) has emerged as a powerful paradigm\n",
            "for collaboratively training machine learning models using distributed data\n",
            "with label deficiency. Advanced FSSL methods predominantly focus on training a\n",
            "single model on each client. However, this approach could lead to a discrepancy\n",
            "between the objective functions of labeled and unlabeled data, resulting in\n",
            "gradient conflicts. To alleviate gradient conflict, we propose a novel\n",
            "twin-model paradigm, called Twin-sight, designed to enhance mutual guidance by\n",
            "providing insights from different perspectives of labeled and unlabeled data.\n",
            "In particular, Twin-sight concurrently trains a supervised model with a\n",
            "supervised objective function while training an unsupervised model using an\n",
            "unsupervised objective function. To enhance the synergy between these two\n",
            "models, Twin-sight introduces a neighbourhood-preserving constraint, which\n",
            "encourages the preservation of the neighbourhood relationship among data\n",
            "features extracted by both models. Our comprehensive experiments on four\n",
            "benchmark datasets provide substantial evidence that Twin-sight can\n",
            "significantly outperform state-of-the-art methods across various experimental\n",
            "settings, demonstrating the efficacy of the proposed Twin-sight.\n",
            "\n",
            "1388. Title: UniversalNER: Targeted Distillation from Large Language Models for Open Named Entity Recognition\n",
            "   Abstract: The rapid progress in open-source large language models (LLMs) is\n",
            "significantly advancing AI development. Extensive efforts have been made before\n",
            "model release to align their behavior with human values, with the primary goal\n",
            "of ensuring their helpfulness and harmlessness. However, even carefully aligned\n",
            "models can be manipulated maliciously, leading to unintended behaviors, known\n",
            "as \"jailbreaks\". These jailbreaks are typically triggered by specific text\n",
            "inputs, often referred to as adversarial prompts. In this work, we propose the\n",
            "generation exploitation attack, an extremely simple approach that disrupts\n",
            "model alignment by only manipulating variations of decoding methods. By\n",
            "exploiting different generation strategies, including varying decoding\n",
            "hyper-parameters and sampling methods, we increase the misalignment rate from\n",
            "0% to more than 95% across 11 language models including LLaMA2, Vicuna, Falcon,\n",
            "and MPT families, outperforming state-of-the-art attacks with $30\\times$ lower\n",
            "computational cost. Finally, we propose an effective alignment method that\n",
            "explores diverse generation strategies, which can reasonably reduce the\n",
            "misalignment rate under our attack. Altogether, our study underscores a major\n",
            "failure in current safety evaluation and alignment procedures for open-source\n",
            "LLMs, strongly advocating for more comprehensive red teaming and better\n",
            "alignment before releasing such models. Our code is available at\n",
            "https://github.com/Princeton-SysML/Jailbreak_LLM.\n",
            "\n",
            "1389. Title: Object centric architectures enable efficient causal representation learning\n",
            "   Abstract: Neural Implicit Representation (NIR) has recently gained significant\n",
            "attention due to its remarkable ability to encode complex and high-dimensional\n",
            "data into representation space and easily reconstruct it through a trainable\n",
            "mapping function. However, NIR methods assume a one-to-one mapping between the\n",
            "target data and representation models regardless of data relevancy or\n",
            "similarity. This results in poor generalization over multiple complex data and\n",
            "limits their efficiency and scalability. Motivated by continual learning, this\n",
            "work investigates how to accumulate and transfer neural implicit\n",
            "representations for multiple complex video data over sequential encoding\n",
            "sessions. To overcome the limitation of NIR, we propose a novel method,\n",
            "Progressive Fourier Neural Representation (PFNR), that aims to find an adaptive\n",
            "and compact sub-module in Fourier space to encode videos in each training\n",
            "session. This sparsified neural encoding allows the neural network to hold free\n",
            "weights, enabling an improved adaptation for future videos. In addition, when\n",
            "learning a representation for a new video, PFNR transfers the representation of\n",
            "previous videos with frozen weights. This design allows the model to\n",
            "continuously accumulate high-quality neural representations for multiple videos\n",
            "while ensuring lossless decoding that perfectly preserves the learned\n",
            "representations for previous videos. We validate our PFNR method on the UVG8/17\n",
            "and DAVIS50 video sequence benchmarks and achieve impressive performance gains\n",
            "over strong continual learning baselines. The PFNR code is available at\n",
            "https://github.com/ihaeyong/PFNR.git.\n",
            "\n",
            "1390. Title: S$2$AC: Energy-Based Reinforcement Learning with Stein Soft Actor Critic\n",
            "   Abstract: Dueling bandits is a prominent framework for decision-making involving\n",
            "preferential feedback, a valuable feature that fits various applications\n",
            "involving human interaction, such as ranking, information retrieval, and\n",
            "recommendation systems. While substantial efforts have been made to minimize\n",
            "the cumulative regret in dueling bandits, a notable gap in the current research\n",
            "is the absence of regret bounds that account for the inherent uncertainty in\n",
            "pairwise comparisons between the dueling arms. Intuitively, greater uncertainty\n",
            "suggests a higher level of difficulty in the problem. To bridge this gap, this\n",
            "paper studies the problem of contextual dueling bandits, where the binary\n",
            "comparison of dueling arms is generated from a generalized linear model (GLM).\n",
            "We propose a new SupLinUCB-type algorithm that enjoys computational efficiency\n",
            "and a variance-aware regret bound $\\tilde O\\big(d\\sqrt{\\sum_{t=1}^T\\sigma_t^2}\n",
            "+ d\\big)$, where $\\sigma_t$ is the variance of the pairwise comparison in round\n",
            "$t$, $d$ is the dimension of the context vectors, and $T$ is the time horizon.\n",
            "Our regret bound naturally aligns with the intuitive expectation in scenarios\n",
            "where the comparison is deterministic, the algorithm only suffers from an\n",
            "$\\tilde O(d)$ regret. We perform empirical experiments on synthetic data to\n",
            "confirm the advantage of our method over previous variance-agnostic algorithms.\n",
            "\n",
            "1391. Title: Random Sparse Lifts: Construction, Analysis and Convergence of finite sparse networks\n",
            "   Abstract: Recent work has showcased the significant potential of diffusion models in\n",
            "pose-guided person image synthesis. However, owing to the inconsistency in pose\n",
            "between the source and target images, synthesizing an image with a distinct\n",
            "pose, relying exclusively on the source image and target pose information,\n",
            "remains a formidable challenge. This paper presents Progressive Conditional\n",
            "Diffusion Models (PCDMs) that incrementally bridge the gap between person\n",
            "images under the target and source poses through three stages. Specifically, in\n",
            "the first stage, we design a simple prior conditional diffusion model that\n",
            "predicts the global features of the target image by mining the global alignment\n",
            "relationship between pose coordinates and image appearance. Then, the second\n",
            "stage establishes a dense correspondence between the source and target images\n",
            "using the global features from the previous stage, and an inpainting\n",
            "conditional diffusion model is proposed to further align and enhance the\n",
            "contextual features, generating a coarse-grained person image. In the third\n",
            "stage, we propose a refining conditional diffusion model to utilize the\n",
            "coarsely generated image from the previous stage as a condition, achieving\n",
            "texture restoration and enhancing fine-detail consistency. The three-stage\n",
            "PCDMs work progressively to generate the final high-quality and high-fidelity\n",
            "synthesized image. Both qualitative and quantitative results demonstrate the\n",
            "consistency and photorealism of our proposed PCDMs under challenging\n",
            "scenarios.The code and model will be available at\n",
            "https://github.com/tencent-ailab/PCDMs.\n",
            "\n",
            "1392. Title: Variance-aware Regret Bounds for Stochastic Contextual Dueling Bandits\n",
            "   Abstract: Recent studies have highlighted the potential of Lipschitz-based methods for\n",
            "training certifiably robust neural networks against adversarial attacks. A key\n",
            "challenge, supported both theoretically and empirically, is that robustness\n",
            "demands greater network capacity and more data than standard training. However,\n",
            "effectively adding capacity under stringent Lipschitz constraints has proven\n",
            "more difficult than it may seem, evident by the fact that state-of-the-art\n",
            "approach tend more towards \\emph{underfitting} than overfitting. Moreover, we\n",
            "posit that a lack of careful exploration of the design space for Lipshitz-based\n",
            "approaches has left potential performance gains on the table. In this work, we\n",
            "provide a more comprehensive evaluation to better uncover the potential of\n",
            "Lipschitz-based certification methods. Using a combination of novel techniques,\n",
            "design optimizations, and synthesis of prior work, we are able to significantly\n",
            "improve the state-of-the-art VRA for deterministic certification on a variety\n",
            "of benchmark datasets, and over a range of perturbation sizes. Of particular\n",
            "note, we discover that the addition of large ``Cholesky-orthogonalized residual\n",
            "dense'' layers to the end of existing state-of-the-art Lipschitz-controlled\n",
            "ResNet architectures is especially effective for increasing network capacity\n",
            "and performance. Combined with filtered generative data augmentation, our final\n",
            "results further the state of the art deterministic VRA by up to 8.5 percentage\n",
            "points\\footnote{Code is available at \\url{https://github.com/hukkai/liresnet}}.\n",
            "\n",
            "1393. Title: Progressive Fourier Neural Representation for Sequential Video Compilation\n",
            "   Abstract: The key challenge in the noisy intermediate-scale quantum era is finding\n",
            "useful circuits compatible with current device limitations. Variational quantum\n",
            "algorithms (VQAs) offer a potential solution by fixing the circuit architecture\n",
            "and optimizing individual gate parameters in an external loop. However,\n",
            "parameter optimization can become intractable, and the overall performance of\n",
            "the algorithm depends heavily on the initially chosen circuit architecture.\n",
            "Several quantum architecture search (QAS) algorithms have been developed to\n",
            "design useful circuit architectures automatically. In the case of parameter\n",
            "optimization alone, noise effects have been observed to dramatically influence\n",
            "the performance of the optimizer and final outcomes, which is a key line of\n",
            "study. However, the effects of noise on the architecture search, which could be\n",
            "just as critical, are poorly understood. This work addresses this gap by\n",
            "introducing a curriculum-based reinforcement learning QAS (CRLQAS) algorithm\n",
            "designed to tackle challenges in realistic VQA deployment. The algorithm\n",
            "incorporates (i) a 3D architecture encoding and restrictions on environment\n",
            "dynamics to explore the search space of possible circuits efficiently, (ii) an\n",
            "episode halting scheme to steer the agent to find shorter circuits, and (iii) a\n",
            "novel variant of simultaneous perturbation stochastic approximation as an\n",
            "optimizer for faster convergence. To facilitate studies, we developed an\n",
            "optimized simulator for our algorithm, significantly improving computational\n",
            "efficiency in simulating noisy quantum circuits by employing the Pauli-transfer\n",
            "matrix formalism in the Pauli-Liouville basis. Numerical experiments focusing\n",
            "on quantum chemistry tasks demonstrate that CRLQAS outperforms existing QAS\n",
            "algorithms across several metrics in both noiseless and noisy environments.\n",
            "\n",
            "1394. Title: Advancing Pose-Guided Image Synthesis with Progressive Conditional Diffusion Models\n",
            "   Abstract: We present a framework to define a large class of neural networks for which,\n",
            "by construction, training by gradient flow provably reaches arbitrarily low\n",
            "loss when the number of parameters grows. Distinct from the fixed-space global\n",
            "optimality of non-convex optimization, this new form of convergence, and the\n",
            "techniques introduced to prove such convergence, pave the way for a usable deep\n",
            "learning convergence theory in the near future, without overparameterization\n",
            "assumptions relating the number of parameters and training samples. We define\n",
            "these architectures from a simple computation graph and a mechanism to lift it,\n",
            "thus increasing the number of parameters, generalizing the idea of increasing\n",
            "the widths of multi-layer perceptrons. We show that architectures similar to\n",
            "most common deep learning models are present in this class, obtained by\n",
            "sparsifying the weight tensors of usual architectures at initialization.\n",
            "Leveraging tools of algebraic topology and random graph theory, we use the\n",
            "computation graph's geometry to propagate properties guaranteeing convergence\n",
            "to any precision for these large sparse models.\n",
            "\n",
            "1395. Title: Curriculum reinforcement learning for quantum architecture search under hardware errors\n",
            "   Abstract: Understanding the inner workings of machine learning models like Transformers\n",
            "is vital for their safe and ethical use. This paper provides a comprehensive\n",
            "analysis of a one-layer Transformer model trained to perform n-digit integer\n",
            "addition. Our findings suggest that the model dissects the task into parallel\n",
            "streams dedicated to individual digits, employing varied algorithms tailored to\n",
            "different positions within the digits. Furthermore, we identify a rare scenario\n",
            "characterized by high loss, which we explain. By thoroughly elucidating the\n",
            "model's algorithm, we provide new insights into its functioning. These findings\n",
            "are validated through rigorous testing and mathematical modeling, thereby\n",
            "contributing to the broader fields of model understanding and interpretability.\n",
            "Our approach opens the door for analyzing more complex tasks and multi-layer\n",
            "Transformer models.\n",
            "\n",
            "1396. Title: Masked Structural Growth for 2x Faster Language Model Pre-training\n",
            "   Abstract: Denoising diffusions are a powerful method to generate approximate samples\n",
            "from high-dimensional data distributions. Recent results provide polynomial\n",
            "bounds on their convergence rate, assuming $L^2$-accurate scores. Until now,\n",
            "the tightest bounds were either superlinear in the data dimension or required\n",
            "strong smoothness assumptions. We provide the first convergence bounds which\n",
            "are linear in the data dimension (up to logarithmic factors) assuming only\n",
            "finite second moments of the data distribution. We show that diffusion models\n",
            "require at most $\\tilde O(\\frac{d \\log^2(1/\\delta)}{\\varepsilon^2})$ steps to\n",
            "approximate an arbitrary distribution on $\\mathbb{R}^d$ corrupted with Gaussian\n",
            "noise of variance $\\delta$ to within $\\varepsilon^2$ in KL divergence. Our\n",
            "proof extends the Girsanov-based methods of previous works. We introduce a\n",
            "refined treatment of the error from discretizing the reverse SDE inspired by\n",
            "stochastic localization.\n",
            "\n",
            "1397. Title: Like Oil and Water: Group Robustness Methods and Poisoning Defenses May Be at Odds\n",
            "   Abstract: Accelerating large language model pre-training is a critical issue in present\n",
            "research. In this paper, we focus on speeding up pre-training by progressively\n",
            "growing from a small Transformer structure to a large one. There are two main\n",
            "research problems associated with progressive growth: determining the optimal\n",
            "growth schedule, and designing efficient growth operators. In terms of growth\n",
            "schedule, the impact of each single dimension on a schedule's efficiency is\n",
            "under-explored by existing work. Regarding the growth operators, existing\n",
            "methods rely on the initialization of new weights to inherit knowledge, and\n",
            "achieve only non-strict function preservation, limiting further improvements on\n",
            "training dynamics. To address these issues, we propose Masked Structural Growth\n",
            "(MSG), including (i) growth schedules involving all possible dimensions and\n",
            "(ii) strictly function-preserving growth operators that is independent of the\n",
            "initialization of new weights. Experiments show that MSG is significantly\n",
            "faster than related work: we achieve up to 2.2x speedup in pre-training\n",
            "different types of language models while maintaining comparable or better\n",
            "downstream performances. Code is publicly available at\n",
            "https://github.com/cofe-ai/MSG.\n",
            "\n",
            "1398. Title: Octavius: Mitigating Task Interference in MLLMs via LoRA-MoE\n",
            "   Abstract: The ultimate goal of Dataset Distillation is to synthesize a small synthetic\n",
            "dataset such that a model trained on this synthetic set will perform equally\n",
            "well as a model trained on the full, real dataset. Until now, no method of\n",
            "Dataset Distillation has reached this completely lossless goal, in part due to\n",
            "the fact that previous methods only remain effective when the total number of\n",
            "synthetic samples is extremely small. Since only so much information can be\n",
            "contained in such a small number of samples, it seems that to achieve truly\n",
            "loss dataset distillation, we must develop a distillation method that remains\n",
            "effective as the size of the synthetic dataset grows. In this work, we present\n",
            "such an algorithm and elucidate why existing methods fail to generate larger,\n",
            "high-quality synthetic sets. Current state-of-the-art methods rely on\n",
            "trajectory-matching, or optimizing the synthetic data to induce similar\n",
            "long-term training dynamics as the real data. We empirically find that the\n",
            "training stage of the trajectories we choose to match (i.e., early or late)\n",
            "greatly affects the effectiveness of the distilled dataset. Specifically, early\n",
            "trajectories (where the teacher network learns easy patterns) work well for a\n",
            "low-cardinality synthetic set since there are fewer examples wherein to\n",
            "distribute the necessary information. Conversely, late trajectories (where the\n",
            "teacher network learns hard patterns) provide better signals for larger\n",
            "synthetic sets since there are now enough samples to represent the necessary\n",
            "complex patterns. Based on our findings, we propose to align the difficulty of\n",
            "the generated patterns with the size of the synthetic dataset. In doing so, we\n",
            "successfully scale trajectory matching-based methods to larger synthetic\n",
            "datasets, achieving lossless dataset distillation for the very first time. Code\n",
            "and distilled datasets are available at https://gzyaftermath.github.io/DATM.\n",
            "\n",
            "1399. Title: Image Inpainting via Iteratively Decoupled Probabilistic Modeling\n",
            "   Abstract: Deep image classification models trained on vast amounts of web-scraped data\n",
            "are susceptible to data poisoning - a mechanism for backdooring models. A small\n",
            "number of poisoned samples seen during training can severely undermine a\n",
            "model's integrity during inference. Existing work considers an effective\n",
            "defense as one that either (i) restores a model's integrity through repair or\n",
            "(ii) detects an attack. We argue that this approach overlooks a crucial\n",
            "trade-off: Attackers can increase robustness at the expense of detectability\n",
            "(over-poisoning) or decrease detectability at the cost of robustness\n",
            "(under-poisoning). In practice, attacks should remain both undetectable and\n",
            "robust. Detectable but robust attacks draw human attention and rigorous model\n",
            "evaluation or cause the model to be re-trained or discarded. In contrast,\n",
            "attacks that are undetectable but lack robustness can be repaired with minimal\n",
            "impact on model accuracy. Our research points to intrinsic flaws in current\n",
            "attack evaluation methods and raises the bar for all data poisoning attackers\n",
            "who must delicately balance this trade-off to remain robust and undetectable.\n",
            "To demonstrate the existence of more potent defenders, we propose defenses\n",
            "designed to (i) detect or (ii) repair poisoned models using a limited amount of\n",
            "trusted image-label pairs. Our results show that an attacker who needs to be\n",
            "robust and undetectable is substantially less threatening. Our defenses\n",
            "mitigate all tested attacks with a maximum accuracy decline of 2% using only 1%\n",
            "of clean data on CIFAR-10 and 2.5% on ImageNet. We demonstrate the scalability\n",
            "of our defenses by evaluating large vision-language models, such as CLIP.\n",
            "Attackers who can manipulate the model's parameters pose an elevated risk as\n",
            "they can achieve higher robustness at low detectability compared to data\n",
            "poisoning attackers.\n",
            "\n",
            "1400. Title: Does Progress On Object Recognition Benchmarks Improve Generalization on Crowdsourced, Global Data?\n",
            "   Abstract: Preventing the performance decay of Transformers on inputs longer than those\n",
            "used for training has been an important challenge in extending the context\n",
            "length of these models. Though the Transformer architecture has fundamentally\n",
            "no limits on the input sequence lengths it can process, the choice of position\n",
            "encoding used during training can limit the performance of these models on\n",
            "longer inputs. We propose a novel functional relative position encoding with\n",
            "progressive interpolation, FIRE, to improve Transformer generalization to\n",
            "longer contexts. We theoretically prove that this can represent some of the\n",
            "popular relative position encodings, such as T5's RPE, Alibi, and Kerple. We\n",
            "next empirically show that FIRE models have better generalization to longer\n",
            "contexts on both zero-shot language modeling and long text benchmarks.\n",
            "\n",
            "1401. Title: A Branching Decoder for Set Generation\n",
            "   Abstract: Generative adversarial networks (GANs) have made great success in image\n",
            "inpainting yet still have difficulties tackling large missing regions. In\n",
            "contrast, iterative probabilistic algorithms, such as autoregressive and\n",
            "denoising diffusion models, have to be deployed with massive computing\n",
            "resources for decent effect. To achieve high-quality results with low\n",
            "computational cost, we present a novel pixel spread model (PSM) that\n",
            "iteratively employs decoupled probabilistic modeling, combining the\n",
            "optimization efficiency of GANs with the prediction tractability of\n",
            "probabilistic models. As a result, our model selectively spreads informative\n",
            "pixels throughout the image in a few iterations, largely enhancing the\n",
            "completion quality and efficiency. On multiple benchmarks, we achieve new\n",
            "state-of-the-art performance. Code is released at\n",
            "https://github.com/fenglinglwb/PSM.\n",
            "\n",
            "1402. Title: Gen-Z: Generative Zero-Shot Text Classification with Contextualized Label Descriptions\n",
            "   Abstract: For more than a decade, researchers have measured progress in object\n",
            "recognition on ImageNet-based generalization benchmarks such as ImageNet-A, -C,\n",
            "and -R. Recent advances in foundation models, trained on orders of magnitude\n",
            "more data, have begun to saturate these standard benchmarks, but remain brittle\n",
            "in practice. This suggests standard benchmarks, which tend to focus on\n",
            "predefined or synthetic changes, may not be sufficient for measuring real world\n",
            "generalization. Consequently, we propose studying generalization across\n",
            "geography as a more realistic measure of progress using two datasets of objects\n",
            "from households across the globe. We conduct an extensive empirical evaluation\n",
            "of progress across nearly 100 vision models up to most recent foundation\n",
            "models. We first identify a progress gap between standard benchmarks and\n",
            "real-world, geographical shifts: progress on ImageNet results in up to 2.5x\n",
            "more progress on standard generalization benchmarks than real-world\n",
            "distribution shifts. Second, we study model generalization across geographies\n",
            "by measuring the disparities in performance across regions, a more fine-grained\n",
            "measure of real world generalization. We observe all models have large\n",
            "geographic disparities, even foundation CLIP models, with differences of 7-20%\n",
            "in accuracy between regions. Counter to modern intuition, we discover progress\n",
            "on standard benchmarks fails to improve geographic disparities and often\n",
            "exacerbates them: geographic disparities between the least performant models\n",
            "and today's best models have more than tripled. Our results suggest scaling\n",
            "alone is insufficient for consistent robustness to real-world distribution\n",
            "shifts. Finally, we highlight in early experiments how simple last layer\n",
            "retraining on more representative, curated data can complement scaling as a\n",
            "promising direction of future work, reducing geographic disparity on both\n",
            "benchmarks by over two-thirds.\n",
            "\n",
            "1403. Title: On the Provable Advantage of Unsupervised Pretraining\n",
            "   Abstract: Deep learning models in computer vision have made remarkable progress, but\n",
            "their lack of transparency and interpretability remains a challenge. The\n",
            "development of explainable AI can enhance the understanding and performance of\n",
            "these models. However, existing techniques often struggle to provide convincing\n",
            "explanations that non-experts easily understand, and they cannot accurately\n",
            "identify models' intrinsic decision-making processes. To address these\n",
            "challenges, we propose to develop a counterfactual explanation (CE) model that\n",
            "balances plausibility and faithfulness. This model generates easy-to-understand\n",
            "visual explanations by making minimum changes necessary in images without\n",
            "altering the pixel data. Instead, the proposed method identifies internal\n",
            "concepts and filters learned by models and leverages them to produce plausible\n",
            "counterfactual explanations. The provided explanations reflect the internal\n",
            "decision-making process of the model, thus ensuring faithfulness to the model.\n",
            "\n",
            "1404. Title: Intriguing Properties of Generative Classifiers\n",
            "   Abstract: Unsupervised pretraining, which learns a useful representation using a large\n",
            "amount of unlabeled data to facilitate the learning of downstream tasks, is a\n",
            "critical component of modern large-scale machine learning systems. Despite its\n",
            "tremendous empirical success, the rigorous theoretical understanding of why\n",
            "unsupervised pretraining generally helps remains rather limited -- most\n",
            "existing results are restricted to particular methods or approaches for\n",
            "unsupervised pretraining with specialized structural assumptions. This paper\n",
            "studies a generic framework, where the unsupervised representation learning\n",
            "task is specified by an abstract class of latent variable models $\\Phi$ and the\n",
            "downstream task is specified by a class of prediction functions $\\Psi$. We\n",
            "consider a natural approach of using Maximum Likelihood Estimation (MLE) for\n",
            "unsupervised pretraining and Empirical Risk Minimization (ERM) for learning\n",
            "downstream tasks. We prove that, under a mild ''informative'' condition, our\n",
            "algorithm achieves an excess risk of\n",
            "$\\tilde{\\mathcal{O}}(\\sqrt{\\mathcal{C}_\\Phi/m} + \\sqrt{\\mathcal{C}_\\Psi/n})$\n",
            "for downstream tasks, where $\\mathcal{C}_\\Phi, \\mathcal{C}_\\Psi$ are complexity\n",
            "measures of function classes $\\Phi, \\Psi$, and $m, n$ are the number of\n",
            "unlabeled and labeled data respectively. Comparing to the baseline of\n",
            "$\\tilde{\\mathcal{O}}(\\sqrt{\\mathcal{C}_{\\Phi \\circ \\Psi}/n})$ achieved by\n",
            "performing supervised learning using only the labeled data, our result\n",
            "rigorously shows the benefit of unsupervised pretraining when $m \\gg n$ and\n",
            "$\\mathcal{C}_{\\Phi\\circ \\Psi} > \\mathcal{C}_\\Psi$. This paper further shows\n",
            "that our generic framework covers a wide range of approaches for unsupervised\n",
            "pretraining, including factor models, Gaussian mixture models, and contrastive\n",
            "learning.\n",
            "\n",
            "1405. Title: Stylized Offline Reinforcement Learning: Extracting Diverse High-Quality Behaviors from Heterogeneous Datasets\n",
            "   Abstract: We propose a new challenging task namely IDentity Stylization (IDS) across\n",
            "heterogeneous domains. IDS focuses on stylizing the content identity, rather\n",
            "than completely swapping it using the reference identity. We use an effective\n",
            "heterogeneous-network-based framework $Styleverse$ that uses a single\n",
            "domain-aware generator to exploit the Metaverse of diverse heterogeneous faces,\n",
            "based on the proposed dataset FS13 with limited data. FS13 means 13 kinds of\n",
            "Face Styles considering diverse lighting conditions, art representations and\n",
            "life dimensions. Previous similar tasks, \\eg, image style transfer can handle\n",
            "textural style transfer based on a reference image. This task usually ignores\n",
            "the high structure-aware facial area and high-fidelity preservation of the\n",
            "content. However, Styleverse intends to controllably create topology-aware\n",
            "faces in the Parallel Style Universe, where the source facial identity is\n",
            "adaptively styled via AdaIN guided by the domain-aware and reference-aware\n",
            "style embeddings from heterogeneous pretrained models. We first establish the\n",
            "IDS quantitative benchmark as well as the qualitative Styleverse matrix.\n",
            "Extensive experiments demonstrate that Styleverse achieves higher-fidelity\n",
            "identity stylization compared with other state-of-the-art methods.\n",
            "\n",
            "1406. Title: Faithful Vision-Language Interpretation via Concept Bottleneck Models\n",
            "   Abstract: Recently, multimodal contrastive learning (MMCL) approaches, such as CLIP,\n",
            "have achieved a remarkable success in learning representations that are robust\n",
            "against distribution shift and generalize to new domains. Despite the empirical\n",
            "success, the mechanism behind learning such generalizable representations is\n",
            "not understood. In this work, we rigorously analyze this problem and uncover\n",
            "two mechanisms behind MMCL's robustness: \\emph{intra-class contrasting}, which\n",
            "allows the model to learn features with a high variance, and \\emph{inter-class\n",
            "feature sharing}, where annotated details in one class help learning other\n",
            "classes better. Both mechanisms prevent spurious features that are\n",
            "over-represented in the training data to overshadow the generalizable core\n",
            "features. This yields superior zero-shot classification accuracy under\n",
            "distribution shift. Furthermore, we theoretically demonstrate the benefits of\n",
            "using rich captions on robustness and explore the effect of annotating\n",
            "different types of details in the captions. We validate our theoretical\n",
            "findings through experiments, including a well-designed synthetic experiment\n",
            "and an experiment involving training CLIP models on MSCOCO/Conceptual Captions\n",
            "and evaluating them on shifted ImageNets.\n",
            "\n",
            "1407. Title: An Efficient Membership Inference Attack for the Diffusion Model by Proximal Initialization\n",
            "   Abstract: We introduce a new class of circuits for constructing efficiently decodable\n",
            "error-correction codes, based on a recently discovered contractible tensor\n",
            "network. We perform an in-depth study of a particular example that can be\n",
            "thought of as an extension to Arikan's polar code. Notably, our numerical\n",
            "simulation show that this code polarizes the logical channels more strongly\n",
            "while retaining the log-linear decoding complexity using the successive\n",
            "cancellation decoder. These codes also display improved error-correcting\n",
            "capability with only a minor impact on decoding complexity. Efficient decoding\n",
            "is realized using powerful graphical calculus tools developed in the field of\n",
            "quantum many-body physics. In a companion paper, we generalize our construction\n",
            "to the quantum setting and describe more in-depth the relation between\n",
            "classical and quantum error correction and the graphical calculus of tensor\n",
            "networks.\n",
            "\n",
            "1408. Title: DistillSpec: Improving Speculative Decoding via Knowledge Distillation\n",
            "   Abstract: Recently, diffusion models have achieved remarkable success in generating\n",
            "tasks, including image and audio generation. However, like other generative\n",
            "models, diffusion models are prone to privacy issues. In this paper, we propose\n",
            "an efficient query-based membership inference attack (MIA), namely Proximal\n",
            "Initialization Attack (PIA), which utilizes groundtruth trajectory obtained by\n",
            "$\\epsilon$ initialized in $t=0$ and predicted point to infer memberships.\n",
            "Experimental results indicate that the proposed method can achieve competitive\n",
            "performance with only two queries on both discrete-time and continuous-time\n",
            "diffusion models. Moreover, previous works on the privacy of diffusion models\n",
            "have focused on vision tasks without considering audio tasks. Therefore, we\n",
            "also explore the robustness of diffusion models to MIA in the text-to-speech\n",
            "(TTS) task, which is an audio generation task. To the best of our knowledge,\n",
            "this work is the first to study the robustness of diffusion models to MIA in\n",
            "the TTS task. Experimental results indicate that models with mel-spectrogram\n",
            "(image-like) output are vulnerable to MIA, while models with audio output are\n",
            "relatively robust to MIA. {Code is available at\n",
            "\\url{https://github.com/kong13661/PIA}}.\n",
            "\n",
            "1409. Title: Federated Wasserstein Distance\n",
            "   Abstract: Despite extensive research since the community learned about adversarial\n",
            "examples 10 years ago, we still do not know how to train high-accuracy\n",
            "classifiers that are guaranteed to be robust to small perturbations of their\n",
            "inputs. Previous works often argued that this might be because no classifier\n",
            "exists that is robust and accurate at the same time. However, in computer\n",
            "vision this assumption does not match reality where humans are usually accurate\n",
            "and robust on most tasks of interest. We offer an alternative explanation and\n",
            "show that in certain settings robust generalization is only possible with\n",
            "unrealistically large amounts of data. More precisely we find a setting where a\n",
            "robust classifier exists, it is easy to learn an accurate classifier, yet it\n",
            "requires an exponential amount of data to learn a robust classifier. Based on\n",
            "this theoretical result, we explore how well robust classifiers generalize on\n",
            "datasets such as CIFAR-10. We come to the conclusion that on this datasets, the\n",
            "limitation of current robust models also lies in the generalization, and that\n",
            "they require a lot of data to do well on the test set. We also show that the\n",
            "problem is not in the expressiveness or generalization capabilities of current\n",
            "architectures, and that there are low magnitude features in the data which are\n",
            "useful for non-robust generalization but are not available for robust\n",
            "classifiers.\n",
            "\n",
            "1410. Title: Understanding the Robustness of Multi-modal Contrastive Learning to Distribution Shift\n",
            "   Abstract: We introduce a principled way of computing the Wasserstein distance between\n",
            "two distributions in a federated manner. Namely, we show how to estimate the\n",
            "Wasserstein distance between two samples stored and kept on different\n",
            "devices/clients whilst a central entity/server orchestrates the computations\n",
            "(again, without having access to the samples). To achieve this feat, we take\n",
            "advantage of the geometric properties of the Wasserstein distance -- in\n",
            "particular, the triangle inequality -- and that of the associated {\\em\n",
            "geodesics}: our algorithm, FedWad (for Federated Wasserstein Distance),\n",
            "iteratively approximates the Wasserstein distance by manipulating and\n",
            "exchanging distributions from the space of geodesics in lieu of the input\n",
            "samples. In addition to establishing the convergence properties of FedWad, we\n",
            "provide empirical results on federated coresets and federate optimal transport\n",
            "dataset distance, that we respectively exploit for building a novel federated\n",
            "model and for boosting performance of popular federated learning algorithms.\n",
            "\n",
            "1411. Title: Variational Inference for SDEs Driven by Fractional Noise\n",
            "   Abstract: This study examines portfolio selection using predictive models for portfolio\n",
            "returns. Portfolio selection is a fundamental task in finance, and a variety of\n",
            "methods have been developed to achieve this goal. For instance, the\n",
            "mean-variance approach constructs portfolios by balancing the trade-off between\n",
            "the mean and variance of asset returns, while the quantile-based approach\n",
            "optimizes portfolios by considering tail risk. These methods often depend on\n",
            "distributional information estimated from historical data using predictive\n",
            "models, each of which carries its own uncertainty. To address this, we propose\n",
            "a framework for predictive portfolio selection via conformal prediction ,\n",
            "called \\emph{Conformal Predictive Portfolio Selection} (CPPS). Our approach\n",
            "forecasts future portfolio returns, computes the corresponding prediction\n",
            "intervals, and selects the portfolio of interest based on these intervals. The\n",
            "framework is flexible and can accommodate a wide range of predictive models,\n",
            "including autoregressive (AR) models, random forests, and neural networks. We\n",
            "demonstrate the effectiveness of the CPPS framework by applying it to an AR\n",
            "model and validate its performance through empirical studies, showing that it\n",
            "delivers superior returns compared to simpler strategies.\n",
            "\n",
            "1412. Title: A Data-Driven Measure of Relative Uncertainty for Misclassification Detection\n",
            "   Abstract: We present a novel variational framework for performing inference in (neural)\n",
            "stochastic differential equations (SDEs) driven by Markov-approximate\n",
            "fractional Brownian motion (fBM). SDEs offer a versatile tool for modeling\n",
            "real-world continuous-time dynamic systems with inherent noise and randomness.\n",
            "Combining SDEs with the powerful inference capabilities of variational methods,\n",
            "enables the learning of representative function distributions through\n",
            "stochastic gradient descent. However, conventional SDEs typically assume the\n",
            "underlying noise to follow a Brownian motion (BM), which hinders their ability\n",
            "to capture long-term dependencies. In contrast, fractional Brownian motion\n",
            "(fBM) extends BM to encompass non-Markovian dynamics, but existing methods for\n",
            "inferring fBM parameters are either computationally demanding or statistically\n",
            "inefficient. In this paper, building upon the Markov approximation of fBM, we\n",
            "derive the evidence lower bound essential for efficient variational inference\n",
            "of posterior path measures, drawing from the well-established field of\n",
            "stochastic analysis. Additionally, we provide a closed-form expression to\n",
            "determine optimal approximation coefficients. Furthermore, we propose the use\n",
            "of neural networks to learn the drift, diffusion and control terms within our\n",
            "variational posterior, leading to the variational training of neural-SDEs. In\n",
            "this framework, we also optimize the Hurst index, governing the nature of our\n",
            "fractional noise. Beyond validation on synthetic data, we contribute a novel\n",
            "architecture for variational latent video prediction,-an approach that, to the\n",
            "best of our knowledge, enables the first variational neural-SDE application to\n",
            "video perception.\n",
            "\n",
            "1413. Title: Long-Short-Range Message-Passing: A Physics-Informed Framework to Capture Non-Local Interaction for Scalable Molecular Dynamics Simulation\n",
            "   Abstract: Misclassification detection is an important problem in machine learning, as\n",
            "it allows for the identification of instances where the model's predictions are\n",
            "unreliable. However, conventional uncertainty measures such as Shannon entropy\n",
            "do not provide an effective way to infer the real uncertainty associated with\n",
            "the model's predictions. In this paper, we introduce a novel data-driven\n",
            "measure of uncertainty relative to an observer for misclassification detection.\n",
            "By learning patterns in the distribution of soft-predictions, our uncertainty\n",
            "measure can identify misclassified samples based on the predicted class\n",
            "probabilities. Interestingly, according to the proposed measure,\n",
            "soft-predictions corresponding to misclassified instances can carry a large\n",
            "amount of uncertainty, even though they may have low Shannon entropy. We\n",
            "demonstrate empirical improvements over multiple image classification tasks,\n",
            "outperforming state-of-the-art misclassification detection methods.\n",
            "\n",
            "1414. Title: Learning to Act without Actions\n",
            "   Abstract: A signed distance function (SDF) is a useful representation for\n",
            "continuous-space geometry and many related operations, including rendering,\n",
            "collision checking, and mesh generation. Hence, reconstructing SDF from image\n",
            "observations accurately and efficiently is a fundamental problem. Recently,\n",
            "neural implicit SDF (SDF-NeRF) techniques, trained using volumetric rendering,\n",
            "have gained a lot of attention. Compared to earlier truncated SDF (TSDF) fusion\n",
            "algorithms that rely on depth maps and voxelize continuous space, SDF-NeRF\n",
            "enables continuous-space SDF reconstruction with better geometric and\n",
            "photometric accuracy. However, the accuracy and convergence speed of\n",
            "scene-level SDF reconstruction require further improvements for many\n",
            "applications. With the advent of 3D Gaussian Splatting (3DGS) as an explicit\n",
            "representation with excellent rendering quality and speed, several works have\n",
            "focused on improving SDF-NeRF by introducing consistency losses on depth and\n",
            "surface normals between 3DGS and SDF-NeRF. However, loss-level connections\n",
            "alone lead to incremental improvements. We propose a novel neural implicit SDF\n",
            "called \"SplatSDF\" to fuse 3DGSandSDF-NeRF at an architecture level with\n",
            "significant boosts to geometric and photometric accuracy and convergence speed.\n",
            "Our SplatSDF relies on 3DGS as input only during training, and keeps the same\n",
            "complexity and efficiency as the original SDF-NeRF during inference. Our method\n",
            "outperforms state-of-the-art SDF-NeRF models on geometric and photometric\n",
            "evaluation by the time of submission.\n",
            "\n",
            "1415. Title: Selective Mixup Fine-Tuning for Optimizing Non-Decomposable Objectives\n",
            "   Abstract: Given a pretrained encoder-based language model, how can we accurately\n",
            "compress it without retraining? Retraining-free structured pruning algorithms\n",
            "are crucial in pretrained language model compression due to their significantly\n",
            "reduced pruning cost and capability to prune large language models. However,\n",
            "existing retraining-free algorithms encounter severe accuracy degradation, as\n",
            "they fail to handle pruning errors, especially at high compression rates. In\n",
            "this paper, we propose K-prune (Knowledge-preserving pruning), an accurate\n",
            "retraining-free structured pruning algorithm for pretrained encoder-based\n",
            "language models. K-prune focuses on preserving the useful knowledge of the\n",
            "pretrained model to minimize pruning errors through a carefully designed\n",
            "iterative pruning process composed of knowledge measurement,\n",
            "knowledge-preserving mask search, and knowledge-preserving weight-tuning. As a\n",
            "result, K-prune shows significant accuracy improvements up to 58.02%p higher F1\n",
            "score compared to existing retraining-free pruning algorithms under a high\n",
            "compression rate of 80% on the SQuAD benchmark without any retraining process.\n",
            "\n",
            "1416. Title: CAMIL: Context-Aware Multiple Instance Learning for Cancer Detection and Subtyping in Whole Slide Images\n",
            "   Abstract: The ability to engineer novel proteins with higher fitness for a desired\n",
            "property would be revolutionary for biotechnology and medicine. Modeling the\n",
            "combinatorially large space of sequences is infeasible; prior methods often\n",
            "constrain optimization to a small mutational radius, but this drastically\n",
            "limits the design space. Instead of heuristics, we propose smoothing the\n",
            "fitness landscape to facilitate protein optimization. First, we formulate\n",
            "protein fitness as a graph signal then use Tikunov regularization to smooth the\n",
            "fitness landscape. We find optimizing in this smoothed landscape leads to\n",
            "improved performance across multiple methods in the GFP and AAV benchmarks.\n",
            "Second, we achieve state-of-the-art results utilizing discrete energy-based\n",
            "models and MCMC in the smoothed landscape. Our method, called Gibbs sampling\n",
            "with Graph-based Smoothing (GGS), demonstrates a unique ability to achieve 2.5\n",
            "fold fitness improvement (with in-silico evaluation) over its training set. GGS\n",
            "demonstrates potential to optimize proteins in the limited data regime. Code:\n",
            "https://github.com/kirjner/GGS\n",
            "\n",
            "1417. Title: Neural SDF Flow for 3D Reconstruction of Dynamic Scenes\n",
            "   Abstract: Computational simulation of chemical and biological systems using ab initio\n",
            "molecular dynamics has been a challenge over decades. Researchers have\n",
            "attempted to address the problem with machine learning and fragmentation-based\n",
            "methods. However, the two approaches fail to give a satisfactory description of\n",
            "long-range and many-body interactions, respectively. Inspired by\n",
            "fragmentation-based methods, we propose the Long-Short-Range Message-Passing\n",
            "(LSR-MP) framework as a generalization of the existing equivariant graph neural\n",
            "networks (EGNNs) with the intent to incorporate long-range interactions\n",
            "efficiently and effectively. We apply the LSR-MP framework to the recently\n",
            "proposed ViSNet and demonstrate the state-of-the-art results with up to 40% MAE\n",
            "reduction for molecules in MD22 and Chignolin datasets. Consistent improvements\n",
            "to various EGNNs will also be discussed to illustrate the general applicability\n",
            "and robustness of our LSR-MP framework. The code for our experiments and\n",
            "trained model weights could be found at https://github.com/liyy2/LSR-MP.\n",
            "\n",
            "1418. Title: The Human-AI Substitution game: active learning from a strategic labeler\n",
            "   Abstract: The act of bluffing confounds game designers to this day. The very nature of\n",
            "bluffing is even open for debate, adding further complication to the process of\n",
            "creating intelligent virtual players that can bluff, and hence play,\n",
            "realistically. Through the use of intelligent, learning agents, and carefully\n",
            "designed agent outlooks, an agent can in fact learn to predict its opponents\n",
            "reactions based not only on its own cards, but on the actions of those around\n",
            "it. With this wider scope of understanding, an agent can in learn to bluff its\n",
            "opponents, with the action representing not an illogical action, as bluffing is\n",
            "often viewed, but rather as an act of maximising returns through an effective\n",
            "statistical optimisation. By using a tee dee lambda learning algorithm to\n",
            "continuously adapt neural network agent intelligence, agents have been shown to\n",
            "be able to learn to bluff without outside prompting, and even to learn to call\n",
            "each others bluffs in free, competitive play.\n",
            "\n",
            "1419. Title: Towards Eliminating Hard Label Constraints in Gradient Inversion Attacks\n",
            "   Abstract: The rise in internet usage has led to the generation of massive amounts of\n",
            "data, resulting in the adoption of various supervised and semi-supervised\n",
            "machine learning algorithms, which can effectively utilize the colossal amount\n",
            "of data to train models. However, before deploying these models in the real\n",
            "world, these must be strictly evaluated on performance measures like worst-case\n",
            "recall and satisfy constraints such as fairness. We find that current\n",
            "state-of-the-art empirical techniques offer sub-optimal performance on these\n",
            "practical, non-decomposable performance objectives. On the other hand, the\n",
            "theoretical techniques necessitate training a new model from scratch for each\n",
            "performance objective. To bridge the gap, we propose SelMix, a selective\n",
            "mixup-based inexpensive fine-tuning technique for pre-trained models, to\n",
            "optimize for the desired objective. The core idea of our framework is to\n",
            "determine a sampling distribution to perform a mixup of features between\n",
            "samples from particular classes such that it optimizes the given objective. We\n",
            "comprehensively evaluate our technique against the existing empirical and\n",
            "theoretically principled methods on standard benchmark datasets for imbalanced\n",
            "classification. We find that proposed SelMix fine-tuning significantly improves\n",
            "the performance for various practical non-decomposable objectives across\n",
            "benchmarks.\n",
            "\n",
            "1420. Title: f-FERM: A Scalable Framework for Robust Fair Empirical Risk Minimization\n",
            "   Abstract: The legality of training language models (LMs) on copyrighted or otherwise\n",
            "restricted data is under intense debate. However, as we show, model performance\n",
            "significantly degrades if trained only on low-risk text (e.g., out-of-copyright\n",
            "books or government documents), due to its limited size and domain coverage. We\n",
            "present SILO, a new language model that manages this risk-performance tradeoff\n",
            "during inference. SILO is built by (1) training a parametric LM on Open License\n",
            "Corpus (OLC), a new corpus we curate with 228B tokens of public domain and\n",
            "permissively licensed text and (2) augmenting it with a more general and easily\n",
            "modifiable nonparametric datastore (e.g., containing copyrighted books or news)\n",
            "that is only queried during inference. The datastore allows use of high-risk\n",
            "data without training on it, supports sentence-level data attribution, and\n",
            "enables data producers to opt out from the model by removing content from the\n",
            "store. These capabilities can foster compliance with data-use regulations such\n",
            "as the fair use doctrine in the United States and the GDPR in the European\n",
            "Union. Our experiments show that the parametric LM struggles on domains not\n",
            "covered by OLC. However, access to the datastore greatly improves out of domain\n",
            "performance, closing 90% of the performance gap with an LM trained on the Pile,\n",
            "a more diverse corpus with mostly high-risk text. We also analyze which\n",
            "nonparametric approach works best, where the remaining errors lie, and how\n",
            "performance scales with datastore size. Our results suggest that it is possible\n",
            "to build high quality language models while mitigating their legal risk.\n",
            "\n",
            "1421. Title: SocioDojo: Building Lifelong Analytical Agents with Real-world Text and Time Series\n",
            "   Abstract: We consider the problem of Probably Approximate Correct (PAC) learning of a\n",
            "binary classifier from noisy labeled examples acquired from multiple annotators\n",
            "(each characterized by a respective classification noise rate). First, we\n",
            "consider the complete information scenario, where the learner knows the noise\n",
            "rates of all the annotators. For this scenario, we derive sample complexity\n",
            "bound for the Minimum Disagreement Algorithm (MDA) on the number of labeled\n",
            "examples to be obtained from each annotator. Next, we consider the incomplete\n",
            "information scenario, where each annotator is strategic and holds the\n",
            "respective noise rate as a private information. For this scenario, we design a\n",
            "cost optimal procurement auction mechanism along the lines of Myerson's optimal\n",
            "auction design framework in a non-trivial manner. This mechanism satisfies\n",
            "incentive compatibility property, thereby facilitating the learner to elicit\n",
            "true noise rates of all the annotators.\n",
            "\n",
            "1422. Title: MagicDrive: Street View Generation with Diverse 3D Geometry Control\n",
            "   Abstract: Gradient inversion attacks aim to reconstruct local training data from\n",
            "intermediate gradients exposed in the federated learning framework. Despite\n",
            "successful attacks, all previous methods, starting from reconstructing a single\n",
            "data point and then relaxing the single-image limit to batch level, are only\n",
            "tested under hard label constraints. Even for single-image reconstruction, we\n",
            "still lack an analysis-based algorithm to recover augmented soft labels. In\n",
            "this work, we change the focus from enlarging batchsize to investigating the\n",
            "hard label constraints, considering a more realistic circumstance where label\n",
            "smoothing and mixup techniques are used in the training process. In particular,\n",
            "we are the first to initiate a novel algorithm to simultaneously recover the\n",
            "ground-truth augmented label and the input feature of the last fully-connected\n",
            "layer from single-input gradients, and provide a necessary condition for any\n",
            "analytical-based label recovery methods. Extensive experiments testify to the\n",
            "label recovery accuracy, as well as the benefits to the following image\n",
            "reconstruction. We believe soft labels in classification tasks are worth\n",
            "further attention in gradient inversion attacks.\n",
            "\n",
            "1423. Title: Addressing Loss of Plasticity and Catastrophic Forgetting in Continual Learning\n",
            "   Abstract: The data article presents the large bilingual parallel corpus of\n",
            "low-resourced language pair Sanskrit-Hindi, named SAHAAYAK 2023. The corpus\n",
            "contains total of 1.5M sentence pairs between Sanskrit and Hindi. To make the\n",
            "universal usability of the corpus and to make it balanced, data from multiple\n",
            "domain has been incorporated into the corpus that includes, News, Daily\n",
            "conversations, Politics, History, Sport, and Ancient Indian Literature. The\n",
            "multifaceted approach has been adapted to make a sizable multi-domain corpus of\n",
            "low-resourced languages like Sanskrit. Our development approach is spanned from\n",
            "creating a small hand-crafted dataset to applying a wide range of mining,\n",
            "cleaning, and verification. We have used the three-fold process of mining:\n",
            "mining from machine-readable sources, mining from non-machine readable sources,\n",
            "and collation from existing corpora sources. Post mining, the dedicated\n",
            "pipeline for normalization, alignment, and corpus cleaning is developed and\n",
            "applied to the corpus to make it ready to use on machine translation\n",
            "algorithms.\n",
            "\n",
            "1424. Title: Fair and Efficient Contribution Valuation for Vertical Federated Learning\n",
            "   Abstract: This is a report on a qualitative study of students' learning where a physics\n",
            "computer simulation session is used to supplement lectures on the topic.\n",
            "Drawing on phenomenography as the analytical framework, the students'\n",
            "learning-focuses were analysed. The result is a description of four distinctly\n",
            "different learning-focuses that emerged when the students involved in the study\n",
            "interacted with the computer simulations. These learning-focuses were then\n",
            "analysed in terms of the level of interaction, the nature of physics knowledge\n",
            "and views of learning experienced by the students. These results were then used\n",
            "to identify advantages and disadvantages of learning through interaction with\n",
            "simulations.\n",
            "\n",
            "1425. Title: NuwaDynamics: Discovering and Updating in Causal Spatio-Temporal Modeling\n",
            "   Abstract: Deep representation learning methods struggle with continual learning,\n",
            "suffering from both catastrophic forgetting of useful units and loss of\n",
            "plasticity, often due to rigid and unuseful units. While many methods address\n",
            "these two issues separately, only a few currently deal with both\n",
            "simultaneously. In this paper, we introduce Utility-based Perturbed Gradient\n",
            "Descent (UPGD) as a novel approach for the continual learning of\n",
            "representations. UPGD combines gradient updates with perturbations, where it\n",
            "applies smaller modifications to more useful units, protecting them from\n",
            "forgetting, and larger modifications to less useful units, rejuvenating their\n",
            "plasticity. We use a challenging streaming learning setup where continual\n",
            "learning problems have hundreds of non-stationarities and unknown task\n",
            "boundaries. We show that many existing methods suffer from at least one of the\n",
            "issues, predominantly manifested by their decreasing accuracy over tasks. On\n",
            "the other hand, UPGD continues to improve performance and surpasses or is\n",
            "competitive with all methods in all problems. Finally, in extended\n",
            "reinforcement learning experiments with PPO, we show that while Adam exhibits a\n",
            "performance drop after initial learning, UPGD avoids it by addressing both\n",
            "continual learning issues.\n",
            "\n",
            "1426. Title: Transformers can optimally learn regression mixture models\n",
            "   Abstract: Mixture models arise in many regression problems, but most methods have seen\n",
            "limited adoption partly due to these algorithms' highly-tailored and\n",
            "model-specific nature. On the other hand, transformers are flexible, neural\n",
            "sequence models that present the intriguing possibility of providing\n",
            "general-purpose prediction methods, even in this mixture setting. In this work,\n",
            "we investigate the hypothesis that transformers can learn an optimal predictor\n",
            "for mixtures of regressions. We construct a generative process for a mixture of\n",
            "linear regressions for which the decision-theoretic optimal procedure is given\n",
            "by data-driven exponential weights on a finite set of parameters. We observe\n",
            "that transformers achieve low mean-squared error on data generated via this\n",
            "process. By probing the transformer's output at inference time, we also show\n",
            "that transformers typically make predictions that are close to the optimal\n",
            "predictor. Our experiments also demonstrate that transformers can learn\n",
            "mixtures of regressions in a sample-efficient fashion and are somewhat robust\n",
            "to distribution shifts. We complement our experimental observations by proving\n",
            "constructively that the decision-theoretic optimal procedure is indeed\n",
            "implementable by a transformer.\n",
            "\n",
            "1427. Title: Towards Meta-Pruning via Optimal Transport\n",
            "   Abstract: Leveraging connections between diffusion-based sampling, optimal transport,\n",
            "and stochastic optimal control through their shared links to the Schr\\\"odinger\n",
            "bridge problem, we propose novel objective functions that can be used to\n",
            "transport $\\nu$ to $\\mu$, consequently sample from the target $\\mu$, via\n",
            "optimally controlled dynamics. We highlight the importance of the pathwise\n",
            "perspective and the role various optimality conditions on the path measure can\n",
            "play for the design of valid training losses, the careful choice of which offer\n",
            "numerical advantages in implementation. Basing the formalism on Schr\\\"odinger\n",
            "bridge comes with the additional practical capability of baking in inductive\n",
            "bias when it comes to Neural Network training.\n",
            "\n",
            "1428. Title: Neural Common Neighbor with Completion for Link Prediction\n",
            "   Abstract: Federated learning is a popular technology for training machine learning\n",
            "models on distributed data sources without sharing data. Vertical federated\n",
            "learning or feature-based federated learning applies to the cases that\n",
            "different data sources share the same sample ID space but differ in feature\n",
            "space. To ensure the data owners' long-term engagement, it is critical to\n",
            "objectively assess the contribution from each data source and recompense them\n",
            "accordingly. The Shapley value (SV) is a provably fair contribution valuation\n",
            "metric originated from cooperative game theory. However, computing the SV\n",
            "requires extensively retraining the model on each subset of data sources, which\n",
            "causes prohibitively high communication costs in federated learning. We propose\n",
            "a contribution valuation metric called vertical federated Shapley value\n",
            "(VerFedSV) based on SV. We show that VerFedSV not only satisfies many desirable\n",
            "properties for fairness but is also efficient to compute, and can be adapted to\n",
            "both synchronous and asynchronous vertical federated learning algorithms. Both\n",
            "theoretical analysis and extensive experimental results verify the fairness,\n",
            "efficiency, and adaptability of VerFedSV.\n",
            "\n",
            "1429. Title: NeRM: Learning Neural Representations for High-Framerate Human Motion Synthesis\n",
            "   Abstract: In this work, we propose a novel link prediction model and further boost it\n",
            "by studying graph incompleteness. First, we introduce MPNN-then-SF, an\n",
            "innovative architecture leveraging structural feature (SF) to guide MPNN's\n",
            "representation pooling, with its implementation, namely Neural Common Neighbor\n",
            "(NCN). NCN exhibits superior expressiveness and scalability compared with\n",
            "existing models, which can be classified into two categories: SF-then-MPNN,\n",
            "augmenting MPNN's input with SF, and SF-and-MPNN, decoupling SF and MPNN.\n",
            "Second, we investigate the impact of graph incompleteness -- the phenomenon\n",
            "that some links are unobserved in the input graph -- on SF, like the common\n",
            "neighbor. Through dataset visualization, we observe that incompleteness reduces\n",
            "common neighbors and induces distribution shifts, significantly affecting model\n",
            "performance. To address this issue, we propose to use a link prediction model\n",
            "to complete the common neighbor structure. Combining this method with NCN, we\n",
            "propose Neural Common Neighbor with Completion (NCNC). NCN and NCNC outperform\n",
            "recent strong baselines by large margins, and NCNC further surpasses\n",
            "state-of-the-art models in standard link prediction benchmarks. Our code is\n",
            "available at https://github.com/GraphPKU/NeuralCommonNeighbor.\n",
            "\n",
            "1430. Title: Fiber Monte Carlo\n",
            "   Abstract: This paper considers a variation of the classical two-user interference\n",
            "channel where the communication of two interfering source-destination pairs is\n",
            "aided by an additional node that has a priori knowledge of the messages to be\n",
            "transmitted, which is referred to as the it cognitive relay. For this\n",
            "Interference Channel with a Cognitive Relay (ICCR) In particular, for the class\n",
            "of injective semi-deterministic ICCRs, a sum-rate upper bound is derived for\n",
            "the general memoryless ICCR and further tightened for the Linear Deterministic\n",
            "Approximation (LDA) of the Gaussian noise channel at high SNR, which disregards\n",
            "the noise and focuses on the interaction among the users' signals. The capacity\n",
            "region of the symmetric LDA is completely characterized except for the regime\n",
            "of moderately weak interference and weak links from the CR to the destinations.\n",
            "The insights gained from the analysis of the LDA are then translated back to\n",
            "the symmetric Gaussian noise channel (GICCR). For the symmetric GICCR, an\n",
            "approximate characterization (to within a constant gap) of the capacity region\n",
            "is provided for a parameter regime where capacity was previously unknown. The\n",
            "approximately optimal scheme suggests that message cognition at a relay is\n",
            "beneficial for interference management as it enables simultaneous over the air\n",
            "neutralization of the interference at both destinations.\n",
            "\n",
            "1431. Title: Consistent4D: Consistent 360° Dynamic Object Generation from Monocular Video\n",
            "   Abstract: We use a simple dynamical model and explore coherent dynamics of wavepackets\n",
            "in complex networks of optical fibers. We start from a symmetric lattice and\n",
            "through the application of a Monte-Carlo criterion we introduce structural\n",
            "disorder and deform the lattice into a small-world network regime. We\n",
            "investigate in the latter both structural (correlation length) as well as\n",
            "dynamical (diffusion exponent) properties and find that both exhibit a rapid\n",
            "crossover from the ordered to the fully random regime. For a critical value of\n",
            "the structural disorder parameter $\\rho \\approx 0.25$ transport changes from\n",
            "ballistic to sub-diffusive due to the creation strongly connected local\n",
            "clusters and channels of preferential transport in the small world regime.\n",
            "\n",
            "1432. Title: Understanding when Dynamics-Invariant Data Augmentations Benefit Model-free Reinforcement Learning Updates\n",
            "   Abstract: Recently, data augmentation (DA) has emerged as a method for leveraging\n",
            "domain knowledge to inexpensively generate additional data in reinforcement\n",
            "learning (RL) tasks, often yielding substantial improvements in data\n",
            "efficiency. While prior work has demonstrated the utility of incorporating\n",
            "augmented data directly into model-free RL updates, it is not well-understood\n",
            "when a particular DA strategy will improve data efficiency. In this paper, we\n",
            "seek to identify general aspects of DA responsible for observed learning\n",
            "improvements. Our study focuses on sparse-reward tasks with dynamics-invariant\n",
            "data augmentation functions, serving as an initial step towards a more general\n",
            "understanding of DA and its integration into RL training. Experimentally, we\n",
            "isolate three relevant aspects of DA: state-action coverage, reward density,\n",
            "and the number of augmented transitions generated per update (the augmented\n",
            "replay ratio). From our experiments, we draw two conclusions: (1) increasing\n",
            "state-action coverage often has a much greater impact on data efficiency than\n",
            "increasing reward density, and (2) decreasing the augmented replay ratio\n",
            "substantially improves data efficiency. In fact, certain tasks in our empirical\n",
            "study are solvable only when the replay ratio is sufficiently low.\n",
            "\n",
            "1433. Title: DSPy: Compiling Declarative Language Model Calls into State-of-the-Art Pipelines\n",
            "   Abstract: Discussion and debate among Large Language Models (LLMs) have gained\n",
            "considerable attention due to their potential to enhance the reasoning ability\n",
            "of LLMs. Although natural language is an obvious choice for communication due\n",
            "to LLM's language understanding capability, the token sampling step needed when\n",
            "generating natural language poses a potential risk of information loss, as it\n",
            "uses only one token to represent the model's belief across the entire\n",
            "vocabulary. In this paper, we introduce a communication regime named CIPHER\n",
            "(Communicative Inter-Model Protocol Through Embedding Representation) to\n",
            "address this issue. Specifically, we remove the token sampling step from LLMs\n",
            "and let them communicate their beliefs across the vocabulary through the\n",
            "expectation of the raw transformer output embeddings. Remarkably, by deviating\n",
            "from natural language, CIPHER offers an advantage of encoding a broader\n",
            "spectrum of information without any modification to the model weights,\n",
            "outperforming the state-of-the-art LLM debate methods using natural language by\n",
            "0.5-5.0% across five reasoning tasks and multiple open-source LLMs of varying\n",
            "sizes. This showcases the superiority and robustness of embeddings as an\n",
            "alternative \"language\" for communication among LLMs. We anticipate that CIPHER\n",
            "will inspire further exploration for the design of interactions within LLM\n",
            "agent systems, offering a new direction that could significantly influence\n",
            "future developments in the field.\n",
            "\n",
            "1434. Title: NeuroBack: Improving CDCL SAT Solving using Graph Neural Networks\n",
            "   Abstract: Class-incremental learning is becoming more popular as it helps models widen\n",
            "their applicability while not forgetting what they already know. A trend in\n",
            "this area is to use a mixture-of-expert technique, where different models work\n",
            "together to solve the task. However, the experts are usually trained all at\n",
            "once using whole task data, which makes them all prone to forgetting and\n",
            "increasing computational burden. To address this limitation, we introduce a\n",
            "novel approach named SEED. SEED selects only one, the most optimal expert for a\n",
            "considered task, and uses data from this task to fine-tune only this expert.\n",
            "For this purpose, each expert represents each class with a Gaussian\n",
            "distribution, and the optimal expert is selected based on the similarity of\n",
            "those distributions. Consequently, SEED increases diversity and heterogeneity\n",
            "within the experts while maintaining the high stability of this ensemble\n",
            "method. The extensive experiments demonstrate that SEED achieves\n",
            "state-of-the-art performance in exemplar-free settings across various\n",
            "scenarios, showing the potential of expert diversification through data in\n",
            "continual learning.\n",
            "\n",
            "1435. Title: Let Models Speak Ciphers: Multiagent Debate through Embeddings\n",
            "   Abstract: The visual prompts have provided an efficient manner in addressing visual\n",
            "cross-domain problems. In previous works, Visual Domain Prompt (VDP) first\n",
            "introduces domain prompts to tackle the classification Test-Time Adaptation\n",
            "(TTA) problem by warping image-level prompts on the input and fine-tuning\n",
            "prompts for each target domain. However, since the image-level prompts mask out\n",
            "continuous spatial details in the prompt-allocated region, it will suffer from\n",
            "inaccurate contextual information and limited domain knowledge extraction,\n",
            "particularly when dealing with dense prediction TTA problems. To overcome these\n",
            "challenges, we propose a novel Sparse Visual Domain Prompts (SVDP) approach,\n",
            "which holds minimal trainable parameters (e.g., 0.1\\%) in the image-level\n",
            "prompt and reserves more spatial information of the input. To better apply SVDP\n",
            "in extracting domain-specific knowledge, we introduce the Domain Prompt\n",
            "Placement (DPP) method to adaptively allocates trainable parameters of SVDP on\n",
            "the pixels with large distribution shifts. Furthermore, recognizing that each\n",
            "target domain sample exhibits a unique domain shift, we design Domain Prompt\n",
            "Updating (DPU) strategy to optimize prompt parameters differently for each\n",
            "sample, facilitating efficient adaptation to the target domain. Extensive\n",
            "experiments were conducted on widely-used TTA and continual TTA benchmarks, and\n",
            "our proposed method achieves state-of-the-art performance in both semantic\n",
            "segmentation and depth estimation tasks.\n",
            "\n",
            "1436. Title: Large Language Models Are Not Robust Multiple Choice Selectors\n",
            "   Abstract: In this paper, we propose a novel probabilistic self-supervised learning via\n",
            "Scoring Rule Minimization (ProSMIN), which leverages the power of probabilistic\n",
            "models to enhance representation quality and mitigate collapsing\n",
            "representations. Our proposed approach involves two neural networks; the online\n",
            "network and the target network, which collaborate and learn the diverse\n",
            "distribution of representations from each other through knowledge distillation.\n",
            "By presenting the input samples in two augmented formats, the online network is\n",
            "trained to predict the target network representation of the same sample under a\n",
            "different augmented view. The two networks are trained via our new loss\n",
            "function based on proper scoring rules. We provide a theoretical justification\n",
            "for ProSMIN's convergence, demonstrating the strict propriety of its modified\n",
            "scoring rule. This insight validates the method's optimization process and\n",
            "contributes to its robustness and effectiveness in improving representation\n",
            "quality. We evaluate our probabilistic model on various downstream tasks, such\n",
            "as in-distribution generalization, out-of-distribution detection, dataset\n",
            "corruption, low-shot learning, and transfer learning. Our method achieves\n",
            "superior accuracy and calibration, surpassing the self-supervised baseline in a\n",
            "wide range of experiments on large-scale datasets like ImageNet-O and\n",
            "ImageNet-C, ProSMIN demonstrates its scalability and real-world applicability.\n",
            "\n",
            "1437. Title: Probabilistic Self-supervised Representation Learning via Scoring Rules Minimization\n",
            "   Abstract: We introduce ProteinWorkshop, a comprehensive benchmark suite for\n",
            "representation learning on protein structures with Geometric Graph Neural\n",
            "Networks. We consider large-scale pre-training and downstream tasks on both\n",
            "experimental and predicted structures to enable the systematic evaluation of\n",
            "the quality of the learned structural representation and their usefulness in\n",
            "capturing functional relationships for downstream tasks. We find that: (1)\n",
            "large-scale pretraining on AlphaFold structures and auxiliary tasks\n",
            "consistently improve the performance of both rotation-invariant and equivariant\n",
            "GNNs, and (2) more expressive equivariant GNNs benefit from pretraining to a\n",
            "greater extent compared to invariant models. We aim to establish a common\n",
            "ground for the machine learning and computational biology communities to\n",
            "rigorously compare and advance protein structure representation learning. Our\n",
            "open-source codebase reduces the barrier to entry for working with large\n",
            "protein structure datasets by providing: (1) storage-efficient dataloaders for\n",
            "large-scale structural databases including AlphaFoldDB and ESM Atlas, as well\n",
            "as (2) utilities for constructing new tasks from the entire PDB.\n",
            "ProteinWorkshop is available at: github.com/a-r-j/ProteinWorkshop.\n",
            "\n",
            "1438. Title: Whole-Song Hierarchical Generation of Symbolic Music Using Cascaded Diffusion Models\n",
            "   Abstract: Multiple choice questions (MCQs) serve as a common yet important task format\n",
            "in the evaluation of large language models (LLMs). This work shows that modern\n",
            "LLMs are vulnerable to option position changes in MCQs due to their inherent\n",
            "\"selection bias\", namely, they prefer to select specific option IDs as answers\n",
            "(like \"Option A\"). Through extensive empirical analyses with 20 LLMs on three\n",
            "benchmarks, we pinpoint that this behavioral bias primarily stems from LLMs'\n",
            "token bias, where the model a priori assigns more probabilistic mass to\n",
            "specific option ID tokens (e.g., A/B/C/D) when predicting answers from the\n",
            "option IDs. To mitigate selection bias, we propose a label-free, inference-time\n",
            "debiasing method, called PriDe, which separates the model's prior bias for\n",
            "option IDs from the overall prediction distribution. PriDe first estimates the\n",
            "prior by permutating option contents on a small number of test samples, and\n",
            "then applies the estimated prior to debias the remaining samples. We\n",
            "demonstrate that it achieves interpretable and transferable debiasing with high\n",
            "computational efficiency. We hope this work can draw broader research attention\n",
            "to the bias and robustness of modern LLMs.\n",
            "\n",
            "1439. Title: Likelihood Training of Cascaded Diffusion Models via Hierarchical Volume-preserving Maps\n",
            "   Abstract: In theoretical neuroscience, recent work leverages deep learning tools to\n",
            "explore how some network attributes critically influence its learning dynamics.\n",
            "Notably, initial weight distributions with small (resp. large) variance may\n",
            "yield a rich (resp. lazy) regime, where significant (resp. minor) changes to\n",
            "network states and representation are observed over the course of learning.\n",
            "However, in biology, neural circuit connectivity could exhibit a low-rank\n",
            "structure and therefore differs markedly from the random initializations\n",
            "generally used for these studies. As such, here we investigate how the\n",
            "structure of the initial weights -- in particular their effective rank --\n",
            "influences the network learning regime. Through both empirical and theoretical\n",
            "analyses, we discover that high-rank initializations typically yield smaller\n",
            "network changes indicative of lazier learning, a finding we also confirm with\n",
            "experimentally-driven initial connectivity in recurrent neural networks.\n",
            "Conversely, low-rank initialization biases learning towards richer learning.\n",
            "Importantly, however, as an exception to this rule, we find lazier learning can\n",
            "still occur with a low-rank initialization that aligns with task and data\n",
            "statistics. Our research highlights the pivotal role of initial weight\n",
            "structures in shaping learning regimes, with implications for metabolic costs\n",
            "of plasticity and risks of catastrophic forgetting.\n",
            "\n",
            "1440. Title: Measuring Vision-Language STEM Skills of Neural Models\n",
            "   Abstract: Recent deep music generation studies have put much emphasis on long-term\n",
            "generation with structures. However, we are yet to see high-quality,\n",
            "well-structured whole-song generation. In this paper, we make the first attempt\n",
            "to model a full music piece under the realization of compositional hierarchy.\n",
            "With a focus on symbolic representations of pop songs, we define a hierarchical\n",
            "language, in which each level of hierarchy focuses on the semantics and context\n",
            "dependency at a certain music scope. The high-level languages reveal whole-song\n",
            "form, phrase, and cadence, whereas the low-level languages focus on notes,\n",
            "chords, and their local patterns. A cascaded diffusion model is trained to\n",
            "model the hierarchical language, where each level is conditioned on its upper\n",
            "levels. Experiments and analysis show that our model is capable of generating\n",
            "full-piece music with recognizable global verse-chorus structure and cadences,\n",
            "and the music quality is higher than the baselines. Additionally, we show that\n",
            "the proposed model is controllable in a flexible way. By sampling from the\n",
            "interpretable hierarchical languages or adjusting pre-trained external\n",
            "representations, users can control the music flow via various features such as\n",
            "phrase harmonic structures, rhythmic patterns, and accompaniment texture.\n",
            "\n",
            "1441. Title: VFLAIR: A Research Library and Benchmark for Vertical Federated Learning\n",
            "   Abstract: We introduce a new challenge to test the STEM skills of neural models. The\n",
            "problems in the real world often require solutions, combining knowledge from\n",
            "STEM (science, technology, engineering, and math). Unlike existing datasets,\n",
            "our dataset requires the understanding of multimodal vision-language\n",
            "information of STEM. Our dataset features one of the largest and most\n",
            "comprehensive datasets for the challenge. It includes 448 skills and 1,073,146\n",
            "questions spanning all STEM subjects. Compared to existing datasets that often\n",
            "focus on examining expert-level ability, our dataset includes fundamental\n",
            "skills and questions designed based on the K-12 curriculum. We also add\n",
            "state-of-the-art foundation models such as CLIP and GPT-3.5-Turbo to our\n",
            "benchmark. Results show that the recent model advances only help master a very\n",
            "limited number of lower grade-level skills (2.5% in the third grade) in our\n",
            "dataset. In fact, these models are still well below (averaging 54.7%) the\n",
            "performance of elementary students, not to mention near expert-level\n",
            "performance. To understand and increase the performance on our dataset, we\n",
            "teach the models on a training split of our dataset. Even though we observe\n",
            "improved performance, the model performance remains relatively low compared to\n",
            "average elementary students. To solve STEM problems, we will need novel\n",
            "algorithmic innovations from the community.\n",
            "\n",
            "1442. Title: Real-Fake: Effective Training Data Synthesis Through Distribution Matching\n",
            "   Abstract: Cascaded models are multi-scale generative models with a marked capacity for\n",
            "producing perceptually impressive samples at high resolutions. In this work, we\n",
            "show that they can also be excellent likelihood models, so long as we overcome\n",
            "a fundamental difficulty with probabilistic multi-scale models: the\n",
            "intractability of the likelihood function. Chiefly, in cascaded models each\n",
            "intermediary scale introduces extraneous variables that cannot be tractably\n",
            "marginalized out for likelihood evaluation. This issue vanishes by modeling the\n",
            "diffusion process on latent spaces induced by a class of transformations we\n",
            "call hierarchical volume-preserving maps, which decompose spatially structured\n",
            "data in a hierarchical fashion without introducing local distortions in the\n",
            "latent space. We demonstrate that two such maps are well-known in the\n",
            "literature for multiscale modeling: Laplacian pyramids and wavelet transforms.\n",
            "Not only do such reparameterizations allow the likelihood function to be\n",
            "directly expressed as a joint likelihood over the scales, we show that the\n",
            "Laplacian pyramid and wavelet transform also produces significant improvements\n",
            "to the state-of-the-art on a selection of benchmarks in likelihood modeling,\n",
            "including density estimation, lossless compression, and out-of-distribution\n",
            "detection. Investigating the theoretical basis of our empirical gains we\n",
            "uncover deep connections to score matching under the Earth Mover's Distance\n",
            "(EMD), which is a well-known surrogate for perceptual similarity. Code can be\n",
            "found at \\href{https://github.com/lihenryhfl/pcdm}{this https url}.\n",
            "\n",
            "1443. Title: BatteryML: An Open-source Platform for Machine Learning on Battery Degradation\n",
            "   Abstract: Lattices are architected metamaterials whose properties strongly depend on\n",
            "their geometrical design. The analogy between lattices and graphs enables the\n",
            "use of graph neural networks (GNNs) as a faster surrogate model compared to\n",
            "traditional methods such as finite element modelling. In this work, we generate\n",
            "a big dataset of structure-property relationships for strut-based lattices. The\n",
            "dataset is made available to the community which can fuel the development of\n",
            "methods anchored in physical principles for the fitting of fourth-order\n",
            "tensors. In addition, we present a higher-order GNN model trained on this\n",
            "dataset. The key features of the model are (i) SE(3) equivariance, and (ii)\n",
            "consistency with the thermodynamic law of conservation of energy. We compare\n",
            "the model to non-equivariant models based on a number of error metrics and\n",
            "demonstrate its benefits in terms of predictive performance and reduced\n",
            "training requirements. Finally, we demonstrate an example application of the\n",
            "model to an architected material design task. The methods which we developed\n",
            "are applicable to fourth-order tensors beyond elasticity such as piezo-optical\n",
            "tensor etc.\n",
            "\n",
            "1444. Title: Frozen Transformers in Language Models Are Effective Visual Encoder Layers\n",
            "   Abstract: This paper reveals that large language models (LLMs), despite being trained\n",
            "solely on textual data, are surprisingly strong encoders for purely visual\n",
            "tasks in the absence of language. Even more intriguingly, this can be achieved\n",
            "by a simple yet previously overlooked strategy -- employing a frozen\n",
            "transformer block from pre-trained LLMs as a constituent encoder layer to\n",
            "directly process visual tokens. Our work pushes the boundaries of leveraging\n",
            "LLMs for computer vision tasks, significantly departing from conventional\n",
            "practices that typically necessitate a multi-modal vision-language setup with\n",
            "associated language prompts, inputs, or outputs. We demonstrate that our\n",
            "approach consistently enhances performance across a diverse range of tasks,\n",
            "encompassing pure 2D and 3D visual recognition tasks (e.g., image and point\n",
            "cloud classification), temporal modeling tasks (e.g., action recognition),\n",
            "non-semantic tasks (e.g., motion forecasting), and multi-modal tasks (e.g.,\n",
            "2D/3D visual question answering and image-text retrieval). Such improvements\n",
            "are a general phenomenon, applicable to various types of LLMs (e.g., LLaMA and\n",
            "OPT) and different LLM transformer blocks. We additionally propose the\n",
            "information filtering hypothesis to explain the effectiveness of pre-trained\n",
            "LLMs in visual encoding -- the pre-trained LLM transformer blocks discern\n",
            "informative visual tokens and further amplify their effect. This hypothesis is\n",
            "empirically supported by the observation that the feature activation, after\n",
            "training with LLM transformer blocks, exhibits a stronger focus on relevant\n",
            "regions. We hope that our work inspires new perspectives on utilizing LLMs and\n",
            "deepening our understanding of their underlying mechanisms. Code is available\n",
            "at https://github.com/ziqipang/LM4VisualEncoding.\n",
            "\n",
            "1445. Title: Controlling Vision-Language Models for Multi-Task Image Restoration\n",
            "   Abstract: We propose a new approach to promote safety in classification tasks with\n",
            "established concepts. Our approach -- called a conceptual safeguard -- acts as\n",
            "a verification layer for models that predict a target outcome by first\n",
            "predicting the presence of intermediate concepts. Given this architecture, a\n",
            "safeguard ensures that a model meets a minimal level of accuracy by abstaining\n",
            "from uncertain predictions. In contrast to a standard selective classifier, a\n",
            "safeguard provides an avenue to improve coverage by allowing a human to confirm\n",
            "the presence of uncertain concepts on instances on which it abstains. We\n",
            "develop methods to build safeguards that maximize coverage without compromising\n",
            "safety, namely techniques to propagate the uncertainty in concept predictions\n",
            "and to flag salient concepts for human review. We benchmark our approach on a\n",
            "collection of real-world and synthetic datasets, showing that it can improve\n",
            "performance and coverage in deep learning tasks.\n",
            "\n",
            "1446. Title: Classification with Conceptual Safeguards\n",
            "   Abstract: Large language models (LMs) are capable of generating free-text rationales to\n",
            "aid question answering. However, prior work 1) suggests that useful\n",
            "self-rationalization is emergent only at significant scales (e.g., 175B\n",
            "parameter GPT-3); and 2) focuses largely on downstream performance, ignoring\n",
            "the semantics of the rationales themselves, e.g., are they faithful, true, and\n",
            "helpful for humans? In this work, we enable small-scale LMs (approx. 200x\n",
            "smaller than GPT-3) to generate rationales that not only improve downstream\n",
            "task performance, but are also more plausible, consistent, and diverse,\n",
            "assessed both by automatic and human evaluation. Our method, MaRio\n",
            "(Multi-rewArd RatIOnalization), is a multi-reward conditioned\n",
            "self-rationalization algorithm that optimizes multiple distinct properties like\n",
            "plausibility, diversity and consistency. Results on five difficult\n",
            "question-answering datasets StrategyQA, QuaRel, OpenBookQA, NumerSense and QASC\n",
            "show that not only does MaRio improve task accuracy, but it also improves the\n",
            "self-rationalization quality of small LMs across the aforementioned axes better\n",
            "than a supervised fine-tuning (SFT) baseline. Extensive human evaluations\n",
            "confirm that MaRio rationales are preferred vs. SFT rationales, as well as\n",
            "qualitative improvements in plausibility and consistency.\n",
            "\n",
            "1447. Title: Tailoring Self-Rationalizers with Multi-Reward Distillation\n",
            "   Abstract: Language models produce a distribution over the next token; can we use this\n",
            "information to recover the prompt tokens? We consider the problem of language\n",
            "model inversion and show that next-token probabilities contain a surprising\n",
            "amount of information about the preceding text. Often we can recover the text\n",
            "in cases where it is hidden from the user, motivating a method for recovering\n",
            "unknown prompts given only the model's current distribution output. We consider\n",
            "a variety of model access scenarios, and show how even without predictions for\n",
            "every token in the vocabulary we can recover the probability vector through\n",
            "search. On Llama-2 7b, our inversion method reconstructs prompts with a BLEU of\n",
            "$59$ and token-level F1 of $78$ and recovers $27\\%$ of prompts exactly. Code\n",
            "for reproducing all experiments is available at\n",
            "http://github.com/jxmorris12/vec2text.\n",
            "\n",
            "1448. Title: Language Model Inversion\n",
            "   Abstract: Image restoration refers to the process of restoring a damaged low-quality\n",
            "image back to its corresponding high-quality image. Typically, we use\n",
            "convolutional neural networks to directly learn the mapping from low-quality\n",
            "images to high-quality images achieving image restoration. Recently, a special\n",
            "type of diffusion bridge model has achieved more advanced results in image\n",
            "restoration. It can transform the direct mapping from low-quality to\n",
            "high-quality images into a diffusion process, restoring low-quality images\n",
            "through a reverse process. However, the current diffusion bridge restoration\n",
            "models do not emphasize the idea of conditional control, which may affect\n",
            "performance. This paper introduces the ECDB model enhancing the control of the\n",
            "diffusion bridge with low-quality images as conditions. Moreover, in response\n",
            "to the characteristic of diffusion models having low denoising level at larger\n",
            "values of \\(\\bm t \\), we also propose a Conditional Fusion Schedule, which more\n",
            "effectively handles the conditional feature information of various modules.\n",
            "Experimental results prove that the ECDB model has achieved state-of-the-art\n",
            "results in many image restoration tasks, including deraining, inpainting and\n",
            "super-resolution. Code is avaliable at https://github.com/Hammour-steak/ECDB.\n",
            "\n",
            "1449. Title: Partitioning Message Passing for Graph Fraud Detection\n",
            "   Abstract: Deep Generative Models (DGMs) have been shown to be powerful tools for\n",
            "generating tabular data, as they have been increasingly able to capture the\n",
            "complex distributions that characterize them. However, to generate realistic\n",
            "synthetic data, it is often not enough to have a good approximation of their\n",
            "distribution, as it also requires compliance with constraints that encode\n",
            "essential background knowledge on the problem at hand. In this paper, we\n",
            "address this limitation and show how DGMs for tabular data can be transformed\n",
            "into Constrained Deep Generative Models (C-DGMs), whose generated samples are\n",
            "guaranteed to be compliant with the given constraints. This is achieved by\n",
            "automatically parsing the constraints and transforming them into a Constraint\n",
            "Layer (CL) seamlessly integrated with the DGM. Our extensive experimental\n",
            "analysis with various DGMs and tasks reveals that standard DGMs often violate\n",
            "constraints, some exceeding $95\\%$ non-compliance, while their corresponding\n",
            "C-DGMs are never non-compliant. Then, we quantitatively demonstrate that, at\n",
            "training time, C-DGMs are able to exploit the background knowledge expressed by\n",
            "the constraints to outperform their standard counterparts with up to $6.5\\%$\n",
            "improvement in utility and detection. Further, we show how our CL does not\n",
            "necessarily need to be integrated at training time, as it can be also used as a\n",
            "guardrail at inference time, still producing some improvements in the overall\n",
            "performance of the models. Finally, we show that our CL does not hinder the\n",
            "sample generation time of the models.\n",
            "\n",
            "1450. Title: Generalization of Scaled Deep ResNets in the Mean-Field Regime\n",
            "   Abstract: Diffusion models are the de facto approach for generating high-quality images\n",
            "and videos, but learning high-dimensional models remains a formidable task due\n",
            "to computational and optimization challenges. Existing methods often resort to\n",
            "training cascaded models in pixel space or using a downsampled latent space of\n",
            "a separately trained auto-encoder. In this paper, we introduce Matryoshka\n",
            "Diffusion Models(MDM), an end-to-end framework for high-resolution image and\n",
            "video synthesis. We propose a diffusion process that denoises inputs at\n",
            "multiple resolutions jointly and uses a NestedUNet architecture where features\n",
            "and parameters for small-scale inputs are nested within those of large scales.\n",
            "In addition, MDM enables a progressive training schedule from lower to higher\n",
            "resolutions, which leads to significant improvements in optimization for\n",
            "high-resolution generation. We demonstrate the effectiveness of our approach on\n",
            "various benchmarks, including class-conditioned image generation,\n",
            "high-resolution text-to-image, and text-to-video applications. Remarkably, we\n",
            "can train a single pixel-space model at resolutions of up to 1024x1024 pixels,\n",
            "demonstrating strong zero-shot generalization using the CC12M dataset, which\n",
            "contains only 12 million images. Our code is released at\n",
            "https://github.com/apple/ml-mdm\n",
            "\n",
            "1451. Title: Matryoshka Diffusion Models\n",
            "   Abstract: Generalizing to out-of-distribution (OOD) data or unseen domain, termed OOD\n",
            "generalization, still lacks appropriate theoretical guarantees. Canonical OOD\n",
            "bounds focus on different distance measurements between source and target\n",
            "domains but fail to consider the optimization property of the learned model. As\n",
            "empirically shown in recent work, the sharpness of learned minima influences\n",
            "OOD generalization. To bridge this gap between optimization and OOD\n",
            "generalization, we study the effect of sharpness on how a model tolerates data\n",
            "change in domain shift which is usually captured by \"robustness\" in\n",
            "generalization. In this paper, we give a rigorous connection between sharpness\n",
            "and robustness, which gives better OOD guarantees for robust algorithms. It\n",
            "also provides a theoretical backing for \"flat minima leads to better OOD\n",
            "generalization\". Overall, we propose a sharpness-based OOD generalization bound\n",
            "by taking robustness into consideration, resulting in a tighter bound than\n",
            "non-robust guarantees. Our findings are supported by the experiments on a ridge\n",
            "regression model, as well as the experiments on deep learning classification\n",
            "tasks.\n",
            "\n",
            "1452. Title: Towards Robust Out-of-Distribution Generalization Bounds via Sharpness\n",
            "   Abstract: Offline reinforcement learning (RL) holds promise as a means to learn\n",
            "high-reward policies from a static dataset, without the need for further\n",
            "environment interactions. However, a key challenge in offline RL lies in\n",
            "effectively stitching portions of suboptimal trajectories from the static\n",
            "dataset while avoiding extrapolation errors arising due to a lack of support in\n",
            "the dataset. Existing approaches use conservative methods that are tricky to\n",
            "tune and struggle with multi-modal data (as we show) or rely on noisy Monte\n",
            "Carlo return-to-go samples for reward conditioning. In this work, we propose a\n",
            "novel approach that leverages the expressiveness of latent diffusion to model\n",
            "in-support trajectory sequences as compressed latent skills. This facilitates\n",
            "learning a Q-function while avoiding extrapolation error via\n",
            "batch-constraining. The latent space is also expressive and gracefully copes\n",
            "with multi-modal data. We show that the learned temporally-abstract latent\n",
            "space encodes richer task-specific information for offline RL tasks as compared\n",
            "to raw state-actions. This improves credit assignment and facilitates faster\n",
            "reward propagation during Q-learning. Our method demonstrates state-of-the-art\n",
            "performance on the D4RL benchmarks, particularly excelling in long-horizon,\n",
            "sparse-reward tasks.\n",
            "\n",
            "1453. Title: Text2Reward: Reward Shaping with Language Models for Reinforcement Learning\n",
            "   Abstract: In this paper, we investigate the problem of offline Preference-based\n",
            "Reinforcement Learning (PbRL) with human feedback where feedback is available\n",
            "in the form of preference between trajectory pairs rather than explicit\n",
            "rewards. Our proposed algorithm consists of two main steps: (1) estimate the\n",
            "implicit reward using Maximum Likelihood Estimation (MLE) with general function\n",
            "approximation from offline data and (2) solve a distributionally robust\n",
            "planning problem over a confidence set around the MLE. We consider the general\n",
            "reward setting where the reward can be defined over the whole trajectory and\n",
            "provide a novel guarantee that allows us to learn any target policy with a\n",
            "polynomial number of samples, as long as the target policy is covered by the\n",
            "offline data. This guarantee is the first of its kind with general function\n",
            "approximation. To measure the coverage of the target policy, we introduce a new\n",
            "single-policy concentrability coefficient, which can be upper bounded by the\n",
            "per-trajectory concentrability coefficient. We also establish lower bounds that\n",
            "highlight the necessity of such concentrability and the difference from\n",
            "standard RL, where state-action-wise rewards are directly observed. We further\n",
            "extend and analyze our algorithm when the feedback is given over action pairs.\n",
            "\n",
            "1454. Title: Learning Conditional Invariances through Non-Commutativity\n",
            "   Abstract: Label imbalance and homophily-heterophily mixture are the fundamental\n",
            "problems encountered when applying Graph Neural Networks (GNNs) to Graph Fraud\n",
            "Detection (GFD) tasks. Existing GNN-based GFD models are designed to augment\n",
            "graph structure to accommodate the inductive bias of GNNs towards homophily, by\n",
            "excluding heterophilic neighbors during message passing. In our work, we argue\n",
            "that the key to applying GNNs for GFD is not to exclude but to {\\em\n",
            "distinguish} neighbors with different labels. Grounded in this perspective, we\n",
            "introduce Partitioning Message Passing (PMP), an intuitive yet effective\n",
            "message passing paradigm expressly crafted for GFD. Specifically, in the\n",
            "neighbor aggregation stage of PMP, neighbors with different classes are\n",
            "aggregated with distinct node-specific aggregation functions. By this means,\n",
            "the center node can adaptively adjust the information aggregated from its\n",
            "heterophilic and homophilic neighbors, thus avoiding the model gradient being\n",
            "dominated by benign nodes which occupy the majority of the population. We\n",
            "theoretically establish a connection between the spatial formulation of PMP and\n",
            "spectral analysis to characterize that PMP operates an adaptive node-specific\n",
            "spectral graph filter, which demonstrates the capability of PMP to handle\n",
            "heterophily-homophily mixed graphs. Extensive experimental results show that\n",
            "PMP can significantly boost the performance on GFD tasks.\n",
            "\n",
            "1455. Title: Generative Modeling with Phase Stochastic Bridge\n",
            "   Abstract: In this paper, we investigate properties and limitations of invariance\n",
            "learned by neural networks from the data compared to the genuine invariance\n",
            "achieved through invariant weight-tying. To do so, we adopt a group theoretical\n",
            "perspective and analyze invariance learning in neural networks without\n",
            "weight-tying constraints. We demonstrate that even when a network learns to\n",
            "correctly classify samples on a group orbit, the underlying decision-making in\n",
            "such a model does not attain genuine invariance. Instead, learned invariance is\n",
            "strongly conditioned on the input data, rendering it unreliable if the input\n",
            "distribution shifts. We next demonstrate how to guide invariance learning\n",
            "toward genuine invariance by regularizing the invariance of a model at the\n",
            "training. To this end, we propose several metrics to quantify learned\n",
            "invariance: (i) predictive distribution invariance, (ii) logit invariance, and\n",
            "(iii) saliency invariance similarity. We show that the invariance learned with\n",
            "the invariance error regularization closely reassembles the genuine invariance\n",
            "of weight-tying models and reliably holds even under a severe input\n",
            "distribution shift. Closer analysis of the learned invariance also reveals the\n",
            "spectral decay phenomenon, when a network chooses to achieve the invariance to\n",
            "a specific transformation group by reducing the sensitivity to any input\n",
            "perturbation.\n",
            "\n",
            "1456. Title: Provable Offline Preference-Based Reinforcement Learning\n",
            "   Abstract: Diffusion models (DMs) represent state-of-the-art generative models for\n",
            "continuous inputs. DMs work by constructing a Stochastic Differential Equation\n",
            "(SDE) in the input space (ie, position space), and using a neural network to\n",
            "reverse it. In this work, we introduce a novel generative modeling framework\n",
            "grounded in \\textbf{phase space dynamics}, where a phase space is defined as\n",
            "{an augmented space encompassing both position and velocity.} Leveraging\n",
            "insights from Stochastic Optimal Control, we construct a path measure in the\n",
            "phase space that enables efficient sampling. {In contrast to DMs, our framework\n",
            "demonstrates the capability to generate realistic data points at an early stage\n",
            "of dynamics propagation.} This early prediction sets the stage for efficient\n",
            "data generation by leveraging additional velocity information along the\n",
            "trajectory. On standard image generation benchmarks, our model yields favorable\n",
            "performance over baselines in the regime of small Number of Function\n",
            "Evaluations (NFEs). Furthermore, our approach rivals the performance of\n",
            "diffusion models equipped with efficient sampling techniques, underscoring its\n",
            "potential as a new tool generative modeling.\n",
            "\n",
            "1457. Title: GraphCare: Enhancing Healthcare Predictions with Personalized Knowledge Graphs\n",
            "   Abstract: Despite the widespread empirical success of ResNet, the generalization\n",
            "properties of deep ResNet are rarely explored beyond the lazy training regime.\n",
            "In this work, we investigate \\emph{scaled} ResNet in the limit of infinitely\n",
            "deep and wide neural networks, of which the gradient flow is described by a\n",
            "partial differential equation in the large-neural network limit, i.e., the\n",
            "\\emph{mean-field} regime. To derive the generalization bounds under this\n",
            "setting, our analysis necessitates a shift from the conventional time-invariant\n",
            "Gram matrix employed in the lazy training regime to a time-variant,\n",
            "distribution-dependent version. To this end, we provide a global lower bound on\n",
            "the minimum eigenvalue of the Gram matrix under the mean-field regime. Besides,\n",
            "for the traceability of the dynamic of Kullback-Leibler (KL) divergence, we\n",
            "establish the linear convergence of the empirical error and estimate the upper\n",
            "bound of the KL divergence over parameters distribution. Finally, we build the\n",
            "uniform convergence for generalization bound via Rademacher complexity. Our\n",
            "results offer new insights into the generalization ability of deep ResNet\n",
            "beyond the lazy training regime and contribute to advancing the understanding\n",
            "of the fundamental properties of deep neural networks.\n",
            "\n",
            "1458. Title: $\\mathbb{D}^2$ Pruning: Message Passing for Balancing Diversity & Difficulty in Data Pruning\n",
            "   Abstract: Most people participate in meetings almost every day, multiple times a day.\n",
            "The study of meetings is important, but also challenging, as it requires an\n",
            "understanding of social signals and complex interpersonal dynamics. Our aim\n",
            "this work is to use a data-driven approach to the science of meetings. We\n",
            "provide tentative evidence that: i) it is possible to automatically detect when\n",
            "during the meeting a key decision is taking place, from analyzing only the\n",
            "local dialogue acts, ii) there are common patterns in the way social dialogue\n",
            "acts are interspersed throughout a meeting, iii) at the time key decisions are\n",
            "made, the amount of time left in the meeting can be predicted from the amount\n",
            "of time that has passed, iv) it is often possible to predict whether a proposal\n",
            "during a meeting will be accepted or rejected based entirely on the language\n",
            "(the set of persuasive words) used by the speaker.\n",
            "\n",
            "1459. Title: Compositional Preference Models for Aligning LMs\n",
            "   Abstract: Exploration in high-dimensional, continuous spaces with sparse rewards is an\n",
            "open problem in reinforcement learning. Artificial curiosity algorithms address\n",
            "this by creating rewards that lead to exploration. Given a reinforcement\n",
            "learning algorithm capable of maximizing rewards, the problem reduces to\n",
            "finding an optimization objective consistent with exploration. Maximum entropy\n",
            "exploration uses the entropy of the state visitation distribution as such an\n",
            "objective. However, efficiently estimating the entropy of the state visitation\n",
            "distribution is challenging in high-dimensional, continuous spaces. We\n",
            "introduce an artificial curiosity algorithm based on lower bounding an\n",
            "approximation to the entropy of the state visitation distribution. The bound\n",
            "relies on a result we prove for non-parametric density estimation in arbitrary\n",
            "dimensions using k-means. We show that our approach is both computationally\n",
            "efficient and competitive on benchmarks for exploration in high-dimensional,\n",
            "continuous spaces, especially on tasks where reinforcement learning algorithms\n",
            "are unable to find rewards.\n",
            "\n",
            "1460. Title: Social Reward: Evaluating and Enhancing Generative AI through Million-User Feedback from an Online Creative Community\n",
            "   Abstract: As language models (LMs) become more capable, it is increasingly important to\n",
            "align them with human preferences. However, the dominant paradigm for training\n",
            "Preference Models (PMs) for that purpose suffers from fundamental limitations,\n",
            "such as lack of transparency and scalability, along with susceptibility to\n",
            "overfitting the preference dataset. We propose Compositional Preference Models\n",
            "(CPMs), a novel PM framework that decomposes one global preference assessment\n",
            "into several interpretable features, obtains scalar scores for these features\n",
            "from a prompted LM, and aggregates these scores using a logistic regression\n",
            "classifier. Through these simple steps, CPMs allow to control which properties\n",
            "of the preference data are used to train the preference model and to build it\n",
            "based on features that are believed to underlie the human preference judgment.\n",
            "Our experiments show that CPMs not only improve generalization and are more\n",
            "robust to overoptimization than standard PMs, but also that best-of-n samples\n",
            "obtained using CPMs tend to be preferred over samples obtained using\n",
            "conventional PMs. Overall, our approach demonstrates the benefits of endowing\n",
            "PMs with priors about which features determine human preferences while relying\n",
            "on LM capabilities to extract those features in a scalable and robust way.\n",
            "\n",
            "1461. Title: Motif: Intrinsic Motivation from Artificial Intelligence Feedback\n",
            "   Abstract: One-shot Federated Learning (OFL) has become a promising learning paradigm,\n",
            "enabling the training of a global server model via a single communication\n",
            "round. In OFL, the server model is aggregated by distilling knowledge from all\n",
            "client models (the ensemble), which are also responsible for synthesizing\n",
            "samples for distillation. In this regard, advanced works show that the\n",
            "performance of the server model is intrinsically related to the quality of the\n",
            "synthesized data and the ensemble model. To promote OFL, we introduce a novel\n",
            "framework, Co-Boosting, in which synthesized data and the ensemble model\n",
            "mutually enhance each other progressively. Specifically, Co-Boosting leverages\n",
            "the current ensemble model to synthesize higher-quality samples in an\n",
            "adversarial manner. These hard samples are then employed to promote the quality\n",
            "of the ensemble model by adjusting the ensembling weights for each client\n",
            "model. Consequently, Co-Boosting periodically achieves high-quality data and\n",
            "ensemble models. Extensive experiments demonstrate that Co-Boosting can\n",
            "substantially outperform existing baselines under various settings. Moreover,\n",
            "Co-Boosting eliminates the need for adjustments to the client's local training,\n",
            "requires no additional data or model transmission, and allows client models to\n",
            "have heterogeneous architectures.\n",
            "\n",
            "1462. Title: Maximum Entropy Heterogeneous-Agent Reinforcement Learning\n",
            "   Abstract: Social reward as a form of community recognition provides a strong source of\n",
            "motivation for users of online platforms to engage and contribute with content.\n",
            "The recent progress of text-conditioned image synthesis has ushered in a\n",
            "collaborative era where AI empowers users to craft original visual artworks\n",
            "seeking community validation. Nevertheless, assessing these models in the\n",
            "context of collective community preference introduces distinct challenges.\n",
            "Existing evaluation methods predominantly center on limited size user studies\n",
            "guided by image quality and prompt alignment. This work pioneers a paradigm\n",
            "shift, unveiling Social Reward - an innovative reward modeling framework that\n",
            "leverages implicit feedback from social network users engaged in creative\n",
            "editing of generated images. We embark on an extensive journey of dataset\n",
            "curation and refinement, drawing from Picsart: an online visual creation and\n",
            "editing platform, yielding a first million-user-scale dataset of implicit human\n",
            "preferences for user-generated visual art named Picsart Image-Social. Our\n",
            "analysis exposes the shortcomings of current metrics in modeling community\n",
            "creative preference of text-to-image models' outputs, compelling us to\n",
            "introduce a novel predictive model explicitly tailored to address these\n",
            "limitations. Rigorous quantitative experiments and user study show that our\n",
            "Social Reward model aligns better with social popularity than existing metrics.\n",
            "Furthermore, we utilize Social Reward to fine-tune text-to-image models,\n",
            "yielding images that are more favored by not only Social Reward, but also other\n",
            "established metrics. These findings highlight the relevance and effectiveness\n",
            "of Social Reward in assessing community appreciation for AI-generated artworks,\n",
            "establishing a closer alignment with users' creative goals: creating popular\n",
            "visual art. Codes can be accessed at\n",
            "https://github.com/Picsart-AI-Research/Social-Reward\n",
            "\n",
            "1463. Title: Does CLIP’s generalization performance mainly stem from high train-test similarity?\n",
            "   Abstract: Recently, Chapman et al. argued that holographic complexities for defects\n",
            "distinguish action from volume. Motivated by their work, we study complexity of\n",
            "quantum states in conformal field theory with boundary. In generic\n",
            "two-dimensional BCFT, we work on the path-integral optimization which gives one\n",
            "of field-theoretic definitions for the complexity. We also perform holographic\n",
            "computations of the complexity in Takayanagi's AdS/BCFT model following by the\n",
            "\"complexity $=$ volume\" conjecture and \"complexity $=$ action\" conjecture. We\n",
            "find that increments of the complexity due to the boundary show the same\n",
            "divergent structures in these models except for the CA complexity in the\n",
            "AdS$_3$/BCFT$_2$ model as the argument by Chapman et al. Thus, we conclude that\n",
            "boundary does not distinguish the complexities in general.\n",
            "\n",
            "1464. Title: Diffusion Posterior Sampling for Linear Inverse Problem Solving: A Filtering Perspective\n",
            "   Abstract: Many interesting tasks in image restoration can be cast as linear inverse\n",
            "problems. A recent family of approaches for solving these problems uses\n",
            "stochastic algorithms that sample from the posterior distribution of natural\n",
            "images given the measurements. However, efficient solutions often require\n",
            "problem-specific supervised training to model the posterior, whereas\n",
            "unsupervised methods that are not problem-specific typically rely on\n",
            "inefficient iterative methods. This work addresses these issues by introducing\n",
            "Denoising Diffusion Restoration Models (DDRM), an efficient, unsupervised\n",
            "posterior sampling method. Motivated by variational inference, DDRM takes\n",
            "advantage of a pre-trained denoising diffusion generative model for solving any\n",
            "linear inverse problem. We demonstrate DDRM's versatility on several image\n",
            "datasets for super-resolution, deblurring, inpainting, and colorization under\n",
            "various amounts of measurement noise. DDRM outperforms the current leading\n",
            "unsupervised methods on the diverse ImageNet dataset in reconstruction quality,\n",
            "perceptual quality, and runtime, being 5x faster than the nearest competitor.\n",
            "DDRM also generalizes well for natural images out of the distribution of the\n",
            "observed ImageNet training set.\n",
            "\n",
            "1465. Title: Quick-Tune: Quickly Learning Which Pretrained Model to Finetune and How\n",
            "   Abstract: As research in large language models (LLMs) continues to accelerate,\n",
            "LLM-based evaluation has emerged as a scalable and cost-effective alternative\n",
            "to human evaluations for comparing the ever increasing list of models. This\n",
            "paper investigates the efficacy of these ``LLM evaluators'', particularly in\n",
            "using them to assess instruction following, a metric that gauges how closely\n",
            "generated text adheres to the given instruction. We introduce a challenging\n",
            "meta-evaluation benchmark, LLMBar, designed to test the ability of an LLM\n",
            "evaluator in discerning instruction-following outputs. The authors manually\n",
            "curated 419 pairs of outputs, one adhering to instructions while the other\n",
            "diverging, yet may possess deceptive qualities that mislead an LLM evaluator,\n",
            "e.g., a more engaging tone. Contrary to existing meta-evaluation, we discover\n",
            "that different evaluators (i.e., combinations of LLMs and prompts) exhibit\n",
            "distinct performance on LLMBar and even the highest-scoring ones have\n",
            "substantial room for improvement. We also present a novel suite of prompting\n",
            "strategies that further close the gap between LLM and human evaluators. With\n",
            "LLMBar, we hope to offer more insight into LLM evaluators and foster future\n",
            "research in developing better instruction-following models.\n",
            "\n",
            "1466. Title: Evaluating Large Language Models at Evaluating Instruction Following\n",
            "   Abstract: While VideoQA Transformer models demonstrate competitive performance on\n",
            "standard benchmarks, the reasons behind their success are not fully understood.\n",
            "Do these models capture the rich multimodal structures and dynamics from video\n",
            "and text jointly? Or are they achieving high scores by exploiting biases and\n",
            "spurious features? Hence, to provide insights, we design $\\textit{QUAG}$\n",
            "(QUadrant AveraGe), a lightweight and non-parametric probe, to conduct\n",
            "dataset-model combined representation analysis by impairing modality fusion. We\n",
            "find that the models achieve high performance on many datasets without\n",
            "leveraging multimodal representations. To validate QUAG further, we design\n",
            "$\\textit{QUAG-attention}$, a less-expressive replacement of self-attention with\n",
            "restricted token interactions. Models with QUAG-attention achieve similar\n",
            "performance with significantly fewer multiplication operations without any\n",
            "finetuning. Our findings raise doubts about the current models' abilities to\n",
            "learn highly-coupled multimodal representations. Hence, we design the\n",
            "$\\textit{CLAVI}$ (Complements in LAnguage and VIdeo) dataset, a stress-test\n",
            "dataset curated by augmenting real-world videos to have high modality coupling.\n",
            "Consistent with the findings of QUAG, we find that most of the models achieve\n",
            "near-trivial performance on CLAVI. This reasserts the limitations of current\n",
            "models for learning highly-coupled multimodal representations, that is not\n",
            "evaluated by the current datasets (project page:\n",
            "https://dissect-videoqa.github.io ).\n",
            "\n",
            "1467. Title: Connect, Collapse, Corrupt: Learning Cross-Modal Tasks with Uni-Modal Data\n",
            "   Abstract: Large language models (LLMs) can perform impressive feats with in-context\n",
            "learning or lightweight finetuning. It is natural to wonder how well these\n",
            "models adapt to genuinely new tasks, but how does one find tasks that are\n",
            "unseen in internet-scale training sets? We turn to a field that is explicitly\n",
            "motivated and bottlenecked by a scarcity of web data: low-resource languages.\n",
            "In this paper, we introduce MTOB (Machine Translation from One Book), a\n",
            "benchmark for learning to translate between English and Kalamang -- a language\n",
            "with less than 200 speakers and therefore virtually no presence on the web --\n",
            "using several hundred pages of field linguistics reference materials. This task\n",
            "framing is novel in that it asks a model to learn a language from a single\n",
            "human-readable book of grammar explanations, rather than a large mined corpus\n",
            "of in-domain data, more akin to L2 learning than L1 acquisition. We demonstrate\n",
            "that baselines using current LLMs are promising but fall short of human\n",
            "performance, achieving 44.7 chrF on Kalamang to English translation and 45.8\n",
            "chrF on English to Kalamang translation, compared to 51.6 and 57.0 chrF by a\n",
            "human who learned Kalamang from the same reference materials. We hope that MTOB\n",
            "will help measure LLM capabilities along a new dimension, and that the methods\n",
            "developed to solve it could help expand access to language technology for\n",
            "underserved communities by leveraging qualitatively different kinds of data\n",
            "than traditional machine translation.\n",
            "\n",
            "1468. Title: Zero Bubble (Almost) Pipeline Parallelism\n",
            "   Abstract: Pipeline parallelism is one of the key components for large-scale distributed\n",
            "training, yet its efficiency suffers from pipeline bubbles which were deemed\n",
            "inevitable. In this work, we introduce a scheduling strategy that, to our\n",
            "knowledge, is the first to successfully achieve zero pipeline bubbles under\n",
            "synchronous training semantics. The key idea behind this improvement is to\n",
            "split the backward computation into two parts, one that computes gradient for\n",
            "the input and another that computes for the parameters. Based on this idea, we\n",
            "handcraft novel pipeline schedules that significantly outperform the baseline\n",
            "methods. We further develop an algorithm that automatically finds an optimal\n",
            "schedule based on specific model configuration and memory limit. Additionally,\n",
            "to truly achieve zero bubble, we introduce a novel technique to bypass\n",
            "synchronizations during the optimizer step. Experimental evaluations show that\n",
            "our method outperforms the 1F1B schedule up to 23% in throughput under a\n",
            "similar memory limit. This number can be further pushed to 31% when the memory\n",
            "constraint is relaxed. We believe our results mark a major step forward in\n",
            "harnessing the true potential of pipeline parallelism. We open sourced our\n",
            "implementation based on the popular Megatron-LM repository on\n",
            "https://github.com/sail-sg/zero-bubble-pipeline-parallelism.\n",
            "\n",
            "1469. Title: SWAP-NAS: Sample-Wise Activation Patterns for Ultra-fast NAS\n",
            "   Abstract: Transformer networks have become the preferred architecture for many tasks\n",
            "due to their state-of-the-art performance. However, the optimal way to\n",
            "implement residual connections in Transformer, which are essential for\n",
            "effective training, is still debated. Two widely used variants are the\n",
            "Post-Layer-Normalization (Post-LN) and Pre-Layer-Normalization (Pre-LN)\n",
            "Transformers, which apply layer normalization after each residual block's\n",
            "output or before each residual block's input, respectively. While both variants\n",
            "enjoy their advantages, they also suffer from severe limitations: Post-LN\n",
            "causes gradient vanishing issue that hinders training deep Transformers, and\n",
            "Pre-LN causes representation collapse issue that limits model capacity. In this\n",
            "paper, we propose ResiDual, a novel Transformer architecture with Pre-Post-LN\n",
            "(PPLN), which fuses the connections in Post-LN and Pre-LN together and inherits\n",
            "their advantages while avoids their limitations. We conduct both theoretical\n",
            "analyses and empirical experiments to verify the effectiveness of ResiDual.\n",
            "Theoretically, we prove that ResiDual has a lower bound on the gradient to\n",
            "avoid the vanishing issue due to the residual connection from Pre-LN. Moreover,\n",
            "ResiDual also has diverse model representations to avoid the collapse issue due\n",
            "to the residual connection from Post-LN. Empirically, ResiDual outperforms both\n",
            "Post-LN and Pre-LN on several machine translation benchmarks across different\n",
            "network depths and data sizes. Thanks to the good theoretical and empirical\n",
            "performance, ResiDual Transformer can serve as a foundation architecture for\n",
            "different AI models (e.g., large language models). Our code is available at\n",
            "https://github.com/microsoft/ResiDual.\n",
            "\n",
            "1470. Title: The Joint Effect of Task Similarity and Overparameterization on Catastrophic Forgetting — An Analytical Model\n",
            "   Abstract: Large Vision-Language Models (LVLMs) have shown significant capability in\n",
            "vision-language understanding. However, one critical issue that persists in\n",
            "these models is sycophancy, which means models are unduly influenced by leading\n",
            "or deceptive prompts, resulting in biased outputs and hallucinations. Despite\n",
            "the progress in LVLMs, evaluating and mitigating sycophancy is yet much\n",
            "under-explored. In this work, we fill this gap by systematically analyzing\n",
            "sycophancy on various VL benchmarks with curated leading queries and further\n",
            "proposing a text contrastive decoding method for mitigation. While the specific\n",
            "sycophantic behavior varies significantly among models, our analysis reveals\n",
            "the severe deficiency of all LVLMs in resilience of sycophancy across various\n",
            "tasks. For improvement, we propose Leading Query Contrastive Decoding (LQCD), a\n",
            "model-agnostic method focusing on calibrating the LVLMs' over-reliance on\n",
            "leading cues by identifying and suppressing the probabilities of sycophancy\n",
            "tokens at the decoding stage. Extensive experiments show that LQCD effectively\n",
            "mitigate sycophancy, outperforming both prompt engineering methods and common\n",
            "methods for hallucination mitigation. We further demonstrate that LQCD does not\n",
            "hurt but even slightly improves LVLMs' responses to neutral queries, suggesting\n",
            "it being a more effective strategy for general-purpose decoding but not limited\n",
            "to sycophancy.\n",
            "\n",
            "1471. Title: Scaling physics-informed hard constraints with mixture-of-experts\n",
            "   Abstract: We consider the general class of time-homogeneous stochastic dynamical\n",
            "systems, both discrete and continuous, and study the problem of learning a\n",
            "representation of the state that faithfully captures its dynamics. This is\n",
            "instrumental to learning the transfer operator or the generator of the system,\n",
            "which in turn can be used for numerous tasks, such as forecasting and\n",
            "interpreting the system dynamics. We show that the search for a good\n",
            "representation can be cast as an optimization problem over neural networks. Our\n",
            "approach is supported by recent results in statistical learning theory,\n",
            "highlighting the role of approximation error and metric distortion in the\n",
            "learning problem. The objective function we propose is associated with\n",
            "projection operators from the representation space to the data space, overcomes\n",
            "metric distortion, and can be empirically estimated from data. In the\n",
            "discrete-time setting, we further derive a relaxed objective function that is\n",
            "differentiable and numerically well-conditioned. We compare our method against\n",
            "state-of-the-art approaches on different datasets, showing better performance\n",
            "across the board.\n",
            "\n",
            "1472. Title: ScaleCrafter: Tuning-free Higher-Resolution Visual Generation with Diffusion Models\n",
            "   Abstract: The energy scale of inflation is of much interest, as it suggests the scale\n",
            "of grand unified physics and also governs whether cosmological events such as\n",
            "topological defect formation can occur after inflation. The COBE results are\n",
            "used to limit the energy scale of inflation at around 60 $e$-foldings from the\n",
            "end of inflation. An exact dynamical treatment based on the Hamilton-Jacobi\n",
            "equations is then used to translate this into limits on the energy scale at the\n",
            "end of inflation. General constraints are given, and then tighter constraints\n",
            "based on physically motivated assumptions regarding the allowed forms of\n",
            "density perturbation and gravitational wave spectra. These are also compared\n",
            "with the values of familiar models.\n",
            "\n",
            "1473. Title: Large Language Models as Generalizable Policies for Embodied Tasks\n",
            "   Abstract: We present ReCAT, a recursive composition augmented Transformer that is able\n",
            "to explicitly model hierarchical syntactic structures of raw texts without\n",
            "relying on gold trees during both learning and inference. Existing research\n",
            "along this line restricts data to follow a hierarchical tree structure and thus\n",
            "lacks inter-span communications. To overcome the problem, we propose a novel\n",
            "contextual inside-outside (CIO) layer that learns contextualized\n",
            "representations of spans through bottom-up and top-down passes, where a\n",
            "bottom-up pass forms representations of high-level spans by composing low-level\n",
            "spans, while a top-down pass combines information inside and outside a span. By\n",
            "stacking several CIO layers between the embedding layer and the attention\n",
            "layers in Transformer, the ReCAT model can perform both deep intra-span and\n",
            "deep inter-span interactions, and thus generate multi-grained representations\n",
            "fully contextualized with other spans. Moreover, the CIO layers can be jointly\n",
            "pre-trained with Transformers, making ReCAT enjoy scaling ability, strong\n",
            "performance, and interpretability at the same time. We conduct experiments on\n",
            "various sentence-level and span-level tasks. Evaluation results indicate that\n",
            "ReCAT can significantly outperform vanilla Transformer models on all span-level\n",
            "tasks and baselines that combine recursive networks with Transformers on\n",
            "natural language inference tasks. More interestingly, the hierarchical\n",
            "structures induced by ReCAT exhibit strong consistency with human-annotated\n",
            "syntactic trees, indicating good interpretability brought by the CIO layers.\n",
            "\n",
            "1474. Title: Adversarial Training on Purification (AToP): Advancing Both Robustness and Generalization\n",
            "   Abstract: We show that large language models (LLMs) can be adapted to be generalizable\n",
            "policies for embodied visual tasks. Our approach, called Large LAnguage model\n",
            "Reinforcement Learning Policy (LLaRP), adapts a pre-trained frozen LLM to take\n",
            "as input text instructions and visual egocentric observations and output\n",
            "actions directly in the environment. Using reinforcement learning, we train\n",
            "LLaRP to see and act solely through environmental interactions. We show that\n",
            "LLaRP is robust to complex paraphrasings of task instructions and can\n",
            "generalize to new tasks that require novel optimal behavior. In particular, on\n",
            "1,000 unseen tasks it achieves 42% success rate, 1.7x the success rate of other\n",
            "common learned baselines or zero-shot applications of LLMs. Finally, to aid the\n",
            "community in studying language conditioned, massively multi-task, embodied AI\n",
            "problems we release a novel benchmark, Language Rearrangement, consisting of\n",
            "150,000 training and 1,000 testing tasks for language-conditioned\n",
            "rearrangement. Video examples of LLaRP in unseen Language Rearrangement\n",
            "instructions are at https://llm-rl.github.io.\n",
            "\n",
            "1475. Title: Augmenting Transformers with Recursively Composed Multi-grained Representations\n",
            "   Abstract: The overall objective of the main project is to propose and develop a system\n",
            "of facial authentication in unlocking phones or applications in phones using\n",
            "facial recognition. The system will include four separate architectures: face\n",
            "detection, face recognition, face spoofing, and classification of closed eyes.\n",
            "In which, we consider the problem of face recognition to be the most important,\n",
            "determining the true identity of the person standing in front of the screen\n",
            "with absolute accuracy is what facial recognition systems need to achieve.\n",
            "Along with the development of the face recognition problem, the problem of the\n",
            "anti-fake face is also gradually becoming popular and equally important. Our\n",
            "goal is to propose and develop two loss functions: LMCot and Double Loss. Then\n",
            "apply them to the face authentication process.\n",
            "\n",
            "1476. Title: Entity-Centric Reinforcement Learning for Object Manipulation from Pixels\n",
            "   Abstract: In recent years, differential privacy has seen significant advancements in\n",
            "image classification; however, its application to video activity recognition\n",
            "remains under-explored. This paper addresses the challenges of applying\n",
            "differential privacy to video activity recognition, which primarily stem from:\n",
            "(1) a discrepancy between the desired privacy level for entire videos and the\n",
            "nature of input data processed by contemporary video architectures, which are\n",
            "typically short, segmented clips; and (2) the complexity and sheer size of\n",
            "video datasets relative to those in image classification, which render\n",
            "traditional differential privacy methods inadequate. To tackle these issues, we\n",
            "propose Multi-Clip DP-SGD, a novel framework for enforcing video-level\n",
            "differential privacy through clip-based classification models. This method\n",
            "samples multiple clips from each video, averages their gradients, and applies\n",
            "gradient clipping in DP-SGD without incurring additional privacy loss.\n",
            "Moreover, we incorporate a parameter-efficient transfer learning strategy to\n",
            "make the model scalable for large-scale video datasets. Through extensive\n",
            "evaluations on the UCF-101 and HMDB-51 datasets, our approach exhibits\n",
            "impressive performance, achieving 81% accuracy with a privacy budget of\n",
            "epsilon=5 on UCF-101, marking a 76% improvement compared to a direct\n",
            "application of DP-SGD. Furthermore, we demonstrate that our transfer learning\n",
            "strategy is versatile and can enhance differentially private image\n",
            "classification across an array of datasets including CheXpert, ImageNet,\n",
            "CIFAR-10, and CIFAR-100.\n",
            "\n",
            "1477. Title: Enhanced Face Recognition using Intra-class Incoherence Constraint\n",
            "   Abstract: Manipulating objects is a hallmark of human intelligence, and an important\n",
            "task in domains such as robotics. In principle, Reinforcement Learning (RL)\n",
            "offers a general approach to learn object manipulation. In practice, however,\n",
            "domains with more than a few objects are difficult for RL agents due to the\n",
            "curse of dimensionality, especially when learning from raw image observations.\n",
            "In this work we propose a structured approach for visual RL that is suitable\n",
            "for representing multiple objects and their interaction, and use it to learn\n",
            "goal-conditioned manipulation of several objects. Key to our method is the\n",
            "ability to handle goals with dependencies between the objects (e.g., moving\n",
            "objects in a certain order). We further relate our architecture to the\n",
            "generalization capability of the trained agent, based on a theoretical result\n",
            "for compositional generalization, and demonstrate agents that learn with 3\n",
            "objects but generalize to similar tasks with over 10 objects. Videos and code\n",
            "are available on the project website:\n",
            "https://sites.google.com/view/entity-centric-rl\n",
            "\n",
            "1478. Title: Differentially Private SGD Without Clipping Bias: An Error-Feedback Approach\n",
            "   Abstract: In continual learning, catastrophic forgetting is affected by multiple\n",
            "aspects of the tasks. Previous works have analyzed separately how forgetting is\n",
            "affected by either task similarity or overparameterization. In contrast, our\n",
            "paper examines how task similarity and overparameterization jointly affect\n",
            "forgetting in an analyzable model. Specifically, we focus on two-task continual\n",
            "linear regression, where the second task is a random orthogonal transformation\n",
            "of an arbitrary first task (an abstraction of random permutation tasks). We\n",
            "derive an exact analytical expression for the expected forgetting - and uncover\n",
            "a nuanced pattern. In highly overparameterized models, intermediate task\n",
            "similarity causes the most forgetting. However, near the interpolation\n",
            "threshold, forgetting decreases monotonically with the expected task\n",
            "similarity. We validate our findings with linear regression on synthetic data,\n",
            "and with neural networks on established permutation task benchmarks.\n",
            "\n",
            "1479. Title: Towards Faithful Explanations: Boosting Rationalization with Shortcuts Discovery\n",
            "   Abstract: The remarkable success in neural networks provokes the selective\n",
            "rationalization. It explains the prediction results by identifying a small\n",
            "subset of the inputs sufficient to support them. Since existing methods still\n",
            "suffer from adopting the shortcuts in data to compose rationales and limited\n",
            "large-scale annotated rationales by human, in this paper, we propose a\n",
            "Shortcuts-fused Selective Rationalization (SSR) method, which boosts the\n",
            "rationalization by discovering and exploiting potential shortcuts.\n",
            "Specifically, SSR first designs a shortcuts discovery approach to detect\n",
            "several potential shortcuts. Then, by introducing the identified shortcuts, we\n",
            "propose two strategies to mitigate the problem of utilizing shortcuts to\n",
            "compose rationales. Finally, we develop two data augmentations methods to close\n",
            "the gap in the number of annotated rationales. Extensive experimental results\n",
            "on real-world datasets clearly validate the effectiveness of our proposed\n",
            "method.\n",
            "\n",
            "1480. Title: GTA: A Geometry-Aware Attention Mechanism for Multi-View Transformers\n",
            "   Abstract: We consider the problem of multi-objective alignment of foundation models\n",
            "with human preferences, which is a critical step towards helpful and harmless\n",
            "AI systems. However, it is generally costly and unstable to fine-tune large\n",
            "foundation models using reinforcement learning (RL), and the\n",
            "multi-dimensionality, heterogeneity, and conflicting nature of human\n",
            "preferences further complicate the alignment process. In this paper, we\n",
            "introduce Rewards-in-Context (RiC), which conditions the response of a\n",
            "foundation model on multiple rewards in its prompt context and applies\n",
            "supervised fine-tuning for alignment. The salient features of RiC are\n",
            "simplicity and adaptivity, as it only requires supervised fine-tuning of a\n",
            "single foundation model and supports dynamic adjustment for user preferences\n",
            "during inference time. Inspired by the analytical solution of an abstracted\n",
            "convex optimization problem, our dynamic inference-time adjustment method\n",
            "approaches the Pareto-optimal solution for multiple objectives. Empirical\n",
            "evidence demonstrates the efficacy of our method in aligning both Large\n",
            "Language Models (LLMs) and diffusion models to accommodate diverse rewards with\n",
            "only around 10% GPU hours compared with multi-objective RL baseline.\n",
            "\n",
            "1481. Title: BEND: Benchmarking DNA Language Models on Biologically Meaningful Tasks\n",
            "   Abstract: Generative flow networks (GFlowNets) are sequential sampling models trained\n",
            "to match a given distribution. GFlowNets have been successfully applied to\n",
            "various structured object generation tasks, sampling a diverse set of\n",
            "high-reward objects quickly. We propose expected flow networks (EFlowNets),\n",
            "which extend GFlowNets to stochastic environments. We show that EFlowNets\n",
            "outperform other GFlowNet formulations in stochastic tasks such as protein\n",
            "design. We then extend the concept of EFlowNets to adversarial environments,\n",
            "proposing adversarial flow networks (AFlowNets) for two-player zero-sum games.\n",
            "We show that AFlowNets learn to find above 80% of optimal moves in Connect-4\n",
            "via self-play and outperform AlphaZero in tournaments.\n",
            "\n",
            "1482. Title: Self-supervised Pocket Pretraining via Protein Fragment-Surroundings Alignment\n",
            "   Abstract: Pocket representations play a vital role in various biomedical applications,\n",
            "such as druggability estimation, ligand affinity prediction, and de novo drug\n",
            "design. While existing geometric features and pretrained representations have\n",
            "demonstrated promising results, they usually treat pockets independent of\n",
            "ligands, neglecting the fundamental interactions between them. However, the\n",
            "limited pocket-ligand complex structures available in the PDB database (less\n",
            "than 100 thousand non-redundant pairs) hampers large-scale pretraining\n",
            "endeavors for interaction modeling. To address this constraint, we propose a\n",
            "novel pocket pretraining approach that leverages knowledge from high-resolution\n",
            "atomic protein structures, assisted by highly effective pretrained small\n",
            "molecule representations. By segmenting protein structures into drug-like\n",
            "fragments and their corresponding pockets, we obtain a reasonable simulation of\n",
            "ligand-receptor interactions, resulting in the generation of over 5 million\n",
            "complexes. Subsequently, the pocket encoder is trained in a contrastive manner\n",
            "to align with the representation of pseudo-ligand furnished by some pretrained\n",
            "small molecule encoders. Our method, named ProFSA, achieves state-of-the-art\n",
            "performance across various tasks, including pocket druggability prediction,\n",
            "pocket matching, and ligand binding affinity prediction. Notably, ProFSA\n",
            "surpasses other pretraining methods by a substantial margin. Moreover, our work\n",
            "opens up a new avenue for mitigating the scarcity of protein-ligand complex\n",
            "data through the utilization of high-quality and diverse protein structure\n",
            "databases.\n",
            "\n",
            "1483. Title: Learning with Mixture of Prototypes for Out-of-Distribution Detection\n",
            "   Abstract: Diffusion models are a powerful class of generative models which simulate\n",
            "stochastic differential equations (SDEs) to generate data from noise. While\n",
            "diffusion models have achieved remarkable progress, they have limitations in\n",
            "unpaired image-to-image (I2I) translation tasks due to the Gaussian prior\n",
            "assumption. Schr\\\"{o}dinger Bridge (SB), which learns an SDE to translate\n",
            "between two arbitrary distributions, have risen as an attractive solution to\n",
            "this problem. Yet, to our best knowledge, none of SB models so far have been\n",
            "successful at unpaired translation between high-resolution images. In this\n",
            "work, we propose Unpaired Neural Schr\\\"{o}dinger Bridge (UNSB), which expresses\n",
            "the SB problem as a sequence of adversarial learning problems. This allows us\n",
            "to incorporate advanced discriminators and regularization to learn a SB between\n",
            "unpaired data. We show that UNSB is scalable and successfully solves various\n",
            "unpaired I2I translation tasks. Code: \\url{https://github.com/cyclomon/UNSB}\n",
            "\n",
            "1484. Title: Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs\n",
            "   Abstract: The recent advancements in large language models (LLMs) have sparked a\n",
            "growing apprehension regarding the potential misuse. One approach to mitigating\n",
            "this risk is to incorporate watermarking techniques into LLMs, allowing for the\n",
            "tracking and attribution of model outputs. This study examines a crucial aspect\n",
            "of watermarking: how significantly watermarks impact the quality of\n",
            "model-generated outputs. Previous studies have suggested a trade-off between\n",
            "watermark strength and output quality. However, our research demonstrates that\n",
            "it is possible to integrate watermarks without affecting the output probability\n",
            "distribution with appropriate implementation. We refer to this type of\n",
            "watermark as an unbiased watermark. This has significant implications for the\n",
            "use of LLMs, as it becomes impossible for users to discern whether a service\n",
            "provider has incorporated watermarks or not. Furthermore, the presence of\n",
            "watermarks does not compromise the performance of the model in downstream\n",
            "tasks, ensuring that the overall utility of the language model is preserved.\n",
            "Our findings contribute to the ongoing discussion around responsible AI\n",
            "development, suggesting that unbiased watermarks can serve as an effective\n",
            "means of tracking and attributing model outputs without sacrificing output\n",
            "quality.\n",
            "\n",
            "1485. Title: Unpaired Image-to-Image Translation via Neural Schrödinger Bridge\n",
            "   Abstract: High-resolution 3D object generation remains a challenging task primarily due\n",
            "to the limited availability of comprehensive annotated training data. Recent\n",
            "advancements have aimed to overcome this constraint by harnessing image\n",
            "generative models, pretrained on extensive curated web datasets, using\n",
            "knowledge transfer techniques like Score Distillation Sampling (SDS).\n",
            "Efficiently addressing the requirements of high-resolution rendering often\n",
            "necessitates the adoption of latent representation-based models, such as the\n",
            "Latent Diffusion Model (LDM). In this framework, a significant challenge\n",
            "arises: To compute gradients for individual image pixels, it is necessary to\n",
            "backpropagate gradients from the designated latent space through the frozen\n",
            "components of the image model, such as the VAE encoder used within LDM.\n",
            "However, this gradient propagation pathway has never been optimized, remaining\n",
            "uncontrolled during training. We find that the unregulated gradients adversely\n",
            "affect the 3D model's capacity in acquiring texture-related information from\n",
            "the image generative model, leading to poor quality appearance synthesis. To\n",
            "address this overarching challenge, we propose an innovative operation termed\n",
            "Pixel-wise Gradient Clipping (PGC) designed for seamless integration into\n",
            "existing 3D generative models, thereby enhancing their synthesis quality.\n",
            "Specifically, we control the magnitude of stochastic gradients by clipping the\n",
            "pixel-wise gradients efficiently, while preserving crucial texture-related\n",
            "gradient directions. Despite this simplicity and minimal extra cost, extensive\n",
            "experiments demonstrate the efficacy of our PGC in enhancing the performance of\n",
            "existing 3D generative models for high-resolution object rendering.\n",
            "\n",
            "1486. Title: In-context Autoencoder for Context Compression in a Large Language Model\n",
            "   Abstract: In this study, we introduce adaptive KV cache compression, a plug-and-play\n",
            "method that reduces the memory footprint of generative inference for Large\n",
            "Language Models (LLMs). Different from the conventional KV cache that retains\n",
            "key and value vectors for all context tokens, we conduct targeted profiling to\n",
            "discern the intrinsic structure of attention modules. Based on the recognized\n",
            "structure, we then construct the KV cache in an adaptive manner: evicting\n",
            "long-range contexts on attention heads emphasizing local contexts, discarding\n",
            "non-special tokens on attention heads centered on special tokens, and only\n",
            "employing the standard KV cache for attention heads that broadly attend to all\n",
            "tokens. Moreover, with the lightweight attention profiling used to guide the\n",
            "construction of the adaptive KV cache, FastGen can be deployed without\n",
            "resource-intensive fine-tuning or re-training. In our experiments across\n",
            "various asks, FastGen demonstrates substantial reduction on GPU memory\n",
            "consumption with negligible generation quality loss. We will release our code\n",
            "and the compatible CUDA kernel for reproducibility.\n",
            "\n",
            "1487. Title: Unbiased Watermark for Large Language Models\n",
            "   Abstract: We introduce contextual stochastic bilevel optimization (CSBO) -- a\n",
            "stochastic bilevel optimization framework with the lower-level problem\n",
            "minimizing an expectation conditioned on some contextual information and the\n",
            "upper-level decision variable. This framework extends classical stochastic\n",
            "bilevel optimization when the lower-level decision maker responds optimally not\n",
            "only to the decision of the upper-level decision maker but also to some side\n",
            "information and when there are multiple or even infinite many followers. It\n",
            "captures important applications such as meta-learning, personalized federated\n",
            "learning, end-to-end learning, and Wasserstein distributionally robust\n",
            "optimization with side information (WDRO-SI). Due to the presence of contextual\n",
            "information, existing single-loop methods for classical stochastic bilevel\n",
            "optimization are unable to converge. To overcome this challenge, we introduce\n",
            "an efficient double-loop gradient method based on the Multilevel Monte-Carlo\n",
            "(MLMC) technique and establish its sample and computational complexities. When\n",
            "specialized to stochastic nonconvex optimization, our method matches existing\n",
            "lower bounds. For meta-learning, the complexity of our method does not depend\n",
            "on the number of tasks. Numerical experiments further validate our theoretical\n",
            "results.\n",
            "\n",
            "1488. Title: Embodied Active Defense: Leveraging Recurrent Feedback to Counter Adversarial Patches\n",
            "   Abstract: The vulnerability of deep neural networks to adversarial patches has\n",
            "motivated numerous defense strategies for boosting model robustness. However,\n",
            "the prevailing defenses depend on single observation or pre-established\n",
            "adversary information to counter adversarial patches, often failing to be\n",
            "confronted with unseen or adaptive adversarial attacks and easily exhibiting\n",
            "unsatisfying performance in dynamic 3D environments. Inspired by active human\n",
            "perception and recurrent feedback mechanisms, we develop Embodied Active\n",
            "Defense (EAD), a proactive defensive strategy that actively contextualizes\n",
            "environmental information to address misaligned adversarial patches in 3D\n",
            "real-world settings. To achieve this, EAD develops two central recurrent\n",
            "sub-modules, i.e., a perception module and a policy module, to implement two\n",
            "critical functions of active vision. These models recurrently process a series\n",
            "of beliefs and observations, facilitating progressive refinement of their\n",
            "comprehension of the target object and enabling the development of strategic\n",
            "actions to counter adversarial patches in 3D environments. To optimize learning\n",
            "efficiency, we incorporate a differentiable approximation of environmental\n",
            "dynamics and deploy patches that are agnostic to the adversary strategies.\n",
            "Extensive experiments demonstrate that EAD substantially enhances robustness\n",
            "against a variety of patches within just a few steps through its action policy\n",
            "in safety-critical tasks (e.g., face recognition and object detection), without\n",
            "compromising standard accuracy. Furthermore, due to the attack-agnostic\n",
            "characteristic, EAD facilitates excellent generalization to unseen attacks,\n",
            "diminishing the averaged attack success rate by 95 percent across a range of\n",
            "unseen adversarial attacks.\n",
            "\n",
            "1489. Title: Implicit Neural Representations and the Algebra of Complex Wavelets\n",
            "   Abstract: LoRA has gained widespread acceptance in the fine-tuning of large pre-trained\n",
            "models to cater to a diverse array of downstream tasks, showcasing notable\n",
            "effectiveness and efficiency, thereby solidifying its position as one of the\n",
            "most prevalent fine-tuning techniques. Due to the modular nature of LoRA's\n",
            "plug-and-play plugins, researchers have delved into the amalgamation of\n",
            "multiple LoRAs to empower models to excel across various downstream tasks.\n",
            "Nonetheless, extant approaches for LoRA fusion grapple with inherent\n",
            "challenges. Direct arithmetic merging may result in the loss of the original\n",
            "pre-trained model's generative capabilities or the distinct identity of LoRAs,\n",
            "thereby yielding suboptimal outcomes. On the other hand, Reference tuning-based\n",
            "fusion exhibits limitations concerning the requisite flexibility for the\n",
            "effective combination of multiple LoRAs. In response to these challenges, this\n",
            "paper introduces the Mixture of LoRA Experts (MoLE) approach, which harnesses\n",
            "hierarchical control and unfettered branch selection. The MoLE approach not\n",
            "only achieves superior LoRA fusion performance in comparison to direct\n",
            "arithmetic merging but also retains the crucial flexibility for combining LoRAs\n",
            "effectively. Extensive experimental evaluations conducted in both the Natural\n",
            "Language Processing (NLP) and Vision & Language (V&L) domains substantiate the\n",
            "efficacy of MoLE.\n",
            "\n",
            "1490. Title: Mathematical Justification of Hard Negative Mining via Isometric Approximation Theorem\n",
            "   Abstract: Addressing the limitation of context length in large language models for\n",
            "code-related tasks is the primary focus of this paper. Existing LLMs are\n",
            "constrained by their pre-trained context lengths, leading to performance issues\n",
            "in handling long complex code sequences. Inspired by how human programmers\n",
            "navigate code, we introduce Hierarchical Rotary Position Embedding (HiRoPE), a\n",
            "novel approach that enhances the traditional rotary position embedding into a\n",
            "hierarchical format based on the hierarchical structure of source code. HiRoPE\n",
            "offers easy integration into existing LLMs without extra training costs. Our\n",
            "method is extensively evaluated with various LLMs, demonstrating stable\n",
            "performance in tasks such as language modeling and long code completion. We\n",
            "also introduce a new long code understanding task with real-world code\n",
            "projects, in hopes of promoting further development in this code-related field.\n",
            "Theoretically and experimentally, we find that HiRoPE also addresses the\n",
            "out-of-distribution issue in position encoding. Our HiRoPE significantly\n",
            "expands the context length capabilities of LLMs, enabling inference at lengths\n",
            "exponentially greater than the training length.\n",
            "\n",
            "1491. Title: A Discretization Framework for Robust Contextual Stochastic Optimization\n",
            "   Abstract: In deep metric learning, the Triplet Loss has emerged as a popular method to\n",
            "learn many computer vision and natural language processing tasks such as facial\n",
            "recognition, object detection, and visual-semantic embeddings. One issue that\n",
            "plagues the Triplet Loss is network collapse, an undesirable phenomenon where\n",
            "the network projects the embeddings of all data onto a single point.\n",
            "Researchers predominately solve this problem by using triplet mining\n",
            "strategies. While hard negative mining is the most effective of these\n",
            "strategies, existing formulations lack strong theoretical justification for\n",
            "their empirical success. In this paper, we utilize the mathematical theory of\n",
            "isometric approximation to show an equivalence between the Triplet Loss sampled\n",
            "by hard negative mining and an optimization problem that minimizes a\n",
            "Hausdorff-like distance between the neural network and its ideal counterpart\n",
            "function. This provides the theoretical justifications for hard negative\n",
            "mining's empirical efficacy. In addition, our novel application of the\n",
            "isometric approximation theorem provides the groundwork for future forms of\n",
            "hard negative mining that avoid network collapse. Our theory can also be\n",
            "extended to analyze other Euclidean space-based metric learning methods like\n",
            "Ladder Loss or Contrastive Learning.\n",
            "\n",
            "1492. Title: Enhancing High-Resolution 3D Generation through Pixel-wise Gradient Clipping\n",
            "   Abstract: Graph Neural Networks (GNNs) are neural models that leverage the dependency\n",
            "structure in graphical data via message passing among the graph nodes. GNNs\n",
            "have emerged as pivotal architectures in analyzing graph-structured data, and\n",
            "their expansive application in sensitive domains requires a comprehensive\n",
            "understanding of their decision-making processes -- necessitating a framework\n",
            "for GNN explainability. An explanation function for GNNs takes a pre-trained\n",
            "GNN along with a graph as input, to produce a `sufficient statistic' subgraph\n",
            "with respect to the graph label. A main challenge in studying GNN\n",
            "explainability is to provide fidelity measures that evaluate the performance of\n",
            "these explanation functions. This paper studies this foundational challenge,\n",
            "spotlighting the inherent limitations of prevailing fidelity metrics, including\n",
            "$Fid_+$, $Fid_-$, and $Fid_\\Delta$. Specifically, a formal,\n",
            "information-theoretic definition of explainability is introduced and it is\n",
            "shown that existing metrics often fail to align with this definition across\n",
            "various statistical scenarios. The reason is due to potential distribution\n",
            "shifts when subgraphs are removed in computing these fidelity measures.\n",
            "Subsequently, a robust class of fidelity measures are introduced, and it is\n",
            "shown analytically that they are resilient to distribution shift issues and are\n",
            "applicable in a wide range of scenarios. Extensive empirical analysis on both\n",
            "synthetic and real datasets are provided to illustrate that the proposed\n",
            "metrics are more coherent with gold standard metrics. The source code is\n",
            "available at https://trustai4s-lab.github.io/fidelity.\n",
            "\n",
            "1493. Title: Hierarchical Context Merging: Better Long Context Understanding for Pre-trained LLMs\n",
            "   Abstract: Current domain-independent, classical planners require symbolic models of the\n",
            "problem domain and instance as input, resulting in a knowledge acquisition\n",
            "bottleneck. Meanwhile, although deep learning has achieved significant success\n",
            "in many fields, the knowledge is encoded in a subsymbolic representation which\n",
            "is incompatible with symbolic systems such as planners. We propose LatPlan, an\n",
            "unsupervised architecture combining deep learning and classical planning. Given\n",
            "only an unlabeled set of image pairs showing a subset of transitions allowed in\n",
            "the environment (training inputs), and a pair of images representing the\n",
            "initial and the goal states (planning inputs), LatPlan finds a plan to the goal\n",
            "state in a symbolic latent space and returns a visualized plan execution. The\n",
            "contribution of this paper is twofold: (1) State Autoencoder, which finds a\n",
            "propositional state representation of the environment using a Variational\n",
            "Autoencoder. It generates a discrete latent vector from the images, based on\n",
            "which a PDDL model can be constructed and then solved by an off-the-shelf\n",
            "planner. (2) Action Autoencoder / Discriminator, a neural architecture which\n",
            "jointly finds the action symbols and the implicit action models\n",
            "(preconditions/effects), and provides a successor function for the implicit\n",
            "graph search. We evaluate LatPlan using image-based versions of 3 planning\n",
            "domains: 8-puzzle, Towers of Hanoi and LightsOut.\n",
            "\n",
            "1494. Title: GROOT: Learning to Follow Instructions by Watching Gameplay Videos\n",
            "   Abstract: With the emergence of large-scale models trained on diverse datasets,\n",
            "in-context learning has emerged as a promising paradigm for multitasking,\n",
            "notably in natural language processing and image processing. However, its\n",
            "application in 3D point cloud tasks remains largely unexplored. In this work,\n",
            "we introduce Point-In-Context (PIC), a novel framework for 3D point cloud\n",
            "understanding via in-context learning. We address the technical challenge of\n",
            "effectively extending masked point modeling to 3D point clouds by introducing a\n",
            "Joint Sampling module and proposing a vanilla version of PIC called\n",
            "Point-In-Context-Generalist (PIC-G). PIC-G is designed as a generalist model\n",
            "for various 3D point cloud tasks, with inputs and outputs modeled as\n",
            "coordinates. In this paradigm, the challenging segmentation task is achieved by\n",
            "assigning label points with XYZ coordinates for each category; the final\n",
            "prediction is then chosen based on the label point closest to the predictions.\n",
            "To break the limitation by the fixed label-coordinate assignment, which has\n",
            "poor generalization upon novel classes, we propose two novel training\n",
            "strategies, In-Context Labeling and In-Context Enhancing, forming an extended\n",
            "version of PIC named Point-In-Context-Segmenter (PIC-S), targeting improving\n",
            "dynamic context labeling and model training. By utilizing dynamic in-context\n",
            "labels and extra in-context pairs, PIC-S achieves enhanced performance and\n",
            "generalization capability in and across part segmentation datasets. PIC is a\n",
            "general framework so that other tasks or datasets can be seamlessly introduced\n",
            "into our PIC through a unified data format. We conduct extensive experiments to\n",
            "validate the versatility and adaptability of our proposed methods in handling a\n",
            "wide range of tasks and segmenting multi-datasets. Our PIC-S is capable of\n",
            "generalizing unseen datasets and performing novel part segmentation by\n",
            "customizing prompts.\n",
            "\n",
            "1495. Title: Towards Robust Fidelity for Evaluating Explainability of Graph Neural Networks\n",
            "   Abstract: We propose infinite mixture prototypes to adaptively represent both simple\n",
            "and complex data distributions for few-shot learning. Our infinite mixture\n",
            "prototypes represent each class by a set of clusters, unlike existing\n",
            "prototypical methods that represent each class by a single cluster. By\n",
            "inferring the number of clusters, infinite mixture prototypes interpolate\n",
            "between nearest neighbor and prototypical representations, which improves\n",
            "accuracy and robustness in the few-shot regime. We show the importance of\n",
            "adaptive capacity for capturing complex data distributions such as alphabets,\n",
            "with 25% absolute accuracy improvements over prototypical networks, while still\n",
            "maintaining or improving accuracy on the standard Omniglot and mini-ImageNet\n",
            "benchmarks. In clustering labeled and unlabeled data by the same clustering\n",
            "rule, infinite mixture prototypes achieves state-of-the-art semi-supervised\n",
            "accuracy. As a further capability, we show that infinite mixture prototypes can\n",
            "perform purely unsupervised clustering, unlike existing prototypical methods.\n",
            "\n",
            "1496. Title: Bridging Neural and Symbolic Representations with Transitional Dictionary Learning\n",
            "   Abstract: Inspired by the outstanding zero-shot capability of vision language models\n",
            "(VLMs) in image classification tasks, open-vocabulary object detection has\n",
            "attracted increasing interest by distilling the broad VLM knowledge into\n",
            "detector training. However, most existing open-vocabulary detectors learn by\n",
            "aligning region embeddings with categorical labels (e.g., bicycle) only,\n",
            "disregarding the capability of VLMs on aligning visual embeddings with\n",
            "fine-grained text description of object parts (e.g., pedals and bells). This\n",
            "paper presents DVDet, a Descriptor-Enhanced Open Vocabulary Detector that\n",
            "introduces conditional context prompts and hierarchical textual descriptors\n",
            "that enable precise region-text alignment as well as open-vocabulary detection\n",
            "training in general. Specifically, the conditional context prompt transforms\n",
            "regional embeddings into image-like representations that can be directly\n",
            "integrated into general open vocabulary detection training. In addition, we\n",
            "introduce large language models as an interactive and implicit knowledge\n",
            "repository which enables iterative mining and refining visually oriented\n",
            "textual descriptors for precise region-text alignment. Extensive experiments\n",
            "over multiple large-scale benchmarks show that DVDet outperforms the\n",
            "state-of-the-art consistently by large margins.\n",
            "\n",
            "1497. Title: LLMs Meet VLMs: Boost Open Vocabulary Object Detection with Fine-grained Descriptors\n",
            "   Abstract: Implicit neural representations (INRs) have recently advanced numerous\n",
            "vision-related areas. INR performance depends strongly on the choice of the\n",
            "nonlinear activation function employed in its multilayer perceptron (MLP)\n",
            "network. A wide range of nonlinearities have been explored, but, unfortunately,\n",
            "current INRs designed to have high accuracy also suffer from poor robustness\n",
            "(to signal noise, parameter variation, etc.). Inspired by harmonic analysis, we\n",
            "develop a new, highly accurate and robust INR that does not exhibit this\n",
            "tradeoff. Wavelet Implicit neural REpresentation (WIRE) uses a continuous\n",
            "complex Gabor wavelet activation function that is well-known to be optimally\n",
            "concentrated in space-frequency and to have excellent biases for representing\n",
            "images. A wide range of experiments (image denoising, image inpainting,\n",
            "super-resolution, computed tomography reconstruction, image overfitting, and\n",
            "novel view synthesis with neural radiance fields) demonstrate that WIRE defines\n",
            "the new state of the art in INR accuracy, training time, and robustness.\n",
            "\n",
            "1498. Title: Beyond Spatio-Temporal Representations: Evolving Fourier Transform for Temporal Graphs\n",
            "   Abstract: Inducing and leveraging sparse activations during training and inference is a\n",
            "promising avenue for improving the computational efficiency of deep networks,\n",
            "which is increasingly important as network sizes continue to grow and their\n",
            "application becomes more widespread. Here we use the large width Gaussian\n",
            "process limit to analyze the behaviour, at random initialization, of nonlinear\n",
            "activations that induce sparsity in the hidden outputs. A previously unreported\n",
            "form of training instability is proven for arguably two of the most natural\n",
            "candidates for hidden layer sparsification; those being a shifted ReLU\n",
            "($\\phi(x)=\\max(0, x-\\tau)$ for $\\tau\\ge 0$) and soft thresholding ($\\phi(x)=0$\n",
            "for $|x|\\le\\tau$ and $x-\\text{sign}(x)\\tau$ for $|x|>\\tau$). We show that this\n",
            "instability is overcome by clipping the nonlinear activation magnitude, at a\n",
            "level prescribed by the shape of the associated Gaussian process variance map.\n",
            "Numerical experiments verify the theory and show that the proposed magnitude\n",
            "clipped sparsifying activations can be trained with training and test\n",
            "fractional sparsity as high as 85\\% while retaining close to full accuracy.\n",
            "\n",
            "1499. Title: Bayesian Coreset Optimization for Personalized Federated Learning\n",
            "   Abstract: We present the Evolving Graph Fourier Transform (EFT), the first invertible\n",
            "spectral transform that captures evolving representations on temporal graphs.\n",
            "We motivate our work by the inadequacy of existing methods for capturing the\n",
            "evolving graph spectra, which are also computationally expensive due to the\n",
            "temporal aspect along with the graph vertex domain. We view the problem as an\n",
            "optimization over the Laplacian of the continuous time dynamic graph.\n",
            "Additionally, we propose pseudo-spectrum relaxations that decompose the\n",
            "transformation process, making it highly computationally efficient. The EFT\n",
            "method adeptly captures the evolving graph's structural and positional\n",
            "properties, making it effective for downstream tasks on evolving graphs. Hence,\n",
            "as a reference implementation, we develop a simple neural model induced with\n",
            "EFT for capturing evolving graph spectra. We empirically validate our\n",
            "theoretical findings on a number of large-scale and standard temporal graph\n",
            "benchmarks and demonstrate that our model achieves state-of-the-art\n",
            "performance.\n",
            "\n",
            "1500. Title: On Stationary Point Convergence of PPO-Clip\n",
            "   Abstract: We consider the problem of sampling from a log-concave distribution\n",
            "$\\pi(\\theta) \\propto e^{-f(\\theta)}$ constrained to a polytope $K:=\\{\\theta \\in\n",
            "\\mathbb{R}^d: A\\theta \\leq b\\}$, where $A\\in \\mathbb{R}^{m\\times d}$ and $b \\in\n",
            "\\mathbb{R}^m$.The fastest-known algorithm \\cite{mangoubi2022faster} for the\n",
            "setting when $f$ is $O(1)$-Lipschitz or $O(1)$-smooth runs in roughly $O(md\n",
            "\\times md^{\\omega -1})$ arithmetic operations, where the $md^{\\omega -1}$ term\n",
            "arises because each Markov chain step requires computing a matrix inversion and\n",
            "determinant (here $\\omega \\approx 2.37$ is the matrix multiplication constant).\n",
            "We present a nearly-optimal implementation of this Markov chain with per-step\n",
            "complexity which is roughly the number of non-zero entries of $A$ while the\n",
            "number of Markov chain steps remains the same. The key technical ingredients\n",
            "are 1) to show that the matrices that arise in this Dikin walk change slowly,\n",
            "2) to deploy efficient linear solvers that can leverage this slow change to\n",
            "speed up matrix inversion by using information computed in previous steps, and\n",
            "3) to speed up the computation of the determinantal term in the Metropolis\n",
            "filter step via a randomized Taylor series-based estimator.\n",
            "\n",
            "1501. Title: How to Capture Higher-order Correlations? Generalizing Matrix Softmax Attention to Kronecker Computation\n",
            "   Abstract: In the classical transformer attention scheme, we are given three $n \\times\n",
            "d$ size matrices $Q, K, V$ (the query, key, and value tokens), and the goal is\n",
            "to compute a new $n \\times d$ size matrix $D^{-1} \\exp(QK^\\top) V$ where $D =\n",
            "\\mathrm{diag}( \\exp(QK^\\top) {\\bf 1}_n )$. In this work, we study a\n",
            "generalization of attention which captures triple-wise correlations. This\n",
            "generalization is able to solve problems about detecting triple-wise\n",
            "connections that were shown to be impossible for transformers. The potential\n",
            "downside of this generalization is that it appears as though computations are\n",
            "even more difficult, since the straightforward algorithm requires cubic time in\n",
            "$n$. However, we show that in the bounded-entry setting (which arises in\n",
            "practice, and which is well-studied in both theory and practice), there is\n",
            "actually a near-linear time algorithm. More precisely, we show that bounded\n",
            "entries are both necessary and sufficient for quickly performing generalized\n",
            "computations:\n",
            "  $\\bullet$ On the positive side, if all entries of the input matrices are\n",
            "bounded above by $o(\\sqrt[3]{\\log n})$ then we show how to approximate the\n",
            "``tensor-type'' attention matrix in $n^{1+o(1)}$ time.\n",
            "  $\\bullet$ On the negative side, we show that if the entries of the input\n",
            "matrices may be as large as $\\Omega(\\sqrt[3]{\\log n})$, then there is no\n",
            "algorithm that runs faster than $n^{3-o(1)}$ (assuming the Strong Exponential\n",
            "Time Hypothesis from fine-grained complexity theory).\n",
            "  We also show that our construction, algorithms, and lower bounds naturally\n",
            "generalize to higher-order tensors and correlations. Interestingly, the higher\n",
            "the order of the tensors, the lower the bound on the entries needs to be for an\n",
            "efficient algorithm. Our results thus yield a natural tradeoff between the\n",
            "boundedness of the entries, and order of the tensor one may use for more\n",
            "expressive, efficient attention computation.\n",
            "\n",
            "1502. Title: Towards Characterizing Domain Counterfactuals for Invertible Latent Causal Models\n",
            "   Abstract: Bayesian methods provide an elegant framework for estimating parameter\n",
            "posteriors and quantification of uncertainty associated with probabilistic\n",
            "models. However, they often suffer from slow inference times. To address this\n",
            "challenge, Bayesian Pseudo-Coresets (BPC) have emerged as a promising solution.\n",
            "BPC methods aim to create a small synthetic dataset, known as pseudo-coresets,\n",
            "that approximates the posterior inference achieved with the original dataset.\n",
            "This approximation is achieved by optimizing a divergence measure between the\n",
            "true posterior and the pseudo-coreset posterior. Various divergence measures\n",
            "have been proposed for constructing pseudo-coresets, with forward\n",
            "Kullback-Leibler (KL) divergence being the most successful. However, using\n",
            "forward KL divergence necessitates sampling from the pseudo-coreset posterior,\n",
            "often accomplished through approximate Gaussian variational distributions.\n",
            "Alternatively, one could employ Markov Chain Monte Carlo (MCMC) methods for\n",
            "sampling, but this becomes challenging in high-dimensional parameter spaces due\n",
            "to slow mixing. In this study, we introduce a novel approach for constructing\n",
            "pseudo-coresets by utilizing contrastive divergence. Importantly, optimizing\n",
            "contrastive divergence eliminates the need for approximations in the\n",
            "pseudo-coreset construction process. Furthermore, it enables the use of\n",
            "finite-step MCMC methods, alleviating the requirement for extensive mixing to\n",
            "reach a stationary distribution. To validate our method's effectiveness, we\n",
            "conduct extensive experiments on multiple datasets, demonstrating its\n",
            "superiority over existing BPC techniques.\n",
            "\n",
            "1503. Title: AutomaTikZ: Text-Guided Synthesis of Scientific Vector Graphics with TikZ\n",
            "   Abstract: Answering counterfactual queries has important applications such as\n",
            "explainability, robustness, and fairness but is challenging when the causal\n",
            "variables are unobserved and the observations are non-linear mixtures of these\n",
            "latent variables, such as pixels in images. One approach is to recover the\n",
            "latent Structural Causal Model (SCM), which may be infeasible in practice due\n",
            "to requiring strong assumptions, e.g., linearity of the causal mechanisms or\n",
            "perfect atomic interventions. Meanwhile, more practical ML-based approaches\n",
            "using naive domain translation models to generate counterfactual samples lack\n",
            "theoretical grounding and may construct invalid counterfactuals. In this work,\n",
            "we strive to strike a balance between practicality and theoretical guarantees\n",
            "by analyzing a specific type of causal query called domain counterfactuals,\n",
            "which hypothesizes what a sample would have looked like if it had been\n",
            "generated in a different domain (or environment). We show that recovering the\n",
            "latent SCM is unnecessary for estimating domain counterfactuals, thereby\n",
            "sidestepping some of the theoretic challenges. By assuming invertibility and\n",
            "sparsity of intervention, we prove domain counterfactual estimation error can\n",
            "be bounded by a data fit term and intervention sparsity term. Building upon our\n",
            "theoretical results, we develop a theoretically grounded practical algorithm\n",
            "that simplifies the modeling process to generative model estimation under\n",
            "autoregressive and shared parameter constraints that enforce intervention\n",
            "sparsity. Finally, we show an improvement in counterfactual estimation over\n",
            "baseline methods through extensive simulated and image-based experiments.\n",
            "\n",
            "1504. Title: RLCD: Reinforcement Learning from Contrastive Distillation for LM Alignment\n",
            "   Abstract: Treatment effect estimation in continuous time is crucial for personalized\n",
            "medicine. However, existing methods for this task are limited to point\n",
            "estimates of the potential outcomes, whereas uncertainty estimates have been\n",
            "ignored. Needless to say, uncertainty quantification is crucial for reliable\n",
            "decision-making in medical applications. To fill this gap, we propose a novel\n",
            "Bayesian neural controlled differential equation (BNCDE) for treatment effect\n",
            "estimation in continuous time. In our BNCDE, the time dimension is modeled\n",
            "through a coupled system of neural controlled differential equations and neural\n",
            "stochastic differential equations, where the neural stochastic differential\n",
            "equations allow for tractable variational Bayesian inference. Thereby, for an\n",
            "assigned sequence of treatments, our BNCDE provides meaningful posterior\n",
            "predictive distributions of the potential outcomes. To the best of our\n",
            "knowledge, ours is the first tailored neural method to provide uncertainty\n",
            "estimates of treatment effects in continuous time. As such, our method is of\n",
            "direct practical value for promoting reliable decision-making in medicine.\n",
            "\n",
            "1505. Title: Faster Sampling from Log-Concave Densities over Polytopes via Efficient Linear Solvers\n",
            "   Abstract: In this work, a convergence lemma for function $f$ being finite compositions\n",
            "of analytic mappings and the maximum operator is proved. The lemma shows that\n",
            "the set of $\\delta$-stationary points near an isolated local minimum point\n",
            "$x^*$ is shrinking to $x^*$ as $\\delta\\to 0$. It is a natural extension of the\n",
            "version for strongly convex $C^1$ functions. However, the correctness of the\n",
            "lemma is subtle. Analytic mappings are necessary for the lemma in the sense\n",
            "that replacing it with differentiable or $C^\\infty$ mappings makes the lemma\n",
            "false. The proof is based on stratification theorems of semi-analytic sets by\n",
            "{\\L}ojasiewicz. An extension of this proof presents a geometric\n",
            "characterization of the set of stationary points of $f$. Finally, a notion of\n",
            "stability on stationary points, called convergence stability, is proposed. It\n",
            "asks, under small numerical errors, whether a reasonable convergent\n",
            "optimization method started near a stationary point should eventually converge\n",
            "to the same stationary point. The concept of convergence stability becomes\n",
            "nontrivial qualitatively only when the objective function is both nonsmooth and\n",
            "nonconvex. Via the convergence lemma, an intuitive equivalent condition for\n",
            "convergence stability of $f$ is proved. These results together provide a new\n",
            "geometric perspective to study the problem of \"where-to-converge\" in nonsmooth\n",
            "nonconvex optimization.\n",
            "\n",
            "1506. Title: Statistically Optimal $K$-means Clustering via Nonnegative Low-rank Semidefinite Programming\n",
            "   Abstract: $K$-means clustering is a widely used machine learning method for identifying\n",
            "patterns in large datasets. Recently, semidefinite programming (SDP)\n",
            "relaxations have been proposed for solving the $K$-means optimization problem,\n",
            "which enjoy strong statistical optimality guarantees. However, the prohibitive\n",
            "cost of implementing an SDP solver renders these guarantees inaccessible to\n",
            "practical datasets. In contrast, nonnegative matrix factorization (NMF) is a\n",
            "simple clustering algorithm widely used by machine learning practitioners, but\n",
            "it lacks a solid statistical underpinning and theoretical guarantees. In this\n",
            "paper, we consider an NMF-like algorithm that solves a nonnegative low-rank\n",
            "restriction of the SDP-relaxed $K$-means formulation using a nonconvex\n",
            "Burer--Monteiro factorization approach. The resulting algorithm is as simple\n",
            "and scalable as state-of-the-art NMF algorithms while also enjoying the same\n",
            "strong statistical optimality guarantees as the SDP. In our experiments, we\n",
            "observe that our algorithm achieves significantly smaller mis-clustering errors\n",
            "compared to the existing state-of-the-art while maintaining scalability.\n",
            "\n",
            "1507. Title: Let's Verify Step by Step\n",
            "   Abstract: In recent years, large language models have greatly improved in their ability\n",
            "to perform complex multi-step reasoning. However, even state-of-the-art models\n",
            "still regularly produce logical mistakes. To train more reliable models, we can\n",
            "turn either to outcome supervision, which provides feedback for a final result,\n",
            "or process supervision, which provides feedback for each intermediate reasoning\n",
            "step. Given the importance of training reliable models, and given the high cost\n",
            "of human feedback, it is important to carefully compare the both methods.\n",
            "Recent work has already begun this comparison, but many questions still remain.\n",
            "We conduct our own investigation, finding that process supervision\n",
            "significantly outperforms outcome supervision for training models to solve\n",
            "problems from the challenging MATH dataset. Our process-supervised model solves\n",
            "78% of problems from a representative subset of the MATH test set.\n",
            "Additionally, we show that active learning significantly improves the efficacy\n",
            "of process supervision. To support related research, we also release PRM800K,\n",
            "the complete dataset of 800,000 step-level human feedback labels used to train\n",
            "our best reward model.\n",
            "\n",
            "1508. Title: Consistency Models as a Rich and Efficient Policy Class for Reinforcement Learning\n",
            "   Abstract: Score-based generative models like the diffusion model have been testified to\n",
            "be effective in modeling multi-modal data from image generation to\n",
            "reinforcement learning (RL). However, the inference process of diffusion model\n",
            "can be slow, which hinders its usage in RL with iterative sampling. We propose\n",
            "to apply the consistency model as an efficient yet expressive policy\n",
            "representation, namely consistency policy, with an actor-critic style algorithm\n",
            "for three typical RL settings: offline, offline-to-online and online. For\n",
            "offline RL, we demonstrate the expressiveness of generative models as policies\n",
            "from multi-modal data. For offline-to-online RL, the consistency policy is\n",
            "shown to be more computational efficient than diffusion policy, with a\n",
            "comparable performance. For online RL, the consistency policy demonstrates\n",
            "significant speedup and even higher average performances than the diffusion\n",
            "policy.\n",
            "\n",
            "1509. Title: A Framework for Inference Inspired by Human Memory Mechanisms\n",
            "   Abstract: How humans and machines make sense of current inputs for relation reasoning\n",
            "and question-answering while putting the perceived information into context of\n",
            "our past memories, has been a challenging conundrum in cognitive science and\n",
            "artificial intelligence. Inspired by human brain's memory system and cognitive\n",
            "architectures, we propose a PMI framework that consists of perception, memory\n",
            "and inference components. Notably, the memory module comprises working and\n",
            "long-term memory, with the latter endowed with a higher-order structure to\n",
            "retain extensive and complex relational knowledge and experience. Through a\n",
            "differentiable competitive write access, current perceptions update working\n",
            "memory, which is later merged with long-term memory via outer product\n",
            "associations, reducing information conflicts and averting memory overflow. In\n",
            "the inference module, relevant information is retrieved from two separate\n",
            "memory origins and associatively integrated to attain a more comprehensive and\n",
            "precise interpretation of current perceptions. We exploratively apply our PMI\n",
            "to improve prevailing Transformers and CNN models on question-answering tasks\n",
            "like bAbI-20k and Sort-of-CLEVR datasets, as well as detecting equilateral\n",
            "triangles, language modeling and image classification tasks, and in each case,\n",
            "our PMI enhancements consistently outshine their original counterparts\n",
            "significantly. Visualization analyses reveal that relational memory\n",
            "consolidation, along with the interaction and integration of information from\n",
            "diverse memory sources, substantially contributes to the model effectiveness on\n",
            "inference tasks.\n",
            "\n",
            "1510. Title: The Expressive Leaky Memory Neuron: an Efficient and Expressive Phenomenological Neuron Model Can Solve Long-Horizon Tasks.\n",
            "   Abstract: Microbes have a profound impact on our health and environment, but our\n",
            "understanding of the diversity and function of microbial communities is\n",
            "severely limited. Through DNA sequencing of microbial communities\n",
            "(metagenomics), DNA fragments (reads) of the individual microbes can be\n",
            "obtained, which through assembly graphs can be combined into long contiguous\n",
            "DNA sequences (contigs). Given the complexity of microbial communities, single\n",
            "contig microbial genomes are rarely obtained. Instead, contigs are eventually\n",
            "clustered into bins, with each bin ideally making up a full genome. This\n",
            "process is referred to as metagenomic binning.\n",
            "  Current state-of-the-art techniques for metagenomic binning rely only on the\n",
            "local features for the individual contigs. These techniques therefore fail to\n",
            "exploit the similarities between contigs as encoded by the assembly graph, in\n",
            "which the contigs are organized. In this paper, we propose to use Graph Neural\n",
            "Networks (GNNs) to leverage the assembly graph when learning contig\n",
            "representations for metagenomic binning. Our method, VaeG-Bin, combines\n",
            "variational autoencoders for learning latent representations of the individual\n",
            "contigs, with GNNs for refining these representations by taking into account\n",
            "the neighborhood structure of the contigs in the assembly graph. We explore\n",
            "several types of GNNs and demonstrate that VaeG-Bin recovers more high-quality\n",
            "genomes than other state-of-the-art binners on both simulated and real-world\n",
            "datasets.\n",
            "\n",
            "1511. Title: Threshold-Consistent Margin Loss for Open-World Deep Metric Learning\n",
            "   Abstract: Person re-identification (ReID) is an important task in computer vision.\n",
            "Recently, deep learning with a metric learning loss has become a common\n",
            "framework for ReID. In this paper, we also propose a new metric learning loss\n",
            "with hard sample mining called margin smaple mining loss (MSML) which can\n",
            "achieve better accuracy compared with other metric learning losses, such as\n",
            "triplet loss. In experi- ments, our proposed methods outperforms most of the\n",
            "state-of-the-art algorithms on Market1501, MARS, CUHK03 and CUHK-SYSU.\n",
            "\n",
            "1512. Title: Learning in reverse causal strategic environments with ramifications on two sided markets\n",
            "   Abstract: Biological cortical neurons are remarkably sophisticated computational\n",
            "devices, temporally integrating their vast synaptic input over an intricate\n",
            "dendritic tree, subject to complex, nonlinearly interacting internal biological\n",
            "processes. A recent study proposed to characterize this complexity by fitting\n",
            "accurate surrogate models to replicate the input-output relationship of a\n",
            "detailed biophysical cortical pyramidal neuron model and discovered it needed\n",
            "temporal convolutional networks (TCN) with millions of parameters. Requiring\n",
            "these many parameters, however, could stem from a misalignment between the\n",
            "inductive biases of the TCN and cortical neuron's computations. In light of\n",
            "this, and to explore the computational implications of leaky memory units and\n",
            "nonlinear dendritic processing, we introduce the Expressive Leaky Memory (ELM)\n",
            "neuron model, a biologically inspired phenomenological model of a cortical\n",
            "neuron. Remarkably, by exploiting such slowly decaying memory-like hidden\n",
            "states and two-layered nonlinear integration of synaptic input, our ELM neuron\n",
            "can accurately match the aforementioned input-output relationship with under\n",
            "ten thousand trainable parameters. To further assess the computational\n",
            "ramifications of our neuron design, we evaluate it on various tasks with\n",
            "demanding temporal structures, including the Long Range Arena (LRA) datasets,\n",
            "as well as a novel neuromorphic dataset based on the Spiking Heidelberg Digits\n",
            "dataset (SHD-Adding). Leveraging a larger number of memory units with\n",
            "sufficiently long timescales, and correspondingly sophisticated synaptic\n",
            "integration, the ELM neuron displays substantial long-range processing\n",
            "capabilities, reliably outperforming the classic Transformer or Chrono-LSTM\n",
            "architectures on LRA, and even solving the Pathfinder-X task with over 70%\n",
            "accuracy (16k context length).\n",
            "\n",
            "1513. Title: Skip-Attention: Improving Vision Transformers by Paying Less Attention\n",
            "   Abstract: While large language models (LLMs) exhibit remarkable capabilities across a\n",
            "wide range of tasks, they pose potential safety concerns, such as the\n",
            "``jailbreak'' problem, wherein malicious instructions can manipulate LLMs to\n",
            "exhibit undesirable behavior. Although several preventive measures have been\n",
            "developed to mitigate the potential risks associated with LLMs, they have\n",
            "primarily focused on English. In this study, we reveal the presence of\n",
            "multilingual jailbreak challenges within LLMs and consider two potential risky\n",
            "scenarios: unintentional and intentional. The unintentional scenario involves\n",
            "users querying LLMs using non-English prompts and inadvertently bypassing the\n",
            "safety mechanisms, while the intentional scenario concerns malicious users\n",
            "combining malicious instructions with multilingual prompts to deliberately\n",
            "attack LLMs. The experimental results reveal that in the unintentional\n",
            "scenario, the rate of unsafe content increases as the availability of languages\n",
            "decreases. Specifically, low-resource languages exhibit about three times the\n",
            "likelihood of encountering harmful content compared to high-resource languages,\n",
            "with both ChatGPT and GPT-4. In the intentional scenario, multilingual prompts\n",
            "can exacerbate the negative impact of malicious instructions, with\n",
            "astonishingly high rates of unsafe output: 80.92\\% for ChatGPT and 40.71\\% for\n",
            "GPT-4. To handle such a challenge in the multilingual context, we propose a\n",
            "novel \\textsc{Self-Defense} framework that automatically generates multilingual\n",
            "training data for safety fine-tuning. Experimental results show that ChatGPT\n",
            "fine-tuned with such data can achieve a substantial reduction in unsafe content\n",
            "generation. Data is available at\n",
            "\\url{https://github.com/DAMO-NLP-SG/multilingual-safety-for-LLMs}.\n",
            "\n",
            "1514. Title: Fine-Tuned Language Models Generate Stable Inorganic Materials as Text\n",
            "   Abstract: Motivated by equilibrium models of labor markets, we develop a formulation of\n",
            "causal strategic classification in which strategic agents can directly\n",
            "manipulate their outcomes. As an application, we compare employers that\n",
            "anticipate the strategic response of a labor force with employers that do not.\n",
            "We show through a combination of theory and experiment that employers with\n",
            "performatively optimal hiring policies improve employer reward, labor force\n",
            "skill level, and in some cases labor force equity. On the other hand, we\n",
            "demonstrate that performative employers harm labor force utility and fail to\n",
            "prevent discrimination in other cases.\n",
            "\n",
            "1515. Title: Provable Memory Efficient Self-Play Algorithm for Model-free Reinforcement Learning\n",
            "   Abstract: Data attribution seeks to trace model outputs back to training data. With the\n",
            "recent development of diffusion models, data attribution has become a desired\n",
            "module to properly assign valuations for high-quality or copyrighted training\n",
            "samples, ensuring that data contributors are fairly compensated or credited.\n",
            "Several theoretically motivated methods have been proposed to implement data\n",
            "attribution, in an effort to improve the trade-off between computational\n",
            "scalability and effectiveness. In this work, we conduct extensive experiments\n",
            "and ablation studies on attributing diffusion models, specifically focusing on\n",
            "DDPMs trained on CIFAR-10 and CelebA, as well as a Stable Diffusion model\n",
            "LoRA-finetuned on ArtBench. Intriguingly, we report counter-intuitive\n",
            "observations that theoretically unjustified design choices for attribution\n",
            "empirically outperform previous baselines by a large margin, in terms of both\n",
            "linear datamodeling score and counterfactual evaluation. Our work presents a\n",
            "significantly more efficient approach for attributing diffusion models, while\n",
            "the unexpected findings suggest that at least in non-convex settings,\n",
            "constructions guided by theoretical assumptions may lead to inferior\n",
            "attribution performance. The code is available at\n",
            "https://github.com/sail-sg/D-TRAK.\n",
            "\n",
            "1516. Title: Efficient Backdoor Attacks for Deep Neural Networks in Real-world Scenarios\n",
            "   Abstract: We study Reinforcement Learning for partially observable dynamical systems\n",
            "using function approximation. We propose a new \\textit{Partially Observable\n",
            "Bilinear Actor-Critic framework}, that is general enough to include models such\n",
            "as observable tabular Partially Observable Markov Decision Processes (POMDPs),\n",
            "observable Linear-Quadratic-Gaussian (LQG), Predictive State Representations\n",
            "(PSRs), as well as a newly introduced model Hilbert Space Embeddings of POMDPs\n",
            "and observable POMDPs with latent low-rank transition. Under this framework, we\n",
            "propose an actor-critic style algorithm that is capable of performing agnostic\n",
            "policy learning. Given a policy class that consists of memory based policies\n",
            "(that look at a fixed-length window of recent observations), and a value\n",
            "function class that consists of functions taking both memory and future\n",
            "observations as inputs, our algorithm learns to compete against the best\n",
            "memory-based policy in the given policy class. For certain examples such as\n",
            "undercomplete observable tabular POMDPs, observable LQGs and observable POMDPs\n",
            "with latent low-rank transition, by implicitly leveraging their special\n",
            "properties, our algorithm is even capable of competing against the globally\n",
            "optimal policy without paying an exponential dependence on the horizon in its\n",
            "sample complexity.\n",
            "\n",
            "1517. Title: How Many Pretraining Tasks Are Needed for In-Context Learning of Linear Regression?\n",
            "   Abstract: Recent deep neural networks (DNNs) have came to rely on vast amounts of\n",
            "training data, providing an opportunity for malicious attackers to exploit and\n",
            "contaminate the data to carry out backdoor attacks. However, existing backdoor\n",
            "attack methods make unrealistic assumptions, assuming that all training data\n",
            "comes from a single source and that attackers have full access to the training\n",
            "data. In this paper, we introduce a more realistic attack scenario where\n",
            "victims collect data from multiple sources, and attackers cannot access the\n",
            "complete training data. We refer to this scenario as data-constrained backdoor\n",
            "attacks. In such cases, previous attack methods suffer from severe efficiency\n",
            "degradation due to the entanglement between benign and poisoning features\n",
            "during the backdoor injection process. To tackle this problem, we introduce\n",
            "three CLIP-based technologies from two distinct streams: Clean Feature\n",
            "Suppression and Poisoning Feature Augmentation.effective solution for\n",
            "data-constrained backdoor attacks. The results demonstrate remarkable\n",
            "improvements, with some settings achieving over 100% improvement compared to\n",
            "existing attacks in data-constrained scenarios. Code is available at\n",
            "https://github.com/sunh1113/Efficient-backdoor-attacks-for-deep-neural-networks-in-real-world-scenarios\n",
            "\n",
            "1518. Title: Enhancing Transfer Learning with Flexible Nonparametric Posterior Sampling\n",
            "   Abstract: We propose fine-tuning large language models for generation of stable\n",
            "materials. While unorthodox, fine-tuning large language models on text-encoded\n",
            "atomistic data is simple to implement yet reliable, with around 90% of sampled\n",
            "structures obeying physical constraints on atom positions and charges. Using\n",
            "energy above hull calculations from both learned ML potentials and\n",
            "gold-standard DFT calculations, we show that our strongest model (fine-tuned\n",
            "LLaMA-2 70B) can generate materials predicted to be metastable at about twice\n",
            "the rate (49% vs 28%) of CDVAE, a competing diffusion model. Because of text\n",
            "prompting's inherent flexibility, our models can simultaneously be used for\n",
            "unconditional generation of stable material, infilling of partial structures\n",
            "and text-conditional generation. Finally, we show that language models' ability\n",
            "to capture key symmetries of crystal structures improves with model scale,\n",
            "suggesting that the biases of pretrained LLMs are surprisingly well-suited for\n",
            "atomistic data.\n",
            "\n",
            "1519. Title: Provably Efficient Iterated CVaR Reinforcement Learning with Function Approximation and Human Feedback\n",
            "   Abstract: Transfer learning has recently shown significant performance across various\n",
            "tasks involving deep neural networks. In these transfer learning scenarios, the\n",
            "prior distribution for downstream data becomes crucial in Bayesian model\n",
            "averaging (BMA). While previous works proposed the prior over the neural\n",
            "network parameters centered around the pre-trained solution, such strategies\n",
            "have limitations when dealing with distribution shifts between upstream and\n",
            "downstream data. This paper introduces nonparametric transfer learning (NPTL),\n",
            "a flexible posterior sampling method to address the distribution shift issue\n",
            "within the context of nonparametric learning. The nonparametric learning (NPL)\n",
            "method is a recent approach that employs a nonparametric prior for posterior\n",
            "sampling, efficiently accounting for model misspecification scenarios, which is\n",
            "suitable for transfer learning scenarios that may involve the distribution\n",
            "shift between upstream and downstream tasks. Through extensive empirical\n",
            "validations, we demonstrate that our approach surpasses other baselines in BMA\n",
            "performance.\n",
            "\n",
            "1520. Title: CodeChain: Towards Modular Code Generation Through Chain of Self-revisions with Representative Sub-modules\n",
            "   Abstract: Risk-sensitive reinforcement learning (RL) aims to optimize policies that\n",
            "balance the expected reward and risk. In this paper, we present a novel\n",
            "risk-sensitive RL framework that employs an Iterated Conditional Value-at-Risk\n",
            "(CVaR) objective under both linear and general function approximations,\n",
            "enriched by human feedback. These new formulations provide a principled way to\n",
            "guarantee safety in each decision making step throughout the control process.\n",
            "Moreover, integrating human feedback into risk-sensitive RL framework bridges\n",
            "the gap between algorithmic decision-making and human participation, allowing\n",
            "us to also guarantee safety for human-in-the-loop systems. We propose provably\n",
            "sample-efficient algorithms for this Iterated CVaR RL and provide rigorous\n",
            "theoretical analysis. Furthermore, we establish a matching lower bound to\n",
            "corroborate the optimality of our algorithms in a linear context.\n",
            "\n",
            "1521. Title: Understanding the Robustness of Randomized Feature Defense Against Query-Based Adversarial Attacks\n",
            "   Abstract: Transformers pretrained on diverse tasks exhibit remarkable in-context\n",
            "learning (ICL) capabilities, enabling them to solve unseen tasks solely based\n",
            "on input contexts without adjusting model parameters. In this paper, we study\n",
            "ICL in one of its simplest setups: pretraining a linearly parameterized\n",
            "single-layer linear attention model for linear regression with a Gaussian\n",
            "prior. We establish a statistical task complexity bound for the attention model\n",
            "pretraining, showing that effective pretraining only requires a small number of\n",
            "independent tasks. Furthermore, we prove that the pretrained model closely\n",
            "matches the Bayes optimal algorithm, i.e., optimally tuned ridge regression, by\n",
            "achieving nearly Bayes optimal risk on unseen tasks under a fixed context\n",
            "length. These theoretical findings complement prior experimental research and\n",
            "shed light on the statistical foundations of ICL.\n",
            "\n",
            "1522. Title: Learning Multi-Agent Communication with Contrastive Learning\n",
            "   Abstract: Communication is a powerful tool for coordination in multi-agent RL. But\n",
            "inducing an effective, common language is a difficult challenge, particularly\n",
            "in the decentralized setting. In this work, we introduce an alternative\n",
            "perspective where communicative messages sent between agents are considered as\n",
            "different incomplete views of the environment state. By examining the\n",
            "relationship between messages sent and received, we propose to learn to\n",
            "communicate using contrastive learning to maximize the mutual information\n",
            "between messages of a given trajectory. In communication-essential\n",
            "environments, our method outperforms previous work in both performance and\n",
            "learning speed. Using qualitative metrics and representation probing, we show\n",
            "that our method induces more symmetric communication and captures global state\n",
            "information from the environment. Overall, we show the power of contrastive\n",
            "learning and the importance of leveraging messages as encodings for effective\n",
            "communication.\n",
            "\n",
            "1523. Title: SAS: Structured Activation Sparsification\n",
            "   Abstract: Recent works have shown that deep neural networks are vulnerable to\n",
            "adversarial examples that find samples close to the original image but can make\n",
            "the model misclassify. Even with access only to the model's output, an attacker\n",
            "can employ black-box attacks to generate such adversarial examples. In this\n",
            "work, we propose a simple and lightweight defense against black-box attacks by\n",
            "adding random noise to hidden features at intermediate layers of the model at\n",
            "inference time. Our theoretical analysis confirms that this method effectively\n",
            "enhances the model's resilience against both score-based and decision-based\n",
            "black-box attacks. Importantly, our defense does not necessitate adversarial\n",
            "training and has minimal impact on accuracy, rendering it applicable to any\n",
            "pre-trained model. Our analysis also reveals the significance of selectively\n",
            "adding noise to different parts of the model based on the gradient of the\n",
            "adversarial objective function, which can be varied during the attack. We\n",
            "demonstrate the robustness of our defense against multiple black-box attacks\n",
            "through extensive empirical experiments involving diverse models with various\n",
            "architectures.\n",
            "\n",
            "1524. Title: Towards Robust and Efficient Cloud-Edge Elastic Model Adaptation via Selective Entropy Distillation\n",
            "   Abstract: The conventional deep learning paradigm often involves training a deep model\n",
            "on a server and then deploying the model or its distilled ones to\n",
            "resource-limited edge devices. Usually, the models shall remain fixed once\n",
            "deployed (at least for some period) due to the potential high cost of model\n",
            "adaptation for both the server and edge sides. However, in many real-world\n",
            "scenarios, the test environments may change dynamically (known as distribution\n",
            "shifts), which often results in degraded performance. Thus, one has to adapt\n",
            "the edge models promptly to attain promising performance. Moreover, with the\n",
            "increasing data collected at the edge, this paradigm also fails to further\n",
            "adapt the cloud model for better performance. To address these, we encounter\n",
            "two primary challenges: 1) the edge model has limited computation power and may\n",
            "only support forward propagation; 2) the data transmission budget between cloud\n",
            "and edge devices is limited in latency-sensitive scenarios. In this paper, we\n",
            "establish a Cloud-Edge Elastic Model Adaptation (CEMA) paradigm in which the\n",
            "edge models only need to perform forward propagation and the edge models can be\n",
            "adapted online. In our CEMA, to reduce the communication burden, we devise two\n",
            "criteria to exclude unnecessary samples from uploading to the cloud, i.e.,\n",
            "dynamic unreliable and low-informative sample exclusion. Based on the uploaded\n",
            "samples, we update and distribute the affine parameters of normalization layers\n",
            "by distilling from the stronger foundation model to the edge model with a\n",
            "sample replay strategy. Extensive experimental results on ImageNet-C and\n",
            "ImageNet-R verify the effectiveness of our CEMA.\n",
            "\n",
            "1525. Title: General Graph Random Features\n",
            "   Abstract: Humans view the world through many sensory channels, e.g., the\n",
            "long-wavelength light channel, viewed by the left eye, or the high-frequency\n",
            "vibrations channel, heard by the right ear. Each view is noisy and incomplete,\n",
            "but important factors, such as physics, geometry, and semantics, tend to be\n",
            "shared between all views (e.g., a \"dog\" can be seen, heard, and felt). We\n",
            "investigate the classic hypothesis that a powerful representation is one that\n",
            "models view-invariant factors. We study this hypothesis under the framework of\n",
            "multiview contrastive learning, where we learn a representation that aims to\n",
            "maximize mutual information between different views of the same scene but is\n",
            "otherwise compact. Our approach scales to any number of views, and is\n",
            "view-agnostic. We analyze key properties of the approach that make it work,\n",
            "finding that the contrastive loss outperforms a popular alternative based on\n",
            "cross-view prediction, and that the more views we learn from, the better the\n",
            "resulting representation captures underlying scene semantics. Our approach\n",
            "achieves state-of-the-art results on image and video unsupervised learning\n",
            "benchmarks. Code is released at: http://github.com/HobbitLong/CMC/.\n",
            "\n",
            "1526. Title: Alice Benchmarks: Connecting Real World Re-Identification with the Synthetic\n",
            "   Abstract: We propose a novel random walk-based algorithm for unbiased estimation of\n",
            "arbitrary functions of a weighted adjacency matrix, coined universal graph\n",
            "random features (u-GRFs). This includes many of the most popular examples of\n",
            "kernels defined on the nodes of a graph. Our algorithm enjoys subquadratic time\n",
            "complexity with respect to the number of nodes, overcoming the notoriously\n",
            "prohibitive cubic scaling of exact graph kernel evaluation. It can also be\n",
            "trivially distributed across machines, permitting learning on much larger\n",
            "networks. At the heart of the algorithm is a modulation function which\n",
            "upweights or downweights the contribution from different random walks depending\n",
            "on their lengths. We show that by parameterising it with a neural network we\n",
            "can obtain u-GRFs that give higher-quality kernel estimates or perform\n",
            "efficient, scalable kernel learning. We provide robust theoretical analysis and\n",
            "support our findings with experiments including pointwise estimation of fixed\n",
            "graph kernels, solving non-homogeneous graph ordinary differential equations,\n",
            "node clustering and kernel regression on triangular meshes.\n",
            "\n",
            "1527. Title: From Bricks to Bridges: Product of Invariances to Enhance Latent Space Communication\n",
            "   Abstract: For object re-identification (re-ID), learning from synthetic data has become\n",
            "a promising strategy to cheaply acquire large-scale annotated datasets and\n",
            "effective models, with few privacy concerns. Many interesting research problems\n",
            "arise from this strategy, e.g., how to reduce the domain gap between synthetic\n",
            "source and real-world target. To facilitate developing more new approaches in\n",
            "learning from synthetic data, we introduce the Alice benchmarks, large-scale\n",
            "datasets providing benchmarks as well as evaluation protocols to the research\n",
            "community. Within the Alice benchmarks, two object re-ID tasks are offered:\n",
            "person and vehicle re-ID. We collected and annotated two challenging real-world\n",
            "target datasets: AlicePerson and AliceVehicle, captured under various\n",
            "illuminations, image resolutions, etc. As an important feature of our real\n",
            "target, the clusterability of its training set is not manually guaranteed to\n",
            "make it closer to a real domain adaptation test scenario. Correspondingly, we\n",
            "reuse existing PersonX and VehicleX as synthetic source domains. The primary\n",
            "goal is to train models from synthetic data that can work effectively in the\n",
            "real world. In this paper, we detail the settings of Alice benchmarks, provide\n",
            "an analysis of existing commonly-used domain adaptation methods, and discuss\n",
            "some interesting future directions. An online server has been set up for the\n",
            "community to evaluate methods conveniently and fairly. Datasets and the online\n",
            "server details are available at https://sites.google.com/view/alice-benchmarks.\n",
            "\n",
            "1528. Title: ModernTCN: A Modern Pure Convolution Structure for General Time Series Analysis\n",
            "   Abstract: It has been observed that representations learned by distinct neural networks\n",
            "conceal structural similarities when the models are trained under similar\n",
            "inductive biases. From a geometric perspective, identifying the classes of\n",
            "transformations and the related invariances that connect these representations\n",
            "is fundamental to unlocking applications, such as merging, stitching, and\n",
            "reusing different neural modules. However, estimating task-specific\n",
            "transformations a priori can be challenging and expensive due to several\n",
            "factors (e.g., weights initialization, training hyperparameters, or data\n",
            "modality). To this end, we introduce a versatile method to directly incorporate\n",
            "a set of invariances into the representations, constructing a product space of\n",
            "invariant components on top of the latent representations without requiring\n",
            "prior knowledge about the optimal invariance to infuse. We validate our\n",
            "solution on classification and reconstruction tasks, observing consistent\n",
            "latent similarity and downstream performance improvements in a zero-shot\n",
            "stitching setting. The experimental analysis comprises three modalities\n",
            "(vision, text, and graphs), twelve pretrained foundational models, nine\n",
            "benchmarks, and several architectures trained from scratch.\n",
            "\n",
            "1529. Title: Can Large Language Models Infer Causation from Correlation?\n",
            "   Abstract: Offline reinforcement learning (RL) allows agents to learn effective,\n",
            "return-maximizing policies from a static dataset. Three popular algorithms for\n",
            "offline RL are Conservative Q-Learning (CQL), Behavior Cloning (BC), and\n",
            "Decision Transformer (DT), from the class of Q-Learning, Imitation Learning,\n",
            "and Sequence Modeling respectively. A key open question is: which algorithm is\n",
            "preferred under what conditions? We study this question empirically by\n",
            "exploring the performance of these algorithms across the commonly used D4RL and\n",
            "Robomimic benchmarks. We design targeted experiments to understand their\n",
            "behavior concerning data suboptimality, task complexity, and stochasticity. Our\n",
            "key findings are: (1) DT requires more data than CQL to learn competitive\n",
            "policies but is more robust; (2) DT is a substantially better choice than both\n",
            "CQL and BC in sparse-reward and low-quality data settings; (3) DT and BC are\n",
            "preferable as task horizon increases, or when data is obtained from human\n",
            "demonstrators; and (4) CQL excels in situations characterized by the\n",
            "combination of high stochasticity and low data quality. We also investigate\n",
            "architectural choices and scaling trends for DT on Atari and D4RL and make\n",
            "design/scaling recommendations. We find that scaling the amount of data for DT\n",
            "by 5x gives a 2.5x average score improvement on Atari.\n",
            "\n",
            "1530. Title: Towards LLM4QPE: Unsupervised Pretraining of Quantum Property Estimation and A Benchmark\n",
            "   Abstract: Causal inference is one of the hallmarks of human intelligence. While the\n",
            "field of CausalNLP has attracted much interest in the recent years, existing\n",
            "causal inference datasets in NLP primarily rely on discovering causality from\n",
            "empirical knowledge (e.g., commonsense knowledge). In this work, we propose the\n",
            "first benchmark dataset to test the pure causal inference skills of large\n",
            "language models (LLMs). Specifically, we formulate a novel task Corr2Cause,\n",
            "which takes a set of correlational statements and determines the causal\n",
            "relationship between the variables. We curate a large-scale dataset of more\n",
            "than 200K samples, on which we evaluate seventeen existing LLMs. Through our\n",
            "experiments, we identify a key shortcoming of LLMs in terms of their causal\n",
            "inference skills, and show that these models achieve almost close to random\n",
            "performance on the task. This shortcoming is somewhat mitigated when we try to\n",
            "re-purpose LLMs for this skill via finetuning, but we find that these models\n",
            "still fail to generalize -- they can only perform causal inference in\n",
            "in-distribution settings when variable names and textual expressions used in\n",
            "the queries are similar to those in the training set, but fail in\n",
            "out-of-distribution settings generated by perturbing these queries. Corr2Cause\n",
            "is a challenging task for LLMs, and would be helpful in guiding future research\n",
            "on improving LLMs' pure reasoning skills and generalizability. Our data is at\n",
            "https://huggingface.co/datasets/causalnlp/corr2cause. Our code is at\n",
            "https://github.com/causalNLP/corr2cause.\n",
            "\n",
            "1531. Title: Grokking as the transition from lazy to rich training dynamics\n",
            "   Abstract: We study the best-arm identification problem in multi-armed bandits with\n",
            "stochastic, potentially private rewards, when the goal is to identify the arm\n",
            "with the highest quantile at a fixed, prescribed level. First, we propose a\n",
            "(non-private) successive elimination algorithm for strictly optimal best-arm\n",
            "identification, we show that our algorithm is $\\delta$-PAC and we characterize\n",
            "its sample complexity. Further, we provide a lower bound on the expected number\n",
            "of pulls, showing that the proposed algorithm is essentially optimal up to\n",
            "logarithmic factors. Both upper and lower complexity bounds depend on a special\n",
            "definition of the associated suboptimality gap, designed in particular for the\n",
            "quantile bandit problem, as we show when the gap approaches zero, best-arm\n",
            "identification is impossible. Second, motivated by applications where the\n",
            "rewards are private, we provide a differentially private successive elimination\n",
            "algorithm whose sample complexity is finite even for distributions with\n",
            "infinite support-size, and we characterize its sample complexity. Our\n",
            "algorithms do not require prior knowledge of either the suboptimality gap or\n",
            "other statistical information related to the bandit problem at hand.\n",
            "\n",
            "1532. Title: CoRe-GD: A Hierarchical Framework for Scalable Graph Visualization with GNNs\n",
            "   Abstract: Large pretrained models such as GPT-3 have had tremendous impact on modern\n",
            "natural language processing by leveraging self-supervised learning to learn\n",
            "salient representations that can be used to readily finetune on a wide variety\n",
            "of downstream tasks. We investigate the possibility of transferring such\n",
            "advances to molecular machine learning by building a chemical foundation model,\n",
            "ChemBERTa-2, using the language of SMILES. While labeled data for molecular\n",
            "prediction tasks is typically scarce, libraries of SMILES strings are readily\n",
            "available. In this work, we build upon ChemBERTa by optimizing the pretraining\n",
            "process. We compare multi-task and self-supervised pretraining by varying\n",
            "hyperparameters and pretraining dataset size, up to 77M compounds from PubChem.\n",
            "To our knowledge, the 77M set constitutes one of the largest datasets used for\n",
            "molecular pretraining to date. We find that with these pretraining\n",
            "improvements, we are competitive with existing state-of-the-art architectures\n",
            "on the MoleculeNet benchmark suite. We analyze the degree to which improvements\n",
            "in pretraining translate to improvement on downstream tasks.\n",
            "\n",
            "1533. Title: GRAPH-CONSTRAINED DIFFUSION FOR END-TO-END PATH PLANNING\n",
            "   Abstract: For safely applying reinforcement learning algorithms on high-dimensional\n",
            "nonlinear dynamical systems, a simplified system model is used to formulate a\n",
            "safe reinforcement learning framework. Based on the simplified system model, a\n",
            "low-dimensional representation of the safe region is identified and is used to\n",
            "provide safety estimates for learning algorithms. However, finding a satisfying\n",
            "simplified system model for complex dynamical systems usually requires a\n",
            "considerable amount of effort. To overcome this limitation, we propose in this\n",
            "work a general data-driven approach that is able to efficiently learn a\n",
            "low-dimensional representation of the safe region. Through an online adaptation\n",
            "method, the low-dimensional representation is updated by using the feedback\n",
            "data such that more accurate safety estimates are obtained. The performance of\n",
            "the proposed approach for identifying the low-dimensional representation of the\n",
            "safe region is demonstrated with a quadcopter example. The results show that,\n",
            "compared to previous work, a more reliable and representative low-dimensional\n",
            "representation of the safe region is derived, which then extends the\n",
            "applicability of the safe reinforcement learning framework.\n",
            "\n",
            "1534. Title: Are Models Biased on Text without Gender-related Language?\n",
            "   Abstract: We propose that the grokking phenomenon, where the train loss of a neural\n",
            "network decreases much earlier than its test loss, can arise due to a neural\n",
            "network transitioning from lazy training dynamics to a rich, feature learning\n",
            "regime. To illustrate this mechanism, we study the simple setting of vanilla\n",
            "gradient descent on a polynomial regression problem with a two layer neural\n",
            "network which exhibits grokking without regularization in a way that cannot be\n",
            "explained by existing theories. We identify sufficient statistics for the test\n",
            "loss of such a network, and tracking these over training reveals that grokking\n",
            "arises in this setting when the network first attempts to fit a kernel\n",
            "regression solution with its initial features, followed by late-time feature\n",
            "learning where a generalizing solution is identified after train loss is\n",
            "already low. We find that the key determinants of grokking are the rate of\n",
            "feature learning -- which can be controlled precisely by parameters that scale\n",
            "the network output -- and the alignment of the initial features with the target\n",
            "function $y(x)$. We argue this delayed generalization arises when (1) the top\n",
            "eigenvectors of the initial neural tangent kernel and the task labels $y(x)$\n",
            "are misaligned, but (2) the dataset size is large enough so that it is possible\n",
            "for the network to generalize eventually, but not so large that train loss\n",
            "perfectly tracks test loss at all epochs, and (3) the network begins training\n",
            "in the lazy regime so does not learn features immediately. We conclude with\n",
            "evidence that this transition from lazy (linear model) to rich training\n",
            "(feature learning) can control grokking in more general settings, like on\n",
            "MNIST, one-layer Transformers, and student-teacher networks.\n",
            "\n",
            "1535. Title: MERT: Acoustic Music Understanding Model with Large-Scale Self-supervised Training\n",
            "   Abstract: The performance of optimization-based robot motion planning algorithms is\n",
            "highly dependent on the initial solutions, commonly obtained by running a\n",
            "sampling-based planner to obtain a collision-free path. However, these methods\n",
            "can be slow in high-dimensional and complex scenes and produce non-smooth\n",
            "solutions. Given previously solved path-planning problems, it is highly\n",
            "desirable to learn their distribution and use it as a prior for new similar\n",
            "problems. Several works propose utilizing this prior to bootstrap the motion\n",
            "planning problem, either by sampling initial solutions from it, or using its\n",
            "distribution in a maximum-a-posterior formulation for trajectory optimization.\n",
            "In this work, we introduce Motion Planning Diffusion (MPD), an algorithm that\n",
            "learns trajectory distribution priors with diffusion models. These generative\n",
            "models have shown increasing success in encoding multimodal data and have\n",
            "desirable properties for gradient-based motion planning, such as cost guidance.\n",
            "Given a motion planning problem, we construct a cost function and sample from\n",
            "the posterior distribution using the learned prior combined with the cost\n",
            "function gradients during the denoising process. Instead of learning the prior\n",
            "on all trajectory waypoints, we propose learning a lower-dimensional\n",
            "representation of a trajectory using linear motion primitives, particularly\n",
            "B-spline curves. This parametrization guarantees that the generated trajectory\n",
            "is smooth, can be interpolated at higher frequencies, and needs fewer\n",
            "parameters than a dense waypoint representation. We demonstrate the results of\n",
            "our method ranging from simple 2D to more complex tasks using a 7-dof robot arm\n",
            "manipulator. In addition to learning from simulated data, we also use human\n",
            "demonstrations on a real-world pick-and-place task.\n",
            "\n",
            "1536. Title: SuRe: Summarizing Retrievals using Answer Candidates for Open-domain QA of LLMs\n",
            "   Abstract: It is often thought that the existence of other worlds cannot be\n",
            "scientifically verified and therefore should be treated as philosophical\n",
            "speculation. In this article, I describe several methods for determining if\n",
            "other worlds exist, even without interacting with them. These methods are based\n",
            "on the following premise: if there are many worlds, then the statistical\n",
            "properties of a natural process are biased when measured by an observer whose\n",
            "existence was influenced by the process. The bias is always in the same\n",
            "direction, making the process appear more beneficial for the existence of the\n",
            "observer than it actually is. I suggest several potential ways of measuring the\n",
            "bias, show through a simple model of population dynamics how the bias is\n",
            "generated, and briefly consider whether our current drop in population growth\n",
            "is evidence of many worlds.\n",
            "\n",
            "1537. Title: Reverse Forward Curriculum Learning for Extreme Sample and Demo Efficiency\n",
            "   Abstract: Low-Rank Adaptation (LoRA) has recently gained attention for fine-tuning\n",
            "foundation models by incorporating trainable low-rank matrices, thereby\n",
            "reducing the number of trainable parameters. While LoRA offers numerous\n",
            "advantages, its applicability for real-time serving to a diverse and global\n",
            "user base is constrained by its incapability to handle multiple task-specific\n",
            "adapters efficiently. This imposes a performance bottleneck in scenarios\n",
            "requiring personalized, task-specific adaptations for each incoming request. To\n",
            "mitigate this constraint, we introduce Fast LoRA (FLoRA), a framework in which\n",
            "each input example in a minibatch can be associated with its unique low-rank\n",
            "adaptation weights, allowing for efficient batching of heterogeneous requests.\n",
            "We empirically demonstrate that FLoRA retains the performance merits of LoRA,\n",
            "showcasing competitive results on the MultiPL-E code generation benchmark\n",
            "spanning over 8 languages and a multilingual speech recognition task across 6\n",
            "languages.\n",
            "\n",
            "1538. Title: Independent-Set Design of Experiments for Estimating Treatment and Spillover Effects under Network Interference\n",
            "   Abstract: Reinforcement learning (RL) presents a promising framework to learn policies\n",
            "through environment interaction, but often requires an infeasible amount of\n",
            "interaction data to solve complex tasks from sparse rewards. One direction\n",
            "includes augmenting RL with offline data demonstrating desired tasks, but past\n",
            "work often require a lot of high-quality demonstration data that is difficult\n",
            "to obtain, especially for domains such as robotics. Our approach consists of a\n",
            "reverse curriculum followed by a forward curriculum. Unique to our approach\n",
            "compared to past work is the ability to efficiently leverage more than one\n",
            "demonstration via a per-demonstration reverse curriculum generated via state\n",
            "resets. The result of our reverse curriculum is an initial policy that performs\n",
            "well on a narrow initial state distribution and helps overcome difficult\n",
            "exploration problems. A forward curriculum is then used to accelerate the\n",
            "training of the initial policy to perform well on the full initial state\n",
            "distribution of the task and improve demonstration and sample efficiency. We\n",
            "show how the combination of a reverse curriculum and forward curriculum in our\n",
            "method, RFCL, enables significant improvements in demonstration and sample\n",
            "efficiency compared against various state-of-the-art\n",
            "learning-from-demonstration baselines, even solving previously unsolvable tasks\n",
            "that require high precision and control.\n",
            "\n",
            "1539. Title: Geometry-Aware Projective Mapping for Unbounded Neural Radiance Fields\n",
            "   Abstract: Neural radiance fields enable state-of-the-art photorealistic view synthesis.\n",
            "However, existing radiance field representations are either too\n",
            "compute-intensive for real-time rendering or require too much memory to scale\n",
            "to large scenes. We present a Memory-Efficient Radiance Field (MERF)\n",
            "representation that achieves real-time rendering of large-scale scenes in a\n",
            "browser. MERF reduces the memory consumption of prior sparse volumetric\n",
            "radiance fields using a combination of a sparse feature grid and\n",
            "high-resolution 2D feature planes. To support large-scale unbounded scenes, we\n",
            "introduce a novel contraction function that maps scene coordinates into a\n",
            "bounded volume while still allowing for efficient ray-box intersection. We\n",
            "design a lossless procedure for baking the parameterization used during\n",
            "training into a model that achieves real-time rendering while still preserving\n",
            "the photorealistic view synthesis quality of a volumetric radiance field.\n",
            "\n",
            "1540. Title: Linearity of Relation Decoding in Transformer Language Models\n",
            "   Abstract: We introduce a method to train vision-language models for remote-sensing\n",
            "images without using any textual annotations. Our key insight is to use\n",
            "co-located internet imagery taken on the ground as an intermediary for\n",
            "connecting remote-sensing images and language. Specifically, we train an image\n",
            "encoder for remote sensing images to align with the image encoder of CLIP using\n",
            "a large amount of paired internet and satellite images. Our unsupervised\n",
            "approach enables the training of a first-of-its-kind large-scale vision\n",
            "language model (VLM) for remote sensing images at two different resolutions. We\n",
            "show that these VLMs enable zero-shot, open-vocabulary image classification,\n",
            "retrieval, segmentation and visual question answering for satellite images. On\n",
            "each of these tasks, our VLM trained without textual annotations outperforms\n",
            "existing VLMs trained with supervision, with gains of up to 20% for\n",
            "classification and 80% for segmentation.\n",
            "\n",
            "1541. Title: On the Hardness of Constrained Cooperative Multi-Agent Reinforcement Learning\n",
            "   Abstract: Much of the knowledge encoded in transformer language models (LMs) may be\n",
            "expressed in terms of relations: relations between words and their synonyms,\n",
            "entities and their attributes, etc. We show that, for a subset of relations,\n",
            "this computation is well-approximated by a single linear transformation on the\n",
            "subject representation. Linear relation representations may be obtained by\n",
            "constructing a first-order approximation to the LM from a single prompt, and\n",
            "they exist for a variety of factual, commonsense, and linguistic relations.\n",
            "However, we also identify many cases in which LM predictions capture relational\n",
            "knowledge accurately, but this knowledge is not linearly encoded in their\n",
            "representations. Our results thus reveal a simple, interpretable, but\n",
            "heterogeneously deployed knowledge representation strategy in transformer LMs.\n",
            "\n",
            "1542. Title: Learning No-Regret Sparse Generalized Linear Models with Varying Observation(s)\n",
            "   Abstract: An essential and challenging problem in causal inference is causal effect\n",
            "estimation from observational data. The problem becomes more difficult with the\n",
            "presence of unobserved confounding variables. The front-door adjustment is a\n",
            "practical approach for dealing with unobserved confounding variables. However,\n",
            "the restriction for the standard front-door adjustment is difficult to satisfy\n",
            "in practice. In this paper, we relax some of the restrictions by proposing the\n",
            "concept of conditional front-door (CFD) adjustment and develop the theorem that\n",
            "guarantees the causal effect identifiability of CFD adjustment. Furthermore, as\n",
            "it is often impossible for a CFD variable to be given in practice, it is\n",
            "desirable to learn it from data. By leveraging the ability of deep generative\n",
            "models, we propose CFDiVAE to learn the representation of the CFD adjustment\n",
            "variable directly from data with the identifiable Variational AutoEncoder and\n",
            "formally prove the model identifiability. Extensive experiments on synthetic\n",
            "datasets validate the effectiveness of CFDiVAE and its superiority over\n",
            "existing methods. The experiments also show that the performance of CFDiVAE is\n",
            "less sensitive to the causal strength of unobserved confounding variables. We\n",
            "further apply CFDiVAE to a real-world dataset to demonstrate its potential\n",
            "application.\n",
            "\n",
            "1543. Title: Batch normalization is sufficient for universal function approximation in CNNs\n",
            "   Abstract: In 2014, Amin, Heidari, and Kearns proved that tree networks can be learned\n",
            "by observing only the infected set of vertices of the contagion process under\n",
            "the independent cascade model, in both the active and passive query models.\n",
            "They also showed empirically that simple extensions of their algorithms work on\n",
            "sparse networks. In this work, we focus on the active model. We prove that a\n",
            "simple modification of Amin et al.'s algorithm works on more general classes of\n",
            "networks, namely (i) networks with large girth and low path growth rate, and\n",
            "(ii) networks with bounded degree. This also provides partial theoretical\n",
            "explanation for Amin et al.'s experiments on sparse networks.\n",
            "\n",
            "1544. Title: STARC: A General Framework For Quantifying Differences Between Reward Functions\n",
            "   Abstract: Multi-agent reinforcement learning (MARL) extends (single-agent)\n",
            "reinforcement learning (RL) by introducing additional agents and (potentially)\n",
            "partial observability of the environment. Consequently, algorithms for solving\n",
            "MARL problems incorporate various extensions beyond traditional RL methods,\n",
            "such as a learned communication protocol between cooperative agents that\n",
            "enables exchange of private information or adaptive modeling of opponents in\n",
            "competitive settings. One popular algorithmic construct is a memory mechanism\n",
            "such that an agent's decisions can depend not only upon the current state but\n",
            "also upon the history of observed states and actions. In this paper, we study\n",
            "how a memory mechanism can be useful in environments with different properties,\n",
            "such as observability, internality and presence of a communication channel.\n",
            "Using both prior work and new experiments, we show that a memory mechanism is\n",
            "helpful when learning agents need to model other agents and/or when\n",
            "communication is constrained in some way; however we must to be cautious of\n",
            "agents achieving effective memoryfulness through other means.\n",
            "\n",
            "1545. Title: MVSFormer++: Revealing the Devil in Transformer's Details for Multi-View Stereo\n",
            "   Abstract: While score-based generative models (SGMs) have achieved remarkable success\n",
            "in enormous image generation tasks, their mathematical foundations are still\n",
            "limited. In this paper, we analyze the approximation and generalization of SGMs\n",
            "in learning a family of sub-Gaussian probability distributions. We introduce a\n",
            "notion of complexity for probability distributions in terms of their relative\n",
            "density with respect to the standard Gaussian measure. We prove that if the\n",
            "log-relative density can be locally approximated by a neural network whose\n",
            "parameters can be suitably bounded, then the distribution generated by\n",
            "empirical score matching approximates the target distribution in total\n",
            "variation with a dimension-independent rate. We illustrate our theory through\n",
            "examples, which include certain mixtures of Gaussians. An essential ingredient\n",
            "of our proof is to derive a dimension-free deep neural network approximation\n",
            "rate for the true score function associated with the forward process, which is\n",
            "interesting in its own right.\n",
            "\n",
            "1546. Title: CLEX: Continuous Length Extrapolation for Large Language Models\n",
            "   Abstract: Interference is ubiquitous when conducting causal experiments over networks.\n",
            "Except for certain network structures, causal inference on the network in the\n",
            "presence of interference is difficult due to the entanglement between the\n",
            "treatment assignments and the interference levels. In this article, we conduct\n",
            "causal inference under interference on an observed, sparse but connected\n",
            "network, and we propose a novel design of experiments based on an independent\n",
            "set. Compared to conventional designs, the independent-set design focuses on an\n",
            "independent subset of data and controls their interference exposures through\n",
            "the assignments to the rest (auxiliary set). We provide a lower bound on the\n",
            "size of the independent set from a greedy algorithm , and justify the\n",
            "theoretical performance of estimators under the proposed design. Our approach\n",
            "is capable of estimating both spillover effects and treatment effects. We\n",
            "justify its superiority over conventional methods and illustrate the empirical\n",
            "performance through simulations.\n",
            "\n",
            "1547. Title: Benign Oscillation of Stochastic Gradient Descent with Large Learning Rate\n",
            "   Abstract: Multi-agent deep reinforcement learning makes optimal decisions dependent on\n",
            "system states observed by agents, but any uncertainty on the observations may\n",
            "mislead agents to take wrong actions. The Mean-Field Actor-Critic reinforcement\n",
            "learning (MFAC) is well-known in the multi-agent field since it can effectively\n",
            "handle a scalability problem. However, it is sensitive to state perturbations\n",
            "that can significantly degrade the team rewards. This work proposes a Robust\n",
            "Mean-field Actor-Critic reinforcement learning (RoMFAC) that has two\n",
            "innovations: 1) a new objective function of training actors, composed of a\n",
            "\\emph{policy gradient function} that is related to the expected cumulative\n",
            "discount reward on sampled clean states and an \\emph{action loss function} that\n",
            "represents the difference between actions taken on clean and adversarial\n",
            "states; and 2) a repetitive regularization of the action loss, ensuring the\n",
            "trained actors to obtain excellent performance. Furthermore, this work proposes\n",
            "a game model named a State-Adversarial Stochastic Game (SASG). Despite the Nash\n",
            "equilibrium of SASG may not exist, adversarial perturbations to states in the\n",
            "RoMFAC are proven to be defensible based on SASG. Experimental results show\n",
            "that RoMFAC is robust against adversarial perturbations while maintaining its\n",
            "competitive performance in environments without perturbations.\n",
            "\n",
            "1548. Title: Mixture of Weak and Strong Experts on Graphs\n",
            "   Abstract: In this work, we theoretically investigate the generalization properties of\n",
            "neural networks (NN) trained by stochastic gradient descent (SGD) algorithm\n",
            "with large learning rates. Under such a training regime, our finding is that,\n",
            "the oscillation of the NN weights caused by the large learning rate SGD\n",
            "training turns out to be beneficial to the generalization of the NN, which\n",
            "potentially improves over the same NN trained by SGD with small learning rates\n",
            "that converges more smoothly. In view of this finding, we call such a\n",
            "phenomenon \"benign oscillation\". Our theory towards demystifying such a\n",
            "phenomenon builds upon the feature learning perspective of deep learning.\n",
            "Specifically, we consider a feature-noise data generation model that consists\n",
            "of (i) weak features which have a small $\\ell_2$-norm and appear in each data\n",
            "point; (ii) strong features which have a larger $\\ell_2$-norm but only appear\n",
            "in a certain fraction of all data points; and (iii) noise. We prove that NNs\n",
            "trained by oscillating SGD with a large learning rate can effectively learn the\n",
            "weak features in the presence of those strong features. In contrast, NNs\n",
            "trained by SGD with a small learning rate can only learn the strong features\n",
            "but makes little progress in learning the weak features. Consequently, when it\n",
            "comes to the new testing data which consist of only weak features, the NN\n",
            "trained by oscillating SGD with a large learning rate could still make correct\n",
            "predictions consistently, while the NN trained by small learning rate SGD\n",
            "fails. Our theory sheds light on how large learning rate training benefits the\n",
            "generalization of NNs. Experimental results demonstrate our finding on \"benign\n",
            "oscillation\".\n",
            "\n",
            "1549. Title: Counterfactual Density Estimation using Kernel Stein Discrepancies\n",
            "   Abstract: Causal effects are usually studied in terms of the means of counterfactual\n",
            "distributions, which may be insufficient in many scenarios. Given a class of\n",
            "densities known up to normalizing constants, we propose to model counterfactual\n",
            "distributions by minimizing kernel Stein discrepancies in a doubly robust\n",
            "manner. This enables the estimation of counterfactuals over large classes of\n",
            "distributions while exploiting the desired double robustness. We present a\n",
            "theoretical analysis of the proposed estimator, providing sufficient conditions\n",
            "for consistency and asymptotic normality, as well as an examination of its\n",
            "empirical performance.\n",
            "\n",
            "1550. Title: Unleashing the Potential of Fractional Calculus in Graph Neural Networks with FROND\n",
            "   Abstract: We study online classification of features into labels with general\n",
            "hypothesis classes. In our setting, true labels are determined by some function\n",
            "within the hypothesis class but are corrupted by unknown stochastic noise, and\n",
            "the features are generated adversarially. Predictions are made using observed\n",
            "noisy labels and noiseless features, while the performance is measured via\n",
            "minimax risk when comparing against true labels. The noise mechanism is modeled\n",
            "via a general noise kernel that specifies, for any individual data point, a set\n",
            "of distributions from which the actual noisy label distribution is chosen. We\n",
            "show that minimax risk is tightly characterized (up to a logarithmic factor of\n",
            "the hypothesis class size) by the Hellinger gap of the noisy label\n",
            "distributions induced by the kernel, independent of other properties such as\n",
            "the means and variances of the noise. Our main technique is based on a novel\n",
            "reduction to an online comparison scheme of two hypotheses, along with a new\n",
            "conditional version of Le Cam-Birg\\'e testing suitable for online settings. Our\n",
            "work provides the first comprehensive characterization for noisy online\n",
            "classification with guarantees with respect to the ground truth while\n",
            "addressing general noisy observations.\n",
            "\n",
            "1551. Title: Robust Classification via Regression for Learning with Noisy Labels\n",
            "   Abstract: Realistic graphs contain both (1) rich self-features of nodes and (2)\n",
            "informative structures of neighborhoods, jointly handled by a Graph Neural\n",
            "Network (GNN) in the typical setup. We propose to decouple the two modalities\n",
            "by Mixture of weak and strong experts (Mowst), where the weak expert is a\n",
            "light-weight Multi-layer Perceptron (MLP), and the strong expert is an\n",
            "off-the-shelf GNN. To adapt the experts' collaboration to different target\n",
            "nodes, we propose a \"confidence\" mechanism based on the dispersion of the weak\n",
            "expert's prediction logits. The strong expert is conditionally activated in the\n",
            "low-confidence region when either the node's classification relies on\n",
            "neighborhood information, or the weak expert has low model quality. We reveal\n",
            "interesting training dynamics by analyzing the influence of the confidence\n",
            "function on loss: our training algorithm encourages the specialization of each\n",
            "expert by effectively generating soft splitting of the graph. In addition, our\n",
            "\"confidence\" design imposes a desirable bias toward the strong expert to\n",
            "benefit from GNN's better generalization capability. Mowst is easy to optimize\n",
            "and achieves strong expressive power, with a computation cost comparable to a\n",
            "single GNN. Empirically, Mowst on 4 backbone GNN architectures show significant\n",
            "accuracy improvement on 6 standard node classification benchmarks, including\n",
            "both homophilous and heterophilous graphs\n",
            "(https://github.com/facebookresearch/mowst-gnn).\n",
            "\n",
            "1552. Title: Navigating Text-To-Image Customization: From LyCORIS Fine-Tuning to Model Evaluation\n",
            "   Abstract: We introduce the FRactional-Order graph Neural Dynamical network (FROND), a\n",
            "new continuous graph neural network (GNN) framework. Unlike traditional\n",
            "continuous GNNs that rely on integer-order differential equations, FROND\n",
            "employs the Caputo fractional derivative to leverage the non-local properties\n",
            "of fractional calculus. This approach enables the capture of long-term\n",
            "dependencies in feature updates, moving beyond the Markovian update mechanisms\n",
            "in conventional integer-order models and offering enhanced capabilities in\n",
            "graph representation learning. We offer an interpretation of the node feature\n",
            "updating process in FROND from a non-Markovian random walk perspective when the\n",
            "feature updating is particularly governed by a diffusion process. We\n",
            "demonstrate analytically that oversmoothing can be mitigated in this setting.\n",
            "Experimentally, we validate the FROND framework by comparing the fractional\n",
            "adaptations of various established integer-order continuous GNNs, demonstrating\n",
            "their consistently improved performance and underscoring the framework's\n",
            "potential as an effective extension to enhance traditional continuous GNNs. The\n",
            "code is available at \\url{https://github.com/zknus/ICLR2024-FROND}.\n",
            "\n",
            "1553. Title: $\\texttt{NAISR}$: A 3D Neural Additive Model for Interpretable Shape Representation\n",
            "   Abstract: Inverse design, where we seek to design input variables in order to optimize\n",
            "an underlying objective function, is an important problem that arises across\n",
            "fields such as mechanical engineering to aerospace engineering. Inverse design\n",
            "is typically formulated as an optimization problem, with recent works\n",
            "leveraging optimization across learned dynamics models. However, as models are\n",
            "optimized they tend to fall into adversarial modes, preventing effective\n",
            "sampling. We illustrate that by instead optimizing over the learned energy\n",
            "function captured by the diffusion model, we can avoid such adversarial\n",
            "examples and significantly improve design performance. We further illustrate\n",
            "how such a design system is compositional, enabling us to combine multiple\n",
            "different diffusion models representing subcomponents of our desired system to\n",
            "design systems with every specified component. In an N-body interaction task\n",
            "and a challenging 2D multi-airfoil design task, we demonstrate that by\n",
            "composing the learned diffusion model at test time, our method allows us to\n",
            "design initial states and boundary shapes that are more complex than those in\n",
            "the training data. Our method generalizes to more objects for N-body dataset\n",
            "and discovers formation flying to minimize drag in the multi-airfoil design\n",
            "task. Project website and code can be found at\n",
            "https://github.com/AI4Science-WestlakeU/cindm.\n",
            "\n",
            "1554. Title: LUM-ViT: Learnable Under-sampling Mask Vision Transformer for Bandwidth Limited Optical Signal Acquisition\n",
            "   Abstract: Spiking neural networks (SNNs) transmit information through discrete spikes,\n",
            "which performs well in processing spatial-temporal information. Due to the\n",
            "non-differentiable characteristic, there still exist difficulties in designing\n",
            "well-performed SNNs. Recently, SNNs trained with backpropagation have shown\n",
            "superior performance due to the proposal of the gradient approximation.\n",
            "However, the performance on complex tasks is still far away from the deep\n",
            "neural networks. Taking inspiration from the autapse in the brain which\n",
            "connects the spiking neurons with a self-feedback connection, we apply an\n",
            "adaptive time-delayed self-feedback on the membrane potential to regulate the\n",
            "spike precisions. As well as, we apply the balanced excitatory and inhibitory\n",
            "neurons mechanism to control the spiking neurons' output dynamically. With the\n",
            "combination of the two mechanisms, we propose a deep spiking neural network\n",
            "with adaptive self-feedback and balanced excitatory and inhibitory neurons\n",
            "(BackEISNN). The experimental results on several standard datasets have shown\n",
            "that the two modules not only accelerate the convergence of the network but\n",
            "also improve the accuracy. For the MNIST, FashionMNIST, and N-MNIST datasets,\n",
            "our model has achieved state-of-the-art performance. For the CIFAR10 dataset,\n",
            "our BackEISNN also gets remarkable performance on a relatively light structure\n",
            "that competes against state-of-the-art SNNs.\n",
            "\n",
            "1555. Title: Scalable Diffusion for Materials Generation\n",
            "   Abstract: Vision-language models (VLMs) mainly rely on contrastive training to learn\n",
            "general-purpose representations of images and captions. We focus on the\n",
            "situation when one image is associated with several captions, each caption\n",
            "containing both information shared among all captions and unique information\n",
            "per caption about the scene depicted in the image. In such cases, it is unclear\n",
            "whether contrastive losses are sufficient for learning task-optimal\n",
            "representations that contain all the information provided by the captions or\n",
            "whether the contrastive learning setup encourages the learning of a simple\n",
            "shortcut that minimizes contrastive loss. We introduce synthetic shortcuts for\n",
            "vision-language: a training and evaluation framework where we inject synthetic\n",
            "shortcuts into image-text data. We show that contrastive VLMs trained from\n",
            "scratch or fine-tuned with data containing these synthetic shortcuts mainly\n",
            "learn features that represent the shortcut. Hence, contrastive losses are not\n",
            "sufficient to learn task-optimal representations, i.e., representations that\n",
            "contain all task-relevant information shared between the image and associated\n",
            "captions. We examine two methods to reduce shortcut learning in our training\n",
            "and evaluation framework: (i) latent target decoding and (ii) implicit feature\n",
            "modification. We show empirically that both methods improve performance on the\n",
            "evaluation task, but only partly reduce shortcut learning when training and\n",
            "evaluating with our shortcut learning framework. Hence, we show the difficulty\n",
            "and challenge of our shortcut learning framework for contrastive\n",
            "vision-language representation learning.\n",
            "\n",
            "1556. Title: Compositional Generative Inverse Design\n",
            "   Abstract: While prior domain generalization (DG) benchmarks consider train-test dataset\n",
            "heterogeneity, we evaluate Federated DG which introduces federated learning\n",
            "(FL) specific challenges. Additionally, we explore domain-based heterogeneity\n",
            "in clients' local datasets - a realistic Federated DG scenario. Prior Federated\n",
            "DG evaluations are limited in terms of the number or heterogeneity of clients\n",
            "and dataset diversity. To address this gap, we propose an Federated DG\n",
            "benchmark methodology that enables control of the number and heterogeneity of\n",
            "clients and provides metrics for dataset difficulty. We then apply our\n",
            "methodology to evaluate 14 Federated DG methods, which include centralized DG\n",
            "methods adapted to the FL context, FL methods that handle client heterogeneity,\n",
            "and methods designed specifically for Federated DG. Our results suggest that\n",
            "despite some progress, there remain significant performance gaps in Federated\n",
            "DG particularly when evaluating with a large number of clients, high client\n",
            "heterogeneity, or more realistic datasets. Please check our extendable\n",
            "benchmark code here: https://github.com/inouye-lab/FedDG_Benchmark.\n",
            "\n",
            "1557. Title: Benchmarking Algorithms for Federated Domain Generalization\n",
            "   Abstract: Optimal Transport is a useful metric to compare probability distributions and\n",
            "to compute a pairing given a ground cost. Its entropic regularization variant\n",
            "(eOT) is crucial to have fast algorithms and reflect fuzzy/noisy matchings.\n",
            "This work focuses on Inverse Optimal Transport (iOT), the problem of inferring\n",
            "the ground cost from samples drawn from a coupling that solves an eOT problem.\n",
            "It is a relevant problem that can be used to infer unobserved/missing links,\n",
            "and to obtain meaningful information about the structure of the ground cost\n",
            "yielding the pairing. On one side, iOT benefits from convexity, but on the\n",
            "other side, being ill-posed, it requires regularization to handle the sampling\n",
            "noise. This work presents an in-depth theoretical study of the l1\n",
            "regularization to model for instance Euclidean costs with sparse interactions\n",
            "between features. Specifically, we derive a sufficient condition for the robust\n",
            "recovery of the sparsity of the ground cost that can be seen as a far reaching\n",
            "generalization of the Lasso's celebrated Irrepresentability Condition. To\n",
            "provide additional insight into this condition, we work out in detail the\n",
            "Gaussian case. We show that as the entropic penalty varies, the iOT problem\n",
            "interpolates between a graphical Lasso and a classical Lasso, thereby\n",
            "establishing a connection between iOT and graph estimation, an important\n",
            "problem in ML.\n",
            "\n",
            "1558. Title: On the Effect of Batch Size in Byzantine-Robust Distributed Learning\n",
            "   Abstract: Text-to-image generative models have garnered immense attention for their\n",
            "ability to produce high-fidelity images from text prompts. Among these, Stable\n",
            "Diffusion distinguishes itself as a leading open-source model in this\n",
            "fast-growing field. However, the intricacies of fine-tuning these models pose\n",
            "multiple challenges from new methodology integration to systematic evaluation.\n",
            "Addressing these issues, this paper introduces LyCORIS (Lora beYond\n",
            "Conventional methods, Other Rank adaptation Implementations for Stable\n",
            "diffusion) [https://github.com/KohakuBlueleaf/LyCORIS], an open-source library\n",
            "that offers a wide selection of fine-tuning methodologies for Stable Diffusion.\n",
            "Furthermore, we present a thorough framework for the systematic assessment of\n",
            "varied fine-tuning techniques. This framework employs a diverse suite of\n",
            "metrics and delves into multiple facets of fine-tuning, including\n",
            "hyperparameter adjustments and the evaluation with different prompt types\n",
            "across various concept categories. Through this comprehensive approach, our\n",
            "work provides essential insights into the nuanced effects of fine-tuning\n",
            "parameters, bridging the gap between state-of-the-art research and practical\n",
            "application.\n",
            "\n",
            "1559. Title: Consistency-guided Prompt Learning for Vision-Language Models\n",
            "   Abstract: Generative models trained on internet-scale data are capable of generating\n",
            "novel and realistic texts, images, and videos. A natural next question is\n",
            "whether these models can advance science, for example by generating novel\n",
            "stable materials. Traditionally, models with explicit structures (e.g., graphs)\n",
            "have been used in modeling structural relationships in scientific data (e.g.,\n",
            "atoms and bonds in crystals), but generating structures can be difficult to\n",
            "scale to large and complex systems. Another challenge in generating materials\n",
            "is the mismatch between standard generative modeling metrics and downstream\n",
            "applications. For instance, common metrics such as the reconstruction error do\n",
            "not correlate well with the downstream goal of discovering stable materials. In\n",
            "this work, we tackle the scalability challenge by developing a unified crystal\n",
            "representation that can represent any crystal structure (UniMat), followed by\n",
            "training a diffusion probabilistic model on these UniMat representations. Our\n",
            "empirical results suggest that despite the lack of explicit structure modeling,\n",
            "UniMat can generate high fidelity crystal structures from larger and more\n",
            "complex chemical systems, outperforming previous graph-based approaches under\n",
            "various generative modeling metrics. To better connect the generation quality\n",
            "of materials to downstream applications, such as discovering novel stable\n",
            "materials, we propose additional metrics for evaluating generative models of\n",
            "materials, including per-composition formation energy and stability with\n",
            "respect to convex hulls through decomposition energy from Density Function\n",
            "Theory (DFT). Lastly, we show that conditional generation with UniMat can scale\n",
            "to previously established crystal datasets with up to millions of crystals\n",
            "structures, outperforming random structure search (the current leading method\n",
            "for structure discovery) in discovering new stable materials.\n",
            "\n",
            "1560. Title: Toward Student-oriented Teacher Network Training for Knowledge Distillation\n",
            "   Abstract: In this paper, we propose a generalization of the Batch Normalization (BN)\n",
            "algorithm, diminishing batch normalization (DBN), where we update the BN\n",
            "parameters in a diminishing moving average way. BN is very effective in\n",
            "accelerating the convergence of a neural network training phase that it has\n",
            "become a common practice. Our proposed DBN algorithm remains the overall\n",
            "structure of the original BN algorithm while introduces a weighted averaging\n",
            "update to some trainable parameters. We provide an analysis of the convergence\n",
            "of the DBN algorithm that converges to a stationary point with respect to\n",
            "trainable parameters. Our analysis can be easily generalized for original BN\n",
            "algorithm by setting some parameters to constant. To the best knowledge of\n",
            "authors, this analysis is the first of its kind for convergence with Batch\n",
            "Normalization introduced. We analyze a two-layer model with arbitrary\n",
            "activation function. The primary challenge of the analysis is the fact that\n",
            "some parameters are updated by gradient while others are not. The convergence\n",
            "analysis applies to any activation function that satisfies our common\n",
            "assumptions. In the numerical experiments, we test the proposed algorithm on\n",
            "complex modern CNN models with stochastic gradients and ReLU activation. We\n",
            "observe that DBN outperforms the original BN algorithm on MNIST, NI and\n",
            "CIFAR-10 datasets with reasonable complex FNN and CNN models.\n",
            "\n",
            "1561. Title: ADOPD: A Large-Scale Document Page Decomposition Dataset\n",
            "   Abstract: Determining the appropriate batch size for mini-batch gradient descent is\n",
            "always time consuming as it often relies on grid search. This paper considers a\n",
            "resizable mini-batch gradient descent (RMGD) algorithm based on a multi-armed\n",
            "bandit for achieving best performance in grid search by selecting an\n",
            "appropriate batch size at each epoch with a probability defined as a function\n",
            "of its previous success/failure. This probability encourages exploration of\n",
            "different batch size and then later exploitation of batch size with history of\n",
            "success. At each epoch, the RMGD samples a batch size from its probability\n",
            "distribution, then uses the selected batch size for mini-batch gradient\n",
            "descent. After obtaining the validation loss at each epoch, the probability\n",
            "distribution is updated to incorporate the effectiveness of the sampled batch\n",
            "size. The RMGD essentially assists the learning process to explore the possible\n",
            "domain of the batch size and exploit successful batch size. Experimental\n",
            "results show that the RMGD achieves performance better than the best performing\n",
            "single batch size. Furthermore, it, obviously, attains this performance in a\n",
            "shorter amount of time than grid search. It is surprising that the RMGD\n",
            "achieves better performance than grid search.\n",
            "\n",
            "1562. Title: Privacy-Preserving In-Context Learning for Large Language Models\n",
            "   Abstract: How to conduct teacher training for knowledge distillation is still an open\n",
            "problem. It has been widely observed that a best-performing teacher does not\n",
            "necessarily yield the best-performing student, suggesting a fundamental\n",
            "discrepancy between the current teacher training practice and the ideal teacher\n",
            "training strategy. To fill this gap, we explore the feasibility of training a\n",
            "teacher that is oriented toward student performance with empirical risk\n",
            "minimization (ERM). Our analyses are inspired by the recent findings that the\n",
            "effectiveness of knowledge distillation hinges on the teacher's capability to\n",
            "approximate the true label distribution of training inputs. We theoretically\n",
            "establish that the ERM minimizer can approximate the true label distribution of\n",
            "training data as long as the feature extractor of the learner network is\n",
            "Lipschitz continuous and is robust to feature transformations. In light of our\n",
            "theory, we propose a teacher training method SoTeacher which incorporates\n",
            "Lipschitz regularization and consistency regularization into ERM. Experiments\n",
            "on benchmark datasets using various knowledge distillation algorithms and\n",
            "teacher-student pairs confirm that SoTeacher can improve student accuracy\n",
            "consistently.\n",
            "\n",
            "1563. Title: PROGRAM: PROtotype GRAph Model based Pseudo-Label Learning for Test-Time Adaptation\n",
            "   Abstract: Large language models (LLMs) exploit in-context learning (ICL) to solve tasks\n",
            "with only a few demonstrations, but its mechanisms are not yet well-understood.\n",
            "Some works suggest that LLMs only recall already learned concepts from\n",
            "pre-training, while others hint that ICL performs implicit learning over\n",
            "demonstrations. We characterize two ways through which ICL leverages\n",
            "demonstrations. Task recognition (TR) captures the extent to which LLMs can\n",
            "recognize a task through demonstrations -- even without ground-truth labels --\n",
            "and apply their pre-trained priors, whereas task learning (TL) is the ability\n",
            "to capture new input-label mappings unseen in pre-training. Using a wide range\n",
            "of classification datasets and three LLM families (GPT-3, LLaMA and OPT), we\n",
            "design controlled experiments to disentangle the roles of TR and TL in ICL. We\n",
            "show that (1) models can achieve non-trivial performance with only TR, and TR\n",
            "does not further improve with larger models or more demonstrations; (2) LLMs\n",
            "acquire TL as the model scales, and TL's performance consistently improves with\n",
            "more demonstrations in context. Our findings unravel two different forces\n",
            "behind ICL and we advocate for discriminating them in future ICL research due\n",
            "to their distinct nature.\n",
            "\n",
            "1564. Title: Synaptic Weight Distributions Depend on the Geometry of Plasticity\n",
            "   Abstract: The alignment tuning process of large language models (LLMs) typically\n",
            "involves instruction learning through supervised fine-tuning (SFT) and\n",
            "preference tuning via reinforcement learning from human feedback (RLHF). A\n",
            "recent study, LIMA (Zhou et al. 2023), shows that using merely 1K examples for\n",
            "SFT can achieve significant alignment performance as well, suggesting that the\n",
            "effect of alignment tuning might be \"superficial.\" This raises questions about\n",
            "how exactly the alignment tuning transforms a base LLM.\n",
            "  We analyze the effect of alignment tuning by examining the token distribution\n",
            "shift between base LLMs and their aligned counterpart. Our findings reveal that\n",
            "base LLMs and their alignment-tuned versions perform nearly identically in\n",
            "decoding on the majority of token positions. Most distribution shifts occur\n",
            "with stylistic tokens. These direct evidence strongly supports the Superficial\n",
            "Alignment Hypothesis suggested by LIMA.\n",
            "  Based on these findings, we rethink the alignment of LLMs by posing the\n",
            "research question: how effectively can we align base LLMs without SFT or RLHF?\n",
            "To address this, we introduce a simple, tuning-free alignment method, URIAL.\n",
            "URIAL achieves effective alignment purely through in-context learning (ICL)\n",
            "with base LLMs, requiring as few as three constant stylistic examples and a\n",
            "system prompt. We conduct a fine-grained and interpretable evaluation on a\n",
            "diverse set of examples, named JUST-EVAL-INSTRUCT. Results demonstrate that\n",
            "base LLMs with URIAL can match or even surpass the performance of LLMs aligned\n",
            "with SFT or SFT+RLHF. We show that the gap between tuning-free and tuning-based\n",
            "alignment methods can be significantly reduced through strategic prompting and\n",
            "ICL. Our findings on the superficial nature of alignment tuning and results\n",
            "with URIAL suggest that deeper analysis and theoretical understanding of\n",
            "alignment is crucial to future LLM research.\n",
            "\n",
            "1565. Title: A Restoration Network as an Implicit Prior\n",
            "   Abstract: A growing literature in computational neuroscience leverages gradient descent\n",
            "and learning algorithms that approximate it to study synaptic plasticity in the\n",
            "brain. However, the vast majority of this work ignores a critical underlying\n",
            "assumption: the choice of distance for synaptic changes - i.e. the geometry of\n",
            "synaptic plasticity. Gradient descent assumes that the distance is Euclidean,\n",
            "but many other distances are possible, and there is no reason that biology\n",
            "necessarily uses Euclidean geometry. Here, using the theoretical tools provided\n",
            "by mirror descent, we show that the distribution of synaptic weights will\n",
            "depend on the geometry of synaptic plasticity. We use these results to show\n",
            "that experimentally-observed log-normal weight distributions found in several\n",
            "brain areas are not consistent with standard gradient descent (i.e. a Euclidean\n",
            "geometry), but rather with non-Euclidean distances. Finally, we show that it\n",
            "should be possible to experimentally test for different synaptic geometries by\n",
            "comparing synaptic weight distributions before and after learning. Overall, our\n",
            "work shows that the current paradigm in theoretical work on synaptic plasticity\n",
            "that assumes Euclidean synaptic geometry may be misguided and that it should be\n",
            "possible to experimentally determine the true geometry of synaptic plasticity\n",
            "in the brain.\n",
            "\n",
            "1566. Title: Are Bert Family Good Instruction Followers? A Study on Their Potential And Limitations\n",
            "   Abstract: Image denoisers have been shown to be powerful priors for solving inverse\n",
            "problems in imaging. In this work, we introduce a generalization of these\n",
            "methods that allows any image restoration network to be used as an implicit\n",
            "prior. The proposed method uses priors specified by deep neural networks\n",
            "pre-trained as general restoration operators. The method provides a principled\n",
            "approach for adapting state-of-the-art restoration models for other inverse\n",
            "problems. Our theoretical result analyzes its convergence to a stationary point\n",
            "of a global functional associated with the restoration operator. Numerical\n",
            "results show that the method using a super-resolution prior achieves\n",
            "state-of-the-art performance both quantitatively and qualitatively. Overall,\n",
            "this work offers a step forward for solving inverse problems by enabling the\n",
            "use of powerful pre-trained restoration models as priors.\n",
            "\n",
            "1567. Title: On the Stability of Expressive Positional Encodings for Graphs\n",
            "   Abstract: Fallacies are defective arguments with faulty reasoning. Detecting and\n",
            "classifying them is a crucial NLP task to prevent misinformation, manipulative\n",
            "claims, and biased decisions. However, existing fallacy classifiers are limited\n",
            "by the requirement for sufficient labeled data for training, which hinders\n",
            "their out-of-distribution (OOD) generalization abilities. In this paper, we\n",
            "focus on leveraging Large Language Models (LLMs) for zero-shot fallacy\n",
            "classification. To elicit fallacy-related knowledge and reasoning abilities of\n",
            "LLMs, we propose diverse single-round and multi-round prompting schemes,\n",
            "applying different task-specific instructions such as extraction,\n",
            "summarization, and Chain-of-Thought reasoning. With comprehensive experiments\n",
            "on benchmark datasets, we suggest that LLMs could be potential zero-shot\n",
            "fallacy classifiers. In general, LLMs under single-round prompting schemes have\n",
            "achieved acceptable zero-shot performances compared to the best full-shot\n",
            "baselines and can outperform them in all OOD inference scenarios and some\n",
            "open-domain tasks. Our novel multi-round prompting schemes can effectively\n",
            "bring about more improvements, especially for small LLMs. Our analysis further\n",
            "underlines the future research on zero-shot fallacy classification. Codes and\n",
            "data are available at: https://github.com/panFJCharlotte98/Fallacy_Detection.\n",
            "\n",
            "1568. Title: GeoDiffusion: Text-Prompted Geometric Control for Object Detection Data Generation\n",
            "   Abstract: Recent developments in offline reinforcement learning have uncovered the\n",
            "immense potential of diffusion modeling, which excels at representing\n",
            "heterogeneous behavior policies. However, sampling from diffusion policies is\n",
            "considerably slow because it necessitates tens to hundreds of iterative\n",
            "inference steps for one action. To address this issue, we propose to extract an\n",
            "efficient deterministic inference policy from critic models and pretrained\n",
            "diffusion behavior models, leveraging the latter to directly regularize the\n",
            "policy gradient with the behavior distribution's score function during\n",
            "optimization. Our method enjoys powerful generative capabilities of diffusion\n",
            "modeling while completely circumventing the computationally intensive and\n",
            "time-consuming diffusion sampling scheme, both during training and evaluation.\n",
            "Extensive results on D4RL tasks show that our method boosts action sampling\n",
            "speed by more than 25 times compared with various leading diffusion-based\n",
            "methods in locomotion tasks, while still maintaining state-of-the-art\n",
            "performance.\n",
            "\n",
            "1569. Title: Navigating Dataset Documentations in AI: A Large-Scale Analysis of Dataset Cards on HuggingFace\n",
            "   Abstract: Designing effective positional encodings for graphs is key to building\n",
            "powerful graph transformers and enhancing message-passing graph neural\n",
            "networks. Although widespread, using Laplacian eigenvectors as positional\n",
            "encodings faces two fundamental challenges: (1) \\emph{Non-uniqueness}: there\n",
            "are many different eigendecompositions of the same Laplacian, and (2)\n",
            "\\emph{Instability}: small perturbations to the Laplacian could result in\n",
            "completely different eigenspaces, leading to unpredictable changes in\n",
            "positional encoding. Despite many attempts to address non-uniqueness, most\n",
            "methods overlook stability, leading to poor generalization on unseen graph\n",
            "structures. We identify the cause of instability to be a ``hard partition'' of\n",
            "eigenspaces. Hence, we introduce Stable and Expressive Positional Encodings\n",
            "(SPE), an architecture for processing eigenvectors that uses eigenvalues to\n",
            "``softly partition'' eigenspaces. SPE is the first architecture that is (1)\n",
            "provably stable, and (2) universally expressive for basis invariant functions\n",
            "whilst respecting all symmetries of eigenvectors. Besides guaranteed stability,\n",
            "we prove that SPE is at least as expressive as existing methods, and highly\n",
            "capable of counting graph structures. Finally, we evaluate the effectiveness of\n",
            "our method on molecular property prediction, and out-of-distribution\n",
            "generalization tasks, finding improved generalization compared to existing\n",
            "positional encoding methods. Our code is available at\n",
            "\\url{https://github.com/Graph-COM/SPE}.\n",
            "\n",
            "1570. Title: Elucidating the Exposure Bias in Diffusion Models\n",
            "   Abstract: Differentially private learning algorithms inject noise into the learning\n",
            "process. While the most common private learning algorithm, DP-SGD, adds\n",
            "independent Gaussian noise in each iteration, recent work on matrix\n",
            "factorization mechanisms has shown empirically that introducing correlations in\n",
            "the noise can greatly improve their utility. We characterize the asymptotic\n",
            "learning utility for any choice of the correlation function, giving precise\n",
            "analytical bounds for linear regression and as the solution to a convex program\n",
            "for general convex functions. We show, using these bounds, how correlated noise\n",
            "provably improves upon vanilla DP-SGD as a function of problem parameters such\n",
            "as the effective dimension and condition number. Moreover, our analytical\n",
            "expression for the near-optimal correlation function circumvents the cubic\n",
            "complexity of the semi-definite program used to optimize the noise correlation\n",
            "matrix in previous work. We validate our theory with experiments on private\n",
            "deep learning. Our work matches or outperforms prior work while being efficient\n",
            "both in terms of compute and memory.\n",
            "\n",
            "1571. Title: How Over-Parameterization Slows Down Gradient Descent in Matrix Sensing: The Curses of Symmetry and Initialization\n",
            "   Abstract: Language Models (LMs) have greatly influenced diverse domains. However, their\n",
            "inherent limitation in comprehending 3D molecular structures has considerably\n",
            "constrained their potential in the biomolecular domain. To bridge this gap, we\n",
            "focus on 3D molecule-text interpretation, and propose 3D-MoLM: 3D-Molecular\n",
            "Language Modeling. Specifically, 3D-MoLM enables an LM to interpret and analyze\n",
            "3D molecules by equipping the LM with a 3D molecular encoder. This integration\n",
            "is achieved by a 3D molecule-text projector, bridging the 3D molecular\n",
            "encoder's representation space and the LM's input space. Moreover, to enhance\n",
            "3D-MoLM's ability of cross-modal molecular understanding and instruction\n",
            "following, we meticulously curated a 3D molecule-centric instruction tuning\n",
            "dataset -- 3D-MoIT. Through 3D molecule-text alignment and 3D molecule-centric\n",
            "instruction tuning, 3D-MoLM establishes an integration of 3D molecular encoder\n",
            "and LM. It significantly surpasses existing baselines on downstream tasks,\n",
            "including molecule-text retrieval, molecule captioning, and more challenging\n",
            "open-text molecular QA tasks, especially focusing on 3D-dependent properties.\n",
            "We release our codes and datasets at https://github.com/lsh0520/3D-MoLM.\n",
            "\n",
            "1572. Title: Correlated Noise Provably Beats Independent Noise for Differentially Private Learning\n",
            "   Abstract: Advances in machine learning are closely tied to the creation of datasets.\n",
            "While data documentation is widely recognized as essential to the reliability,\n",
            "reproducibility, and transparency of ML, we lack a systematic empirical\n",
            "understanding of current dataset documentation practices. To shed light on this\n",
            "question, here we take Hugging Face -- one of the largest platforms for sharing\n",
            "and collaborating on ML models and datasets -- as a prominent case study. By\n",
            "analyzing all 7,433 dataset documentation on Hugging Face, our investigation\n",
            "provides an overview of the Hugging Face dataset ecosystem and insights into\n",
            "dataset documentation practices, yielding 5 main findings: (1) The dataset card\n",
            "completion rate shows marked heterogeneity correlated with dataset popularity.\n",
            "(2) A granular examination of each section within the dataset card reveals that\n",
            "the practitioners seem to prioritize Dataset Description and Dataset Structure\n",
            "sections, while the Considerations for Using the Data section receives the\n",
            "lowest proportion of content. (3) By analyzing the subsections within each\n",
            "section and utilizing topic modeling to identify key topics, we uncover what is\n",
            "discussed in each section, and underscore significant themes encompassing both\n",
            "technical and social impacts, as well as limitations within the Considerations\n",
            "for Using the Data section. (4) Our findings also highlight the need for\n",
            "improved accessibility and reproducibility of datasets in the Usage sections.\n",
            "(5) In addition, our human annotation evaluation emphasizes the pivotal role of\n",
            "comprehensive dataset content in shaping individuals' perceptions of a dataset\n",
            "card's overall quality. Overall, our study offers a unique perspective on\n",
            "analyzing dataset documentation through large-scale data science analysis and\n",
            "underlines the need for more thorough dataset documentation in machine learning\n",
            "research.\n",
            "\n",
            "1573. Title: Towards 3D Molecule-Text Interpretation in Language Models\n",
            "   Abstract: This paper rigorously shows how over-parameterization changes the convergence\n",
            "behaviors of gradient descent (GD) for the matrix sensing problem, where the\n",
            "goal is to recover an unknown low-rank ground-truth matrix from near-isotropic\n",
            "linear measurements. First, we consider the symmetric setting with the\n",
            "symmetric parameterization where $M^* \\in \\mathbb{R}^{n \\times n}$ is a\n",
            "positive semi-definite unknown matrix of rank $r \\ll n$, and one uses a\n",
            "symmetric parameterization $XX^\\top$ to learn $M^*$. Here $X \\in \\mathbb{R}^{n\n",
            "\\times k}$ with $k > r$ is the factor matrix. We give a novel $\\Omega (1/T^2)$\n",
            "lower bound of randomly initialized GD for the over-parameterized case ($k >r$)\n",
            "where $T$ is the number of iterations. This is in stark contrast to the\n",
            "exact-parameterization scenario ($k=r$) where the convergence rate is $\\exp\n",
            "(-\\Omega (T))$. Next, we study asymmetric setting where $M^* \\in\n",
            "\\mathbb{R}^{n_1 \\times n_2}$ is the unknown matrix of rank $r \\ll\n",
            "\\min\\{n_1,n_2\\}$, and one uses an asymmetric parameterization $FG^\\top$ to\n",
            "learn $M^*$ where $F \\in \\mathbb{R}^{n_1 \\times k}$ and $G \\in \\mathbb{R}^{n_2\n",
            "\\times k}$. Building on prior work, we give a global exact convergence result\n",
            "of randomly initialized GD for the exact-parameterization case ($k=r$) with an\n",
            "$\\exp (-\\Omega(T))$ rate. Furthermore, we give the first global exact\n",
            "convergence result for the over-parameterization case ($k>r$) with an\n",
            "$\\exp(-\\Omega(\\alpha^2 T))$ rate where $\\alpha$ is the initialization scale.\n",
            "This linear convergence result in the over-parameterization case is especially\n",
            "significant because one can apply the asymmetric parameterization to the\n",
            "symmetric setting to speed up from $\\Omega (1/T^2)$ to linear convergence. On\n",
            "the other hand, we propose a novel method that only modifies one step of GD and\n",
            "obtains a convergence rate independent of $\\alpha$, recovering the rate in the\n",
            "exact-parameterization case.\n",
            "\n",
            "1574. Title: Subtractive Mixture Models via Squaring: Representation and Learning\n",
            "   Abstract: Diffusion models have demonstrated impressive generative capabilities, but\n",
            "their \\textit{exposure bias} problem, described as the input mismatch between\n",
            "training and sampling, lacks in-depth exploration. In this paper, we\n",
            "systematically investigate the exposure bias problem in diffusion models by\n",
            "first analytically modelling the sampling distribution, based on which we then\n",
            "attribute the prediction error at each sampling step as the root cause of the\n",
            "exposure bias issue. Furthermore, we discuss potential solutions to this issue\n",
            "and propose an intuitive metric for it. Along with the elucidation of exposure\n",
            "bias, we propose a simple, yet effective, training-free method called Epsilon\n",
            "Scaling to alleviate the exposure bias. We show that Epsilon Scaling explicitly\n",
            "moves the sampling trajectory closer to the vector field learned in the\n",
            "training phase by scaling down the network output, mitigating the input\n",
            "mismatch between training and sampling. Experiments on various diffusion\n",
            "frameworks (ADM, DDIM, EDM, LDM, DiT, PFGM++) verify the effectiveness of our\n",
            "method. Remarkably, our ADM-ES, as a state-of-the-art stochastic sampler,\n",
            "obtains 2.17 FID on CIFAR-10 under 100-step unconditional generation. The code\n",
            "is available at \\url{https://github.com/forever208/ADM-ES} and\n",
            "\\url{https://github.com/forever208/EDM-ES}.\n",
            "\n",
            "1575. Title: Constrained Bi-Level Optimization: Proximal Lagrangian Value Function Approach and Hessian-free Algorithm\n",
            "   Abstract: We study constrained reinforcement learning (CRL) from a novel perspective by\n",
            "setting constraints directly on state density functions, rather than the value\n",
            "functions considered by previous works. State density has a clear physical and\n",
            "mathematical interpretation, and is able to express a wide variety of\n",
            "constraints such as resource limits and safety requirements. Density\n",
            "constraints can also avoid the time-consuming process of designing and tuning\n",
            "cost functions required by value function-based constraints to encode system\n",
            "specifications. We leverage the duality between density functions and Q\n",
            "functions to develop an effective algorithm to solve the density constrained RL\n",
            "problem optimally and the constrains are guaranteed to be satisfied. We prove\n",
            "that the proposed algorithm converges to a near-optimal solution with a bounded\n",
            "error even when the policy update is imperfect. We use a set of comprehensive\n",
            "experiments to demonstrate the advantages of our approach over state-of-the-art\n",
            "CRL methods, with a wide range of density constrained tasks as well as standard\n",
            "CRL benchmarks such as Safety-Gym.\n",
            "\n",
            "1576. Title: Impact of Computation in Integral Reinforcement Learning for Continuous-Time Control\n",
            "   Abstract: Mixture models are traditionally represented and learned by adding several\n",
            "distributions as components. Allowing mixtures to subtract probability mass or\n",
            "density can drastically reduce the number of components needed to model complex\n",
            "distributions. However, learning such subtractive mixtures while ensuring they\n",
            "still encode a non-negative function is challenging. We investigate how to\n",
            "learn and perform inference on deep subtractive mixtures by squaring them. We\n",
            "do this in the framework of probabilistic circuits, which enable us to\n",
            "represent tensorized mixtures and generalize several other subtractive models.\n",
            "We theoretically prove that the class of squared circuits allowing subtractions\n",
            "can be exponentially more expressive than traditional additive mixtures; and,\n",
            "we empirically show this increased expressiveness on a series of real-world\n",
            "distribution estimation tasks.\n",
            "\n",
            "1577. Title: SALMON: Self-Alignment with Instructable Reward Models\n",
            "   Abstract: Dynamic hedging is the practice of periodically transacting financial\n",
            "instruments to offset the risk caused by an investment or a liability. Dynamic\n",
            "hedging optimization can be framed as a sequential decision problem; thus,\n",
            "Reinforcement Learning (RL) models were recently proposed to tackle this task.\n",
            "However, existing RL works for hedging do not consider market impact caused by\n",
            "the finite liquidity of traded instruments. Integrating such feature can be\n",
            "crucial to achieve optimal performance when hedging options on stocks with\n",
            "limited liquidity. In this paper, we propose a novel general market impact\n",
            "dynamic hedging model based on Deep Reinforcement Learning (DRL) that considers\n",
            "several realistic features such as convex market impacts, and impact\n",
            "persistence through time. The optimal policy obtained from the DRL model is\n",
            "analysed using several option hedging simulations and compared to commonly used\n",
            "procedures such as delta hedging. Results show our DRL model behaves better in\n",
            "contexts of low liquidity by, among others: 1) learning the extent to which\n",
            "portfolio rebalancing actions should be dampened or delayed to avoid high\n",
            "costs, 2) factoring in the impact of features not considered by conventional\n",
            "approaches, such as previous hedging errors through the portfolio value, and\n",
            "the underlying asset's drift (i.e. the magnitude of its expected return).\n",
            "\n",
            "1578. Title: Negative Label Guided OOD Detection with Pretrained Vision-Language Models\n",
            "   Abstract: Out-of-distribution (OOD) detection aims at identifying samples from unknown\n",
            "classes, playing a crucial role in trustworthy models against errors on\n",
            "unexpected inputs. Extensive research has been dedicated to exploring OOD\n",
            "detection in the vision modality. Vision-language models (VLMs) can leverage\n",
            "both textual and visual information for various multi-modal applications,\n",
            "whereas few OOD detection methods take into account information from the text\n",
            "modality. In this paper, we propose a novel post hoc OOD detection method,\n",
            "called NegLabel, which takes a vast number of negative labels from extensive\n",
            "corpus databases. We design a novel scheme for the OOD score collaborated with\n",
            "negative labels. Theoretical analysis helps to understand the mechanism of\n",
            "negative labels. Extensive experiments demonstrate that our method NegLabel\n",
            "achieves state-of-the-art performance on various OOD detection benchmarks and\n",
            "generalizes well on multiple VLM architectures. Furthermore, our method\n",
            "NegLabel exhibits remarkable robustness against diverse domain shifts. The\n",
            "codes are available at https://github.com/tmlr-group/NegLabel.\n",
            "\n",
            "1579. Title: Privacy Amplification for Matrix Mechanisms\n",
            "   Abstract: Improving the alignment of language models with human preferences remains an\n",
            "active research challenge. Previous approaches have primarily utilized\n",
            "Reinforcement Learning from Human Feedback (RLHF) via online RL methods such as\n",
            "Proximal Policy Optimization (PPO). Recently, offline methods such as Sequence\n",
            "Likelihood Calibration (SLiC) and Direct Preference Optimization (DPO) have\n",
            "emerged as attractive alternatives, offering improvements in stability and\n",
            "scalability while maintaining competitive performance. SLiC refines its loss\n",
            "function using sequence pairs sampled from a supervised fine-tuned (SFT)\n",
            "policy, while DPO directly optimizes language models based on preference data,\n",
            "foregoing the need for a separate reward model. However, the maximum likelihood\n",
            "estimator (MLE) of the target optimal policy requires labeled preference pairs\n",
            "sampled from that policy. DPO's lack of a reward model constrains its ability\n",
            "to sample preference pairs from the optimal policy, and SLiC is restricted to\n",
            "sampling preference pairs only from the SFT policy. To address these\n",
            "limitations, we introduce a novel approach called Statistical Rejection\n",
            "Sampling Optimization (RSO) that aims to source preference data from the target\n",
            "optimal policy using rejection sampling, enabling a more accurate estimation of\n",
            "the optimal policy. We also propose a unified framework that enhances the loss\n",
            "functions used in both SLiC and DPO from a preference modeling standpoint.\n",
            "Through extensive experiments across three diverse tasks, we demonstrate that\n",
            "RSO consistently outperforms both SLiC and DPO on evaluations from both Large\n",
            "Language Model (LLM) and human raters.\n",
            "\n",
            "1580. Title: Tell Your Model Where to Attend: Post-hoc Attention Steering for LLMs\n",
            "   Abstract: Privacy amplification exploits randomness in data selection to provide\n",
            "tighter differential privacy (DP) guarantees. This analysis is key to DP-SGD's\n",
            "success in machine learning, but, is not readily applicable to the newer\n",
            "state-of-the-art algorithms. This is because these algorithms, known as\n",
            "DP-FTRL, use the matrix mechanism to add correlated noise instead of\n",
            "independent noise as in DP-SGD.\n",
            "  In this paper, we propose \"MMCC\", the first algorithm to analyze privacy\n",
            "amplification via sampling for any generic matrix mechanism. MMCC is nearly\n",
            "tight in that it approaches a lower bound as $\\epsilon\\to0$. To analyze\n",
            "correlated outputs in MMCC, we prove that they can be analyzed as if they were\n",
            "independent, by conditioning them on prior outputs. Our \"conditional\n",
            "composition theorem\" has broad utility: we use it to show that the noise added\n",
            "to binary-tree-DP-FTRL can asymptotically match the noise added to DP-SGD with\n",
            "amplification. Our amplification algorithm also has practical empirical\n",
            "utility: we show it leads to significant improvement in the privacy-utility\n",
            "trade-offs for DP-FTRL algorithms on standard benchmarks.\n",
            "\n",
            "1581. Title: Statistical Rejection Sampling Improves Preference Optimization\n",
            "   Abstract: Normalization layers are one of the key building blocks for deep neural\n",
            "networks. Several theoretical studies have shown that batch normalization\n",
            "improves the signal propagation, by avoiding the representations from becoming\n",
            "collinear across the layers. However, results on mean-field theory of batch\n",
            "normalization also conclude that this benefit comes at the expense of exploding\n",
            "gradients in depth. Motivated by these two aspects of batch normalization, in\n",
            "this study we pose the following question: \"Can a batch-normalized network keep\n",
            "the optimal signal propagation properties, but avoid exploding gradients?\" We\n",
            "answer this question in the affirmative by giving a particular construction of\n",
            "an Multi-Layer Perceptron (MLP) with linear activations and batch-normalization\n",
            "that provably has bounded gradients at any depth. Based on Weingarten calculus,\n",
            "we develop a rigorous and non-asymptotic theory for this constructed MLP that\n",
            "gives a precise characterization of forward signal propagation, while proving\n",
            "that gradients remain bounded for linearly independent input samples, which\n",
            "holds in most practical settings. Inspired by our theory, we also design an\n",
            "activation shaping scheme that empirically achieves the same properties for\n",
            "certain non-linear activations.\n",
            "\n",
            "1582. Title: Learning to design protein-protein interactions with enhanced generalization\n",
            "   Abstract: This paper introduces the Structural Optimization gym (SOgym), a novel\n",
            "open-source Reinforcement Learning (RL) environment designed to advance machine\n",
            "learning in Topology Optimization (TO). SOgym enables RL agents to generate\n",
            "physically viable and structurally robust designs by integrating the physics of\n",
            "TO into the reward function. To enhance scalability, SOgym leverages\n",
            "feature-mapping methods as a mesh-independent interface between the environment\n",
            "and the agent, allowing efficient interaction with the design variables\n",
            "regardless of mesh resolution. Baseline results use a model-free Proximal\n",
            "Policy Optimization agent and a model-based DreamerV3 agent. Three observation\n",
            "space configurations were tested. The TopOpt game-inspired configuration, an\n",
            "interactive educational tool that improves students' intuition in designing\n",
            "structures to minimize compliance under volume constraints, performed best in\n",
            "terms of performance and sample efficiency. The 100M parameter version of\n",
            "DreamerV3 produced structures within 54% of the baseline compliance achieved by\n",
            "traditional optimization methods and a 0% disconnection rate, an improvement\n",
            "over supervised learning approaches that often struggle with disconnected load\n",
            "paths. When comparing the learning rates of the agents to those of engineering\n",
            "students from the TopOpt game experiment, the DreamerV3-100M model shows a\n",
            "learning rate approximately four orders of magnitude lower, an impressive feat\n",
            "for a policy trained from scratch through trial and error. These results\n",
            "suggest RL's potential to solve continuous TO problems and its capacity to\n",
            "explore and learn from diverse design solutions. SOgym provides a platform for\n",
            "developing RL agents for complex structural design challenges and is publicly\n",
            "available to support further research in the field.\n",
            "\n",
            "1583. Title: Federated Recommendation with Additive Personalization\n",
            "   Abstract: Protein design, a grand challenge of the day, involves optimization on a\n",
            "fitness landscape, and leading methods adopt a model-based approach where a\n",
            "model is trained on a training set (protein sequences and fitness) and proposes\n",
            "candidates to explore next. These methods are challenged by sparsity of\n",
            "high-fitness samples in the training set, a problem that has been in the\n",
            "literature. A less recognized but equally important problem stems from the\n",
            "distribution of training samples in the design space: leading methods are not\n",
            "designed for scenarios where the desired optimum is in a region that is not\n",
            "only poorly represented in training data, but also relatively far from the\n",
            "highly represented low-fitness regions. We show that this problem of\n",
            "\"separation\" in the design space is a significant bottleneck in existing\n",
            "model-based optimization tools and propose a new approach that uses a novel VAE\n",
            "as its search model to overcome the problem. We demonstrate its advantage over\n",
            "prior methods in robustly finding improved samples, regardless of the imbalance\n",
            "and separation between low- and high-fitness samples. Our comprehensive\n",
            "benchmark on real and semi-synthetic protein datasets as well as solution\n",
            "design for physics-informed neural networks, showcases the generality of our\n",
            "approach in discrete and continuous design spaces. Our implementation is\n",
            "available at https://github.com/sabagh1994/PGVAE.\n",
            "\n",
            "1584. Title: Exploring Target Representations for Masked Autoencoders\n",
            "   Abstract: Building recommendation systems via federated learning (FL) is a new emerging\n",
            "challenge for advancing next-generation Internet service and privacy\n",
            "protection. Existing approaches train shared item embedding by FL while keeping\n",
            "the user embedding private on client side. However, item embedding identical\n",
            "for all clients cannot capture users' individual differences on perceiving the\n",
            "same item and thus leads to poor personalization. Moreover, dense item\n",
            "embedding in FL results in expensive communication cost and latency. To address\n",
            "these challenges, we propose Federated Recommendation with Additive\n",
            "Personalization (FedRAP), which learns a global view of items via FL and a\n",
            "personalized view locally on each user. FedRAP enforces sparsity of the global\n",
            "view to save FL's communication cost and encourages difference between the two\n",
            "views through regularization. We propose an effective curriculum to learn the\n",
            "local and global views progressively with increasing regularization weights. To\n",
            "produce recommendations for an user, FedRAP adds the two views together to\n",
            "obtain a personalized item embedding. FedRAP achieves the best performance in\n",
            "FL setting on multiple benchmarks. It outperforms recent federated\n",
            "recommendation methods and several ablation study baselines.\n",
            "\n",
            "1585. Title: On the Fairness ROAD: Robust Optimization for Adversarial Debiasing\n",
            "   Abstract: Masked autoencoders have become popular training paradigms for\n",
            "self-supervised visual representation learning. These models randomly mask a\n",
            "portion of the input and reconstruct the masked portion according to the target\n",
            "representations. In this paper, we first show that a careful choice of the\n",
            "target representation is unnecessary for learning good representations, since\n",
            "different targets tend to derive similarly behaved models. Driven by this\n",
            "observation, we propose a multi-stage masked distillation pipeline and use a\n",
            "randomly initialized model as the teacher, enabling us to effectively train\n",
            "high-capacity models without any efforts to carefully design target\n",
            "representations. Interestingly, we further explore using teachers of larger\n",
            "capacity, obtaining distilled students with remarkable transferring ability. On\n",
            "different tasks of classification, transfer learning, object detection, and\n",
            "semantic segmentation, the proposed method to perform masked knowledge\n",
            "distillation with bootstrapped teachers (dBOT) outperforms previous\n",
            "self-supervised methods by nontrivial margins. We hope our findings, as well as\n",
            "the proposed method, could motivate people to rethink the roles of target\n",
            "representations in pre-training masked autoencoders.The code and pre-trained\n",
            "models are publicly available at https://github.com/liuxingbin/dbot.\n",
            "\n",
            "1586. Title: Teaching Language Models to Hallucinate Less with Synthetic Tasks\n",
            "   Abstract: In human-written articles, we often leverage the subtleties of text style,\n",
            "such as bold and italics, to guide the attention of readers. These textual\n",
            "emphases are vital for the readers to grasp the conveyed information. When\n",
            "interacting with large language models (LLMs), we have a similar need --\n",
            "steering the model to pay closer attention to user-specified information, e.g.,\n",
            "an instruction. Existing methods, however, are constrained to process plain\n",
            "text and do not support such a mechanism. This motivates us to introduce PASTA\n",
            "-- Post-hoc Attention STeering Approach, a method that allows LLMs to read text\n",
            "with user-specified emphasis marks. To this end, PASTA identifies a small\n",
            "subset of attention heads and applies precise attention reweighting on them,\n",
            "directing the model attention to user-specified parts. Like prompting, PASTA is\n",
            "applied at inference time and does not require changing any model parameters.\n",
            "Experiments demonstrate that PASTA can substantially enhance an LLM's ability\n",
            "to follow user instructions or integrate new knowledge from user inputs,\n",
            "leading to a significant performance improvement on a variety of tasks, e.g.,\n",
            "an average accuracy improvement of 22% for LLAMA-7B. Our code is publicly\n",
            "available at https://github.com/QingruZhang/PASTA .\n",
            "\n",
            "1587. Title: Inherently Interpretable Time Series Classification via Multiple Instance Learning\n",
            "   Abstract: Large language models (LLMs) frequently hallucinate on abstractive\n",
            "summarization tasks such as document-based question-answering, meeting\n",
            "summarization, and clinical report generation, even though all necessary\n",
            "information is included in context. However, optimizing LLMs to hallucinate\n",
            "less on these tasks is challenging, as hallucination is hard to efficiently\n",
            "evaluate at each optimization step. In this work, we show that reducing\n",
            "hallucination on a synthetic task can also reduce hallucination on real-world\n",
            "downstream tasks. Our method, SynTra, first designs a synthetic task where\n",
            "hallucinations are easy to elicit and measure. It next optimizes the LLM's\n",
            "system message via prefix-tuning on the synthetic task, and finally transfers\n",
            "the system message to realistic, hard-to-optimize tasks. Across three realistic\n",
            "abstractive summarization tasks, SynTra reduces hallucination for two\n",
            "13B-parameter LLMs using only a synthetic retrieval task for supervision. We\n",
            "also find that optimizing the system message rather than the model weights can\n",
            "be critical; fine-tuning the entire model on the synthetic task can\n",
            "counterintuitively increase hallucination. Overall, SynTra demonstrates that\n",
            "the extra flexibility of working with synthetic data can help mitigate\n",
            "undesired behaviors in practice.\n",
            "\n",
            "1588. Title: Dual RL: Unification and New Methods for Reinforcement and Imitation Learning\n",
            "   Abstract: Conventional Time Series Classification (TSC) methods are often black boxes\n",
            "that obscure inherent interpretation of their decision-making processes. In\n",
            "this work, we leverage Multiple Instance Learning (MIL) to overcome this issue,\n",
            "and propose a new framework called MILLET: Multiple Instance Learning for\n",
            "Locally Explainable Time series classification. We apply MILLET to existing\n",
            "deep learning TSC models and show how they become inherently interpretable\n",
            "without compromising (and in some cases, even improving) predictive\n",
            "performance. We evaluate MILLET on 85 UCR TSC datasets and also present a novel\n",
            "synthetic dataset that is specially designed to facilitate interpretability\n",
            "evaluation. On these datasets, we show MILLET produces sparse explanations\n",
            "quickly that are of higher quality than other well-known interpretability\n",
            "methods. To the best of our knowledge, our work with MILLET, which is available\n",
            "on GitHub (https://github.com/JAEarly/MILTimeSeriesClassification), is the\n",
            "first to develop general MIL methods for TSC and apply them to an extensive\n",
            "variety of domains\n",
            "\n",
            "1589. Title: TACTiS-2: Better, Faster, Simpler Attentional Copulas for Multivariate Time Series\n",
            "   Abstract: In the field of algorithmic fairness, significant attention has been put on\n",
            "group fairness criteria, such as Demographic Parity and Equalized Odds.\n",
            "Nevertheless, these objectives, measured as global averages, have raised\n",
            "concerns about persistent local disparities between sensitive groups. In this\n",
            "work, we address the problem of local fairness, which ensures that the\n",
            "predictor is unbiased not only in terms of expectations over the whole\n",
            "population, but also within any subregion of the feature space, unknown at\n",
            "training time. To enforce this objective, we introduce ROAD, a novel approach\n",
            "that leverages the Distributionally Robust Optimization (DRO) framework within\n",
            "a fair adversarial learning objective, where an adversary tries to infer the\n",
            "sensitive attribute from the predictions. Using an instance-level re-weighting\n",
            "strategy, ROAD is designed to prioritize inputs that are likely to be locally\n",
            "unfair, i.e. where the adversary faces the least difficulty in reconstructing\n",
            "the sensitive attribute. Numerical experiments demonstrate the effectiveness of\n",
            "our method: it achieves Pareto dominance with respect to local fairness and\n",
            "accuracy for a given global fairness level across three standard datasets, and\n",
            "also enhances fairness generalization under distribution shift.\n",
            "\n",
            "1590. Title: Emergent mechanisms for long timescales depend on training curriculum and affect performance in memory tasks\n",
            "   Abstract: Information Operations (IOs) pose a significant threat to the integrity of\n",
            "democratic processes, with the potential to influence election-related online\n",
            "discourse. In anticipation of the 2024 U.S. presidential election, we present a\n",
            "study aimed at uncovering the digital traces of coordinated IOs on $\\mathbb{X}$\n",
            "(formerly Twitter). Using our machine learning framework for detecting online\n",
            "coordination, we analyze a dataset comprising election-related conversations on\n",
            "$\\mathbb{X}$ from May 2024. This reveals a network of coordinated inauthentic\n",
            "actors, displaying notable similarities in their link-sharing behaviors. Our\n",
            "analysis shows concerted efforts by these accounts to disseminate misleading,\n",
            "redundant, and biased information across the Web through a coordinated\n",
            "cross-platform information operation: The links shared by this network\n",
            "frequently direct users to other social media platforms or suspicious websites\n",
            "featuring low-quality political content and, in turn, promoting the same\n",
            "$\\mathbb{X}$ and YouTube accounts. Members of this network also shared\n",
            "deceptive images generated by AI, accompanied by language attacking political\n",
            "figures and symbolic imagery intended to convey power and dominance. While\n",
            "$\\mathbb{X}$ has suspended a subset of these accounts, more than 75% of the\n",
            "coordinated network remains active. Our findings underscore the critical role\n",
            "of developing computational models to scale up the detection of threats on\n",
            "large social media platforms, and emphasize the broader implications of these\n",
            "techniques to detect IOs across the wider Web.\n",
            "\n",
            "1591. Title: Sparse Model Soups: A Recipe for Improved Pruning via Model Averaging\n",
            "   Abstract: Extending the context window of large language models (LLMs) is getting\n",
            "popular recently, while the solution of augmenting LLMs with retrieval has\n",
            "existed for years. The natural questions are: i) Retrieval-augmentation versus\n",
            "long context window, which one is better for downstream tasks? ii) Can both\n",
            "methods be combined to get the best of both worlds? In this work, we answer\n",
            "these questions by studying both solutions using two state-of-the-art\n",
            "pretrained LLMs, i.e., a proprietary 43B GPT and Llama2-70B. Perhaps\n",
            "surprisingly, we find that LLM with 4K context window using simple\n",
            "retrieval-augmentation at generation can achieve comparable performance to\n",
            "finetuned LLM with 16K context window via positional interpolation on long\n",
            "context tasks, while taking much less computation. More importantly, we\n",
            "demonstrate that retrieval can significantly improve the performance of LLMs\n",
            "regardless of their extended context window sizes. Our best model,\n",
            "retrieval-augmented Llama2-70B with 32K context window, outperforms\n",
            "GPT-3.5-turbo-16k and Davinci003 in terms of average score on nine long context\n",
            "tasks including question answering, query-based summarization, and in-context\n",
            "few-shot learning tasks. It also outperforms its non-retrieval Llama2-70B-32k\n",
            "baseline by a margin, while being much faster at generation. Our study provides\n",
            "general insights on the choice of retrieval-augmentation versus long context\n",
            "extension of LLM for practitioners.\n",
            "\n",
            "1592. Title: Revisiting the Last-Iterate Convergence of Stochastic Gradient Methods\n",
            "   Abstract: We explore how continued pre-training on domain-specific corpora influences\n",
            "large language models, revealing that training on the raw corpora endows the\n",
            "model with domain knowledge, but drastically hurts its prompting ability for\n",
            "question answering. Taken inspiration from human learning via reading\n",
            "comprehension--practice after reading improves the ability to answer questions\n",
            "based on the learned knowledge--we propose a simple method for transforming raw\n",
            "corpora into reading comprehension texts. Each raw text is enriched with a\n",
            "series of tasks related to its content. Our method, highly scalable and\n",
            "applicable to any pre-training corpora, consistently enhances performance\n",
            "across various tasks in three different domains: biomedicine, finance, and law.\n",
            "Notably, our 7B language model achieves competitive performance with\n",
            "domain-specific models of much larger scales, such as BloombergGPT-50B.\n",
            "Furthermore, we demonstrate that domain-specific reading comprehension texts\n",
            "can improve the model's performance even on general benchmarks, showing the\n",
            "potential to develop a general model across even more domains. Our model, code,\n",
            "and data are available at https://github.com/microsoft/LMOps.\n",
            "\n",
            "1593. Title: Un-Mixing Test-Time Normalization Statistics: Combatting Label Temporal Correlation\n",
            "   Abstract: Hierarchical Imitation Learning (HIL) is an effective way for robots to learn\n",
            "sub-skills from long-horizon unsegmented demonstrations. However, the learned\n",
            "hierarchical structure lacks the mechanism to transfer across multi-tasks or to\n",
            "new tasks, which makes them have to learn from scratch when facing a new\n",
            "situation. Transferring and reorganizing modular sub-skills require fast\n",
            "adaptation ability of the whole hierarchical structure. In this work, we\n",
            "propose Dual Meta Imitation Learning (DMIL), a hierarchical meta imitation\n",
            "learning method where the high-level network and sub-skills are iteratively\n",
            "meta-learned with model-agnostic meta-learning. DMIL uses the likelihood of\n",
            "state-action pairs from each sub-skill as the supervision for the high-level\n",
            "network adaptation, and use the adapted high-level network to determine\n",
            "different data set for each sub-skill adaptation. We theoretically prove the\n",
            "convergence of the iterative training process of DMIL and establish the\n",
            "connection between DMIL and Expectation-Maximization algorithm. Empirically, we\n",
            "achieve state-of-the-art few-shot imitation learning performance on the\n",
            "Meta-world \\cite{metaworld} benchmark and competitive results on long-horizon\n",
            "tasks of Kitchen environments.\n",
            "\n",
            "1594. Title: DreamLLM: Synergistic Multimodal Comprehension and Creation\n",
            "   Abstract: We address the challenges of the semi-supervised LiDAR segmentation (SSLS)\n",
            "problem, particularly in low-budget scenarios. The two main issues in\n",
            "low-budget SSLS are the poor-quality pseudo-labels for unlabeled data, and the\n",
            "performance drops due to the significant imbalance between ground-truth and\n",
            "pseudo-labels. This imbalance leads to a vicious training cycle. To overcome\n",
            "these challenges, we leverage the spatio-temporal prior by recognizing the\n",
            "substantial overlap between temporally adjacent LiDAR scans. We propose a\n",
            "proximity-based label estimation, which generates highly accurate pseudo-labels\n",
            "for unlabeled data by utilizing semantic consistency with adjacent labeled\n",
            "data. Additionally, we enhance this method by progressively expanding the\n",
            "pseudo-labels from the nearest unlabeled scans, which helps significantly\n",
            "reduce errors linked to dynamic classes. Additionally, we employ a dual-branch\n",
            "structure to mitigate performance degradation caused by data imbalance.\n",
            "Experimental results demonstrate remarkable performance in low-budget settings\n",
            "(i.e., <= 5%) and meaningful improvements in normal budget settings (i.e., 5 -\n",
            "50%). Finally, our method has achieved new state-of-the-art results on\n",
            "SemanticKITTI and nuScenes in semi-supervised LiDAR segmentation. With only 5%\n",
            "labeled data, it offers competitive results against fully-supervised\n",
            "counterparts. Moreover, it surpasses the performance of the previous\n",
            "state-of-the-art at 100% labeled data (75.2%) using only 20% of labeled data\n",
            "(76.0%) on nuScenes. The code is available on https://github.com/halbielee/PLE.\n",
            "\n",
            "1595. Title: Is Self-Repair a Silver Bullet for Code Generation?\n",
            "   Abstract: Neural networks can be significantly compressed by pruning, yielding sparse\n",
            "models with reduced storage and computational demands while preserving\n",
            "predictive performance. Model soups (Wortsman et al., 2022) enhance\n",
            "generalization and out-of-distribution (OOD) performance by averaging the\n",
            "parameters of multiple models into a single one, without increasing inference\n",
            "time. However, achieving both sparsity and parameter averaging is challenging\n",
            "as averaging arbitrary sparse models reduces the overall sparsity due to\n",
            "differing sparse connectivities. This work addresses these challenges by\n",
            "demonstrating that exploring a single retraining phase of Iterative Magnitude\n",
            "Pruning (IMP) with varied hyperparameter configurations such as batch ordering\n",
            "or weight decay yields models suitable for averaging, sharing identical sparse\n",
            "connectivity by design. Averaging these models significantly enhances\n",
            "generalization and OOD performance over their individual counterparts. Building\n",
            "on this, we introduce Sparse Model Soups (SMS), a novel method for merging\n",
            "sparse models by initiating each prune-retrain cycle with the averaged model\n",
            "from the previous phase. SMS preserves sparsity, exploits sparse network\n",
            "benefits, is modular and fully parallelizable, and substantially improves IMP's\n",
            "performance. We further demonstrate that SMS can be adapted to enhance\n",
            "state-of-the-art pruning-during-training approaches.\n",
            "\n",
            "1596. Title: AdjointDPM: Adjoint Sensitivity Method for Gradient Backpropagation of Diffusion Probabilistic Models\n",
            "   Abstract: The bandits with knapsack (BwK) framework models online decision-making\n",
            "problems in which an agent makes a sequence of decisions subject to resource\n",
            "consumption constraints. The traditional model assumes that each action\n",
            "consumes a non-negative amount of resources and the process ends when the\n",
            "initial budgets are fully depleted. We study a natural generalization of the\n",
            "BwK framework which allows non-monotonic resource utilization, i.e., resources\n",
            "can be replenished by a positive amount. We propose a best-of-both-worlds\n",
            "primal-dual template that can handle any online learning problem with\n",
            "replenishment for which a suitable primal regret minimizer exists. In\n",
            "particular, we provide the first positive results for the case of adversarial\n",
            "inputs by showing that our framework guarantees a constant competitive ratio\n",
            "$\\alpha$ when $B=\\Omega(T)$ or when the possible per-round replenishment is a\n",
            "positive constant. Moreover, under a stochastic input model, our algorithm\n",
            "yields an instance-independent $\\tilde{O}(T^{1/2})$ regret bound which\n",
            "complements existing instance-dependent bounds for the same setting. Finally,\n",
            "we provide applications of our framework to some economic problems of practical\n",
            "relevance.\n",
            "\n",
            "1597. Title: Adapting Large Language Models via Reading Comprehension\n",
            "   Abstract: We prove an inverse approximation theorem for the approximation of nonlinear\n",
            "sequence-to-sequence relationships using recurrent neural networks (RNNs). This\n",
            "is a so-called Bernstein-type result in approximation theory, which deduces\n",
            "properties of a target function under the assumption that it can be effectively\n",
            "approximated by a hypothesis space. In particular, we show that nonlinear\n",
            "sequence relationships that can be stably approximated by nonlinear RNNs must\n",
            "have an exponential decaying memory structure - a notion that can be made\n",
            "precise. This extends the previously identified curse of memory in linear RNNs\n",
            "into the general nonlinear setting, and quantifies the essential limitations of\n",
            "the RNN architecture for learning sequential relationships with long-term\n",
            "memory. Based on the analysis, we propose a principled reparameterization\n",
            "method to overcome the limitations. Our theoretical results are confirmed by\n",
            "numerical experiments. The code has been released in\n",
            "https://github.com/radarFudan/Curse-of-memory\n",
            "\n",
            "1598. Title: Bandits with Replenishable Knapsacks: the Best of both Worlds\n",
            "   Abstract: Large language models have shown remarkable aptitude in code generation, but\n",
            "still struggle to perform complex tasks. Self-repair -- in which the model\n",
            "debugs and repairs its own code -- has recently become a popular way to boost\n",
            "performance in these settings. However, despite its increasing popularity,\n",
            "existing studies of self-repair have been limited in scope; in many settings,\n",
            "its efficacy thus remains poorly understood. In this paper, we analyze Code\n",
            "Llama, GPT-3.5 and GPT-4's ability to perform self-repair on problems taken\n",
            "from HumanEval and APPS. We find that when the cost of carrying out repair is\n",
            "taken into account, performance gains are often modest, vary a lot between\n",
            "subsets of the data, and are sometimes not present at all. We hypothesize that\n",
            "this is because self-repair is bottlenecked by the model's ability to provide\n",
            "feedback on its own code; using a stronger model to artificially boost the\n",
            "quality of the feedback, we observe substantially larger performance gains.\n",
            "Similarly, a small-scale study in which we provide GPT-4 with feedback from\n",
            "human participants suggests that even for the strongest models, self-repair\n",
            "still lags far behind what can be achieved with human-level debugging.\n",
            "\n",
            "1599. Title: Inverse Approximation Theory for Nonlinear Recurrent Neural Networks\n",
            "   Abstract: Recurrent neural networks (RNNs) in the brain and in silico excel at solving\n",
            "tasks with intricate temporal dependencies. Long timescales required for\n",
            "solving such tasks can arise from properties of individual neurons\n",
            "(single-neuron timescale, $\\tau$, e.g., membrane time constant in biological\n",
            "neurons) or recurrent interactions among them (network-mediated timescale).\n",
            "However, the contribution of each mechanism for optimally solving\n",
            "memory-dependent tasks remains poorly understood. Here, we train RNNs to solve\n",
            "$N$-parity and $N$-delayed match-to-sample tasks with increasing memory\n",
            "requirements controlled by $N$ by simultaneously optimizing recurrent weights\n",
            "and $\\tau$s. We find that for both tasks RNNs develop longer timescales with\n",
            "increasing $N$, but depending on the learning objective, they use different\n",
            "mechanisms. Two distinct curricula define learning objectives: sequential\n",
            "learning of a single-$N$ (single-head) or simultaneous learning of multiple\n",
            "$N$s (multi-head). Single-head networks increase their $\\tau$ with $N$ and are\n",
            "able to solve tasks for large $N$, but they suffer from catastrophic\n",
            "forgetting. However, multi-head networks, which are explicitly required to hold\n",
            "multiple concurrent memories, keep $\\tau$ constant and develop longer\n",
            "timescales through recurrent connectivity. Moreover, we show that the\n",
            "multi-head curriculum increases training speed and network stability to\n",
            "ablations and perturbations, and allows RNNs to generalize better to tasks\n",
            "beyond their training regime. This curriculum also significantly improves\n",
            "training GRUs and LSTMs for large-$N$ tasks. Our results suggest that adapting\n",
            "timescales to task requirements via recurrent interactions allows learning more\n",
            "complex objectives and improves the RNN's performance.\n",
            "\n",
            "1600. Title: Faithful and Efficient Explanations for Neural Networks via Neural Tangent Kernel Surrogate Models\n",
            "   Abstract: Simplex identification via split augmented Lagrangian (SISAL) is a\n",
            "popularly-used algorithm in blind unmixing of hyperspectral images. Developed\n",
            "by Jos\\'{e} M. Bioucas-Dias in 2009, the algorithm is fundamentally relevant to\n",
            "tackling simplex-structured matrix factorization, and by extension,\n",
            "non-negative matrix factorization, which have many applications under their\n",
            "umbrellas. In this article, we revisit SISAL and provide new meanings to this\n",
            "quintessential algorithm. The formulation of SISAL was motivated from a\n",
            "geometric perspective, with no noise. We show that SISAL can be explained as an\n",
            "approximation scheme from a probabilistic simplex component analysis framework,\n",
            "which is statistical and is principally more powerful in accommodating the\n",
            "presence of noise. The algorithm for SISAL was designed based on a successive\n",
            "convex approximation method, with a focus on practical utility. It was not\n",
            "known, by analyses, whether the SISAL algorithm has any kind of guarantee of\n",
            "convergence to a stationary point. By establishing associations between the\n",
            "SISAL algorithm and a line-search-based proximal gradient method, we confirm\n",
            "that SISAL can indeed guarantee convergence to a stationary point. Our\n",
            "re-explanation of SISAL also reveals new formulations and algorithms. The\n",
            "performance of these new possibilities is demonstrated by numerical\n",
            "experiments.\n",
            "\n",
            "1601. Title: Transformers as Decision Makers: Provable In-Context Reinforcement Learning via Supervised Pretraining\n",
            "   Abstract: Large transformer models pretrained on offline reinforcement learning\n",
            "datasets have demonstrated remarkable in-context reinforcement learning (ICRL)\n",
            "capabilities, where they can make good decisions when prompted with interaction\n",
            "trajectories from unseen environments. However, when and how transformers can\n",
            "be trained to perform ICRL have not been theoretically well-understood. In\n",
            "particular, it is unclear which reinforcement-learning algorithms transformers\n",
            "can perform in context, and how distribution mismatch in offline training data\n",
            "affects the learned algorithms. This paper provides a theoretical framework\n",
            "that analyzes supervised pretraining for ICRL. This includes two recently\n",
            "proposed training methods -- algorithm distillation and decision-pretrained\n",
            "transformers. First, assuming model realizability, we prove the\n",
            "supervised-pretrained transformer will imitate the conditional expectation of\n",
            "the expert algorithm given the observed trajectory. The generalization error\n",
            "will scale with model capacity and a distribution divergence factor between the\n",
            "expert and offline algorithms. Second, we show transformers with ReLU attention\n",
            "can efficiently approximate near-optimal online reinforcement learning\n",
            "algorithms like LinUCB and Thompson sampling for stochastic linear bandits, and\n",
            "UCB-VI for tabular Markov decision processes. This provides the first\n",
            "quantitative analysis of the ICRL capabilities of transformers pretrained from\n",
            "offline trajectories.\n",
            "\n",
            "1602. Title: Bidirectional Temporal Diffusion Model for Temporally Consistent Human Animation\n",
            "   Abstract: Model-free reinforcement learning (RL) algorithms, such as Q-learning,\n",
            "directly parameterize and update value functions or policies without explicitly\n",
            "modeling the environment. They are typically simpler, more flexible to use, and\n",
            "thus more prevalent in modern deep RL than model-based approaches. However,\n",
            "empirical work has suggested that model-free algorithms may require more\n",
            "samples to learn [Deisenroth and Rasmussen 2011, Schulman et al. 2015]. The\n",
            "theoretical question of \"whether model-free algorithms can be made sample\n",
            "efficient\" is one of the most fundamental questions in RL, and remains unsolved\n",
            "even in the basic scenario with finitely many states and actions.\n",
            "  We prove that, in an episodic MDP setting, Q-learning with UCB exploration\n",
            "achieves regret $\\tilde{O}(\\sqrt{H^3 SAT})$, where $S$ and $A$ are the numbers\n",
            "of states and actions, $H$ is the number of steps per episode, and $T$ is the\n",
            "total number of steps. This sample efficiency matches the optimal regret that\n",
            "can be achieved by any model-based approach, up to a single $\\sqrt{H}$ factor.\n",
            "To the best of our knowledge, this is the first analysis in the model-free\n",
            "setting that establishes $\\sqrt{T}$ regret without requiring access to a\n",
            "\"simulator.\"\n",
            "\n",
            "1603. Title: Conversational Drug Editing Using Retrieval and Domain Feedback\n",
            "   Abstract: A recent trend in explainable AI research has focused on surrogate modeling,\n",
            "where neural networks are approximated as simpler ML algorithms such as kernel\n",
            "machines. A second trend has been to utilize kernel functions in various\n",
            "explain-by-example or data attribution tasks. In this work, we combine these\n",
            "two trends to analyze approximate empirical neural tangent kernels (eNTK) for\n",
            "data attribution. Approximation is critical for eNTK analysis due to the high\n",
            "computational cost to compute the eNTK. We define new approximate eNTK and\n",
            "perform novel analysis on how well the resulting kernel machine surrogate\n",
            "models correlate with the underlying neural network. We introduce two new\n",
            "random projection variants of approximate eNTK which allow users to tune the\n",
            "time and memory complexity of their calculation. We conclude that kernel\n",
            "machines using approximate neural tangent kernel as the kernel function are\n",
            "effective surrogate models, with the introduced trace NTK the most consistent\n",
            "performer. Open source software allowing users to efficiently calculate kernel\n",
            "functions in the PyTorch framework is available\n",
            "(https://github.com/pnnl/projection\\_ntk).\n",
            "\n",
            "1604. Title: Sparse MoE with Language Guided Routing for Multilingual Machine Translation\n",
            "   Abstract: We introduce a method to generate temporally coherent human animation from a\n",
            "single image, a video, or a random noise. This problem has been formulated as\n",
            "modeling of an auto-regressive generation, i.e., to regress past frames to\n",
            "decode future frames. However, such unidirectional generation is highly prone\n",
            "to motion drifting over time, generating unrealistic human animation with\n",
            "significant artifacts such as appearance distortion. We claim that\n",
            "bidirectional temporal modeling enforces temporal coherence on a generative\n",
            "network by largely suppressing the motion ambiguity of human appearance. To\n",
            "prove our claim, we design a novel human animation framework using a denoising\n",
            "diffusion model: a neural network learns to generate the image of a person by\n",
            "denoising temporal Gaussian noises whose intermediate results are\n",
            "cross-conditioned bidirectionally between consecutive frames. In the\n",
            "experiments, our method demonstrates strong performance compared to existing\n",
            "unidirectional approaches with realistic temporal coherence.\n",
            "\n",
            "1605. Title: Provable Reward-Agnostic Preference-Based Reinforcement Learning\n",
            "   Abstract: Excellent tail performance is crucial for modern machine learning tasks, such\n",
            "as algorithmic fairness, class imbalance, and risk-sensitive decision making,\n",
            "as it ensures the effective handling of challenging samples within a dataset.\n",
            "Tail performance is also a vital determinant of success for personalized\n",
            "recommender systems to reduce the risk of losing users with low satisfaction.\n",
            "This study introduces a \"safe\" collaborative filtering method that prioritizes\n",
            "recommendation quality for less-satisfied users rather than focusing on the\n",
            "average performance. Our approach minimizes the conditional value at risk\n",
            "(CVaR), which represents the average risk over the tails of users' loss. To\n",
            "overcome computational challenges for web-scale recommender systems, we develop\n",
            "a robust yet practical algorithm that extends the most scalable method,\n",
            "implicit alternating least squares (iALS). Empirical evaluation on real-world\n",
            "datasets demonstrates the excellent tail performance of our approach while\n",
            "maintaining competitive computational efficiency.\n",
            "\n",
            "1606. Title: Hybrid Sharing for Multi-Label Image Classification\n",
            "   Abstract: Learning discriminative image representations plays a vital role in\n",
            "long-tailed image classification because it can ease the classifier learning in\n",
            "imbalanced cases. Given the promising performance contrastive learning has\n",
            "shown recently in representation learning, in this work, we explore effective\n",
            "supervised contrastive learning strategies and tailor them to learn better\n",
            "image representations from imbalanced data in order to boost the classification\n",
            "accuracy thereon. Specifically, we propose a novel hybrid network structure\n",
            "being composed of a supervised contrastive loss to learn image representations\n",
            "and a cross-entropy loss to learn classifiers, where the learning is\n",
            "progressively transited from feature learning to the classifier learning to\n",
            "embody the idea that better features make better classifiers. We explore two\n",
            "variants of contrastive loss for feature learning, which vary in the forms but\n",
            "share a common idea of pulling the samples from the same class together in the\n",
            "normalized embedding space and pushing the samples from different classes\n",
            "apart. One of them is the recently proposed supervised contrastive (SC) loss,\n",
            "which is designed on top of the state-of-the-art unsupervised contrastive loss\n",
            "by incorporating positive samples from the same class. The other is a\n",
            "prototypical supervised contrastive (PSC) learning strategy which addresses the\n",
            "intensive memory consumption in standard SC loss and thus shows more promise\n",
            "under limited memory budget. Extensive experiments on three long-tailed\n",
            "classification datasets demonstrate the advantage of the proposed contrastive\n",
            "learning based hybrid networks in long-tailed classification.\n",
            "\n",
            "1607. Title: Safe Collaborative Filtering\n",
            "   Abstract: Sparsely activated models (SAMs), such as Mixture-of-Experts (MoE), can\n",
            "easily scale to have outrageously large amounts of parameters without\n",
            "significant increase in computational cost. However, SAMs are reported to be\n",
            "parameter inefficient such that larger models do not always lead to better\n",
            "performance. While most on-going research focuses on improving SAMs models by\n",
            "exploring methods of routing inputs to experts, our analysis reveals that such\n",
            "research might not lead to the solution we expect, i.e., the commonly-used\n",
            "routing methods based on gating mechanisms do not work better than randomly\n",
            "routing inputs to experts. In this paper, we propose a new expert-based model,\n",
            "THOR (Transformer witH StOchastic ExpeRts). Unlike classic expert-based models,\n",
            "such as the Switch Transformer, experts in THOR are randomly activated for each\n",
            "input during training and inference. THOR models are trained using a\n",
            "consistency regularized loss, where experts learn not only from training data\n",
            "but also from other experts as teachers, such that all the experts make\n",
            "consistent predictions. We validate the effectiveness of THOR on machine\n",
            "translation tasks. Results show that THOR models are more parameter efficient\n",
            "in that they significantly outperform the Transformer and MoE models across\n",
            "various settings. For example, in multilingual translation, THOR outperforms\n",
            "the Switch Transformer by 2 BLEU scores, and obtains the same BLEU score as\n",
            "that of a state-of-the-art MoE model that is 18 times larger. Our code is\n",
            "publicly available at:\n",
            "https://github.com/microsoft/Stochastic-Mixture-of-Experts.\n",
            "\n",
            "1608. Title: Towards Assessing and Benchmarking Risk-Return Tradeoff of Off-Policy Evaluation\n",
            "   Abstract: Edge video analytics is becoming the solution to many safety and management\n",
            "tasks. Its wide deployment, however, must first address the tension between\n",
            "inference accuracy and resource (compute/network) cost. This has led to the\n",
            "development of video analytics pipelines (VAPs), which reduce resource cost by\n",
            "combining DNN compression/speedup techniques with video processing heuristics.\n",
            "Our measurement study on existing VAPs, however, shows that today's methods for\n",
            "evaluating VAPs are incomplete, often producing premature conclusions or\n",
            "ambiguous results. This is because each VAP's performance varies substantially\n",
            "across videos and time (even under the same scenario) and is sensitive to\n",
            "different subsets of video content characteristics.\n",
            "  We argue that accurate VAP evaluation must first characterize the complex\n",
            "interaction between VAPs and video characteristics, which we refer to as VAP\n",
            "performance clarity. We design and implement Yoda, the first VAP benchmark to\n",
            "achieve performance clarity. Using primitive-based profiling and a carefully\n",
            "curated benchmark video set, Yoda builds a performance clarity profile for each\n",
            "VAP to precisely define its accuracy/cost tradeoff and its relationship with\n",
            "video characteristics. We show that Yoda substantially improves VAP evaluations\n",
            "by (1) providing a comprehensive, transparent assessment of VAP performance and\n",
            "its dependencies on video characteristics; (2) explicitly identifying\n",
            "fine-grained VAP behaviors that were previously hidden by large performance\n",
            "variance; and (3) revealing strengths/weaknesses among different VAPs and new\n",
            "design opportunities.\n",
            "\n",
            "1609. Title: EmerNeRF: Emergent Spatial-Temporal Scene Decomposition via Self-Supervision\n",
            "   Abstract: The is no other model or hypothesis verification tool in Bayesian statistics\n",
            "that is as widely used as the Bayes factor. We focus on generative models that\n",
            "are likelihood-free and, therefore, render the computation of Bayes factors\n",
            "(marginal likelihood ratios) far from obvious. We propose a deep learning\n",
            "estimator of the Bayes factor based on simulated data from two competing models\n",
            "using the likelihood ratio trick. This estimator is devoid of summary\n",
            "statistics and obviates some of the difficulties with ABC model choice. We\n",
            "establish sufficient conditions for consistency of our Deep Bayes Factor\n",
            "estimator as well as its consistency as a model selection tool. We investigate\n",
            "the performance of our estimator on various examples using a wide range of\n",
            "quality metrics related to estimation and model decision accuracy. After\n",
            "training, our deep learning approach enables rapid evaluations of the Bayes\n",
            "factor estimator at any fictional data arriving from either hypothesized model,\n",
            "not just the observed data $Y_0$. This allows us to inspect entire Bayes factor\n",
            "distributions under the two models and to quantify the relative location of the\n",
            "Bayes factor evaluated at $Y_0$ in light of these distributions. Such tail area\n",
            "evaluations are not possible for Bayes factor estimators tailored to $Y_0$. We\n",
            "find the performance of our Deep Bayes Factors competitive with existing MCMC\n",
            "techniques that require the knowledge of the likelihood function. We also\n",
            "consider variants for posterior or intrinsic Bayes factors estimation. We\n",
            "demonstrate the usefulness of our approach on a relatively high-dimensional\n",
            "real data example about determining cognitive biases.\n",
            "\n",
            "1610. Title: Variance-enlarged Poisson Learning for Graph-based Semi-Supervised Learning with Extremely Sparse Labeled Data\n",
            "   Abstract: In this research paper, weighted / unweighted, directed / undirected graphs\n",
            "are associated with interesting Discrete Time Markov Chains (DTMCs) as well as\n",
            "Continuous Time Markov Chains (CTMCs). The equilibrium / transient behaviour of\n",
            "such Markov chains is studied. Also entropy dynamics (Shannon entropy) of\n",
            "certain structured Markov chains is investigated. Finally certain structured\n",
            "graphs and the associated Markov chains are studied.\n",
            "\n",
            "1611. Title: VDC: Versatile Data Cleanser based on Visual-Linguistic Inconsistency by Multimodal Large Language Models\n",
            "   Abstract: Recent advancements in conversational large language models (LLMs), such as\n",
            "ChatGPT, have demonstrated remarkable promise in various domains, including\n",
            "drug discovery. However, existing works mainly focus on investigating the\n",
            "capabilities of conversational LLMs on chemical reaction and retrosynthesis.\n",
            "While drug editing, a critical task in the drug discovery pipeline, remains\n",
            "largely unexplored. To bridge this gap, we propose ChatDrug, a framework to\n",
            "facilitate the systematic investigation of drug editing using LLMs. ChatDrug\n",
            "jointly leverages a prompt module, a retrieval and domain feedback (ReDF)\n",
            "module, and a conversation module to streamline effective drug editing. We\n",
            "empirically show that ChatDrug reaches the best performance on 33 out of 39\n",
            "drug editing tasks, encompassing small molecules, peptides, and proteins. We\n",
            "further demonstrate, through 10 case studies, that ChatDrug can successfully\n",
            "identify the key substructures (e.g., the molecule functional groups, peptide\n",
            "motifs, and protein structures) for manipulation, generating diverse and valid\n",
            "suggestions for drug editing. Promisingly, we also show that ChatDrug can offer\n",
            "insightful explanations from a domain-specific perspective, enhancing\n",
            "interpretability and enabling informed decision-making. This research sheds\n",
            "light on the potential of ChatGPT and conversational LLMs for drug editing. It\n",
            "paves the way for a more efficient and collaborative drug discovery pipeline,\n",
            "contributing to the advancement of pharmaceutical research and development.\n",
            "\n",
            "1612. Title: Chain of Log-Concave Markov Chains\n",
            "   Abstract: To enhance the efficiency and practicality of federated bandit learning,\n",
            "recent advances have introduced incentives to motivate communication among\n",
            "clients, where a client participates only when the incentive offered by the\n",
            "server outweighs its participation cost. However, existing incentive mechanisms\n",
            "naively assume the clients are truthful: they all report their true cost and\n",
            "thus the higher cost one participating client claims, the more the server has\n",
            "to pay. Therefore, such mechanisms are vulnerable to strategic clients aiming\n",
            "to optimize their own utility by misreporting. To address this issue, we\n",
            "propose an incentive compatible (i.e., truthful) communication protocol, named\n",
            "Truth-FedBan, where the incentive for each participant is independent of its\n",
            "self-reported cost, and reporting the true cost is the only way to achieve the\n",
            "best utility. More importantly, Truth-FedBan still guarantees the sub-linear\n",
            "regret and communication cost without any overheads. In other words, the core\n",
            "conceptual contribution of this paper is, for the first time, demonstrating the\n",
            "possibility of simultaneously achieving incentive compatibility and nearly\n",
            "optimal regret in federated bandit learning. Extensive numerical studies\n",
            "further validate the effectiveness of our proposed solution.\n",
            "\n",
            "1613. Title: Incentivized Truthful Communication for Federated Bandits\n",
            "   Abstract: Foundational deep learning (DL) models are general models, trained on large,\n",
            "diverse, and unlabelled datasets, typically using self-supervised learning\n",
            "techniques have led to significant advancements especially in natural language\n",
            "processing. These pretrained models can be fine-tuned for related downstream\n",
            "tasks, offering faster development and reduced training costs, while often\n",
            "achieving improved performance. In this work, we introduce Masked Spectrogram\n",
            "Modeling, a novel self-supervised learning approach for pretraining\n",
            "foundational DL models on radio signals. Adopting a Convolutional LSTM\n",
            "architecture for efficient spatio-temporal processing, we pretrain the model\n",
            "with an unlabelled radio dataset collected from over-the-air measurements.\n",
            "Subsequently, the pretrained model is fine-tuned for two downstream tasks:\n",
            "spectrum forecasting and segmentation. Experimental results demonstrate that\n",
            "our methodology achieves competitive performance in both forecasting accuracy\n",
            "and segmentation, validating its effectiveness for developing foundational\n",
            "radio models.\n",
            "\n",
            "1614. Title: Pre-Training and Fine-Tuning Generative Flow Networks\n",
            "   Abstract: Extreme multi-label classification refers to supervised multi-label learning\n",
            "involving hundreds of thousands or even millions of labels. Datasets in extreme\n",
            "classification exhibit fit to power-law distribution, i.e. a large fraction of\n",
            "labels have very few positive instances in the data distribution. Most\n",
            "state-of-the-art approaches for extreme multi-label classification attempt to\n",
            "capture correlation among labels by embedding the label matrix to a\n",
            "low-dimensional linear sub-space. However, in the presence of power-law\n",
            "distributed extremely large and diverse label spaces, structural assumptions\n",
            "such as low rank can be easily violated.\n",
            "  In this work, we present DiSMEC, which is a large-scale distributed framework\n",
            "for learning one-versus-rest linear classifiers coupled with explicit capacity\n",
            "control to control model size. Unlike most state-of-the-art methods, DiSMEC\n",
            "does not make any low rank assumptions on the label matrix. Using double layer\n",
            "of parallelization, DiSMEC can learn classifiers for datasets consisting\n",
            "hundreds of thousands labels within few hours. The explicit capacity control\n",
            "mechanism filters out spurious parameters which keep the model compact in size,\n",
            "without losing prediction accuracy. We conduct extensive empirical evaluation\n",
            "on publicly available real-world datasets consisting upto 670,000 labels. We\n",
            "compare DiSMEC with recent state-of-the-art approaches, including - SLEEC which\n",
            "is a leading approach for learning sparse local embeddings, and FastXML which\n",
            "is a tree-based approach optimizing ranking based loss function. On some of the\n",
            "datasets, DiSMEC can significantly boost prediction accuracies - 10% better\n",
            "compared to SLECC and 15% better compared to FastXML, in absolute terms.\n",
            "\n",
            "1615. Title: Image Background Serves as Good Proxy for Out-of-distribution Data\n",
            "   Abstract: Out-of-distribution (OOD) detection empowers the model trained on the closed\n",
            "image set to identify unknown data in the open world. Though many prior\n",
            "techniques have yielded considerable improvements in this research direction,\n",
            "two crucial obstacles still remain. Firstly, a unified perspective has yet to\n",
            "be presented to view the developed arts with individual designs, which is vital\n",
            "for providing insights into future work. Secondly, we expect sufficient natural\n",
            "OOD supervision to promote the generation of compact boundaries between the\n",
            "in-distribution (ID) and OOD data without collecting explicit OOD samples. To\n",
            "tackle these issues, we propose a general probabilistic framework to interpret\n",
            "many existing methods and an OOD-data-free model, namely\n",
            "\\textbf{S}elf-supervised \\textbf{S}ampling for \\textbf{O}OD \\textbf{D}etection\n",
            "(SSOD). SSOD efficiently exploits natural OOD signals from the ID data based on\n",
            "the local property of convolution. With these supervisions, it jointly\n",
            "optimizes the OOD detection and conventional ID classification in an end-to-end\n",
            "manner. Extensive experiments reveal that SSOD establishes competitive\n",
            "state-of-the-art performance on many large-scale benchmarks, outperforming the\n",
            "best previous method by a large margin, \\eg, reporting \\textbf{-6.28\\%} FPR95\n",
            "and \\textbf{+0.77\\%} AUROC on ImageNet, \\textbf{-19.01\\%} FPR95 and\n",
            "\\textbf{+3.04\\%} AUROC on CIFAR-10, and top-ranked performance on hard OOD\n",
            "datasets, \\ie, ImageNet-O and OpenImage-O.\n",
            "\n",
            "1616. Title: Consistency Trajectory Models: Learning Probability Flow ODE Trajectory of Diffusion\n",
            "   Abstract: Diffusion models (DMs) have become the dominant paradigm of generative\n",
            "modeling in a variety of domains by learning stochastic processes from noise to\n",
            "data. Recently, diffusion denoising bridge models (DDBMs), a new formulation of\n",
            "generative modeling that builds stochastic processes between fixed data\n",
            "endpoints based on a reference diffusion process, have achieved empirical\n",
            "success across tasks with coupled data distribution, such as image-to-image\n",
            "translation. However, DDBM's sampling process typically requires hundreds of\n",
            "network evaluations to achieve decent performance, which may impede their\n",
            "practical deployment due to high computational demands. In this work, inspired\n",
            "by the recent advance of consistency models in DMs, we tackle this problem by\n",
            "learning the consistency function of the probability-flow ordinary differential\n",
            "equation (PF-ODE) of DDBMs, which directly predicts the solution at a starting\n",
            "step given any point on the ODE trajectory. Based on a dedicated general-form\n",
            "ODE solver, we propose two paradigms: consistency bridge distillation and\n",
            "consistency bridge training, which is flexible to apply on DDBMs with broad\n",
            "design choices. Experimental results show that our proposed method could sample\n",
            "$4\\times$ to $50\\times$ faster than the base DDBM and produce better visual\n",
            "quality given the same step in various tasks with pixel resolution ranging from\n",
            "$64 \\times 64$ to $256 \\times 256$, as well as supporting downstream tasks such\n",
            "as semantic interpolation in the data space.\n",
            "\n",
            "1617. Title: Image Translation as Diffusion Visual Programmers\n",
            "   Abstract: Fairness plays a crucial role in various multi-agent systems (e.g.,\n",
            "communication networks, financial markets, etc.). Many multi-agent dynamical\n",
            "interactions can be cast as Markov Decision Processes (MDPs). While existing\n",
            "research has focused on studying fairness in known environments, the\n",
            "exploration of fairness in such systems for unknown environments remains open.\n",
            "In this paper, we propose a Reinforcement Learning (RL) approach to achieve\n",
            "fairness in multi-agent finite-horizon episodic MDPs. Instead of maximizing the\n",
            "sum of individual agents' value functions, we introduce a fairness function\n",
            "that ensures equitable rewards across agents. Since the classical Bellman's\n",
            "equation does not hold when the sum of individual value functions is not\n",
            "maximized, we cannot use traditional approaches. Instead, in order to explore,\n",
            "we maintain a confidence bound of the unknown environment and then propose an\n",
            "online convex optimization based approach to obtain a policy constrained to\n",
            "this confidence region. We show that such an approach achieves sub-linear\n",
            "regret in terms of the number of episodes. Additionally, we provide a probably\n",
            "approximately correct (PAC) guarantee based on the obtained regret bound. We\n",
            "also propose an offline RL algorithm and bound the optimality gap with respect\n",
            "to the optimal fair solution. To mitigate computational complexity, we\n",
            "introduce a policy-gradient type method for the fair objective. Simulation\n",
            "experiments also demonstrate the efficacy of our approach.\n",
            "\n",
            "1618. Title: In defense of parameter sharing for model-compression\n",
            "   Abstract: We introduce the novel Diffusion Visual Programmer (DVP), a neuro-symbolic\n",
            "image translation framework. Our proposed DVP seamlessly embeds a\n",
            "condition-flexible diffusion model within the GPT architecture, orchestrating a\n",
            "coherent sequence of visual programs (i.e., computer vision models) for various\n",
            "pro-symbolic steps, which span RoI identification, style transfer, and position\n",
            "manipulation, facilitating transparent and controllable image translation\n",
            "processes. Extensive experiments demonstrate DVP's remarkable performance,\n",
            "surpassing concurrent arts. This success can be attributed to several key\n",
            "features of DVP: First, DVP achieves condition-flexible translation via\n",
            "instance normalization, enabling the model to eliminate sensitivity caused by\n",
            "the manual guidance and optimally focus on textual descriptions for\n",
            "high-quality content generation. Second, the framework enhances in-context\n",
            "reasoning by deciphering intricate high-dimensional concepts in feature spaces\n",
            "into more accessible low-dimensional symbols (e.g., [Prompt], [RoI object]),\n",
            "allowing for localized, context-free editing while maintaining overall\n",
            "coherence. Last but not least, DVP improves systemic controllability and\n",
            "explainability by offering explicit symbolic representations at each\n",
            "programming stage, empowering users to intuitively interpret and modify\n",
            "results. Our research marks a substantial step towards harmonizing artificial\n",
            "image translation processes with cognitive intelligence, promising broader\n",
            "applications.\n",
            "\n",
            "1619. Title: Equivariant Matrix Function Neural Networks\n",
            "   Abstract: The growing concern over data privacy, the benefits of utilizing data from\n",
            "diverse sources for model training, and the proliferation of networked devices\n",
            "with enhanced computational capabilities have all contributed to the rise of\n",
            "federated learning (FL). The clients in FL collaborate to train a global model\n",
            "by uploading gradients computed on their private datasets without collecting\n",
            "raw data. However, a new attack surface has emerged from gradient sharing,\n",
            "where adversaries can restore the label distribution of a victim's private data\n",
            "by analyzing the obtained gradients. To mitigate this privacy leakage, existing\n",
            "lightweight defenses restrict the sharing of gradients, such as encrypting the\n",
            "final-layer gradients or locally updating the parameters within. In this paper,\n",
            "we introduce a novel attack called Gradient Bridge (GDBR) that recovers the\n",
            "label distribution of training data from the limited gradient information\n",
            "shared in FL. GDBR explores the relationship between the layer-wise gradients,\n",
            "tracks the flow of gradients, and analytically derives the batch training\n",
            "labels. Extensive experiments show that GDBR can accurately recover more than\n",
            "80% of labels in various FL settings. GDBR highlights the inadequacy of\n",
            "restricted gradient sharing-based defenses and calls for the design of\n",
            "effective defense schemes in FL.\n",
            "\n",
            "1620. Title: A Quadratic Synchronization Rule for Distributed Deep Learning\n",
            "   Abstract: Neural Functional Networks (NFNs) have gained increasing interest due to\n",
            "their wide range of applications, including extracting information from\n",
            "implicit representations of data, editing network weights, and evaluating\n",
            "policies. A key design principle of NFNs is their adherence to the permutation\n",
            "and scaling symmetries inherent in the connectionist structure of the input\n",
            "neural networks. Recent NFNs have been proposed with permutation and scaling\n",
            "equivariance based on either graph-based message-passing mechanisms or\n",
            "parameter-sharing mechanisms. However, graph-based equivariant NFNs suffer from\n",
            "high memory consumption and long running times. On the other hand,\n",
            "parameter-sharing-based NFNs built upon equivariant linear layers exhibit lower\n",
            "memory consumption and faster running time, yet their expressivity is limited\n",
            "due to the large size of the symmetric group of the input neural networks. The\n",
            "challenge of designing a permutation and scaling equivariant NFN that maintains\n",
            "low memory consumption and running time while preserving expressivity remains\n",
            "unresolved. In this paper, we propose a novel solution with the development of\n",
            "MAGEP-NFN (Monomial mAtrix Group Equivariant Polynomial NFN). Our approach\n",
            "follows the parameter-sharing mechanism but differs from previous works by\n",
            "constructing a nonlinear equivariant layer represented as a polynomial in the\n",
            "input weights. This polynomial formulation enables us to incorporate additional\n",
            "relationships between weights from different input hidden layers, enhancing the\n",
            "model's expressivity while keeping memory consumption and running time low,\n",
            "thereby addressing the aforementioned challenge. We provide empirical evidence\n",
            "demonstrating that MAGEP-NFN achieves competitive performance and efficiency\n",
            "compared to existing baselines.\n",
            "\n",
            "1621. Title: Empirical Analysis of Model Selection for Heterogeneous Causal Effect Estimation\n",
            "   Abstract: In distributed deep learning with data parallelism, synchronizing gradients\n",
            "at each training step can cause a huge communication overhead, especially when\n",
            "many nodes work together to train large models. Local gradient methods, such as\n",
            "Local SGD, address this issue by allowing workers to compute locally for $H$\n",
            "steps without synchronizing with others, hence reducing communication\n",
            "frequency. While $H$ has been viewed as a hyperparameter to trade optimization\n",
            "efficiency for communication cost, recent research indicates that setting a\n",
            "proper $H$ value can lead to generalization improvement. Yet, selecting a\n",
            "proper $H$ is elusive. This work proposes a theory-grounded method for\n",
            "determining $H$, named the Quadratic Synchronization Rule (QSR), which\n",
            "recommends dynamically setting $H$ in proportion to $\\frac{1}{\\eta^2}$ as the\n",
            "learning rate $\\eta$ decays over time. Extensive ImageNet experiments on ResNet\n",
            "and ViT show that local gradient methods with QSR consistently improve the test\n",
            "accuracy over other synchronization strategies. Compared with the standard data\n",
            "parallel training, QSR enables Local AdamW on ViT-B to cut the training time on\n",
            "16 or 64 GPUs down from 26.7 to 20.2 hours or from 8.6 to 5.5 hours and, at the\n",
            "same time, achieves $1.16\\%$ or $0.84\\%$ higher top-1 validation accuracy.\n",
            "\n",
            "1622. Title: Generating Pragmatic Examples to Train Neural Program Synthesizers\n",
            "   Abstract: We study the problem of model selection in causal inference, specifically for\n",
            "conditional average treatment effect (CATE) estimation. Unlike machine\n",
            "learning, there is no perfect analogue of cross-validation for model selection\n",
            "as we do not observe the counterfactual potential outcomes. Towards this, a\n",
            "variety of surrogate metrics have been proposed for CATE model selection that\n",
            "use only observed data. However, we do not have a good understanding regarding\n",
            "their effectiveness due to limited comparisons in prior studies. We conduct an\n",
            "extensive empirical analysis to benchmark the surrogate model selection metrics\n",
            "introduced in the literature, as well as the novel ones introduced in this\n",
            "work. We ensure a fair comparison by tuning the hyperparameters associated with\n",
            "these metrics via AutoML, and provide more detailed trends by incorporating\n",
            "realistic datasets via generative modeling. Our analysis suggests novel model\n",
            "selection strategies based on careful hyperparameter selection of CATE\n",
            "estimators and causal ensembling.\n",
            "\n",
            "1623. Title: Matcher: Segment Anything with One Shot Using All-Purpose Feature Matching\n",
            "   Abstract: Programming-by-example is the task of synthesizing a program that is\n",
            "consistent with a set of user-provided input-output examples. As examples are\n",
            "often an under-specification of one's intent, a good synthesizer must choose\n",
            "the intended program from the many that are consistent with the given set of\n",
            "examples. Prior work frames program synthesis as a cooperative game between a\n",
            "listener (that synthesizes programs) and a speaker (a user choosing examples),\n",
            "and shows that models of computational pragmatic inference are effective in\n",
            "choosing the user intended programs. However, these models require\n",
            "counterfactual reasoning over a large set of programs and examples, which is\n",
            "infeasible in realistic program spaces. In this paper, we propose a novel way\n",
            "to amortize this search with neural networks. We sample pairs of programs and\n",
            "examples via self-play between listener and speaker models, and use pragmatic\n",
            "inference to choose informative training examples from this sample.We then use\n",
            "the informative dataset to train models to improve the synthesizer's ability to\n",
            "disambiguate user-provided examples without human supervision. We validate our\n",
            "method on the challenging task of synthesizing regular expressions from example\n",
            "strings, and find that our method (1) outperforms models trained without\n",
            "choosing pragmatic examples by 23% (a 51% relative increase) (2) matches the\n",
            "performance of supervised learning on a dataset of pragmatic examples provided\n",
            "by humans, despite using no human data in training.\n",
            "\n",
            "1624. Title: Byzantine Robust Cooperative Multi-Agent Reinforcement Learning as a Bayesian Game\n",
            "   Abstract: Medical Vision-Language Pre-training (VLP) learns representations jointly\n",
            "from medical images and paired radiology reports. It typically requires\n",
            "large-scale paired image-text datasets to achieve effective pre-training for\n",
            "both the image encoder and text encoder. The advent of text-guided generative\n",
            "models raises a compelling question: Can VLP be implemented solely with\n",
            "synthetic images generated from genuine radiology reports, thereby mitigating\n",
            "the need for extensively pairing and curating image-text datasets? In this\n",
            "work, we scrutinize this very question by examining the feasibility and\n",
            "effectiveness of employing synthetic images for medical VLP. We replace real\n",
            "medical images with their synthetic equivalents, generated from authentic\n",
            "medical reports. Utilizing three state-of-the-art VLP algorithms, we\n",
            "exclusively train on these synthetic samples. Our empirical evaluation across\n",
            "three subsequent tasks, namely image classification, semantic segmentation and\n",
            "object detection, reveals that the performance achieved through synthetic data\n",
            "is on par with or even exceeds that obtained with real images. As a pioneering\n",
            "contribution to this domain, we introduce a large-scale synthetic medical image\n",
            "dataset, paired with anonymized real radiology reports. This alleviates the\n",
            "need of sharing medical images, which are not easy to curate and share in\n",
            "practice. The code and the dataset can be found in\n",
            "\\href{https://github.com/cheliu-computation/MedSyn-RepLearn/tree/main}{https://github.com/cheliu-computation/MedSyn-RepLearn/tree/main}.\n",
            "\n",
            "1625. Title: Perceptual Scales Predicted by Fisher Information Metrics\n",
            "   Abstract: Inverse reinforcement learning (IRL) offers a powerful and general framework\n",
            "for learning humans' latent preferences in route recommendation, yet no\n",
            "approach has successfully addressed planetary-scale problems with hundreds of\n",
            "millions of states and demonstration trajectories. In this paper, we introduce\n",
            "scaling techniques based on graph compression, spatial parallelization, and\n",
            "improved initialization conditions inspired by a connection to eigenvector\n",
            "algorithms. We revisit classic IRL methods in the routing context, and make the\n",
            "key observation that there exists a trade-off between the use of cheap,\n",
            "deterministic planners and expensive yet robust stochastic policies. This\n",
            "insight is leveraged in Receding Horizon Inverse Planning (RHIP), a new\n",
            "generalization of classic IRL algorithms that provides fine-grained control\n",
            "over performance trade-offs via its planning horizon. Our contributions\n",
            "culminate in a policy that achieves a 16-24% improvement in route quality at a\n",
            "global scale, and to the best of our knowledge, represents the largest\n",
            "published study of IRL algorithms in a real-world setting to date. We conclude\n",
            "by conducting an ablation study of key components, presenting negative results\n",
            "from alternative eigenvalue solvers, and identifying opportunities to further\n",
            "improve scalability via IRL-specific batching strategies.\n",
            "\n",
            "1626. Title: MathCoder: Seamless Code Integration in LLMs for Enhanced Mathematical Reasoning\n",
            "   Abstract: Recent embedding-based methods have achieved great successes in exploiting\n",
            "entity alignment from knowledge graph (KG) embeddings of multiple modalities.\n",
            "In this paper, we study embedding-based entity alignment (EEA) from a\n",
            "perspective of generative models. We show that EEA shares similarities with\n",
            "typical generative models and prove the effectiveness of the recently developed\n",
            "generative adversarial network (GAN)-based EEA methods theoretically. We then\n",
            "reveal that their incomplete objective limits the capacity on both entity\n",
            "alignment and entity synthesis (i.e., generating new entities). We mitigate\n",
            "this problem by introducing a generative EEA (GEEA) framework with the proposed\n",
            "mutual variational autoencoder (M-VAE) as the generative model. M-VAE enables\n",
            "entity conversion between KGs and generation of new entities from random noise\n",
            "vectors. We demonstrate the power of GEEA with theoretical analysis and\n",
            "empirical experiments on both entity alignment and entity synthesis tasks.\n",
            "\n",
            "1627. Title: AgentBench: Evaluating LLMs as Agents\n",
            "   Abstract: We study pool-based active learning of half-spaces. We revisit the aggressive\n",
            "approach for active learning in the realizable case, and show that it can be\n",
            "made efficient and practical, while also having theoretical guarantees under\n",
            "reasonable assumptions. We further show, both theoretically and experimentally,\n",
            "that it can be preferable to mellow approaches. Our efficient aggressive active\n",
            "learner of half-spaces has formal approximation guarantees that hold when the\n",
            "pool is separable with a margin. While our analysis is focused on the\n",
            "realizable setting, we show that a simple heuristic allows using the same\n",
            "algorithm successfully for pools with low error as well. We further compare the\n",
            "aggressive approach to the mellow approach, and prove that there are cases in\n",
            "which the aggressive approach results in significantly better label complexity\n",
            "compared to the mellow approach. We demonstrate experimentally that substantial\n",
            "improvements in label complexity can be achieved using the aggressive approach,\n",
            "for both realizable and low-error settings.\n",
            "\n",
            "1628. Title: SetCSE: Set Operations using Contrastive Learning of Sentence Embeddings\n",
            "   Abstract: In this study, we explore the robustness of cooperative multi-agent\n",
            "reinforcement learning (c-MARL) against Byzantine failures, where any agent can\n",
            "enact arbitrary, worst-case actions due to malfunction or adversarial attack.\n",
            "To address the uncertainty that any agent can be adversarial, we propose a\n",
            "Bayesian Adversarial Robust Dec-POMDP (BARDec-POMDP) framework, which views\n",
            "Byzantine adversaries as nature-dictated types, represented by a separate\n",
            "transition. This allows agents to learn policies grounded on their posterior\n",
            "beliefs about the type of other agents, fostering collaboration with identified\n",
            "allies and minimizing vulnerability to adversarial manipulation. We define the\n",
            "optimal solution to the BARDec-POMDP as an ex post robust Bayesian Markov\n",
            "perfect equilibrium, which we proof to exist and weakly dominates the\n",
            "equilibrium of previous robust MARL approaches. To realize this equilibrium, we\n",
            "put forward a two-timescale actor-critic algorithm with almost sure convergence\n",
            "under specific conditions. Experimentation on matrix games, level-based\n",
            "foraging and StarCraft II indicate that, even under worst-case perturbations,\n",
            "our method successfully acquires intricate micromanagement skills and\n",
            "adaptively aligns with allies, demonstrating resilience against non-oblivious\n",
            "adversaries, random allies, observation-based attacks, and transfer-based\n",
            "attacks.\n",
            "\n",
            "1629. Title: Protein Discovery with Discrete Walk-Jump Sampling\n",
            "   Abstract: Perception is often viewed as a process that transforms physical variables,\n",
            "external to an observer, into internal psychological variables. Such a process\n",
            "can be modeled by a function coined perceptual scale. The perceptual scale can\n",
            "be deduced from psychophysical measurements that consist in comparing the\n",
            "relative differences between stimuli (i.e. difference scaling experiments).\n",
            "However, this approach is often overlooked by the modeling and experimentation\n",
            "communities. Here, we demonstrate the value of measuring the perceptual scale\n",
            "of classical (spatial frequency, orientation) and less classical physical\n",
            "variables (interpolation between textures) by embedding it in recent\n",
            "probabilistic modeling of perception. First, we show that the assumption that\n",
            "an observer has an internal representation of univariate parameters such as\n",
            "spatial frequency or orientation while stimuli are high-dimensional does not\n",
            "lead to contradictory predictions when following the theoretical framework.\n",
            "Second, we show that the measured perceptual scale corresponds to the\n",
            "transduction function hypothesized in this framework. In particular, we\n",
            "demonstrate that it is related to the Fisher information of the generative\n",
            "model that underlies perception and we test the predictions given by the\n",
            "generative model of different stimuli in a set a of difference scaling\n",
            "experiments. Our main conclusion is that the perceptual scale is mostly driven\n",
            "by the stimulus power spectrum. Finally, we propose that this measure of\n",
            "perceptual scale is a way to push further the notion of perceptual distances by\n",
            "estimating the perceptual geometry of images i.e. the path between images\n",
            "instead of simply the distance between those.\n",
            "\n",
            "1630. Title: CADS: Unleashing the Diversity of Diffusion Models through Condition-Annealed Sampling\n",
            "   Abstract: We resolve difficulties in training and sampling from a discrete generative\n",
            "model by learning a smoothed energy function, sampling from the smoothed data\n",
            "manifold with Langevin Markov chain Monte Carlo (MCMC), and projecting back to\n",
            "the true data manifold with one-step denoising. Our Discrete Walk-Jump Sampling\n",
            "formalism combines the contrastive divergence training of an energy-based model\n",
            "and improved sample quality of a score-based model, while simplifying training\n",
            "and sampling by requiring only a single noise level. We evaluate the robustness\n",
            "of our approach on generative modeling of antibody proteins and introduce the\n",
            "distributional conformity score to benchmark protein generative models. By\n",
            "optimizing and sampling from our models for the proposed distributional\n",
            "conformity score, 97-100% of generated samples are successfully expressed and\n",
            "purified and 70% of functional designs show equal or improved binding affinity\n",
            "compared to known functional antibodies on the first attempt in a single round\n",
            "of laboratory experiments. We also report the first demonstration of long-run\n",
            "fast-mixing MCMC chains where diverse antibody protein classes are visited in a\n",
            "single MCMC chain.\n",
            "\n",
            "1631. Title: Neural Processing of Tri-Plane Hybrid Neural Fields\n",
            "   Abstract: Text generation models are notoriously vulnerable to errors in the training\n",
            "data. With the wide-spread availability of massive amounts of web-crawled data\n",
            "becoming more commonplace, how can we enhance the robustness of models trained\n",
            "on a massive amount of noisy web-crawled text? In our work, we propose Error\n",
            "Norm Truncation (ENT), a robust enhancement method to the standard training\n",
            "objective that truncates noisy data. Compared to methods that only uses the\n",
            "negative log-likelihood loss to estimate data quality, our method provides a\n",
            "more accurate estimation by considering the distribution of non-target tokens,\n",
            "which is often overlooked by previous work. Through comprehensive experiments\n",
            "across language modeling, machine translation, and text summarization, we show\n",
            "that equipping text generation models with ENT improves generation quality over\n",
            "standard training and previous soft and hard truncation methods. Furthermore,\n",
            "we show that our method improves the robustness of models against two of the\n",
            "most detrimental types of noise in machine translation, resulting in an\n",
            "increase of more than 2 BLEU points over the MLE baseline when up to 50% of\n",
            "noise is added to the data.\n",
            "\n",
            "1632. Title: Task Adaptation from Skills: Information Geometry, Disentanglement, and New Objectives for Unsupervised Reinforcement Learning\n",
            "   Abstract: Driven by the appealing properties of neural fields for storing and\n",
            "communicating 3D data, the problem of directly processing them to address tasks\n",
            "such as classification and part segmentation has emerged and has been\n",
            "investigated in recent works. Early approaches employ neural fields\n",
            "parameterized by shared networks trained on the whole dataset, achieving good\n",
            "task performance but sacrificing reconstruction quality. To improve the latter,\n",
            "later methods focus on individual neural fields parameterized as large\n",
            "Multi-Layer Perceptrons (MLPs), which are, however, challenging to process due\n",
            "to the high dimensionality of the weight space, intrinsic weight space\n",
            "symmetries, and sensitivity to random initialization. Hence, results turn out\n",
            "significantly inferior to those achieved by processing explicit\n",
            "representations, e.g., point clouds or meshes. In the meantime, hybrid\n",
            "representations, in particular based on tri-planes, have emerged as a more\n",
            "effective and efficient alternative to realize neural fields, but their direct\n",
            "processing has not been investigated yet. In this paper, we show that the\n",
            "tri-plane discrete data structure encodes rich information, which can be\n",
            "effectively processed by standard deep-learning machinery. We define an\n",
            "extensive benchmark covering a diverse set of fields such as occupancy,\n",
            "signed/unsigned distance, and, for the first time, radiance fields. While\n",
            "processing a field with the same reconstruction quality, we achieve task\n",
            "performance far superior to frameworks that process large MLPs and, for the\n",
            "first time, almost on par with architectures handling explicit representations.\n",
            "\n",
            "1633. Title: Detecting Pretraining Data from Large Language Models\n",
            "   Abstract: Hyperbolic space has proven to be well-suited for capturing hierarchical\n",
            "relations in data, such as trees and directed acyclic graphs. Prior work\n",
            "introduced the concept of entailment cones, which uses partial orders defined\n",
            "by nested cones in the Poincar\\'e ball to model hierarchies. Here, we introduce\n",
            "the ``shadow cones\" framework, a physics-inspired entailment cone construction.\n",
            "Specifically, we model partial orders as subset relations between shadows\n",
            "formed by a light source and opaque objects in hyperbolic space. The shadow\n",
            "cones framework generalizes entailment cones to a broad class of formulations\n",
            "and hyperbolic space models beyond the Poincar\\'e ball. This results in clear\n",
            "advantages over existing constructions: for example, shadow cones possess\n",
            "better optimization properties over constructions limited to the Poincar\\'e\n",
            "ball. Our experiments on datasets of various sizes and hierarchical structures\n",
            "show that shadow cones consistently and significantly outperform existing\n",
            "entailment cone constructions. These results indicate that shadow cones are an\n",
            "effective way to model partial orders in hyperbolic space, offering physically\n",
            "intuitive and novel insights about the nature of such structures.\n",
            "\n",
            "1634. Title: FROSTER: Frozen CLIP is A Strong Teacher for Open-Vocabulary Action Recognition\n",
            "   Abstract: An important aspect of intelligence is the ability to adapt to a novel task\n",
            "without any direct experience (zero-shot), based on its relationship to\n",
            "previous tasks. Humans can exhibit this cognitive flexibility. By contrast,\n",
            "models that achieve superhuman performance in specific tasks often fail to\n",
            "adapt to even slight task alterations. To address this, we propose a general\n",
            "computational framework for adapting to novel tasks based on their relationship\n",
            "to prior tasks. We begin by learning vector representations of tasks. To adapt\n",
            "to new tasks, we propose meta-mappings, higher-order tasks that transform basic\n",
            "task representations. We demonstrate the effectiveness of this framework across\n",
            "a wide variety of tasks and computational paradigms, ranging from regression to\n",
            "image classification and reinforcement learning. We compare to both human\n",
            "adaptability and language-based approaches to zero-shot learning. Across these\n",
            "domains, meta-mapping is successful, often achieving 80-90% performance,\n",
            "without any data, on a novel task, even when the new task directly contradicts\n",
            "prior experience. We further show that meta-mapping can not only generalize to\n",
            "new tasks via learned relationships, but can also generalize using novel\n",
            "relationships unseen during training. Finally, using meta-mapping as a starting\n",
            "point can dramatically accelerate later learning on a new task, and reduce\n",
            "learning time and cumulative error substantially. Our results provide insight\n",
            "into a possible computational basis of intelligent adaptability and offer a\n",
            "possible framework for modeling cognitive flexibility and building more\n",
            "flexible artificial intelligence systems.\n",
            "\n",
            "1635. Title: On the Limitations of Temperature Scaling for Distributions with Overlaps\n",
            "   Abstract: Learning features from data is one of the defining characteristics of deep\n",
            "learning, but our theoretical understanding of the role features play in deep\n",
            "learning is still rudimentary. To address this gap, we introduce a new tool,\n",
            "the interaction tensor, for empirically analyzing the interaction between data\n",
            "and model through features. With the interaction tensor, we make several key\n",
            "observations about how features are distributed in data and how models with\n",
            "different random seeds learn different features. Based on these observations,\n",
            "we propose a conceptual framework for feature learning. Under this framework,\n",
            "the expected accuracy for a single hypothesis and agreement for a pair of\n",
            "hypotheses can both be derived in closed-form. We demonstrate that the proposed\n",
            "framework can explain empirically observed phenomena, including the recently\n",
            "discovered Generalization Disagreement Equality (GDE) that allows for\n",
            "estimating the generalization error with only unlabeled data. Further, our\n",
            "theory also provides explicit construction of natural data distributions that\n",
            "break the GDE. Thus, we believe this work provides valuable new insight into\n",
            "our understanding of feature learning.\n",
            "\n",
            "1636. Title: How do Language Models Bind Entities in Context?\n",
            "   Abstract: Despite the impressive generalization capabilities of deep neural networks,\n",
            "they have been repeatedly shown to be overconfident when they are wrong. Fixing\n",
            "this issue is known as model calibration, and has consequently received much\n",
            "attention in the form of modified training schemes and post-training\n",
            "calibration procedures such as temperature scaling. While temperature scaling\n",
            "is frequently used because of its simplicity, it is often outperformed by\n",
            "modified training schemes. In this work, we identify a specific bottleneck for\n",
            "the performance of temperature scaling. We show that for empirical risk\n",
            "minimizers for a general set of distributions in which the supports of classes\n",
            "have overlaps, the performance of temperature scaling degrades with the amount\n",
            "of overlap between classes, and asymptotically becomes no better than random\n",
            "when there are a large number of classes. On the other hand, we prove that\n",
            "optimizing a modified form of the empirical risk induced by the Mixup data\n",
            "augmentation technique can in fact lead to reasonably good calibration\n",
            "performance, showing that training-time calibration may be necessary in some\n",
            "situations. We also verify that our theoretical results reflect practice by\n",
            "showing that Mixup significantly outperforms empirical risk minimization (with\n",
            "respect to multiple calibration metrics) on image classification benchmarks\n",
            "with class overlaps introduced in the form of label noise.\n",
            "\n",
            "1637. Title: Shadow Cones: A Generalized Framework for Partial Order Embeddings\n",
            "   Abstract: To correctly use in-context information, language models (LMs) must bind\n",
            "entities to their attributes. For example, given a context describing a \"green\n",
            "square\" and a \"blue circle\", LMs must bind the shapes to their respective\n",
            "colors. We analyze LM representations and identify the binding ID mechanism: a\n",
            "general mechanism for solving the binding problem, which we observe in every\n",
            "sufficiently large model from the Pythia and LLaMA families. Using causal\n",
            "interventions, we show that LMs' internal activations represent binding\n",
            "information by attaching binding ID vectors to corresponding entities and\n",
            "attributes. We further show that binding ID vectors form a continuous subspace,\n",
            "in which distances between binding ID vectors reflect their discernability.\n",
            "Overall, our results uncover interpretable strategies in LMs for representing\n",
            "symbolic knowledge in-context, providing a step towards understanding general\n",
            "in-context reasoning in large-scale LMs.\n",
            "\n",
            "1638. Title: A ROBUST DIFFERENTIAL NEURAL ODE OPTIMIZER\n",
            "   Abstract: The continuous dynamics of natural systems has been effectively modelled\n",
            "using Neural Ordinary Differential Equations (Neural ODEs). However, for\n",
            "accurate and meaningful predictions, it is crucial that the models follow the\n",
            "underlying rules or laws that govern these systems. In this work, we propose a\n",
            "self-adaptive penalty algorithm for Neural ODEs to enable modelling of\n",
            "constrained natural systems. The proposed self-adaptive penalty function can\n",
            "dynamically adjust the penalty parameters. The explicit introduction of prior\n",
            "knowledge helps to increase the interpretability of Neural ODE -based models.\n",
            "We validate the proposed approach by modelling three natural systems with prior\n",
            "knowledge constraints: population growth, chemical reaction evolution, and\n",
            "damped harmonic oscillator motion. The numerical experiments and a comparison\n",
            "with other penalty Neural ODE approaches and \\emph{vanilla} Neural ODE,\n",
            "demonstrate the effectiveness of the proposed self-adaptive penalty algorithm\n",
            "for Neural ODEs in modelling constrained natural systems. Moreover, the\n",
            "self-adaptive penalty approach provides more accurate and robust models with\n",
            "reliable and meaningful predictions.\n",
            "\n",
            "1639. Title: Rigid Protein-Protein Docking via Equivariant Elliptic-Paraboloid Interface Prediction\n",
            "   Abstract: The study of rigid protein-protein docking plays an essential role in a\n",
            "variety of tasks such as drug design and protein engineering. Recently, several\n",
            "learning-based methods have been proposed for the task, exhibiting much faster\n",
            "docking speed than those computational methods. In this paper, we propose a\n",
            "novel learning-based method called ElliDock, which predicts an elliptic\n",
            "paraboloid to represent the protein-protein docking interface. To be specific,\n",
            "our model estimates elliptic paraboloid interfaces for the two input proteins\n",
            "respectively, and obtains the roto-translation transformation for docking by\n",
            "making two interfaces coincide. By its design, ElliDock is independently\n",
            "equivariant with respect to arbitrary rotations/translations of the proteins,\n",
            "which is an indispensable property to ensure the generalization of the docking\n",
            "process. Experimental evaluations show that ElliDock achieves the fastest\n",
            "inference time among all compared methods and is strongly competitive with\n",
            "current state-of-the-art learning-based models such as DiffDock-PP and Multimer\n",
            "particularly for antibody-antigen docking.\n",
            "\n",
            "1640. Title: Long-Term Typhoon Trajectory Prediction: A Physics-Conditioned Approach Without Reanalysis Data\n",
            "   Abstract: Open-vocabulary object detection has benefited greatly from pretrained\n",
            "vision-language models, but is still limited by the amount of available\n",
            "detection training data. While detection training data can be expanded by using\n",
            "Web image-text pairs as weak supervision, this has not been done at scales\n",
            "comparable to image-level pretraining. Here, we scale up detection data with\n",
            "self-training, which uses an existing detector to generate pseudo-box\n",
            "annotations on image-text pairs. Major challenges in scaling self-training are\n",
            "the choice of label space, pseudo-annotation filtering, and training\n",
            "efficiency. We present the OWLv2 model and OWL-ST self-training recipe, which\n",
            "address these challenges. OWLv2 surpasses the performance of previous\n",
            "state-of-the-art open-vocabulary detectors already at comparable training\n",
            "scales (~10M examples). However, with OWL-ST, we can scale to over 1B examples,\n",
            "yielding further large improvement: With an L/14 architecture, OWL-ST improves\n",
            "AP on LVIS rare classes, for which the model has seen no human box annotations,\n",
            "from 31.2% to 44.6% (43% relative improvement). OWL-ST unlocks Web-scale\n",
            "training for open-world localization, similar to what has been seen for image\n",
            "classification and language modelling.\n",
            "\n",
            "1641. Title: Fast and unified path gradient estimators for normalizing flows\n",
            "   Abstract: We present a novel podcast recommender system deployed at industrial scale.\n",
            "This system successfully optimizes personal listening journeys that unfold over\n",
            "months for hundreds of millions of listeners. In deviating from the pervasive\n",
            "industry practice of optimizing machine learning algorithms for short-term\n",
            "proxy metrics, the system substantially improves long-term performance in A/B\n",
            "tests. The paper offers insights into how our methods cope with attribution,\n",
            "coordination, and measurement challenges that usually hinder such long-term\n",
            "optimization. To contextualize these practical insights within a broader\n",
            "academic framework, we turn to reinforcement learning (RL). Using the language\n",
            "of RL, we formulate a comprehensive model of users' recurring relationships\n",
            "with a recommender system. Then, within this model, we identify our approach as\n",
            "a policy improvement update to a component of the existing recommender system,\n",
            "enhanced by tailored modeling of value functions and user-state\n",
            "representations. Illustrative offline experiments suggest this specialized\n",
            "modeling reduces data requirements by as much as a factor of 120,000 compared\n",
            "to black-box approaches.\n",
            "\n",
            "1642. Title: Nemesis: Normalizing the Soft-prompt Vectors of Vision-Language Models\n",
            "   Abstract: The self-attention mechanism traditionally relies on the softmax operator,\n",
            "necessitating positional embeddings like RoPE, or position biases to account\n",
            "for token order. But current methods using still face length generalisation\n",
            "challenges. We propose an alternative attention mechanism based on the\n",
            "stick-breaking process: For each token before the current, we determine a break\n",
            "point $\\beta_{i,j}$, which represents the proportion of the remaining stick to\n",
            "allocate to the current token. We repeat the process until the stick is fully\n",
            "allocated, resulting in a sequence of attention weights. This process naturally\n",
            "incorporates recency bias, which has linguistic motivations for grammar parsing\n",
            "(Shen et. al., 2017). We study the implications of replacing the conventional\n",
            "softmax-based attention mechanism with stick-breaking attention. We then\n",
            "discuss implementation of numerically stable stick-breaking attention and adapt\n",
            "Flash Attention to accommodate this mechanism. When used as a drop-in\n",
            "replacement for current softmax+RoPE attention systems, we find that\n",
            "stick-breaking attention performs competitively with current methods on length\n",
            "generalisation and downstream tasks. Stick-breaking also performs well at\n",
            "length generalisation, allowing a model trained with $2^{11}$ context window to\n",
            "perform well at $2^{14}$ with perplexity improvements.\n",
            "\n",
            "1643. Title: Get What You Want, Not What You Don't: Image Content Suppression for Text-to-Image Diffusion Models\n",
            "   Abstract: Recent work shows that path gradient estimators for normalizing flows have\n",
            "lower variance compared to standard estimators for variational inference,\n",
            "resulting in improved training. However, they are often prohibitively more\n",
            "expensive from a computational point of view and cannot be applied to maximum\n",
            "likelihood training in a scalable manner, which severely hinders their\n",
            "widespread adoption. In this work, we overcome these crucial limitations.\n",
            "Specifically, we propose a fast path gradient estimator which improves\n",
            "computational efficiency significantly and works for all normalizing flow\n",
            "architectures of practical relevance. We then show that this estimator can also\n",
            "be applied to maximum likelihood training for which it has a regularizing\n",
            "effect as it can take the form of a given target energy function into account.\n",
            "We empirically establish its superior performance and reduced variance for\n",
            "several natural sciences applications.\n",
            "\n",
            "1644. Title: Enhancing Small Medical Learners with Privacy-preserving Contextual Prompting\n",
            "   Abstract: The success of recent text-to-image diffusion models is largely due to their\n",
            "capacity to be guided by a complex text prompt, which enables users to\n",
            "precisely describe the desired content. However, these models struggle to\n",
            "effectively suppress the generation of undesired content, which is explicitly\n",
            "requested to be omitted from the generated image in the prompt. In this paper,\n",
            "we analyze how to manipulate the text embeddings and remove unwanted content\n",
            "from them. We introduce two contributions, which we refer to as\n",
            "$\\textit{soft-weighted regularization}$ and $\\textit{inference-time text\n",
            "embedding optimization}$. The first regularizes the text embedding matrix and\n",
            "effectively suppresses the undesired content. The second method aims to further\n",
            "suppress the unwanted content generation of the prompt, and encourages the\n",
            "generation of desired content. We evaluate our method quantitatively and\n",
            "qualitatively on extensive experiments, validating its effectiveness.\n",
            "Furthermore, our method is generalizability to both the pixel-space diffusion\n",
            "models (i.e. DeepFloyd-IF) and the latent-space diffusion models (i.e. Stable\n",
            "Diffusion).\n",
            "\n",
            "1645. Title: Out-of-Variable Generalisation for Discriminative Models\n",
            "   Abstract: Large language models (LLMs) demonstrate remarkable medical expertise, but\n",
            "data privacy concerns impede their direct use in healthcare environments.\n",
            "Although offering improved data privacy protection, domain-specific small\n",
            "language models (SLMs) often underperform LLMs, emphasizing the need for\n",
            "methods that reduce this performance gap while alleviating privacy concerns. In\n",
            "this paper, we present a simple yet effective method that harnesses LLMs'\n",
            "medical proficiency to boost SLM performance in medical tasks under\n",
            "privacy-restricted scenarios. Specifically, we mitigate patient privacy issues\n",
            "by extracting keywords from medical data and prompting the LLM to generate a\n",
            "medical knowledge-intensive context by simulating clinicians' thought\n",
            "processes. This context serves as additional input for SLMs, augmenting their\n",
            "decision-making capabilities. Our method significantly enhances performance in\n",
            "both few-shot and full training settings across three medical\n",
            "knowledge-intensive tasks, achieving up to a 22.57% increase in absolute\n",
            "accuracy compared to SLM fine-tuning without context, and sets new\n",
            "state-of-the-art results in two medical tasks within privacy-restricted\n",
            "scenarios. Further out-of-domain testing and experiments in two general domain\n",
            "datasets showcase its generalizability and broad applicability. Our code can be\n",
            "found at https://github.com/XZhang97666/PrivacyBoost-SLM.\n",
            "\n",
            "1646. Title: Learning Mean Field Games on Sparse Graphs: A Hybrid Graphex Approach\n",
            "   Abstract: Particle distinguishability is a significant challenge for quantum\n",
            "technologies, in particular photonics where the Hong-Ou-Mandel (HOM) effect\n",
            "clearly demonstrates it is detrimental to quantum interference. We take a\n",
            "representation theoretic approach in first quantisation, separating particles'\n",
            "Hilbert spaces into degrees of freedom that we control and those we do not,\n",
            "yielding a quantum information inspired bipartite model where\n",
            "distinguishability can arise as correlation with an environment carried by the\n",
            "particles themselves. This makes clear that the HOM experiment is an instance\n",
            "of a (mixed) state discrimination protocol, which can be generalised to\n",
            "interferometers that discriminate unambiguously between ideal indistinguishable\n",
            "states and interesting distinguishable states, leading to bounds on the success\n",
            "probability of an arbitrary HOM generalisation for multiple particles and\n",
            "modes. After setting out the first quantised formalism in detail, we consider\n",
            "several scenarios and provide a combination of analytical and numerical results\n",
            "for up to nine photons in nine modes. Although the Quantum Fourier Transform\n",
            "features prominently, we see that it is suboptimal for discriminating\n",
            "completely distinguishable states.\n",
            "\n",
            "1647. Title: On the generalization capacity of neural networks during generic multimodal reasoning\n",
            "   Abstract: Learning the behavior of large agent populations is an important task for\n",
            "numerous research areas. Although the field of multi-agent reinforcement\n",
            "learning (MARL) has made significant progress towards solving these systems,\n",
            "solutions for many agents often remain computationally infeasible and lack\n",
            "theoretical guarantees. Mean Field Games (MFGs) address both of these issues\n",
            "and can be extended to Graphon MFGs (GMFGs) to include network structures\n",
            "between agents. Despite their merits, the real world applicability of GMFGs is\n",
            "limited by the fact that graphons only capture dense graphs. Since most\n",
            "empirically observed networks show some degree of sparsity, such as power law\n",
            "graphs, the GMFG framework is insufficient for capturing these network\n",
            "topologies. Thus, we introduce the novel concept of Graphex MFGs (GXMFGs) which\n",
            "builds on the graph theoretical concept of graphexes. Graphexes are the\n",
            "limiting objects to sparse graph sequences that also have other desirable\n",
            "features such as the small world property. Learning equilibria in these games\n",
            "is challenging due to the rich and sparse structure of the underlying graphs.\n",
            "To tackle these challenges, we design a new learning algorithm tailored to the\n",
            "GXMFG setup. This hybrid graphex learning approach leverages that the system\n",
            "mainly consists of a highly connected core and a sparse periphery. After\n",
            "defining the system and providing a theoretical analysis, we state our learning\n",
            "approach and demonstrate its learning capabilities on both synthetic graphs and\n",
            "real-world networks. This comparison shows that our GXMFG learning algorithm\n",
            "successfully extends MFGs to a highly relevant class of hard, realistic\n",
            "learning problems that are not accurately addressed by current MARL and MFG\n",
            "methods.\n",
            "\n",
            "1648. Title: Breaking Physical and Linguistic Borders: Multilingual Federated Prompt Tuning for Low-Resource Languages\n",
            "   Abstract: The advent of the Transformer has led to the development of large language\n",
            "models (LLM), which appear to demonstrate human-like capabilities. To assess\n",
            "the generality of this class of models and a variety of other base neural\n",
            "network architectures to multimodal domains, we evaluated and compared their\n",
            "capacity for multimodal generalization. We introduce a multimodal\n",
            "question-answer benchmark to evaluate three specific types of\n",
            "out-of-distribution (OOD) generalization performance: distractor generalization\n",
            "(generalization in the presence of distractors), systematic compositional\n",
            "generalization (generalization to new task permutations), and productive\n",
            "compositional generalization (generalization to more complex tasks structures).\n",
            "We found that across model architectures (e.g., RNNs, Transformers, Perceivers,\n",
            "etc.), models with multiple attention layers, or models that leveraged\n",
            "cross-attention mechanisms between input domains, fared better. Our positive\n",
            "results demonstrate that for multimodal distractor and systematic\n",
            "generalization, either cross-modal attention or models with deeper attention\n",
            "layers are key architectural features required to integrate multimodal inputs.\n",
            "On the other hand, neither of these architectural features led to productive\n",
            "generalization, suggesting fundamental limitations of existing architectures\n",
            "for specific types of multimodal generalization. These results demonstrate the\n",
            "strengths and limitations of specific architectural components underlying\n",
            "modern neural models for multimodal reasoning. Finally, we provide Generic COG\n",
            "(gCOG), a configurable benchmark with several multimodal generalization splits,\n",
            "for future studies to explore.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_path =f'/content/drive/MyDrive/{loadFile}.csv'\n",
        "\n",
        "# 将 DataFrame 保存为 CSV 文件\n",
        "df.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"文件已保存到：{output_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mg3NKV6JNYtO",
        "outputId": "d6efcfa8-a2eb-48f2-f90e-a97430e42a88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "文件已保存到：/content/drive/MyDrive/iclr2024.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[df['title'].str.contains(\"MOF\", na=False)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "id": "Cpvh1Mt0DN7u",
        "outputId": "c3fb6141-3765-46df-af3b-16350905f8bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              id                                              title track  \\\n",
              "46    0VBsoluxR2  MOFDiff: Coarse-grained Diffusion for Metal-Or...  main   \n",
              "3092  QQYpgReSRk  MOFI: Learning Image Representations from Nois...  main   \n",
              "\n",
              "      status                                           keywords  \\\n",
              "46    Poster  Materials design;diffusion model;metal-organic...   \n",
              "3092  Poster  Image representation;image embedding;image bas...   \n",
              "\n",
              "                                           primary_area  \\\n",
              "46    applications to physical sciences (physics, ch...   \n",
              "3092  representation learning for computer vision, a...   \n",
              "\n",
              "                                                 author  \\\n",
              "46    Xiang Fu;Tian Xie;Andrew Scott Rosen;Tommi S. ...   \n",
              "3092  Wentao Wu;Aleksei Timofeev;Chen Chen;Bowen Zha...   \n",
              "\n",
              "                                              authorids  \\\n",
              "46    ~Xiang_Fu4;~Tian_Xie2;~Andrew_Scott_Rosen1;~To...   \n",
              "3092  ~Wentao_Wu3;~Aleksei_Timofeev1;~Chen_Chen38;~B...   \n",
              "\n",
              "                                                    aff  \\\n",
              "46    Massachusetts Institute of Technology;Microsof...   \n",
              "3092     Apple;Apple;Apple;Apple;Apple;Apple;Apple Inc.   \n",
              "\n",
              "                                             aff_domain  ... confidence_avg  \\\n",
              "46     mit.edu;microsoft.com;berkeley.edu;microsoft.com  ...           3.50   \n",
              "3092  apple.com;apple.com;apple.com;apple.com;apple....  ...           4.25   \n",
              "\n",
              "     soundness_avg contribution_avg presentation_avg replies_avg  \\\n",
              "46            2.75              2.5             3.25          20   \n",
              "3092          3.00              2.5             3.50          25   \n",
              "\n",
              "     corr_rating_confidence  project  github  \\\n",
              "46                 0.000000                    \n",
              "3092              -0.132453                    \n",
              "\n",
              "                                           site  \\\n",
              "46    https://iclr.cc/virtual/2024/poster/19615   \n",
              "3092  https://iclr.cc/virtual/2024/poster/18677   \n",
              "\n",
              "                                               abstract  \n",
              "46    The great success of Large Language Models (LL...  \n",
              "3092  We propose Hyper-Dimensional Function Encoding...  \n",
              "\n",
              "[2 rows x 27 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d937b1c2-f120-414f-bc58-59cc4644e924\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>title</th>\n",
              "      <th>track</th>\n",
              "      <th>status</th>\n",
              "      <th>keywords</th>\n",
              "      <th>primary_area</th>\n",
              "      <th>author</th>\n",
              "      <th>authorids</th>\n",
              "      <th>aff</th>\n",
              "      <th>aff_domain</th>\n",
              "      <th>...</th>\n",
              "      <th>confidence_avg</th>\n",
              "      <th>soundness_avg</th>\n",
              "      <th>contribution_avg</th>\n",
              "      <th>presentation_avg</th>\n",
              "      <th>replies_avg</th>\n",
              "      <th>corr_rating_confidence</th>\n",
              "      <th>project</th>\n",
              "      <th>github</th>\n",
              "      <th>site</th>\n",
              "      <th>abstract</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>0VBsoluxR2</td>\n",
              "      <td>MOFDiff: Coarse-grained Diffusion for Metal-Or...</td>\n",
              "      <td>main</td>\n",
              "      <td>Poster</td>\n",
              "      <td>Materials design;diffusion model;metal-organic...</td>\n",
              "      <td>applications to physical sciences (physics, ch...</td>\n",
              "      <td>Xiang Fu;Tian Xie;Andrew Scott Rosen;Tommi S. ...</td>\n",
              "      <td>~Xiang_Fu4;~Tian_Xie2;~Andrew_Scott_Rosen1;~To...</td>\n",
              "      <td>Massachusetts Institute of Technology;Microsof...</td>\n",
              "      <td>mit.edu;microsoft.com;berkeley.edu;microsoft.com</td>\n",
              "      <td>...</td>\n",
              "      <td>3.50</td>\n",
              "      <td>2.75</td>\n",
              "      <td>2.5</td>\n",
              "      <td>3.25</td>\n",
              "      <td>20</td>\n",
              "      <td>0.000000</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>https://iclr.cc/virtual/2024/poster/19615</td>\n",
              "      <td>The great success of Large Language Models (LL...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3092</th>\n",
              "      <td>QQYpgReSRk</td>\n",
              "      <td>MOFI: Learning Image Representations from Nois...</td>\n",
              "      <td>main</td>\n",
              "      <td>Poster</td>\n",
              "      <td>Image representation;image embedding;image bas...</td>\n",
              "      <td>representation learning for computer vision, a...</td>\n",
              "      <td>Wentao Wu;Aleksei Timofeev;Chen Chen;Bowen Zha...</td>\n",
              "      <td>~Wentao_Wu3;~Aleksei_Timofeev1;~Chen_Chen38;~B...</td>\n",
              "      <td>Apple;Apple;Apple;Apple;Apple;Apple;Apple Inc.</td>\n",
              "      <td>apple.com;apple.com;apple.com;apple.com;apple....</td>\n",
              "      <td>...</td>\n",
              "      <td>4.25</td>\n",
              "      <td>3.00</td>\n",
              "      <td>2.5</td>\n",
              "      <td>3.50</td>\n",
              "      <td>25</td>\n",
              "      <td>-0.132453</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>https://iclr.cc/virtual/2024/poster/18677</td>\n",
              "      <td>We propose Hyper-Dimensional Function Encoding...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2 rows × 27 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d937b1c2-f120-414f-bc58-59cc4644e924')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d937b1c2-f120-414f-bc58-59cc4644e924 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d937b1c2-f120-414f-bc58-59cc4644e924');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-3b412e58-b6a8-4967-b8e8-40178013c9e4\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3b412e58-b6a8-4967-b8e8-40178013c9e4')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-3b412e58-b6a8-4967-b8e8-40178013c9e4 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[df['title'].str.contains(\"MOF\", na=False)].to_markdown()\n",
        "# 解析这个表格的内容，我想知道每篇paper的标题，机构（简略），Abstract简要概括，评分\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "k70jz06m1Et2",
        "outputId": "a97666fa-8be8-4920-9a0b-432c2c202bd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'|      | id         | title                                                                   | track   | status   | keywords                                                                                                | primary_area                                                                       | author                                                                                                                       | authorids                                                                                                                                         | aff                                                                                                                  | aff_domain                                                            | position                                                                                    | rating   | confidence   | soundness   | contribution   | presentation   |   rating_avg |   confidence_avg |   soundness_avg |   contribution_avg |   presentation_avg |   replies_avg |   corr_rating_confidence | project   | github   | site                                      | abstract                                                                        |\\n|-----:|:-----------|:------------------------------------------------------------------------|:--------|:---------|:--------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:--------------------------------------------------------------------------------------------|:---------|:-------------|:------------|:---------------|:---------------|-------------:|-----------------:|----------------:|-------------------:|-------------------:|--------------:|-------------------------:|:----------|:---------|:------------------------------------------|:--------------------------------------------------------------------------------|\\n|   46 | 0VBsoluxR2 | MOFDiff: Coarse-grained Diffusion for Metal-Organic Framework Design    | main    | Poster   | Materials design;diffusion model;metal-organic framework;carbon capture;generative model;AI for Science | applications to physical sciences (physics, chemistry, biology, etc.)              | Xiang Fu;Tian Xie;Andrew Scott Rosen;Tommi S. Jaakkola;Jake Allen Smith                                                      | ~Xiang_Fu4;~Tian_Xie2;~Andrew_Scott_Rosen1;~Tommi_S._Jaakkola1;~Jake_Allen_Smith1                                                                 | Massachusetts Institute of Technology;Microsoft Research AI for Science;University of California, Berkeley;Microsoft | mit.edu;microsoft.com;berkeley.edu;microsoft.com                      | PhD student;Senior Researcher;Postdoc;Researcher                                            | 8;8;8;8  | 4;3;3;4      | 3;3;2;3     | 3;3;2;2        | 3;3;3;4        |         8    |             3.5  |            2.75 |                2.5 |               3.25 |            20 |                 0        |           |          | https://iclr.cc/virtual/2024/poster/19615 | The great success of Large Language Models (LLMs) has expanded the potential    |\\n|      |            |                                                                         |         |          |                                                                                                         |                                                                                    |                                                                                                                              |                                                                                                                                                   |                                                                                                                      |                                                                       |                                                                                             |          |              |             |                |                |              |                  |                 |                    |                    |               |                          |           |          |                                           | of multimodality, contributing to the gradual evolution of General Artificial   |\\n|      |            |                                                                         |         |          |                                                                                                         |                                                                                    |                                                                                                                              |                                                                                                                                                   |                                                                                                                      |                                                                       |                                                                                             |          |              |             |                |                |              |                  |                 |                    |                    |               |                          |           |          |                                           | Intelligence (AGI). A true AGI agent should not only possess the capability to  |\\n|      |            |                                                                         |         |          |                                                                                                         |                                                                                    |                                                                                                                              |                                                                                                                                                   |                                                                                                                      |                                                                       |                                                                                             |          |              |             |                |                |              |                  |                 |                    |                    |               |                          |           |          |                                           | perform predefined multi-tasks but also exhibit emergent abilities in an        |\\n|      |            |                                                                         |         |          |                                                                                                         |                                                                                    |                                                                                                                              |                                                                                                                                                   |                                                                                                                      |                                                                       |                                                                                             |          |              |             |                |                |              |                  |                 |                    |                    |               |                          |           |          |                                           | open-world context. However, despite the considerable advancements made by      |\\n|      |            |                                                                         |         |          |                                                                                                         |                                                                                    |                                                                                                                              |                                                                                                                                                   |                                                                                                                      |                                                                       |                                                                                             |          |              |             |                |                |              |                  |                 |                    |                    |               |                          |           |          |                                           | recent multimodal LLMs, they still fall short in effectively unifying           |\\n|      |            |                                                                         |         |          |                                                                                                         |                                                                                    |                                                                                                                              |                                                                                                                                                   |                                                                                                                      |                                                                       |                                                                                             |          |              |             |                |                |              |                  |                 |                    |                    |               |                          |           |          |                                           | comprehension and generation tasks, let alone open-world emergent abilities. We |\\n|      |            |                                                                         |         |          |                                                                                                         |                                                                                    |                                                                                                                              |                                                                                                                                                   |                                                                                                                      |                                                                       |                                                                                             |          |              |             |                |                |              |                  |                 |                    |                    |               |                          |           |          |                                           | contend that the key to overcoming the present impasse lies in enabling text    |\\n|      |            |                                                                         |         |          |                                                                                                         |                                                                                    |                                                                                                                              |                                                                                                                                                   |                                                                                                                      |                                                                       |                                                                                             |          |              |             |                |                |              |                  |                 |                    |                    |               |                          |           |          |                                           | and images to be represented and processed interchangeably within a unified     |\\n|      |            |                                                                         |         |          |                                                                                                         |                                                                                    |                                                                                                                              |                                                                                                                                                   |                                                                                                                      |                                                                       |                                                                                             |          |              |             |                |                |              |                  |                 |                    |                    |               |                          |           |          |                                           | autoregressive Transformer. To this end, we introduce SEED, an elaborate image  |\\n|      |            |                                                                         |         |          |                                                                                                         |                                                                                    |                                                                                                                              |                                                                                                                                                   |                                                                                                                      |                                                                       |                                                                                             |          |              |             |                |                |              |                  |                 |                    |                    |               |                          |           |          |                                           | tokenizer that empowers LLMs with the ability to SEE and Draw at the same time. |\\n|      |            |                                                                         |         |          |                                                                                                         |                                                                                    |                                                                                                                              |                                                                                                                                                   |                                                                                                                      |                                                                       |                                                                                             |          |              |             |                |                |              |                  |                 |                    |                    |               |                          |           |          |                                           | We identify two crucial design principles: (1) Image tokens should be           |\\n|      |            |                                                                         |         |          |                                                                                                         |                                                                                    |                                                                                                                              |                                                                                                                                                   |                                                                                                                      |                                                                       |                                                                                             |          |              |             |                |                |              |                  |                 |                    |                    |               |                          |           |          |                                           | independent of 2D physical patch positions and instead be produced with a 1D    |\\n|      |            |                                                                         |         |          |                                                                                                         |                                                                                    |                                                                                                                              |                                                                                                                                                   |                                                                                                                      |                                                                       |                                                                                             |          |              |             |                |                |              |                  |                 |                    |                    |               |                          |           |          |                                           | causal dependency, exhibiting intrinsic interdependence that aligns with the    |\\n|      |            |                                                                         |         |          |                                                                                                         |                                                                                    |                                                                                                                              |                                                                                                                                                   |                                                                                                                      |                                                                       |                                                                                             |          |              |             |                |                |              |                  |                 |                    |                    |               |                          |           |          |                                           | left-to-right autoregressive prediction mechanism in LLMs. (2) Image tokens     |\\n|      |            |                                                                         |         |          |                                                                                                         |                                                                                    |                                                                                                                              |                                                                                                                                                   |                                                                                                                      |                                                                       |                                                                                             |          |              |             |                |                |              |                  |                 |                    |                    |               |                          |           |          |                                           | should capture high-level semantics consistent with the degree of semantic      |\\n|      |            |                                                                         |         |          |                                                                                                         |                                                                                    |                                                                                                                              |                                                                                                                                                   |                                                                                                                      |                                                                       |                                                                                             |          |              |             |                |                |              |                  |                 |                    |                    |               |                          |           |          |                                           | abstraction in words, and be optimized for both discriminativeness and          |\\n|      |            |                                                                         |         |          |                                                                                                         |                                                                                    |                                                                                                                              |                                                                                                                                                   |                                                                                                                      |                                                                       |                                                                                             |          |              |             |                |                |              |                  |                 |                    |                    |               |                          |           |          |                                           | reconstruction during the tokenizer training phase. With SEED tokens, LLM is    |\\n|      |            |                                                                         |         |          |                                                                                                         |                                                                                    |                                                                                                                              |                                                                                                                                                   |                                                                                                                      |                                                                       |                                                                                             |          |              |             |                |                |              |                  |                 |                    |                    |               |                          |           |          |                                           | able to perform scalable multimodal autoregression under its original training  |\\n|      |            |                                                                         |         |          |                                                                                                         |                                                                                    |                                                                                                                              |                                                                                                                                                   |                                                                                                                      |                                                                       |                                                                                             |          |              |             |                |                |              |                  |                 |                    |                    |               |                          |           |          |                                           | recipe, i.e., next-word prediction. SEED-LLaMA is therefore produced by         |\\n|      |            |                                                                         |         |          |                                                                                                         |                                                                                    |                                                                                                                              |                                                                                                                                                   |                                                                                                                      |                                                                       |                                                                                             |          |              |             |                |                |              |                  |                 |                    |                    |               |                          |           |          |                                           | large-scale pretraining and instruction tuning on the interleaved textual and   |\\n|      |            |                                                                         |         |          |                                                                                                         |                                                                                    |                                                                                                                              |                                                                                                                                                   |                                                                                                                      |                                                                       |                                                                                             |          |              |             |                |                |              |                  |                 |                    |                    |               |                          |           |          |                                           | visual data, demonstrating impressive performance on a broad range of           |\\n|      |            |                                                                         |         |          |                                                                                                         |                                                                                    |                                                                                                                              |                                                                                                                                                   |                                                                                                                      |                                                                       |                                                                                             |          |              |             |                |                |              |                  |                 |                    |                    |               |                          |           |          |                                           | multimodal comprehension and generation tasks. More importantly, SEED-LLaMA has |\\n|      |            |                                                                         |         |          |                                                                                                         |                                                                                    |                                                                                                                              |                                                                                                                                                   |                                                                                                                      |                                                                       |                                                                                             |          |              |             |                |                |              |                  |                 |                    |                    |               |                          |           |          |                                           | exhibited compositional emergent abilities such as multi-turn in-context        |\\n|      |            |                                                                         |         |          |                                                                                                         |                                                                                    |                                                                                                                              |                                                                                                                                                   |                                                                                                                      |                                                                       |                                                                                             |          |              |             |                |                |              |                  |                 |                    |                    |               |                          |           |          |                                           | multimodal generation, acting like your AI assistant.                           |\\n| 3092 | QQYpgReSRk | MOFI: Learning Image Representations from Noisy Entity Annotated Images | main    | Poster   | Image representation;image embedding;image based search                                                 | representation learning for computer vision, audio, language, and other modalities | Wentao Wu;Aleksei Timofeev;Chen Chen;Bowen Zhang;Kun Duan;Shuangning Liu;Yantao Zheng;Jonathon Shlens;Xianzhi Du;Yinfei Yang | ~Wentao_Wu3;~Aleksei_Timofeev1;~Chen_Chen38;~Bowen_Zhang2;~Kun_Duan1;~Shuangning_Liu1;~Yantao_Zheng1;~Jonathon_Shlens1;~Xianzhi_Du4;~Yinfei_Yang1 | Apple;Apple;Apple;Apple;Apple;Apple;Apple Inc.                                                                       | apple.com;apple.com;apple.com;apple.com;apple.com;apple.com;apple.com | Researcher;Researcher;Research Scientist;Researcher;Researcher;Researcher;Software Engineer | 5;6;6;8  | 4;4;5;4      | 3;3;3;3     | 2;3;2;3        | 3;3;4;4        |         6.25 |             4.25 |            3    |                2.5 |               3.5  |            25 |                -0.132453 |           |          | https://iclr.cc/virtual/2024/poster/18677 | We propose Hyper-Dimensional Function Encoding (HDFE). Given samples of a       |\\n|      |            |                                                                         |         |          |                                                                                                         |                                                                                    |                                                                                                                              |                                                                                                                                                   |                                                                                                                      |                                                                       |                                                                                             |          |              |             |                |                |              |                  |                 |                    |                    |               |                          |           |          |                                           | continuous object (e.g. a function), HDFE produces an explicit vector           |\\n|      |            |                                                                         |         |          |                                                                                                         |                                                                                    |                                                                                                                              |                                                                                                                                                   |                                                                                                                      |                                                                       |                                                                                             |          |              |             |                |                |              |                  |                 |                    |                    |               |                          |           |          |                                           | representation of the given object, invariant to the sample distribution and    |\\n|      |            |                                                                         |         |          |                                                                                                         |                                                                                    |                                                                                                                              |                                                                                                                                                   |                                                                                                                      |                                                                       |                                                                                             |          |              |             |                |                |              |                  |                 |                    |                    |               |                          |           |          |                                           | density. Sample distribution and density invariance enables HDFE to             |\\n|      |            |                                                                         |         |          |                                                                                                         |                                                                                    |                                                                                                                              |                                                                                                                                                   |                                                                                                                      |                                                                       |                                                                                             |          |              |             |                |                |              |                  |                 |                    |                    |               |                          |           |          |                                           | consistently encode continuous objects regardless of their sampling, and        |\\n|      |            |                                                                         |         |          |                                                                                                         |                                                                                    |                                                                                                                              |                                                                                                                                                   |                                                                                                                      |                                                                       |                                                                                             |          |              |             |                |                |              |                  |                 |                    |                    |               |                          |           |          |                                           | therefore allows neural networks to receive continuous objects as inputs for    |\\n|      |            |                                                                         |         |          |                                                                                                         |                                                                                    |                                                                                                                              |                                                                                                                                                   |                                                                                                                      |                                                                       |                                                                                             |          |              |             |                |                |              |                  |                 |                    |                    |               |                          |           |          |                                           | machine learning tasks, such as classification and regression. Besides, HDFE    |\\n|      |            |                                                                         |         |          |                                                                                                         |                                                                                    |                                                                                                                              |                                                                                                                                                   |                                                                                                                      |                                                                       |                                                                                             |          |              |             |                |                |              |                  |                 |                    |                    |               |                          |           |          |                                           | does not require any training and is proved to map the object into an organized |\\n|      |            |                                                                         |         |          |                                                                                                         |                                                                                    |                                                                                                                              |                                                                                                                                                   |                                                                                                                      |                                                                       |                                                                                             |          |              |             |                |                |              |                  |                 |                    |                    |               |                          |           |          |                                           | embedding space, which facilitates the training of the downstream tasks. In     |\\n|      |            |                                                                         |         |          |                                                                                                         |                                                                                    |                                                                                                                              |                                                                                                                                                   |                                                                                                                      |                                                                       |                                                                                             |          |              |             |                |                |              |                  |                 |                    |                    |               |                          |           |          |                                           | addition, the encoding is decodable, which enables neural networks to regress   |\\n|      |            |                                                                         |         |          |                                                                                                         |                                                                                    |                                                                                                                              |                                                                                                                                                   |                                                                                                                      |                                                                       |                                                                                             |          |              |             |                |                |              |                  |                 |                    |                    |               |                          |           |          |                                           | continuous objects by regressing their encodings. Therefore, HDFE serves as an  |\\n|      |            |                                                                         |         |          |                                                                                                         |                                                                                    |                                                                                                                              |                                                                                                                                                   |                                                                                                                      |                                                                       |                                                                                             |          |              |             |                |                |              |                  |                 |                    |                    |               |                          |           |          |                                           | interface for processing continuous objects.                                    |\\n|      |            |                                                                         |         |          |                                                                                                         |                                                                                    |                                                                                                                              |                                                                                                                                                   |                                                                                                                      |                                                                       |                                                                                             |          |              |             |                |                |              |                  |                 |                    |                    |               |                          |           |          |                                           |   We apply HDFE to function-to-function mapping, where vanilla HDFE achieves    |\\n|      |            |                                                                         |         |          |                                                                                                         |                                                                                    |                                                                                                                              |                                                                                                                                                   |                                                                                                                      |                                                                       |                                                                                             |          |              |             |                |                |              |                  |                 |                    |                    |               |                          |           |          |                                           | competitive performance as the state-of-the-art algorithm. We apply HDFE to     |\\n|      |            |                                                                         |         |          |                                                                                                         |                                                                                    |                                                                                                                              |                                                                                                                                                   |                                                                                                                      |                                                                       |                                                                                             |          |              |             |                |                |              |                  |                 |                    |                    |               |                          |           |          |                                           | point cloud surface normal estimation, where a simple replacement from PointNet |\\n|      |            |                                                                         |         |          |                                                                                                         |                                                                                    |                                                                                                                              |                                                                                                                                                   |                                                                                                                      |                                                                       |                                                                                             |          |              |             |                |                |              |                  |                 |                    |                    |               |                          |           |          |                                           | to HDFE leads to immediate 12% and 15% error reductions in two benchmarks. In   |\\n|      |            |                                                                         |         |          |                                                                                                         |                                                                                    |                                                                                                                              |                                                                                                                                                   |                                                                                                                      |                                                                       |                                                                                             |          |              |             |                |                |              |                  |                 |                    |                    |               |                          |           |          |                                           | addition, by integrating HDFE into the PointNet-based SOTA network, we improve  |\\n|      |            |                                                                         |         |          |                                                                                                         |                                                                                    |                                                                                                                              |                                                                                                                                                   |                                                                                                                      |                                                                       |                                                                                             |          |              |             |                |                |              |                  |                 |                    |                    |               |                          |           |          |                                           | the SOTA baseline by 2.5% and 1.7% in the same benchmarks.                      |'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    }
  ]
}